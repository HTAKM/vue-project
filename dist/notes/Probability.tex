\documentclass{huhtakm-template-book-v2}
\usepackage{enumitem}
\newcommand{\independent}{\perp\!\!\!\perp}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\expect}{\mathbb{E}}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\NBin}{NBin}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\Poisson}{Poisson}
\DeclareMathOperator{\Hypergeometric}{Hypergeometric}
\DeclareMathOperator{\U}{U}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\Cauchy}{Cauchy}
\DeclareMathOperator{\Gam}{Gamma}
\DeclareMathOperator{\Beta}{Beta}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\setlength{\parindent}{0pt}
\title{
	\Huge Probability\\
	\small Version 2.1
}
\author{
	HU-HTAKM\\
	\small Website: \url{https://htakm.github.io/htakm_test/}
}
\date{
	Last major change: September 29, 2025\\
	Last small update: October 6, 2025
}
\begin{document}
\maketitle

    This is a rewritten version of my lecture notes titled "MATH 2431: Honor Probability." I realized that the ordering of topics was somewhat disorganized, and there were some missing topics that might be necessary for future courses.\\
    My plan is to make these notes a combination of MATH 2421 (Probability) and MATH 2431 (Honor Probability). Again, if you can find any typos, you are either already proficient in the topics or have sharp eyes. ;)\\
    Please note that all proofs that could be combinatorial proofs are omitted.
    \begin{figure}[h]
        \begin{subfigure}[h]{0.45\textwidth}
            \centering
            \begin{tabular}{cc}
                Notations & Meaning\\
                \hline
                $\mathbb{N}_{+}$ & Set of positive integers\\
                $\mathbb{N}$ & Set of natural numbers\\
                $\mathbb{Z}$ & Set of integers\\
                $\mathbb{Q}$ & Set of rational numbers\\
                $\mathbb{R}$ & Set of real numbers\\
                $\emptyset$ & Empty set\\
                $\Omega$ & Sample space / Entire set\\
                $\omega$ & Outcome\\
                $\mathcal{F},\mathcal{G},\mathcal{H}$ & $\sigma$-field / $\sigma$-algebra\\
                $A,B,C,\cdots$ & Events\\
                $A^{\complement}$ & Complement of events\\
                $\prob$ & Probability measure\\
                $X$ & Random variable\\
                $\mathcal{B}(\mathbb{R})$ & Borel $\sigma$-field of $\mathbb{R}$ \\
                $f_{X}$ & PMF/PDF of $X$\\
                $F_{X}$ & CDF of $X$\\
                $\mathbf{1}_{A}$ & Indicator function\\
                $\mathbb{E}$ & Expectation\\
                $\psi$ & Conditional expectation\\
                $\mathbf{u},\mathbf{v},\mathbf{w},\cdots$ & Vector\\
                $\mathbf{A},\mathbf{B},\mathbf{C},\cdots$ & Matrix\\
                $\mathbf{X}$ & Random vector\\
                $\overline{X}$ & Sample mean of $X$\\
                $S_{n-1}^{2}$ & Sample variance of $X$\\
                $G_{X}$ & Probability generating function of $X$\\
                $M_{X}$ & Moment generating function of $X$\\
                $\phi$ & CF / PDF of $X \sim \N(0,1)$\\
                $\Phi$ & CDF of $X \sim \N(0,1)$
            \end{tabular}
            \caption{Notations}
        \end{subfigure}
        \begin{subfigure}[h]{0.45\textwidth}
            \centering
            \begin{tabular}{cc}
                Abbreviations & Meaning\\
                \hline
                CDF & Cumulative distribution function\\
                JCDF & Joint cumulative distribution function\\
                PMF & Probability mass function\\
                JPMF & Joint probability mass function\\
                PDF & Probability density function\\
                JPDF & Joint probability density function\\
                PGF & Probability generating function\\
                MGF & Moment generating function\\
                JMGF & Joint moment generating function\\
                CF & Characteristic function\\
                JCF & Joint characteristic function\\
                i.i.d. & Independent and identically distributed\\
                WLLN & Weak Law of Large Numbers\\
                SLLN & Strong Law of Large Numbers\\
                CLT & Central Limit Theorem\\
                BCI & Borel-Cantelli Lemma I\\
                BCII & Borel-Cantelli Lemma II\\
                i.o. & Infinitely often\\
                f.o. & Finitely often\\
                a.s. & Almost surely
            \end{tabular}
            \caption{Abbreviations}
        \end{subfigure}
    \end{figure}
    \begin{defn}
        This is a definition.
    \end{defn}
    \begin{rem}
        This is a remark.
    \end{rem}
    \begin{lem}
        This is a lemma.
    \end{lem}
    \begin{prop}
        This is a proposition.
    \end{prop}
    \begin{thm}
        This is a theorem.
    \end{thm}
    \begin{cla}
        This is a claim.
    \end{cla}
    \begin{cor}
        This is a corollary.
    \end{cor}
    \begin{eg}
        This is an example.
    \end{eg}

\tableofcontents

\chapter{Combinatorial Analysis}
    \label{Chapter 1 (Combinatorial Analysis)}
\section{Probabilities}
    In our lives, we often believe that the future is largely unpredictable. We express this belief through chance behavior and assign both quantitative and qualitative meanings to its usage. Therefore, we introduce the concept of "probability," which aims to provide a numerical description of how likely an event is to occur.
    \begin{defn}
        \textbf{Probability} is a numerical measurement of how likely an event is to occur.
    \end{defn}
    To determine probabilities, we can use random experiments.
    \begin{defn}
        An \textbf{experiment} is a process that produces a random outcome.
    \end{defn}
    \begin{eg}
        Examples of experiments:
        \begin{enumerate}
            \item Randomly picking a number from $1$ to $10$.
            \item Randomly tossing a coin.
        \end{enumerate}
    \end{eg}
    The most basic way to calculate probability is by counting.
    \begin{thm}\named{Fundamental Principle of Counting}
        Suppose that $m_{i}$ represents the number of outcomes of the $i$-th event. The total number of outcomes of $n$ independent events is the product of the number of outcomes for each individual event:
        \begin{equation*}
            \prod_{i = 1}^{n}m_{i}
        \end{equation*} 
    \end{thm}
    \begin{eg}
        Assume that we choose a president and a vice-president from $30$ people. By the Fundamental Principle of Counting, the total number of possible outcomes is:
        \begin{equation*}
            30\times 29 = 870
        \end{equation*}
    \end{eg}
    \begin{eg}
        Assume that we toss a coin $5$ times. By the Fundamental Principle of Counting, the total number of possible outcomes is:
        \begin{equation*}
            2^{5} = 32
        \end{equation*}
    \end{eg}
    \begin{eg}
        Assume that we roll a die $3$ times. By the Fundamental Principle of Counting, the total number of possible outcomes is:
        \begin{equation*}
            6^{3} = 216
        \end{equation*}
    \end{eg}
    \newpage

    Sometimes, we want to focus on how many ways we can arrange a set of objects.
    \begin{defn}
        Given a set with $n$ distinct elements:
        \begin{enumerate}
            \item A \textbf{permutation} of the set is an ordered arrangement of all elements of the set.
            \item If $k \leq n$, a \textbf{$k$-permutation} of the set is an ordered arrangement of $k$ elements of the set.
        \end{enumerate}
    \end{defn}
    \begin{rem}
        The $n$-permutation is simply the regular permutation.
    \end{rem}
    \begin{rem}
        To find the total number of permutations, we can think of it as determining the number of outcomes for placing one object in the 1st position, 2nd position, and so on.
    \end{rem}
    \begin{eg}
        Given a set $\{1,2,3\}$:
        \begin{enumerate}
            \item The ordered arrangement $(3,1,2)$ is a permutation of the set.
            \item The ordered arrangement $(3,2)$ is a $2$-permutation of the set.
        \end{enumerate}
    \end{eg}
    We have a formula to find the number of permutations.
    \begin{thm}
        Given a set with $n$ distinct elements, the number of permutations of the set is:
        \begin{equation*}
            n!
        \end{equation*}
        where $n! = n\times(n-1)\times(n-2)\times\cdots\times 2\times 1$ and $0! = 1$.
    \end{thm}
    \begin{thm}
        Let $n$ and $k$ be integers with $0 \leq k \leq n$. The number of $k$-permutations of a set with $n$ distinct elements, denoted by $P_{k}^{n}$, is given by:
        \begin{equation*}
            P_{k}^{n} = \frac{n!}{(n-k)!}
        \end{equation*}
    \end{thm}
    \begin{eg}
        Given a set $\{1,2,3,4\}$:
        \begin{enumerate}
            \item The number of permutations is:
            \begin{equation*}
                P_{4}^{4} = \frac{4!}{(4-4)!} = \frac{24}{1} = 24
            \end{equation*}
            \item The number of $2$-permutations is:
            \begin{equation*}
                P_{2}^{4} = \frac{4!}{(4-2)!} = \frac{24}{2} = 12
            \end{equation*}
        \end{enumerate}
    \end{eg}
    We also consider cases where the two ends of the ordered arrangement are connected together. For example, when arranging people around a circular table, rotating clockwise or counterclockwise results in the same arrangement.
    \begin{thm}
        Given a set with $n$ distinct elements, the number of arrangements of elements in a circle is:
        \begin{equation*}
            (n-1)!
        \end{equation*}
    \end{thm}
    \begin{eg}
        Given a set $\{1,2,3,4\}$, the number of arrangements in a circle is:
        \begin{equation*}
            (4-1)! = 3! = 6
        \end{equation*}
    \end{eg}
    \newpage

\section{Combinations}
    Now, assume that we do not care about the ordering of chosen objects. We only want to choose $k$ objects from $n$ objects. Then, we have the following definition.
    \begin{defn}
        If $k \leq n$, a \textbf{$k$-combination} of a set with $n$ distinct elements is an unordered arrangement of $k$ elements of the set.
    \end{defn}
    \begin{thm}
        Let $n$ and $k$ be integers with $0 \leq k \leq n$. The number of $k$-combinations of a set with $n$ distinct elements, denoted by $C_{k}^{n}$ or $\binom{n}{k}$, is given by:
        \begin{equation*}
            C_{k}^{n} = \binom{n}{k} = \frac{n!}{k!(n-k)!}
        \end{equation*}
    \end{thm}
    \begin{proofing}
        We know that the number of permutations of $k$ objects is $k!$, and the number of ordered arrangements of choosing $k$ objects from $n$ objects is:
        \begin{equation*}
            P_{k}^{n} = \frac{n!}{(n-k)!}
        \end{equation*}
        Since we do not care about the order, the number of unordered arrangements of choosing $k$ objects from $n$ objects is:
        \begin{equation*}
            \binom{n}{k} = \frac{P_{k}^{n}}{k!} = \frac{n!}{k!(n-k)!}
        \end{equation*}
    \end{proofing}
    \begin{rem}
        By convention, when $n$ is a non-negative integer and $k < 0$ or $k > n$, we define:
        \begin{equation*}
            \binom{n}{k} = 0
        \end{equation*}
    \end{rem}
    \begin{eg}
        Given a set $\{1,2,3,4\}$:
        \begin{enumerate}
            \item The number of $2$-combinations is:
            \begin{equation*}
                \binom{4}{2} = \frac{4!}{2!(4-2)!} = \frac{24}{4} = 6
            \end{equation*}
            \item The number of $3$-combinations is:
            \begin{equation*}
                \binom{4}{3} = \frac{4!}{3!(4-3)!} = \frac{24}{6} = 4
            \end{equation*}
        \end{enumerate}
    \end{eg}
    We can immediately derive the following corollary.
    \begin{cor}
        Let $n$ be an integer. For all integers $k$ with $0 \leq k \leq n$, we have:
        \begin{equation*}
            \binom{n}{k} = \binom{n}{n-k}
        \end{equation*}
    \end{cor}
    \begin{proofing}
        \begin{equation*}
            \binom{n}{n-k} = \frac{n!}{(n-k)!(n-n+k)!} = \frac{n!}{k!(n-k)!} = \binom{n}{k}
        \end{equation*}
    \end{proofing}
    \newpage

    From here, we will provide some important combinatorial identities that could be very useful.
    \begin{thm}\named{Pascal's Identity}
        Let $n$ and $k$ be integers with $0 < k < n$. Then:
        \begin{equation*}
            \binom{n}{k} = \binom{n-1}{k-1}+\binom{n-1}{k}
        \end{equation*}
    \end{thm}
    \begin{proofing}
        From the right-hand side, we have:
        \begin{equation*}
            \binom{n-1}{k-1}+\binom{n-1}{k} = \frac{(n-1)!}{(k-1)!(n-k)!}+\frac{(n-1)!}{k!(n-k-1)!} = \frac{(k+n-k)(n-1)!}{k!(n-k)!} = \frac{n!}{k!(n-k)!} = \binom{n}{k}
        \end{equation*}
    \end{proofing}
    We now introduce the famous Binomial Theorem.
    \begin{thm}\named{Binomial Theorem}
        Let $n$ be a non-negative integer. Then:
        \begin{equation*}
            (x+y)^{n} = \sum_{k = 0}^{n}\binom{n}{k}x^{k}y^{n-k}
        \end{equation*} 
        where for all $k$, $\binom{n}{k}$ is called the \textbf{binomial coefficient}.
    \end{thm}
    \begin{cor}
        Let $n$ be a non-negative integer. Then:
        \begin{equation*}
            \sum_{k = 0}^{n}\binom{n}{k} = 2^{n}
        \end{equation*}
    \end{cor}
    \begin{proofing}
        Using the Binomial Theorem and substituting $x = 1$ and $y = 1$, we have:
        \begin{equation*}
            2^{n} = (1+1)^{n} = \sum_{k = 0}^{n}\binom{n}{k}(1)^{k}(1)^{n-k} = \sum_{k = 0}^{n}\binom{n}{k}
        \end{equation*}
    \end{proofing}
    \begin{cor}
        Let $n$ be a positive integer. Then:
        \begin{equation*}
            \sum_{k = 0}^{n}(-1)^{k}\binom{n}{k} = 0
        \end{equation*}
    \end{cor}
    \begin{proofing}
        Using the Binomial Theorem and substituting $x = -1$ and $y = 1$, we have:
        \begin{equation*}
            0 = (-1+1)^{n} = \sum_{k = 0}^{n}\binom{n}{k}(-1)^{k}(1)^{n-k} = \sum_{k = 0}^{n}(-1)^{k}\binom{n}{k}
        \end{equation*}
    \end{proofing}
    \begin{cor}
        Let $n$ be a positive integer. Then:
        \begin{equation*}
            \sum_{k = 0}^{n}2^{k}\binom{n}{k} = 3^{n}
        \end{equation*}
    \end{cor}
    \begin{proofing}
        Using the Binomial Theorem and substituting $x = 2$ and $y = 1$, we have:
        \begin{equation*}
            3^{n} = (2+1)^{n} = \sum_{k = 0}^{n}\binom{n}{k}(2)^{k}(1)^{n-k} = \sum_{k = 0}^{n}2^{k}\binom{n}{k}
        \end{equation*}
    \end{proofing}
    \newpage

    Using the Binomial Theorem, we can prove the following identity.
    \begin{thm}\named{Vandermonde's Identity}
        Let $m,n,r$ be positive integers with $0 \leq r \leq m$ and $0 \leq r \leq n$. Then:
        \begin{equation*}
            \binom{m+n}{r} = \sum_{k = 0}^{r}\binom{m}{r-k}\binom{n}{k}
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Assume that we want to find $(x+y)^{m+n}$. Using the Binomial Theorem, we have:
        \begin{align*}
            \sum_{r = 0}^{m+n}\binom{m+n}{r}x^{r}y^{m+n-r} = (x+y)^{m+n} &= \left(\sum_{i = 0}^{m}\binom{m}{i}x^{i}y^{m-i}\right)\left(\sum_{j = 0}^{n}\binom{n}{j}x^{j}y^{n-j}\right)\\
            &= \sum_{i = 0}^{m}\sum_{j = 0}^{n}\binom{m}{i}\binom{n}{j}x^{i+j}y^{m+n-i-j}\\
            \tag{Setting $r = i+j$ and $k = j$}
            &= \sum_{r = 0}^{m+n}\sum_{k = 0}^{r}\binom{m}{r-k}\binom{n}{k}x^{r}y^{m+n-r}
        \end{align*}
        If we look at each binomial coefficient, we find that:
        \begin{equation*}
            \binom{m+n}{r} = \sum_{k = 0}^{r}\binom{m}{r-k}\binom{n}{k}
        \end{equation*}
    \end{proofing}
    \begin{cor}
        Let $n$ be a non-negative integer. Then:
        \begin{equation*}
            \binom{2n}{n} = \sum_{k = 0}^{n}\binom{n}{k}^{2}
        \end{equation*}
    \end{cor}
    \begin{proofing}
        Using Vandermonde's Identity and substituting $m = n$ and $r = n$, we have:
        \begin{equation*}
            \binom{2n}{n} = \sum_{k = 0}^{n}\binom{n}{n-k}\binom{n}{k} = \sum_{k = 0}^{n}\binom{n}{k}^{2}
        \end{equation*}
    \end{proofing}
    \begin{thm}
        Let $n$ and $r$ be integers such that $0 \leq r \leq n$. Then:
        \begin{equation*}
            \binom{n+1}{r+1} = \sum_{i = r}^{n}\binom{i}{r}
        \end{equation*}
    \end{thm}
    \begin{proofing}
        We can use Pascal's Identity on the right-hand side:
        \begin{equation*}
            \sum_{i = r}^{n}\binom{i}{r} = \sum_{i = r+1}^{n}\binom{i}{r}+1 = \sum_{i = r+1}^{n}\binom{i}{r}+\binom{r+1}{r+1} = \binom{n}{r}+\binom{n}{r+1} = \binom{n+1}{r+1}
        \end{equation*}
    \end{proofing}
    \newpage

\section{Multinomial}
    What about when some objects are of the same type?
    \begin{thm}
        Given a set with $n$ elements, if we divide $n_{i}$ objects into the $i$-th group for all $i$ and $n_{1}+n_{2}+\cdots+n_{k} = n$, then the number of different combinations, denoted by $\binom{n}{n_{1},n_{2},\cdots,n_{k}}$, is:
        \begin{equation*}
            \binom{n}{n_{1},n_{2},\cdots,n_{k}} = \frac{n!}{n_{1}!n_{2}!\cdots n_{k}!}
        \end{equation*}
    \end{thm}
    \begin{rem}
        When you choose $k$ objects from $n$ objects, you may consider it as classifying $k$ objects as chosen and the remaining $n-k$ objects as not chosen. 
    \end{rem}
    We have a more generalized version of the Binomial Theorem.
    \begin{thm}\named{Multinomial Theorem}
        Let $n$ be a non-negative integer. Then:
        \begin{equation*}
            (x_{1}+x_{2}+\cdots+x_{k})^{n} = \sum_{(n_{1},n_{2},\cdots,n_{k}):n_{1}+n_{2}+\cdots+n_{k} = n}\binom{n}{n_{1},n_{2},\cdots,n_{k}}x_{1}^{n_{1}}x_{2}^{n_{2}}\cdots x_{k}^{n_{k}}
        \end{equation*}
        where $(n_{1},n_{2},\cdots,n_{k})$ are all non-negative integer-valued vectors.
    \end{thm}
    The formula is quite complex! Fortunately, we have a formula to determine how many terms the above equation contains.
    \begin{thm}
        There are $\binom{n+r-1}{r-1}$ distinct non-negative integer-valued vectors $(x_{1},x_{2},\cdots,x_{r})$ that satisfy:
        \begin{equation*}
            x_{1}+x_{2}+\cdots+x_{r} = n
        \end{equation*}
        where $x_{i} \geq 0$ for all $i$.
    \end{thm}
    \begin{eg}
        Suppose that a cookie shop has $4$ different kinds of cookies. How many different ways can $6$ cookies be chosen? The number of ways to choose $6$ cookies is the number of $6$-combinations with repetition from a set with $4$ elements, which is:
        \begin{equation*}
            \binom{4+6-1}{6} = \binom{9}{6} = 84
        \end{equation*}
    \end{eg}
    We can also use the above to find the number of ways to distribute $n$ objects into $r$ boxes. If, in addition, every box must contain at least one object, we have the following.
    \begin{thm}
        There are $\binom{n-1}{r-1}$ distinct positive integer-valued vectors $(x_{1},x_{2},\cdots,x_{r})$ that satisfy:
        \begin{equation*}
            x_{1}+x_{2}+\cdots+x_{r} = n
        \end{equation*}
        where $x_{i} > 0$ for all $i$.
    \end{thm}
    \begin{eg}
        Suppose that a cookie shop has $4$ different kinds of cookies. How many different ways can $6$ cookies be chosen if at least one cookie of each kind must be chosen? The number of ways to choose $6$ cookies is the number of $6$-combinations with repetition from a set with $4$ elements, where each element must be chosen at least once, which is:
        \begin{equation*}
            \binom{6-1}{4-1} = \binom{5}{3} = 10
        \end{equation*}
    \end{eg}

\chapter{Events and Their Probabilities}
    \label{Chapter 2 (Events and Their Probabilities)}
    We have now understood how to find the number of possibilities. However, we still have not rigorously formalized probability. Therefore, we need to define some basic terminology related to probability.
\section{Fundamentals}
    We start with some basic terminology. Many statements in probability take the form of "the probability of event $A$ is $p$," where events usually include some of the elements of the sample space.
    \begin{defn}
        These are the basic objects of probability:
        \begin{enumerate}
            \item An \textbf{experiment} is an activity that produces distinct and well-defined possibilities called \textbf{outcomes}, denoted by $\omega$.
            \item The \textbf{sample space} is the set of all outcomes of an experiment, denoted by $\Omega$.
            \item An \textbf{event} is a subset of the sample space and is usually represented by $A,B,C,\cdots$.
            \item Outcomes are called \textbf{elementary events}.
        \end{enumerate}
    \end{defn}
    \begin{eg}
        Examples of sample spaces:
        \begin{enumerate}
            \item Rolling a die: $\Omega = \{1,2,3,4,5,6\}$
            \item Lifetime of a bulb: $\Omega = [0,\infty)$
            \item Flipping two coins: $\Omega = \{(H,H),(H,T),(T,H),(T,T)\}$
        \end{enumerate} 
    \end{eg}
    \begin{rem}
        If the outcome $\omega$ is in event $A$, we say that event $A$ has occurred.
    \end{rem}
    \begin{rem}
        It is not necessary for all subsets of $\Omega$ to be events. However, we do not discuss this issue for the moment.
    \end{rem}
    \begin{eg}
        Events for rolling a die: Odd ($A = \{1,3,5\}$), Even ($A = \{2,4,6\}$), $\cdots$
    \end{eg}
    \begin{eg}
        Events for flipping two coins: At least one head ($A = \{(H,H),(H,T),(T,H)\}$), Two tails ($A = \{(T,T)\}$), $\cdots$
    \end{eg}
    \begin{rem}
        If only the outcome $\omega = 2$ is given, then there exist many events that can include this outcome, e.g., $\{2\}$, $\{2,4\}$, $\cdots$
    \end{rem}
    \begin{eg}
        If we roll a die and the outcome is $2$, then the event that occurred could be $\{2\}$, $\{1,2\}$, $\{2,4,6\}$, $\{1,2,3,4,5,6\}$, etc.
    \end{eg}
    \newpage

\section{Event Operations}
    We can perform operations on events, similar to sets.
    \begin{defn}
        Given two events $A$ and $B$:
        \begin{enumerate}
            \item The \textbf{union} of $A$ and $B$ is an event:
            \begin{equation*}
                A\cup B = \{\omega \in \Omega:\omega \in A\text{ or }\omega \in B\}
            \end{equation*}
            \item The \textbf{intersection} of $A$ and $B$ is an event:
            \begin{equation*}
                A\cap B = \{\omega \in \Omega:\omega \in A\text{ and }\omega \in B\}
            \end{equation*}
            \item The \textbf{complement} of $A$ is an event containing all elements in the sample space $\Omega$ that are not in $A$, or equivalently:
            \begin{equation*}
                A^{\complement} = \{\omega \in \Omega:\omega \not \in A\}
            \end{equation*}
            \item The \textbf{relative complement} of $B$ in $A$ is an event:
            \begin{equation*}
                A\setminus B = \{\omega \in \Omega:\omega \in A\text{ and }\omega \not \in B\}
            \end{equation*}
            \item The \textbf{symmetric difference} of $A$ and $B$ is an event:
            \begin{equation*}
                A\Delta B = \{\omega \in \Omega:\omega \in A\cup B\text{ and }\omega \not \in A\cap B\}
            \end{equation*}
        \end{enumerate}
    \end{defn}
    We also need to define the inclusion of all outcomes of one event in another event.
    \begin{defn}
        For any two events $A$ and $B$, if all of the outcomes in $A$ are also in $B$, then we say $A$ is \textbf{contained} in $B$, written as $A\subset B$ or $B\supset A$.
    \end{defn}
    \begin{rem}
        If $A\subset B$, the occurrence of $A$ necessarily implies the occurrence of $B$.
    \end{rem}
    We can describe the events in a sample space.
    \begin{defn}
        Given a sequence of events $A_{1},A_{2},\cdots,A_{k}$:
        \begin{enumerate}
            \item For any $i$ and $j$, if $A_{i}\cap A_{j} = \emptyset$, then $A_{i}$ and $A_{j}$ are called \textbf{disjoint}.
            \item If $A_{i}\cap A_{j} = \emptyset$ for all $i$ and $j$, the sequence of events is called \textbf{mutually exclusive}.
            \item If $A_{1}\cup A_{2}\cup\cdots\cup A_{k} = \Omega$, the sequence of events is called \textbf{exhaustive}.
            \item If the sequence is both mutually exclusive and exhaustive, it is called a \textbf{partition}.
        \end{enumerate}
    \end{defn}
    We have some fundamental laws for event operations.
    \begin{thm}
        Let $A,B,C$ be any three events:
        \begin{enumerate}
            \item Commutative Law:
            \begin{align*}
                A\cup B &= B\cup A & A\cap B &= B\cap A
            \end{align*}
            \item Associative Law:
            \begin{align*}
                A\cup(B\cup C) &= (A\cup B)\cup C & A\cap(B\cap C) &= (A\cap B)\cap C
            \end{align*}
            \item Distributive Law:
            \begin{align*}
                A\cup(B\cap C) &= (A\cup B)\cap(A\cup C) & A\cap(B\cup C) &= (A\cap B)\cup(A\cap C)
            \end{align*}
        \end{enumerate}
    \end{thm}
    \newpage

    \begin{thm}\named{De Morgan's Law}
        For any $k$, the sequence of events $A_{1},A_{2},\cdots,A_{k}$ satisfies:
        \begin{align*}
            \left(\bigcup_{i = 1}^{k}A_{i}\right)^{\complement} &= \bigcap_{i = 1}^{k}A_{i}^{\complement} & \left(\bigcap_{i = 1}^{k}A_{i}\right)^{\complement} = \bigcup_{i = 1}^{k}A_{i}^{\complement}
        \end{align*}
    \end{thm}
    We can also split any event into a union of two events.
    \begin{lem}
        For any events $A$ and $B$, we have:
        \begin{equation*}
            A = (A\cap B)\cup(A\cap B^{\complement})
        \end{equation*}
    \end{lem}
    \begin{proofing}
        By the distributive law:
        \begin{equation*}
            (A\cap B)\cup(A\cap B^{\complement}) = A\cap(B\cup B^{\complement}) = A\cap\Omega = A
        \end{equation*}
    \end{proofing}
    We may now start defining probability. Let us begin by defining a collection of subsets of the sample space.
    \begin{defn}
        A \textbf{field} $\mathcal{F}$ is any collection of subsets of the sample space $\Omega$ that satisfies the following conditions:
        \begin{enumerate}
            \item If $A \in \mathcal{F}$, then $A^{\complement} \in \mathcal{F}$.
            \item If $A,B \in \mathcal{F}$, then $A\cup B \in \mathcal{F}$ and $A\cap B = (A^{\complement}\cup B^{\complement})^{\complement} \in \mathcal{F}$. (Closed under \textit{finite} unions or intersections)
            \item $\emptyset \in \mathcal{F}$ and $\Omega = A\cup A^{\complement} \in \mathcal{F}$.
        \end{enumerate}
    \end{defn}
    We are more interested in $\sigma$-fields that are closed under countably infinite unions.
    \begin{defn}
        A \textbf{$\sigma$-field} (or \textbf{$\sigma$-algebra}) $\mathcal{F}$ is any collection of subsets of the sample space $\Omega$ that satisfies the following conditions:
        \begin{enumerate}
            \item If $A \in \mathcal{F}$, then $A^{\complement} \in \mathcal{F}$.
            \item If $A_{1},A_{2},\cdots \in \mathcal{F}$, then $\bigcup_{i = 1}^{\infty}A_{i} \in \mathcal{F}$. (Closed under \textit{countably infinite} unions)
            \item $\emptyset \in \mathcal{F}$ and $\Omega = A\cup A^{\complement}\cup\cdots \in \mathcal{F}$.
        \end{enumerate}
    \end{defn}
    \begin{rem}
        All $\sigma$-fields are fields. The converse is not necessarily true.
    \end{rem}
    \begin{rem}
        From this point onwards, $\mathcal{F}$ represents the $\sigma$-field.
    \end{rem}
    \begin{eg}
        Smallest $\sigma$-field: $\mathcal{F} = \{\emptyset,\Omega\}$
    \end{eg}
    \begin{eg}
        If $A$ is any subset of $\Omega$, then $\mathcal{F} = \{\emptyset,A,A^{\complement},\Omega\}$ is a $\sigma$-field.
    \end{eg}
    \begin{eg}
        Largest $\sigma$-field: Power set of $\Omega$: $2^{\Omega} = \{0,1\}^{\Omega}: = \{\text{All subsets of }\Omega\}$\\
        When $\Omega$ is infinite, the power set is too large a collection for probabilities to be assigned reasonably.
    \end{eg}
    \begin{rem}
        For any $a < b$:
        \begin{align*}
            (a, b) &= \bigcup_{n = 1}^{\infty}\left[a+\frac{1}{n},b-\frac{1}{n}\right] &            [a, b] &= \bigcap_{n = 1}^{\infty}\left[a-\frac{1}{n},b+\frac{1}{n}\right]
        \end{align*}
    \end{rem}
    \newpage

\section{Probability Measure and Kolmogorov Axioms}
    Now that we have defined some fundamental terminology, we can finally define probability.
    \begin{defn}
        A \textbf{measurable space} $(\Omega,\mathcal{F})$ is a pair comprising a sample space $\Omega$ and a $\sigma$-field $\mathcal{F}$.\\
        A \textbf{measure} $\mu$ on a measurable space $(\Omega,\mathcal{F})$ is a function $\mu:\mathcal{F} \to [0,\infty]$ satisfying:
        \begin{enumerate}
            \item $\mu(\emptyset) = 0$.
            \item (Countable additivity) If $A_{i} \in \mathcal{F}$ for all $i$ and they are disjoint, then 
            \begin{equation*}
                \mu\left(\bigcup_{i = 1}^{\infty}A_{i}\right) = \sum_{i = 1}^{\infty}\mu(A_{i})
            \end{equation*}
        \end{enumerate}
        A \textbf{probability measure} $\prob$ is a measure with $\prob(\Omega) = 1$.
    \end{defn}
    You may ask, "Isn't it just probability?" The probability that we know is indeed a probability measure, which we will define soon. However, there are other measures that satisfy the definition of a probability measure, e.g., the risk-neutral measure.
    \begin{eg}
        Let $\Omega = \{1,2,3,4,5,6\}$ and $\mathcal{F} = \{0,1\}^{\Omega}$. Define $\prob(A) = \frac{|A|}{6}$ for all $A \in \mathcal{F}$. Then $\prob$ is a probability measure.
    \end{eg}
    The following measures are not probability measures:
    \begin{eg}
        Lebesgue measure: $\mu((a, b)) = b-a$, $\Omega = \mathbb{R}$
    \end{eg}
    \begin{eg}
        Counting measure: $\mu(A) = \#\{A\}$, $\Omega = \mathbb{R}$
    \end{eg}
    We can combine a measurable space and a measure into a measure space.
    \begin{defn}
        A \textbf{measure space} is the triple $(\Omega,\mathcal{F},\mu)$, comprising:
        \begin{enumerate}
            \item A sample space $\Omega$.
            \item A $\sigma$-field $\mathcal{F}$ of certain subsets of $\Omega$.
            \item A measure $\mu$ on $(\Omega,\mathcal{F})$.
        \end{enumerate}
        A \textbf{probability space} $(\Omega,\mathcal{F},\prob)$ is a measure space with a probability measure $\prob$ as the measure.
    \end{defn}
    Kolmogorov's axioms of probability use axioms to formalize probability.
    \begin{defn}\named{Kolmogorov Axioms of Probability}
        Let $(\Omega,\mathcal{F},\prob)$ be a probability space, with sample space $\Omega$, $\sigma$-field $\mathcal{F}$, and probability measure $\prob$.
        \begin{enumerate}
            \item The probability of an event is a non-negative real number. For all $E \in \mathcal{F}$:
            \begin{align*}
                \prob(E) &\in \mathbb{R} & \prob(E) &\geq 0
            \end{align*}
            \item The probability that at least one of the elementary events in the entire sample space will occur is $1$:
            \begin{equation*}
                \prob(\Omega) = 1
            \end{equation*}
            \item Any countable sequence of disjoint events $\{E_{i}\}_{i}\subset\mathcal{F}$ satisfies:
            \begin{equation*}
                \prob\left(\bigcup_{i = 1}^{\infty}E_{i}\right) = \sum_{i = 1}^{\infty}\prob(E_{i})
            \end{equation*}
        \end{enumerate}
        By this definition, we call $\prob(A)$ the \textbf{probability} of the event $A$.
    \end{defn}
    \newpage

    \begin{eg}
        Consider a coin flip. The sample space is $\Omega = \{H,T\}$, and the $\sigma$-field is $\mathcal{F} = \{\emptyset,H,T,\Omega\}$. Let $\prob(H) = p$, where $p \in [0,1]$. Define $A = \{\omega \in \Omega:\omega = H\}$. Then:
        \begin{equation*}
            \prob(A) = \begin{cases}
                0, &A = \emptyset\\
                p, &A = \{H\}\\
                1-p, &A = \{T\}\\
                1, &A = \Omega
            \end{cases}
        \end{equation*}
        If $p = \frac{1}{2}$, then the coin is fair.   
    \end{eg}
    \begin{eg}
        Consider a die roll. The sample space is $\Omega = \{1,2,3,4,5,6\}$, and the $\sigma$-field is $\mathcal{F} = \{0,1\}^{\Omega}$. Let $p_{i} = \prob(\{i\})$, where $i \in \Omega$. For all $A \in \mathcal{F}$:
        \begin{equation*}
            \prob(A) = \sum_{i \in A}p_{i}
        \end{equation*}
        If $p_{i} = \frac{1}{6}$ for all $i$, then the die is fair, and we have:
        \begin{equation*}
            \prob(A) = \frac{|A|}{6}
        \end{equation*}
    \end{eg}
    The following properties are important and form the foundation of probability.
    \begin{lem}
        For any $A,B \in \mathcal{F}$, $\prob$ satisfies the following properties:
        \begin{enumerate}
            \item $\prob(A^{\complement}) = 1-\prob(A)$.
            \item If $A\subseteq B$, then $\prob(B) = \prob(A)+\prob(B\setminus A) \geq \prob(A)$.
            \item $\prob(A\cup B) = \prob(A)+\prob(B)-\prob(A\cap B)$. If $A$ and $B$ are disjoint, then $\prob(A\cup B) = \prob(A)+\prob(B)$.
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item $A\cup A^{\complement} = \Omega$ and $A\cap A^{\complement} = \emptyset \Longrightarrow \prob(A\cup A^{\complement}) = \prob(A)+\prob(A^{\complement}) = 1$
            \item $A\subseteq B \Longrightarrow B = A\cup (B\setminus A) \Longrightarrow \prob(B) = \prob(A)+\prob(B\setminus A)$
            \item $A\cup B = A\cup (B\setminus A) \Longrightarrow \prob(A\cup B) = \prob(A)+\prob(B\setminus A) = \prob(A)+\prob(B\setminus(A\cap B)) = \prob(A)+\prob(B)-\prob(A\cap B)$
        \end{enumerate}
    \end{proofing}
    \begin{thm}\named{Inclusion-Exclusion Formula}
        For any set of events $\{A_{1},\cdots,A_{n}\} \in \mathcal{F}$:
        \begin{equation*}
            \prob\left(\bigcup_{i = 1}^{n}A_{i}\right) = \sum_{i}\prob(A_{i})-\sum_{i < j}\prob(A_{i}\cap A_{j})+\cdots+(-1)^{n+1}\prob(A_{1}\cap A_{2}\cap\cdots\cap A_{n})
        \end{equation*}
    \end{thm}
    \begin{proofing}
        By induction. When $n = 1$, it is obviously true. Assume it is true for some positive integer $m$. When $n = m+1$:
        \begin{align*}
            \prob\left(\bigcup_{i = 1}^{m+1}A_{i}\right) &= \prob\left(\bigcup_{i = 1}^{m}A_{i}\right)+\prob(A_{m+1})-\prob\left(\bigcup_{i = 1}^{m}A_{i}\cap A_{m+1}\right)\\
            &= \sum_{i = 1}^{m+1}\prob(A_{i})-\sum_{1 \leq i < j \leq m}\prob(A_{i}\cap A_{j})+\cdots(-1)^{m+1}\prob\left(\bigcap_{i = 1}^{m}A_{i}\right)-\prob\left(\bigcup_{i = 1}^{m}A_{i}\cap A_{m+1}\right)\\
            &= \sum_{i = 1}^{m+1}\prob(A_{i})-\sum_{1 \leq i < j \leq m+1}\prob(A_{i}\cap A_{j})+\cdots+(-1)^{m+2}\prob\left(\bigcap_{i = 1}^{m+1}A_{i}\right)
        \end{align*}
        Therefore, by induction, the formula is true for any set of events $\{A_{1},\cdots,A_{n}\}$ for any $n \in \mathbb{N}_{+}$.
    \end{proofing}
    \newpage
    
    We recall the continuity of a function $f:\mathbb{R} \to \mathbb{R}$. The function $f$ is continuous at some point $x$ if, for all sequences $x_{n}$ such that $x_{n} \to x$ as $n \to \infty$, we have:
    \begin{equation*}
        \lim_{n \to \infty}f(x_{n}) = f\left(\lim_{n \to \infty}x_{n}\right) = f(x).
    \end{equation*}
    Similarly, we say a set function $\mu$ is continuous if, for all sequences of sets $\{A_{i}\}_{i}$ with $A = \lim_{n \to \infty}A_{n}$, we have:
    \begin{equation*}
        \lim_{n \to \infty}\mu(A_{n}) = \mu\left(\lim_{n \to \infty}A_{n}\right) = \mu(A).
    \end{equation*}
    \begin{rem} 
        Given a sequence of sets $\{A_{i}\}_{i}\subset\mathcal{F}$, we have two types of set limits:
        \begin{align*}
            \limsup_{n \to \infty}A_{n} &= \lim_{n \to \infty}\sup_{m \geq n}A_{m} = \bigcap_{n = 1}^{\infty}\bigcup_{m = n}^{\infty}A_{m} = \{\omega \in \Omega: \omega \in A_{n}\text{ for infinitely many }n\},\\
            \liminf_{n \to \infty}A_{n} &= \lim_{n \to \infty}\inf_{m \geq n}A_{m} = \bigcup_{n = 1}^{\infty}\bigcap_{m = n}^{\infty}A_{m} = \{\omega \in \Omega: \omega \in A_{n}\text{ for all but finitely many }n\}.
        \end{align*}
        Clearly, $\liminf_{n \to \infty}A_{n}\subseteq\limsup_{n \to \infty}A_{n}$.
    \end{rem}
    \begin{defn}
        We say a sequence of events $\{A_{i}\}_{i}\subset\mathcal{F}$ \textbf{converges}, and $\lim_{n \to \infty}A_{n}$ exists if:
        \begin{equation*}
            \limsup_{n \to \infty}A_{n} = \liminf_{n \to \infty}A_{n}.
        \end{equation*}
        Given a probability space $(\Omega,\mathcal{F},\prob)$, if $\{A_{i}\}_{i}\subset\mathcal{F}$ such that $A = \lim_{n \to \infty}A_{n}$ exists, then:
        \begin{equation*}
            \lim_{n \to \infty}\prob(A_{n}) = \prob\left(\lim_{n \to \infty}A_{n}\right).
        \end{equation*}
    \end{defn}
    From this definition, we derive the following important lemma.
    \begin{lem}
        \label{Chapter 2 (Lemma) Continuity of Probability Measure}
        If $\{A_{i}\}_{i}\subset\mathcal{F}$ is an increasing sequence of events ($A_{1}\subseteq A_{2}\subseteq\cdots$), then:
        \begin{equation*}
            \prob(A) = \prob\left(\bigcup_{n = 1}^{\infty}A_{n}\right) = \lim_{n \to \infty}\prob(A_{n}).
        \end{equation*}
        Similarly, if $\{A_{i}\}_{i}\subset\mathcal{F}$ is a decreasing sequence of events ($A_{1}\supseteq A_{2}\supseteq\cdots$), then:
        \begin{equation*}
            \prob(A) = \prob\left(\bigcap_{n = 1}^{\infty}A_{n}\right) = \lim_{n \to \infty}\prob(A_{n}).
        \end{equation*}
    \end{lem}
    \begin{proofing}
        For $A_{1}\subseteq A_{2}\subseteq\cdots$, let $B_{n} = A_{n}\setminus A_{n-1}$. Then:
        \begin{equation*}
            \prob\left(\bigcup_{n = 1}^{\infty}A_{n}\right) = \prob\left(\bigcup_{n = 1}^{\infty}B_{n}\right) = \sum_{m = 1}^{\infty}\prob(B_{m}) = \lim_{n \to \infty}\sum_{m = 1}^{n}\prob(B_{m}) = \lim_{n \to \infty}\prob\left(\bigcup_{m = 1}^{n}B_{m}\right) = \lim_{n \to \infty}\prob(A_{n}).
        \end{equation*}
        For $A_{1}\supseteq A_{2}\supseteq\cdots$, we have $A^{\complement} = \bigcup_{i = 1}^{\infty}A_{i}^{\complement}$ and $A_{1}^{\complement}\subseteq A_{2}^{\complement}\subseteq\cdots$. Therefore:
        \begin{equation*}
            \prob\left(\bigcap_{n = 1}^{\infty}A_{n}\right) = 1-\prob\left(\bigcup_{n = 1}^{\infty}A_{n}^{\complement}\right) = 1-\lim_{n \to \infty}\prob(A_{n}^{\complement}) = \lim_{n \to \infty}\prob(A_{n}).
        \end{equation*}
    \end{proofing}
    We can assign terminology to some special probabilities.
    \begin{defn}
        An event $A$ is \textbf{null} if $\prob(A) = 0$.
    \end{defn}
    \begin{rem}
        Null events need not be impossible. For example, $\prob(\text{Choosing a specific point in a plane}) = 0$.
    \end{rem}
    \begin{defn}
        An event $A$ occurs \textbf{almost surely} if $\prob(A) = 1$.
    \end{defn}
    \newpage

\section{Conditional Probability}
    Let $\mathcal{F}$ be a $\sigma$-field of the sample space $\Omega$. Sometimes, we are interested in the probability of a certain event given that another event has occurred.
    \begin{defn}
        For any $A,B \in \mathcal{F}$, if $\prob(B) > 0$, then the \textbf{conditional probability} that $A$ occurs given that $B$ occurs is:
        \begin{equation*}
            \prob(A|B) = \frac{\prob(A\cap B)}{\prob(B)}.
        \end{equation*}
    \end{defn}
    \begin{rem}
        For any $A \in \mathcal{F}$, $\prob(A)$ can be regarded as $\prob(A|\Omega)$.
    \end{rem}
    \begin{rem}
        For any $E,F \in \mathcal{F}$, when $\prob(E) = \prob(E|F)$, $E$ and $F$ are said to be \textbf{independent}.
    \end{rem}
    \begin{eg}
        Two fair dice are thrown. Given that the first shows $3$, what is the probability that the sum of the numbers shown exceeds $6$?
        \begin{equation*}
            \prob(\text{Sum} > 6|\text{First die shows }3) = \frac{\frac{3}{36}}{\frac{1}{6}} = \frac{1}{6}.
        \end{equation*}
    \end{eg}
    \begin{lem}
        For any $B \in \mathcal{F}$, if $\prob(B) > 0$, $\prob(\cdot|B)$ is a probability measure on $\mathcal{F}$.
    \end{lem}
    \begin{proofing}
        We prove this from the definition of a probability measure.
        \begin{enumerate}
            \item We prove $\prob(\emptyset|B) = 0$. Since $\prob(B) > 0$:
            \begin{equation*}
                \prob(\emptyset|B) = \frac{\prob(\emptyset\cap B)}{\prob(B)} = \frac{\prob(\emptyset)}{\prob(B)} = 0.
            \end{equation*}
            \item We prove $\prob(\Omega|B) = 1$. Since $\prob(B) > 0$:
            \begin{equation*}
                \prob(\Omega|B) = \frac{\prob(\Omega\cap B)}{\prob(B)} = \frac{\prob(B)}{\prob(B)} = 1.
            \end{equation*}
            \item We prove countable additivity. Since $\prob(B) > 0$, for any disjoint sequence of events $A_{i} \in \mathcal{F}$ for all $i$:
            \begin{equation*}
                \prob\left(\left.\bigcup_{i = 1}^{\infty}A_{i}\right|B\right) = \frac{1}{\prob(B)}\prob\left(\bigcup_{i = 1}^{\infty}A_{i}\cap B\right) = \frac{1}{\prob(B)}\prob\left(\bigcup_{i = 1}^{\infty}(A_{i}\cap B)\right) = \frac{1}{\prob(B)}\sum_{i = 1}^{\infty}\prob(A_{i}\cap B) = \sum_{i = 1}^{\infty}\prob(A_{i}|B).
            \end{equation*}
        \end{enumerate}
        Therefore, for any $B \in \mathcal{F}$, if $\prob(B) > 0$, then $\prob(\cdot|B)$ is a probability measure.
    \end{proofing}
    We may create a series of probabilities based on previous events. This is useful when dealing with a sequence of events over time.
    \begin{lem}\named{General Multiplication Rule}
        Let $\{A_{1},\cdots,A_{n}\}\subset\mathcal{F}$ be a sequence of events. We have:
        \begin{equation*}
            \prob\left(\bigcap_{i = 1}^{n}A_{i}\right) = \prob(A_{1})\prob(A_{2}|A_{1})\prob(A_{3}|A_{1}\cap A_{2})\cdots\prob(A_{n}|A_{1}\cap A_{2}\cap\cdots\cap A_{n-1}).
        \end{equation*}
    \end{lem}
    \begin{proofing}
        \begin{align*}
            \prob(A_{1})\prob(A_{2}|A_{1})\prob(A_{3}|A_{1}\cap A_{2})\cdots\prob(A_{n}|A_{1}\cap A_{2}\cap\cdots\cap A_{n-1}) &= \prob(A_{1}\cap A_{2})\prob(A_{3}|A_{1}\cap A_{2})\cdots\prob(A_{n}|A_{1}\cap A_{2}\cap\cdots\cap A_{n-1})\\
            &= \prob(A_{1}\cap A_{2}\cap A_{3})\cdots\prob(A_{n}|A_{1}\cap A_{2}\cap\cdots\cap A_{n-1})\\
            &= \prob\left(\bigcap_{i = 1}^{n}A_{i}\right).
        \end{align*}
    \end{proofing}
    \newpage

    It is evident that a certain event occurs when another event either occurs or does not occur.
    \begin{lem}
        For any $A,B \in \mathcal{F}$ such that $0 < \prob(B) < 1$:
        \begin{equation*}
            \prob(A) = \prob(A|B)\prob(B)+\prob(A|B^{\complement})\prob(B^{\complement}).
        \end{equation*}
    \end{lem}
    \begin{proofing}
        \begin{equation*}
            A = (A\cap B)\cup(A\cap B^{\complement}) \Longrightarrow \prob(A) = \prob(A\cap B)+\prob(A\cap B^{\complement}) = \prob(A|B)\prob(B)+\prob(A|B^{\complement})\prob(B^{\complement}).
        \end{equation*}
    \end{proofing}
    \begin{eg}
        In medical cases, we often evaluate the efficiency and effectiveness of a medical test. Each type of result has a specific name:
        \begin{enumerate}
            \item True positive (TP): Sick individuals correctly identified as sick (Found positive and correct).
            \item False positive (FP): Healthy individuals incorrectly identified as sick (Found positive but incorrect).
            \item True negative (TN): Healthy individuals correctly identified as healthy (Found negative and correct).
            \item False negative (FN): Sick individuals incorrectly identified as healthy (Found negative but incorrect).
        \end{enumerate}
    \end{eg}
    There are cases where multiple events contribute to the occurrence of a certain event.
    \begin{lem}\named{Law of Total Probability}
        Let $\{B_{1},\cdots,B_{n}\}\subset\mathcal{F}$ be a partition of $\Omega$. Suppose that $\prob(B_{i}) > 0$ for all $i$. Then:
        \begin{equation*}
            \prob(A) = \sum_{i = 1}^{n}\prob(A|B_{i})\prob(B_{i}).
        \end{equation*}
    \end{lem}
    \begin{proofing}
        \begin{align*}
            \prob(A) = \prob(A\cap\Omega) &= \prob\left(A\cap\left(\bigcup_{i = 1}^{n}B_{i}\right)\right)\\
            &= \prob\left(\bigcup_{i = 1}^{n}(A\cap B_{i})\right)\\
            &= \sum_{i = 1}^{n}\prob(A\cap B_{i})\\
            &= \sum_{i = 1}^{n}\prob(A|B_{i})\prob(B_{i}).
        \end{align*}
    \end{proofing}
    At this point, we can prove a theorem widely used in fields outside mathematics. Imagine knowing the probability of each type of disease and the probability of having a specific symptom given the disease. If a patient has the symptom, what is the probability they have the disease you are considering?
    \begin{thm}\named{Bayes' Theorem}
        Suppose that a sequence of events $\{A_{1},\cdots,A_{n}\}\subset\mathcal{F}$ forms a partition of the sample space. Assume further that $\prob(A_{i}) > 0$ for all $i$. For any $B \in \mathcal{F}$:
        \begin{equation*}
            \prob(A_{i}|B) = \frac{\prob(B|A_{i})\prob(A_{i})}{\sum_{k = 1}^{n}\prob(B|A_{k})\prob(A_{k})}, \qquad\text{for }i = 1,\cdots,n.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        \begin{equation*}
            \prob(A_{i}|B) = \frac{\prob(A_{i}\cap B)}{\prob(B)} = \frac{\prob(B|A_{i})\prob(A_{i})}{\prob(B)} = \frac{\prob(B|A_{i})\prob(A_{i})}{\sum_{k = 1}^{n}\prob(B|A_{k})\prob(A_{k})}.
        \end{equation*}
    \end{proofing}
    \newpage

\section{Independence}
    In general, the probability of a certain event is influenced by the occurrence of other events. However, there are exceptions.
    \begin{defn}
        Two events $A$ and $B$ are \textbf{independent}, denoted by $A \independent B$, if:
        \begin{equation*}
            \prob(A\cap B) = \prob(A)\prob(B).
        \end{equation*}
    \end{defn}
    \begin{rem}
        If either $\prob(A) = 0$ or $\prob(B) = 0$, then $A \independent B$.
    \end{rem}
    \begin{rem}
        If events $A$ and $B$ are independent and $A\cap B = \emptyset$, then either $\prob(A) = 0$ or $\prob(B) = 0$.
    \end{rem}
    The following is relatively simple to prove.
    \begin{lem}
        For any $A,B \in \mathcal{F}$, if $A \independent B$, then:
        \begin{equation*}
            \prob(A|B) = \prob(A).
        \end{equation*}
    \end{lem}
    \begin{proofing}
        \begin{equation*}
            \prob(A|B) = \frac{\prob(A\cap B)}{\prob(B)} = \frac{\prob(A)\prob(B)}{\prob(B)} = \prob(A).
        \end{equation*}
    \end{proofing}
    \begin{prop}
        If events $A$ and $B$ are independent, then $A \independent B^{\complement}$ and $A^{\complement} \independent B^{\complement}$.
    \end{prop}
    \begin{proofing}
        \begin{align*}
            \prob(A\cap B^{\complement}) &= \prob(A)-\prob(A\cap B)\\
            &= \prob(A)-\prob(A)\prob(B)\\
            &= \prob(A)(1-\prob(B))\\
            &= \prob(A)\prob(B^{\complement}).
        \end{align*}
        Therefore, $A \independent B^{\complement}$ and also $A^{\complement} \independent B^{\complement}$.
    \end{proofing}
    \begin{prop}
        If events $A,B,C$ are independent, then $A \independent (B\cup C)$ and $A \independent (B\cap C)$.
    \end{prop}
    \begin{proofing}
        Using the properties of probability:
        \begin{align*}
            \prob(A\cap(B\cup C)) &= \prob((A\cap B)\cup(A\cap C))\\
            &= \prob(A\cap B)+\prob(A\cap C)-\prob(A\cap B\cap C)\\
            &= \prob(A)\prob(B)+\prob(A)\prob(C)-\prob(A)\prob(B)\prob(C)\\
            &= \prob(A)\prob(B\cup C).
            \prob(A\cap(B\cap C)) &= \prob(A)\prob(B)\prob(C)\\
            &= \prob(A)\prob(B\cap C).
        \end{align*}
    \end{proofing}
    \newpage

    Sometimes, we may deal with more than two events. We have a more specific way to describe their relationships.
    \begin{defn}
        Given a family of events $\{A_{i}:i \in I\}\subset\mathcal{F}$ for some $I\subset\mathbb{N}_{+}$:
        \begin{enumerate}
            \item If $\prob(A_{i}\cap A_{j}) = \prob(A_{i})\prob(A_{j})$ for any $i \neq j$, the events are \textbf{pairwise independent}.
            \item If, additionally, for all subsets $J$ of $I$:
            \begin{equation*}
                \prob\left(\bigcap_{i \in J}A_{i}\right) = \prod_{i \in J}\prob(A_{i}),
            \end{equation*}
            then the events are \textbf{mutually independent}.
        \end{enumerate}
    \end{defn}
    \begin{rem}
        Usually, when we say multiple events are independent, we mean they are mutually independent.
    \end{rem}
    \begin{eg}
        Consider tossing a fair coin three times. Define the following events:
        \begin{align*}
            A &= \{\text{The first toss is a head}\} = \{HHH,HHT,HTH,HTT\},\\
            B &= \{\text{The second toss is a head}\} = \{HHH,HTH,THH,THT\},\\
            C &= \{\text{The third toss is a head}\} = \{HHH,HHT,THH,TTH\}.
        \end{align*}
        We can observe the intersections between each pair of events and the overall intersection:
        \begin{align*}
            \prob(A\cap B) &= \prob(\{HHH, HHT\}) = \frac{2}{8} = \frac{1}{2}\left(\frac{1}{2}\right) = \prob(A)\prob(B),\\
            \prob(B\cap C) &= \prob(\{HHH, THH\}) = \frac{2}{8} = \frac{1}{2}\left(\frac{1}{2}\right) = \prob(B)\prob(C),\\
            \prob(A\cap C) &= \prob(\{HHH, HHT\}) = \frac{2}{8} = \frac{1}{2}\left(\frac{1}{2}\right) = \prob(A)\prob(C),\\
            \prob(A\cap B\cap C) &= \prob(\{HHH\}) = \frac{1}{8} = \frac{1}{2}\left(\frac{1}{2}\right)\left(\frac{1}{2}\right) = \prob(A)\prob(B)\prob(C).
        \end{align*}
        Therefore, events $A$, $B$, and $C$ are mutually independent.
    \end{eg}
    \begin{rem}
        Not all pairwise independent events are mutually independent.
    \end{rem}
    \begin{eg}
        Consider rolling a die twice: $\Omega = \{1,2,\cdots,6\}\times\{1,2,\cdots,6\}$ and $\mathcal{F} = 2^{\Omega}$. Define the following events:
        \begin{align*}
            A &= \{\text{The sum is }7\} = \{(1,6),(2,5),(3,4),(4,3),(5,2),(6,1)\},\\
            B &= \{\text{The first roll is }4\} = \{(4,1),(4,2),(4,3),(4,4),(4,5),(4,6)\},\\
            C &= \{\text{The second roll is }3\} = \{(1,3),(2,3),(3,3),(4,3),(5,3),(6,3)\}.
        \end{align*}
        We can observe the intersections between each pair of events and the overall intersection:
        \begin{align*}
            \prob(A\cap B) &= \prob((4,3)) = \frac{1}{36} = \frac{1}{6}\left(\frac{1}{6}\right) = \prob(A)\prob(B),\\
            \prob(B\cap C) &= \prob((4,3)) = \frac{1}{36} = \frac{1}{6}\left(\frac{1}{6}\right) = \prob(B)\prob(C),\\
            \prob(A\cap C) &= \prob((4,3)) = \frac{1}{36} = \frac{1}{6}\left(\frac{1}{6}\right) = \prob(A)\prob(C),\\
            \prob(A\cap B\cap C) &= \prob((4,3)) = \frac{1}{36} \neq \prob(A)\prob(B)\prob(C).
        \end{align*}
        Therefore, events $A$, $B$, and $C$ are pairwise independent but not mutually independent.
    \end{eg}
    \newpage

\section{Product Space}
    There are many $\sigma$-fields you can generate using a collection of subsets of $\Omega$. However, many of these may be too large to be useful. Therefore, we have the following definition.
    \begin{defn}
        Let $A$ be a collection of subsets of $\Omega$. The \textbf{$\sigma$-field generated by $A$} is:
        \begin{equation*}
            \sigma(A) = \bigcap_{A\subseteq\mathcal{G}}\mathcal{G},
        \end{equation*}
        where $\mathcal{G}$ is also a $\sigma$-field. 
    \end{defn}
    \begin{rem}
        $\sigma(A)$ is the smallest $\sigma$-field containing $A$.
    \end{rem}
    \begin{eg}
        Let $\Omega = \{1,2,\cdots,6\}$ and $A = \{\{1\}\}\subseteq 2^{\Omega}$. Then $\sigma(A) = \{\emptyset,\{1\},\{2,3,\cdots,6\},\Omega\}$.
    \end{eg}
    \begin{cor}
        Suppose $(\mathcal{F}_{i})_{i \in I}$ is a system of $\sigma$-fields in $\Omega$. Then:
        \begin{equation*}
            \bigcap_{i \in I}\mathcal{F}_{i} = \{A \in \Omega: A \in \mathcal{F}_{i}\text{ for all }i \in I\}.
        \end{equation*}
    \end{cor}
    Now that we know which $\sigma$-field to generate, we can combine two probability spaces to form a new probability space.
    \begin{defn}
        The \textbf{product space} of two probability spaces $(\Omega_{1},\mathcal{F}_{1},\prob_{1})$ and $(\Omega_{2},\mathcal{F}_{2},\prob_{2})$ is the probability space $(\Omega_{1}\times\Omega_{2},\mathcal{G},\prob_{12})$, comprising:
        \begin{enumerate}
            \item A collection of ordered pairs $\Omega_{1}\times\Omega_{2} = \{(\omega_{1},\omega_{2}):\omega_{1} \in \Omega_{1},\omega_{2} \in \Omega_{2}\}$.
            \item A $\sigma$-algebra $\mathcal{G} = \sigma(\mathcal{F}_{1}\times\mathcal{F}_{2})$, where $\mathcal{F}_{1}\times\mathcal{F}_{2} = \{A_{1}\times A_{2}:A_{1} \in \mathcal{F}_{1},A_{2} \in \mathcal{F}_{2}\}$.
            \item A probability measure $\prob_{12}:\mathcal{F}_{1}\times\mathcal{F}_{2} \to [0,1]$ given by:
            \begin{equation*}
                \prob_{12}(A_{1}\times A_{2}) = \prob_{1}(A_{1})\prob_{2}(A_{2}),
            \end{equation*}
            for $A_{1} \in \mathcal{F}_{1},A_{2} \in \mathcal{F}_{2}$.
        \end{enumerate}
    \end{defn}
    \begin{eg}
        Assume that we want to consider the probabilities of getting a head in a coin flip and getting a $5$ in a die toss simultaneously. We already know:
        \begin{align*}
            \tag{Coin flipping}
            \Omega_{1} &= \{H,T\}, & \mathcal{F}_{1} &= \{\emptyset, \{H\}, \{T\}, \Omega_{1}\}, & \prob_{1} &= \prob(\cdot|\Omega_{1}),\\
            \tag{Die tossing}
            \Omega_{2} &= \{1,2,3,4,5,6\}, & \mathcal{F}_{2} &= 2^{\Omega_{2}}, & \prob_{2} &= \prob(\cdot|\Omega_{2}).		
        \end{align*}
        The probability space we are considering is the product space $(\Omega_{1}\times\Omega_{2},\mathcal{G},\prob_{12})$, where:
        \begin{align*}
            \Omega_{1}\times\Omega_{2} &= \{(H,1),(H,2),(H,3),(H,4),(H,5),(H,6),(T,1),(T,2),(T,3),(T,4),(T,5),(T,6)\},\\
            \mathcal{G} &= 2^{\Omega},\\
            \prob_{12} &= \prob(\cdot|\Omega_{1}\times\Omega_{2}) = \prob(\cdot|\Omega_{1})\prob(\cdot|\Omega_{2}).
        \end{align*}
    \end{eg}

\chapter{Random Variables and Their Distribution}
    \label{Chapter 3 (Random Variables and Their Distribution)}
    \textit{In this chapter, let $\mathcal{F}$ be a $\sigma$-field of a sample space $\Omega$, and let $\prob$ be a probability measure.}
\section{Introduction to Random Variables}
    Sometimes, we are not interested in an experiment itself but rather in the consequence of its random outcome. We can consider this consequence as a function that maps a sample space into the real number field. We call these functions "random variables."
    \begin{defn}
        A \textbf{random variable} (r.v.) is a function $X:\Omega \to \mathbb{R}$ with the property that for any $x \in \mathbb{R}$:
        \begin{equation*}
            X^{-1}((-\infty,x]) = \{\omega \in \Omega:X(\omega) \leq x\} \in \mathcal{F}.
        \end{equation*}
    \end{defn}
    \begin{rem}
        More generally, a random variable is a function $X$ with the property that for all intervals $A\subseteq\mathbb{R}$:
        \begin{equation*}
            X^{-1}(A) = \{\omega \in \Omega: X(\omega) \in A\} \in \mathcal{F}.
        \end{equation*}
        We say the function is \textbf{$\mathcal{F}$-measurable}. Any function that is $\mathcal{F}$-measurable is a random variable.
    \end{rem}
    \begin{rem}
        All intervals can be replaced by any of the following classes:
        \begin{enumerate}
            \item $(a, b),(a, b],[a, b),[a, b]$ for all $a < b$.
            \item $(-\infty,x]$ for all $x \in \mathbb{R}$.
        \end{enumerate}
        This is because $X^{-1}$ can be interchanged with any set functions and because $\mathcal{F}$ is a $\sigma$-field.
    \end{rem}
    \begin{cla}
        Suppose $X^{-1}(B) \in \mathcal{F}$ for all open sets $B$. Then $X^{-1}(B') \in \mathcal{F}$ for all closed sets $B'$.
    \end{cla}
    \begin{proofing}
        For any $a,b \in \mathbb{R}$:
        \begin{equation*}
            X^{-1}([a, b]) = X^{-1}\left(\bigcap_{n = 1}^{\infty}\left(a-\frac{1}{n},b+\frac{1}{n}\right)\right) = \bigcap_{n = 1}^{\infty}X^{-1}\left(\left(a-\frac{1}{n},b+\frac{1}{n}\right)\right) \in \mathcal{F}.
        \end{equation*}
    \end{proofing}
    \begin{eg}
        \label{Chapter 3 (Example) Toss coin twice}
        A fair coin is tossed twice. $\Omega = \{HH,HT,TH,TT\}$. For all $\omega \in \Omega$, let $X(\omega)$ be the number of heads.
        \begin{align*}
            X(\omega) &= \begin{cases}
                0, &\omega \in \{TT\},\\
                1, &\omega \in \{HT,TH\},\\
                2, &\omega \in \{HH\}
            \end{cases} & X^{-1}((-\infty,x]) &= \begin{cases}
                \emptyset, & x < 0,\\
                \{TT\}, & x \in [0,1),\\
                \{HT,TH,TT\}, & x \in [1,2),\\
                \Omega, &x \in [2,\infty)
            \end{cases}
        \end{align*}
        If we choose $\mathcal{F} = \{\emptyset,\Omega\}$, then $X$ is not a random variable.
    \end{eg}
    \newpage

    We can create new random variables from $X$.
    \begin{lem}
        Given a random variable $X$:
        \begin{enumerate}
            \item If $Y = cX+d$ for some $c,d \in \mathbb{R}$, then $Y$ is a random variable.
            \item If $Z = X^{2}$, then $Z$ is a random variable.
        \end{enumerate}
    \end{lem} 
    \begin{proofing}
        \begin{enumerate}
            \item Let $y \in \mathbb{R}$. If $c = 0$, then:
            \begin{equation*}
                Y^{-1}((-\infty,y]) = \{\omega \in \Omega:d \leq y\} \in \mathcal{F}.
            \end{equation*}
            Otherwise:
            \begin{equation*}
                Y^{-1}((-\infty,y]) = \{\omega \in \Omega:Y(\omega) \leq y\} = \begin{cases}
                    \bigg\{\omega \in \Omega:X(\omega) \leq \frac{y-d}{c}\bigg\} \in \mathcal{F}, &c > 0,\\
                    \bigg\{\omega \in \Omega:X(\omega) \geq \frac{y-d}{c}\bigg\} \in \mathcal{F}, &c < 0.
                \end{cases}
            \end{equation*}
            Therefore, for any $c,d \in \mathbb{R}$, $Y = cX+d$ is a random variable.
            \item Let $z$ be a positive number. We have:
            \begin{equation*}
                Z^{-1}([0,z]) = \{\omega \in \Omega:0 \leq Z(\omega) \leq z\} = \{\omega \in \Omega:-\sqrt{z} \leq X(\omega) \leq \sqrt{z}\} \in \mathcal{F}.
            \end{equation*}
            Therefore, $Z = X^{2}$ is a random variable.
        \end{enumerate}
    \end{proofing}
    Before we continue, it is best to understand Borel sets first.
    \begin{defn}
        A \textbf{Borel set} is a set that can be obtained by taking countable unions, intersections, or complements repeatedly (countably many steps).
    \end{defn}
    \begin{defn}
        The \textbf{Borel $\sigma$-field} of $\mathbb{R}$ is a $\sigma$-field $\mathcal{B}(\mathbb{R})$ that is generated by all open sets. It is a collection of Borel sets.
    \end{defn}
    \begin{eg}
        $\{(a, b),[a, b],\{a\},\mathbb{Q},\mathbb{R}\setminus\mathbb{Q}\}\subset\mathcal{B}(\mathbb{R})$. Note that closed sets can be generated by open sets.
    \end{eg}
    \begin{rem}
        In modern understanding: $(\Omega,\mathcal{F},\prob)\xrightarrow{X}(\mathbb{R},\mathcal{B},\prob \circ X^{-1})$.
    \end{rem}
    \begin{cla}
        $\prob \circ X^{-1}$ is a probability measure on $(\mathbb{R},\mathcal{B})$.
    \end{cla}
    \begin{proofing}
        \begin{enumerate}
            \item For all $B \in \mathcal{B}$, $\prob \circ X^{-1}(B) = \prob(\{\omega:X(\omega) \in B\}) \in [0,1]$:
            \begin{align*}
                \prob \circ X^{-1}(\emptyset) &= \prob(\{\omega:X(\omega) \in \emptyset\}) = \prob(\emptyset) = 0, & \prob \circ X^{-1}(\mathbb{R}) &= \prob(\{\omega:X(\omega) \in \mathbb{R}\}) = \prob(\Omega) = 1.
            \end{align*}
            \item For any disjoint $\{B_{i}\}_{i}\subset\mathcal{B}$:
            \begin{equation*}
                \prob \circ X^{-1}\left(\bigcup_{i = 1}^{\infty}B_{i}\right) = \prob\left(\bigcup_{i = 1}^{\infty}X^{-1}(B_{i})\right) = \sum_{i = 1}^{\infty}\prob(X^{-1}(B_{i})) = \sum_{i = 1}^{\infty}\prob \circ X^{-1}(B_{i}).
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    \begin{rem}
        We can derive the probability of all $[a, b] \in \mathcal{B}$:
        \begin{align*}
            \prob \circ X^{-1}([a, b]) = \prob \circ X^{-1}((-\infty,b])-\prob \circ X^{-1}((-\infty,a)) &= \prob \circ X^{-1}((-\infty,b])-\prob \circ X^{-1}\left(\bigcup_{n = 1}^{\infty}\left(-\infty,a-\frac{1}{n}\right]\right)\\
            &= \prob \circ X^{-1}((-\infty,b])-\lim_{n \to \infty}\prob \circ X^{-1}\left(\left(-\infty,a-\frac{1}{n}\right]\right).
        \end{align*}
    \end{rem}
    \newpage

\section{CDF of Random Variables}
    Every random variable has its own distribution function.
    \begin{defn}
        The \textbf{(Cumulative) Distribution Function} (CDF) of a random variable $X$ is a function $F_{X}:\mathbb{R} \to [0,1]$ given by:
        \begin{equation*}
            F_{X}(x) = \prob(X \leq x): = \prob \circ X^{-1}((-\infty,x]).
        \end{equation*}
    \end{defn}
    \begin{eg}
        From Example \ref{Chapter 3 (Example) Toss coin twice}:
        \begin{align*}
            \prob(\omega) &= \frac{1}{4}, & F_{X}(x) &= \prob(X \leq x) = \begin{cases}
                0, &x < 0,\\
                \frac{1}{4}, &0 \leq x < 1,\\
                \frac{3}{4}, &1 \leq x < 2,\\
                1, &x \geq 2.
            \end{cases}
        \end{align*}
    \end{eg}
    \begin{rem}
        $F_{X}(x)$ is non-decreasing because if $x < y$, then $\{X \leq x\}\subseteq\{X \leq y\}$.
    \end{rem}
    \begin{rem}
        $F_{X}(x)$ is bounded because $0 \leq F_{X}(x) \leq 1$ for all $x \in \mathbb{R}$.
    \end{rem}
    \begin{lem}
        The CDF $F_{X}$ of a random variable $X$ has the following properties for any $x,y \in \mathbb{R}$:
        \begin{enumerate}
            \item $\lim_{x \to -\infty}F_{X}(x) = 0$ and $\lim_{x \to \infty}F_{X}(x) = 1$.
            \item If $x < y$, then $F_{X}(x) \leq F_{X}(y)$.
            \item $F_{X}$ is right-continuous ($F_{X}(x+h) \to F_{X}(x)$ as $h \to 0$).
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item Let $B_{n} = \{\omega \in \Omega:X(\omega) \leq -n\}$. Since $B_{1}\supseteq B_{2}\supseteq\cdots$, by Lemma \ref{Chapter 2 (Lemma) Continuity of Probability Measure}:
            \begin{equation*}
                \lim_{x \to -\infty}F_{X}(x) = \prob\left(\lim_{n \to \infty}B_{n}\right) = \prob(\emptyset) = 0.
            \end{equation*}
            Alternative proof:
            \begin{equation*}
                \lim_{x \to -\infty}F_{X}(x) = \lim_{x \to -\infty}\prob \circ X^{-1}((-\infty,x]) = \lim_{n \to \infty}\prob \circ X^{-1}((-\infty,-n]) = \prob \circ X^{-1}(\emptyset) = 0.
            \end{equation*}
            Let $C_{n} = \{\omega \in \Omega:X(\omega) \leq n\}$. Since $C_{1}\subseteq C_{2}\subseteq\cdots$, by Lemma \ref{Chapter 2 (Lemma) Continuity of Probability Measure}:
            \begin{equation*}
                \lim_{x \to \infty}F_{X}(x) = \prob\left(\lim_{n \to \infty}C_{n}\right) = \prob(\Omega) = 1.
            \end{equation*}
            Alternative proof:
            \begin{equation*}
                \lim_{x \to \infty}F_{X}(x) = \lim_{x \to \infty}\prob \circ X^{-1}((-\infty,x]) = \prob \circ X^{-1}(\mathbb{R}) = 1.
            \end{equation*}
            \item Let $A(x) = \{X \leq x\}$ and $A(x, y) = \{x < X \leq y\}$. Then $A(y) = A(x)\cup A(x, y)$ is a disjoint union:
            \begin{equation*}
                F_{X}(y) = \prob(A(y)) = \prob(A(x))+\prob(A(x, y)) = F_{X}(x)+\prob(x < X \leq y) \geq F_{X}(x).
            \end{equation*}
            \item Let $B_{n} = \big\{\omega \in \Omega:X(\omega) \leq x+\frac{1}{n}\big\}$. Since $B_{1}\supseteq B_{2}\supseteq\cdots$, by Lemma \ref{Chapter 2 (Lemma) Continuity of Probability Measure}:
            \begin{equation*}
                \lim_{h \to 0^{+}}F_{X}(x+h) = \prob\left(\bigcap_{n = 1}^{\infty}B_{n}\right) = \prob\left(\lim_{n \to \infty}B_{n}\right) = \prob(\{\omega \in \Omega:X(\omega) \leq x\}) = F_{X}(x).
            \end{equation*}
            Alternative proof:
            \begin{equation*}
                \lim_{h \to 0^{+}}F_{X}(x+h) = \lim_{h \to 0^{+}}\prob \circ X^{-1}((-\infty,x+h]) = \lim_{n \to \infty}\prob \circ X^{-1}\left(\left(-\infty,x+\frac{1}{n}\right]\right) = \prob \circ X^{-1}((-\infty,x]) = F_{X}(x).
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    \newpage

    \begin{rem}
        $F$ is not left-continuous because:
        \begin{equation*}
            \lim_{h \to 0^{+}}F_{X}(x-h) = \lim_{n \to \infty}\prob \circ X^{-1}\left(\left(-\infty,x-\frac{1}{n}\right)\right) = \prob \circ X^{-1}((-\infty,x)) = F_{X}(x)-\prob \circ X^{-1}(\{x\}).
        \end{equation*}
    \end{rem}
    \begin{rem}
        Two random variables $X$ and $Y$ are \textbf{identically distributed} if they have the same CDF, i.e., $F_{X}(x) = F_{Y}(x)$ for all $x \in \mathbb{R}$.
    \end{rem}
    \begin{lem}
        Let $F_{X}$ be the CDF of a random variable $X$. Then for any $x,y \in \mathbb{R}$ such that $x < y$:
        \begin{enumerate}
            \item $\prob(X > x) = 1-F_{X}(x)$.
            \item $\prob(x < X \leq y) = F_{X}(y)-F_{X}(x)$.
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item $\prob(X > x) = \prob(\Omega\setminus\{X \leq x\}) = \prob(\Omega)-\prob(X \leq x) = 1-F_{X}(x)$.
            \item $\prob(x < X \leq y) = \prob(\{X \leq y\}\setminus\{X \leq x\}) = \prob(X \leq y)-\prob(X \leq x) = F_{X}(y)-F_{X}(x)$.
        \end{enumerate}
    \end{proofing}
    In some cases, we want to find a number where a specific percentage of outcomes fall below it. This is very useful if you know that the random variable follows a certain distribution.
    \begin{defn}
        The \textbf{$q$-th quantile} of a random variable $X$ is defined as a number $z_{q}$ such that:
        \begin{equation*}
            \prob(X \leq z_{q}) = q.
        \end{equation*}
    \end{defn}
    \begin{rem}
        If $F_{X}$ is continuous and strictly increasing, then $z_{q} = F_{X}^{-1}(q)$.
    \end{rem}
    \begin{eg}
        The median is the $0.5$-th quantile. The quartiles are the $0.25$-th and $0.75$-th quantiles.
    \end{eg}
    \newpage

\section{PMF / PDF of Random Variables}
    We can classify some (not all) random variables as either discrete or continuous. These two types will be further discussed in the next two chapters.
    \begin{defn}
        A random variable $X$ is \textbf{discrete} if it takes values in some countable subset $\{x_{1},x_{2},\cdots\}\subset\mathbb{R}$.
    \end{defn}
    \begin{defn}
        A discrete random variable $X$ has a \textbf{probability mass function} (PMF) $f_{X}:\mathbb{R} \to [0,1]$ given by: 
        \begin{equation*}
            f_{X}(x) = \prob(X = x) = \prob \circ X^{-1}(\{x\}), \qquad\text{for }x \in \mathbb{R}.
        \end{equation*}
    \end{defn}
    \begin{rem}
        Some textbooks use $p_{X}(x)$ to denote the PMF to prevent confusion with the PDF.
    \end{rem}
    \begin{rem}
        For discrete random variables, the distribution is \textbf{atomic} because the distribution function has jump discontinuities at values $x_{1},x_{2},\cdots$ and is constant in between.
    \end{rem}
    This definition is problematic when the random variable $X$ is continuous because using the PMF would yield $f_{X}(x) = 0$ for all $x$. Therefore, we need another definition for continuous random variables.
    \begin{defn}
        A random variable $X$ is called \textbf{continuous} if its distribution function can be expressed as:
        \begin{equation*}
            F_{X}(x) = \int_{-\infty}^{x}f(u)\,du, \qquad\text{for }x \in \mathbb{R},
        \end{equation*}
        for some integrable \textbf{probability density function} (PDF) $f_{X}:\mathbb{R} \to [0,\infty)$ of $X$. 
    \end{defn}
    \begin{rem}
        For small $\delta > 0$:
        \begin{equation*}
            \prob(x < X \leq x+\delta) = F_{X}(x+\delta)-F_{X}(x) = \int_{x}^{x+\delta}f_{X}(u)\,du \approx f_{X}(x)\delta, \qquad\text{for }x \in \mathbb{R}.
        \end{equation*}
    \end{rem}
    \begin{rem}
        For continuous random variables, the CDF is \textbf{absolutely continuous}.
    \end{rem}
    \begin{rem}
        Not every continuous function can be written as $\int_{-\infty}^{x}f_{X}(u)\,du$. For example, the Cantor function.
    \end{rem}
    \begin{rem}
        It is possible for a random variable to be neither continuous nor discrete.
    \end{rem}
    \newpage 

\section{JCDF of Random Variables}
    Often, we want to study the simultaneous behavior of multiple events. In such cases, we involve two or more random variables that share the same sample space.
    \begin{defn}
        Let $X_{1},X_{2},\dots,X_{n}$ be $n$ random variables on $(\Omega,\mathcal{F},\prob)$. We define a \textbf{random vector} $\mathbf{X}$ with the property:
        \begin{equation*}
            \mathbf{X}^{-1}(B) = \{\omega \in \Omega:\mathbf{X}(\omega) = (X_{1}(\omega),\dots,X_{n}(\omega)) \in B\} \in \mathcal{F}, \qquad\text{for }B \in \mathcal{B}(\mathbb{R}^{n}).
        \end{equation*}
        We can also say $\mathbf{X}$ is a random vector if $X_{i}$ are random variables for $i = 1,\dots,n$. This means:
        \begin{equation*}
            X_{i}^{-1}(B) \in \mathcal{F}, \qquad \text{for }B \in \mathcal{B}(\mathbb{R}),
        \end{equation*}
        for $i = 1,\dots,n$.
    \end{defn}
    \begin{cla}
        Both definitions of random vectors are equivalent.
    \end{cla}
    \begin{proofing}
        By the first definition, $\mathbf{X}^{-1}(A_{1}\times\cdots\times A_{n}) \in \mathcal{F}$. If we choose $A_{2} = A_{3} = \cdots = A_{n} = \mathbb{R}$:
        \begin{align*}
            \mathbf{X}^{-1}(A_{1}\times\mathbb{R}\times\cdots\times\mathbb{R}) &= \{\omega \in \Omega:(X_{1}(\omega),\dots,X_{n}(\omega)) \in A_{1}\times\mathbb{R}\times\cdots\times\mathbb{R}\}\\
            &= \{\omega \in \Omega:X_{1}(\omega) \in A_{1}\}\cap\{\omega \in \Omega:X_{2}(\omega) \in \mathbb{R}\}\cap\cdots\cap\{\omega \in \Omega:X_{n}(\omega) \in \mathbb{R}\}\\
            &= X_{1}^{-1}(A_{1}).
        \end{align*}
        This means $X_{1}$ is a random variable. Similarly, we can also find that $X_{i}$ is a random variable for $i = 2,\dots,n$.\\
        Therefore, we can derive the second definition from the first definition.\\
        By the second definition, $X_{i}$ are random variables for $i = 1,\dots,n$. Therefore:
        \begin{align*}
            \mathbf{X}^{-1}(A_{1}\times\cdots\times A_{n}) &= \{\omega \in \Omega:(X_{1}(\omega),\dots,X_{n}(\omega)) \in A_{1}\times\cdots\times A_{n}\}\\
            &= \{\omega \in \Omega:X_{1}(\omega) \in A_{1}\}\cap\cdots\cap\{\omega \in \Omega:X_{n}(\omega) \in A_{n}\}\\
            &= X_{1}^{-1}(A_{1})\cap\cdots\cap X_{n}^{-1}(A_{n}) \in \mathcal{F}.
        \end{align*}
        Therefore, we can derive the first definition from the second definition.\\
        Hence, the two definitions are equivalent.
    \end{proofing}
    \begin{rem}
        Alternatively, for any $B \in \mathcal{B}(\mathbb{R}^{n})$:
        \begin{equation*}
            \prob \circ \mathbf{X}^{-1}(B) = \prob(\mathbf{X} \in B) = \prob(\{\omega \in \Omega:\mathbf{X}(\omega) = (X_{1}(\omega),\dots,X_{n}(\omega)) \in B\}).
        \end{equation*}
    \end{rem}
    \begin{rem}
        We can replace all Borel sets with the form $[a_{1},b_{1}]\times[a_{2},b_{2}]\times\cdots\times[a_{n},b_{n}]$.
    \end{rem}
    In general, we can define a distribution function corresponding to the random vector.
    \begin{defn}
        The \textbf{Joint (Cumulative) Distribution Function} (JCDF) $F_{\mathbf{X}}:\mathbb{R}^{n} \to [0,1]$ is defined as:
        \begin{equation*}
            F_{\mathbf{X}}(x_{1},\dots,x_{n}) = F_{X_{1},\dots,X_{n}}(x_{1},\dots,x_{n}) = \prob \circ \mathbf{X}^{-1}((-\infty,x_{1}]\times\cdots\times(-\infty,x_{n}]) = \prob(X_{1} \leq x_{1},\dots,X_{n} \leq x_{n}),
        \end{equation*}
        for $x_{1},\dots,x_{n} \in \mathbb{R}$.
    \end{defn}
    \begin{rem}
        The random variables being of the same type is not necessary to define the JCDF.
    \end{rem}
    The joint distribution function has properties similar to those of a normal distribution function.
    \begin{lem}
        The JCDF $F_{X,Y}$ of two random variables $X$ and $Y$ has the following properties:
        \begin{enumerate}
            \item $\lim_{(x, y) \to (-\infty,-\infty)}F_{X,Y}(x, y) = 0$ and $\lim_{(x, y) \to (\infty,\infty)}F_{X,Y}(x, y) = 1$.
            \item If $x_{1} \leq y_{1}$ and $x_{2} \leq y_{2}$, then $F_{X,Y}(x_{1},y_{1}) \leq F_{X,Y}(x_{2},y_{2})$.
            \item $F_{X,Y}$ is continuous from above, meaning $F_{X,Y}(x+u,y+v) \to F_{X,Y}(x, y)$ as $u \to 0^{+}$ and $v \to 0^{+}$.
        \end{enumerate}
    \end{lem}
    \newpage

    Most of the time, we consider the case when the random variables are either all discrete or all continuous. For simplicity, we consider only two random variables from the same probability space.
    \begin{defn}
        Two random variables $X$ and $Y$ on $(\Omega,\mathcal{F},\prob)$ are \textbf{jointly discrete} if the vector $(X,Y)$ takes values in some countable subset of $\mathbb{R}^{2}$ only.
    \end{defn}
    \begin{defn}
        Given two jointly discrete random variables $X$ and $Y$, the \textbf{Joint (Probability) Mass Function} (JPMF) $f_{X,Y}:\mathbb{R}^{2} \to [0,1]$ is given by:
        \begin{equation*}
            f_{X,Y}(x, y) = \prob(X = x,Y = y) = \prob \circ (X,Y)^{-1}(\{(x, y)\}), \qquad \text{for }x,y \in \mathbb{R}.
        \end{equation*}
    \end{defn}
    \begin{rem}
        If two random variables $X$ and $Y$ are jointly discrete, we can find the JCDF by:
        \begin{equation*}
            F_{X,Y}(x, y) = \sum_{u \leq x}\sum_{v \leq y}f(u, v), \qquad\text{for }x,y \in \mathbb{R}.
        \end{equation*}
        More generally, for all $B \in \mathcal{B}(\mathbb{R}^{2})$:
        \begin{equation*}
            \prob \circ (X,Y)^{-1}(B) = \sum_{(u, v) \in B}f_{X,Y}(u, v).
        \end{equation*}
    \end{rem}
    \begin{eg}
        Assume that a special three-sided coin is provided. Each toss results in a head (H), tail (T), or edge (E) with equal probabilities. What is the probability of having $h$ heads, $t$ tails, and $e$ edges after $n$ tosses?\\
        Let $H_{n},T_{n},E_{n}$ be the numbers of such outcomes in $n$ tosses of the coin. The vector $(H_{n},T_{n},E_{n})$ satisfies $H_{n}+T_{n}+E_{n} = n$.
        \begin{equation*}
            \prob(H_{n} = h,T_{n} = t,E_{n} = e) = \frac{n!}{h!t!e!}\left(\frac{1}{3}\right)^{n}.
        \end{equation*}
    \end{eg}
    Similar to the idea of defining the probability density function, we can define the joint probability density function for two jointly continuous random variables.
    \begin{defn}
        Two random variables $X$ and $Y$ on $(\Omega,\mathcal{F},\prob)$ are \textbf{jointly continuous} if the \textbf{Joint Probability Density Function} (JPDF) $f: \mathbb{R}^{2} \to [0,\infty)$ of $(X,Y)$ can be expressed as:
        \begin{align*}
            f_{X,Y}(x, y) &= \pdv*{F_{X,Y}(x, y)}{x,y}, & F_{X,Y}(x, y) &= \int_{-\infty}^{x}\int_{-\infty}^{y}f_{X,Y}(u, v)\,du\,dv, \qquad\text{for }x,y \in \mathbb{R}.
        \end{align*}
    \end{defn}
    \begin{rem}
        More generally, for all $B \in \mathcal{B}(\mathbb{R}^{2})$:
        \begin{equation*}
            \prob \circ (X,Y)^{-1}(B) = \prob((X,Y) \in B) = \iint_{B}f_{X,Y}(u, v)\,du\,dv.
        \end{equation*}
    \end{rem}
    \begin{rem}
        It is not generally true for two continuous random variables $X$ and $Y$ to be jointly continuous.
    \end{rem}
    \begin{eg}
        Let $X$ be uniformly distributed on $[0,1]$ ($f_{X}(x) = \mathbf{1}_{[0,1]}$). This means $f_{X}(x) = 1$ when $x \in [0,1]$ and $0$ otherwise. Let $Y = X$. That means $(X,Y) = (X,X)$. Let $B = \{(x, y):x = y\text{ and }x \in [0,1]\} \in \mathcal{B}(\mathbb{R}^{2})$.\\
        Since $y = x$ is just a line:
        \begin{align*}
            \prob \circ (X,Y)^{-1}(B) &= 1, & \iint_{B}f_{X,Y}(u, v)\,du\,dv &= 0 \neq \prob \circ (X,Y)^{-1}(B).
        \end{align*}
        Therefore, $X$ and $Y$ are not jointly continuous.
    \end{eg}
    \begin{rem}
        \begin{equation*}
            f_{X,Y}(x, y) = F_{X,Y}(x, y)-F_{X,Y}(x^{-},y)-F_{X,Y}(x,y^{-})+F_{X,Y}(x^{-},y^{-}), \qquad\text{for } x,y \in \mathbb{R}.
        \end{equation*}
    \end{rem}
    \newpage

\section{Marginal Distribution of Random Variables}
    We can find the probability distribution of one random variable by disregarding another variable. This results in the following distribution:
    \begin{defn}
        Let $X,Y$ be jointly discrete random variables. We can obtain a \textbf{marginal distribution} (marginal CDF) as follows:
        \begin{equation*}
            F_{X}(x) = \prob \circ X^{-1}((-\infty,x]) = \prob\left(X^{-1}((-\infty,x])\cap Y^{-1}((-\infty,\infty))\right), \qquad\text{for }x \in \mathbb{R}.
        \end{equation*}
    \end{defn}
    \begin{rem}
        For all $x \in \mathbb{R}$, the marginal distribution can be obtained by:
        \begin{equation*}
            F_{X}(x) = \lim_{y \to \infty}\prob\left(X^{-1}((-\infty,x])\cap Y^{-1}((-\infty,y])\right) = \lim_{y \to \infty}F_{X,Y}(x, y).
        \end{equation*}
    \end{rem}
    \begin{defn}
        Let $X,Y$ be jointly discrete random variables with a JPMF $f_{X,Y}$. The \textbf{marginal PMF} of $X$ is given by:
        \begin{equation*}
            f_{X}(x) = \prob(X = x) = \sum_{y \in \mathbb{R}}\prob(X = x,Y = y) = \sum_{y \in \mathbb{R}}f_{X,Y}(x, y), \qquad\text{for }x \in \mathbb{R}.
        \end{equation*}
    \end{defn}
    \begin{eg}
        From Example \ref{Chapter 3 (Example) Toss coin twice}, let $X$ be the number of heads and $Y$ be the number of tails when tossing a coin twice. The JPMF is given by:
        \begin{equation*}
            f_{X,Y}(x, y) = \begin{cases}
                \frac{1}{4}, &(x, y) = (0,2),(1,1),(2,0),\\
                0, &\text{otherwise}.
            \end{cases}
        \end{equation*}
        The marginal PMF of $X$ is given by:
        \begin{equation*}
            f_{X}(x) = \sum_{y \in \mathbb{R}}f_{X,Y}(x, y) = \begin{cases}
                \frac{1}{4}, &x = 0,\\
                \frac{1}{2}, &x = 1,\\
                \frac{1}{4}, &x = 2,\\
                0, &\text{otherwise}.
            \end{cases}
        \end{equation*}
    \end{eg}
    \begin{defn}
        Let $X,Y$ be jointly continuous random variables with a JPDF $f_{X,Y}$. The \textbf{marginal PDF} of $X$ is given by:
        \begin{equation*}
            f_{X}(x) = \int_{-\infty}^{\infty}f_{X,Y}(x, y)\,dy, \qquad\text{for }x \in \mathbb{R}.
        \end{equation*}
    \end{defn}
    \begin{eg}
        Let $X$ and $Y$ be jointly continuous random variables with a JPDF:
        \begin{equation*}
            f_{X,Y}(x, y) = \begin{cases}
                2, &0 < x < y < 1,\\
                0, &\text{otherwise}.
            \end{cases}
        \end{equation*}
        The marginal PDF of $X$ is given by:
        \begin{equation*}
            f_{X}(x) = \int_{-\infty}^{\infty}f_{X,Y}(x, y)\,dy = \int_{x}^{1}2\,dy = 2(1-x), \qquad\text{for }0 < x < 1.
        \end{equation*}
    \end{eg}

\chapter{Discrete Random Variables}
    \label{Chapter 4 (Discrete Random Variables)}
\section{Introduction to Discrete Random Variables}
    Let us recall some of the definitions of discrete random variables from the previous chapter.
    \begin{defn}
        A random variable $X$ is \textbf{discrete} if it takes values in some countable subset $\{x_{1},x_{2},\dots\}\subset\mathbb{R}$.
    \end{defn}
    \begin{defn}
        The \textbf{Probability Mass Function} (PMF) of a discrete random variable $X$ is the function $f_{X}:\mathbb{R} \to [0,1]$ given by:
        \begin{equation*}
            f_{X}(x) = \prob(X = x), \qquad\text{for }x \in \mathbb{R}.
        \end{equation*}
    \end{defn}
    \begin{defn}
        The \textbf{(Cumulative) Distribution Function} (CDF) of a discrete random variable $X$ is the function $F_{X}:\mathbb{R} \to [0,1]$ given by:
        \begin{equation*}
            F_{X}(x) = \prob(X \leq x) = \sum_{i:x_{i} \leq x}f_{X}(x_{i}), \qquad\text{for }x \in \mathbb{R}.
        \end{equation*}
    \end{defn}
    \begin{lem}
        \label{Chapter 4 (Lemma) Relationship between PMF and CDF}
        The relationship between the PMF $f_{X}$ and the CDF $F_{X}$ of a random variable $X$ for any $x,y \in \mathbb{R}$ is as follows:
        \begin{enumerate}
            \item $F_{X}(x) = \sum_{y \leq x}f_{X}(y)$.
            \item $f_{X}(x) = F_{X}(x)-\lim_{y \to x^{-}}F_{X}(y)$.
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item 
            \begin{equation*}
                F_{X}(x) = \prob(X \leq x) = \sum_{i:x_{i} \leq x}\prob(X = x_{i}) = \sum_{y \leq x}f_{X}(y).
            \end{equation*}
            \item Let $B_{n} = \big\{x-\frac{1}{n} < X \leq x\big\}$. Since $B_{1}\supseteq B_{2}\supseteq\cdots$, by Lemma \ref{Chapter 2 (Lemma) Continuity of Probability Measure}:
            \begin{align*}
                F_{X}(x)-\lim_{y \to x^{-}}F_{X}(y) &= \prob\left(\bigcap_{n = 1}^{\infty}B_{n}\right)\\
                &= \prob\left(\lim_{n \to \infty}B_{n}\right)\\
                &= \prob\left(\left\{\lim_{n \to \infty}\left(x-\frac{1}{n}\right) < X \leq x\right\}\right)\\
                &= \prob(X = x).
            \end{align*}
        \end{enumerate}
    \end{proofing}
    \begin{lem}
        The PMF $f_{X}:\mathbb{R} \to [0,1]$ of a discrete random variable $X$ satisfies the following properties:
        \begin{enumerate}
            \item $\{x \in \mathbb{R} : f_{X}(x) \neq 0\}$ is countable.
            \item $\sum_{i}f_{X}(x_{i}) = 1$, where $x_{1},x_{2},\dots$ are the values of $x$ such that $f_{X}(x) \neq 0$.
        \end{enumerate}
    \end{lem}
    \newpage

    Let us also recall the definitions of the joint distribution function and the joint mass function.
    \begin{defn}
        For jointly discrete random variables $X$ and $Y$, the \textbf{Joint Probability Mass Function} (JPMF) $f_{X,Y}:\mathbb{R}^{2} \to [0,1]$ is given by:
        \begin{equation*}
            f_{X,Y}(x, y) = \prob((X,Y) = (x, y)) = \prob \circ (X,Y)^{-1}(\{x,y\}), \qquad\text{for }x,y \in \mathbb{R}.
        \end{equation*}
    \end{defn}
    \begin{defn}
        For jointly discrete random variables $X$ and $Y$ with a JPMF $f_{X,Y}$, the \textbf{Joint Cumulative Distribution Function} (JCDF) $F_{X,Y}:\mathbb{R}^{2} \to [0,1]$ is given by:
        \begin{equation*}
            F_{X,Y}(x, y) = \sum_{u \leq x}\sum_{v \leq y}f(u, v), \qquad \text{for }x,y \in \mathbb{R}.
        \end{equation*}
    \end{defn}
    Recall that events $A$ and $B$ are independent if the occurrence of $A$ does not change the probability of $B$ occurring.
    \begin{defn}
        Discrete random variables $X$ and $Y$ are \textbf{independent} if the events $\{X = x\}$ and $\{Y = y\}$ are independent for all $x,y$. Equivalently, $X$ and $Y$ are independent if:
        \begin{enumerate}
            \item $\prob((X,Y) \in A\times B) = \prob(X \in A)\prob(Y \in B)$ for all $A,B \in \mathcal{B}(\mathbb{R})$, or
            \item $F_{X,Y}(x, y) = F_{X}(x)F_{Y}(y)$ for all $x,y \in \mathbb{R}$, or
            \item $f_{X,Y}(x, y) = f_{X}(x)f_{Y}(y)$ for all $x,y \in \mathbb{R}$.
        \end{enumerate}
    \end{defn}
    \begin{cla}
        These three definitions are equivalent.
    \end{cla}
    \begin{proofing}
        \begin{itemize}
            \item[$2 \to 1$]
            \begin{equation*}
                F_{X,Y}(x, y) = \prob(X \leq x,Y \leq y) = \prob(X \leq x)\prob(Y \leq y) = F_{X}(x)F_{Y}(y).
            \end{equation*}
            \item[$3 \to 2$]
            \begin{align*}
                f_{X,Y}(x, y) &= F_{X,Y}(x, y)-F_{X,Y}(x^{-},y)-F_{X,Y}(x,y^{-})+F_{X,Y}(x^{-},y^{-})\\
                &= F_{X}(x)F_{Y}(y)-F_{X}(x^{-})F_{Y}(y)-F_{X}(x)F_{Y}(y^{-})+F_{X}(x^{-})F_{Y}(y^{-})\\
                &= (F_{X}(x)-F_{X}(x^{-}))(F_{Y}(y)-F_{Y}(y^{-})) = f_{X}(x)f_{Y}(y).
            \end{align*}
            \item[$1 \to 3$]
            \begin{align*}
                \prob \circ (X,Y)^{-1}(A\times B) &= \sum_{(x, y) \in A\times B}f_{X,Y}(x, y)\\
                &= \sum_{x \in A}\sum_{y \in B}f_{X}(x)f_{Y}(y)\\
                &= (\prob \circ X^{-1}(A))(\prob \circ Y^{-1}(B)).
            \end{align*}
        \end{itemize}
        Therefore, these three definitions are equivalent.
    \end{proofing}
    \begin{rem}
        More generally, let $X_{1},X_{2},\dots,X_{n}$ be discrete random variables. They are \textbf{independent} if:
        \begin{enumerate}
            \item For all $A_{i} \in \mathcal{B}(\mathbb{R})$:
            \begin{equation*}
                \prob \circ (X_{1},X_{2},\dots,X_{n})^{-1}(A_{1}\times A_{2}\times\cdots\times A_{n}) = \prod_{i = 1}^{n}\prob \circ X_{i}^{-1}(A_{i}).
            \end{equation*}
            \item For all $x_{i} \in \mathbb{R}$:
            \begin{equation*}
                F_{X_{1},X_{2},\dots,X_{n}}(x_{1},x_{2},\dots,x_{n}) = \prod_{i = 1}^{n}F_{X_{i}}(x_{i}).
            \end{equation*}
            \item For all $x_{i} \in \mathbb{R}$:
            \begin{equation*}
                f_{X_{1},X_{2},\dots,X_{n}}(x_{1},x_{2},\dots,x_{n}) = \prod_{i = 1}^{n}f_{X_{i}}(x_{i}).
            \end{equation*}
        \end{enumerate}
    \end{rem}
    \newpage

    \begin{defn}
        Given two discrete random variables $X$ and $Y$, the \textbf{marginal probability mass function} (marginal PMF) of $X$ is given by:
        \begin{equation*}
            f_{X}(x) = \sum_{y \in \mathbb{R}}f_{X,Y}(x, y), \qquad\text{for }x \in \mathbb{R}.
        \end{equation*}
    \end{defn}
    Recall that we say $A_{1},A_{2},\dots,A_{n} \in \mathcal{F}$ are independent if, for any $I\subseteq\{1,2,\dots,n\}$:
    \begin{equation*}
        \prob\left(\bigcap_{i \in I}A_{i}\right) = \prod_{i \in I}\prob(A_{i}).
    \end{equation*}
    \begin{rem}
        From the definition, we can see that $X \independent Y$ means that $X^{-1}(E) \independent Y^{-1}(F)$ for all $E,F \in \mathcal{B}(\mathbb{R})$.
    \end{rem}
    \begin{rem}
        We can generate a $\sigma$-field using random variables by defining the $\sigma$-field generated by a random variable $X$ as:
        \begin{equation*}
            \sigma(X) = \{X^{-1}(E):E \in \mathcal{B}(\mathbb{R})\}\subseteq\mathcal{F}.
        \end{equation*}
    \end{rem}
    From these remarks, we can extend the definition of independence from random variables to $\sigma$-fields.
    \begin{defn}
        Let $\mathcal{G},\mathcal{H}\subseteq\mathcal{F}$ be two $\sigma$-fields. We say $\mathcal{G}$ and $\mathcal{H}$ are \textbf{independent} if, for all $A \in \mathcal{G}$ and $B \in \mathcal{H}$:
        \begin{equation*}
            A \independent B.
        \end{equation*}
    \end{defn}
    \begin{rem}
        For any discrete random variables $X$ and $Y$:
        \begin{equation*}
            \sigma(X) \independent \sigma(Y) \iff X \independent Y.
        \end{equation*}
    \end{rem}
    \begin{thm}
        Given two random variables $X$ and $Y$, if $X \independent Y$ and we have two functions $g,h:\mathbb{R} \to \mathbb{R}$ such that $g(X)$ and $h(Y)$ are still random variables, then:
        \begin{equation*}
            g(X) \independent h(Y).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        For all $A,B \in \mathcal{B}$:
        \begin{align*}
            \prob((g(X),h(Y)) \in A\times B) &= \prob(g(X) \in A,h(Y) \in B)\\
            &= \prob(X \in \{x:g(x) \in A\},Y \in \{y:h(y) \in B\})\\
            &= \prob(X \in \{x:g(x) \in A\})\prob(Y \in \{y:h(y) \in B\})\\
            &= \prob(g(X) \in A)\prob(h(Y) \in B).
        \end{align*}
        Therefore, $g(X) \independent h(Y)$.
    \end{proofing}
    \begin{rem}
        We assume a product space $(\Omega,\mathcal{F},\prob)$ of two probability spaces $(\Omega_{1},\mathcal{F}_{1},\prob_{1})$ and $(\Omega_{2},\mathcal{F}_{2},\prob_{2})$.\\
        Any pair of events of the form $E_{1}\times\Omega_{2}$ and $\Omega_{1}\times E_{2}$ are independent:
        \begin{align*}
            \prob((E_{1}\times\Omega_{2})\cap(\Omega_{1}\times E_{2})) &= \prob(E_{1}\times E_{2})\\
            &= \prob_{1}(E_{1})\prob_{2}(E_{2})\\
            &= \prob(E_{1}\times\Omega_{2})\prob(\Omega_{1}\times E_{2}).
        \end{align*}
    \end{rem}
    \newpage

\section{Conditional Distribution of Discrete Random Variables}
    In the first chapter, we discussed the conditional probability $\prob(B|A)$. We can use this to define a distribution function.
    \begin{defn}
        Suppose $X,Y$ are two discrete random variables. The \textbf{Conditional Distribution} of $Y$ given $X = x$ for any $x \in \mathbb{R}$ such that $\prob(X = x) > 0$ is defined as:
        \begin{equation*}
            \prob(Y \in \cdot|X = x).
        \end{equation*}
        The \textbf{Conditional Mass Function} (Conditional PMF) of $Y$ given $X = x$ for any $x \in \mathbb{R}$ such that $\prob(X = x) > 0$ is defined as:
        \begin{equation*}
            f_{Y|X}(y|x) = \prob(Y = y|X = x).
        \end{equation*}
        The \textbf{Conditional Distribution Function} (Conditional CDF) of $Y$ given $X = x$ for any $x \in \mathbb{R}$ such that $\prob(X = x) > 0$ is defined as:
        \begin{equation*}
            F_{Y|X}(y|x) = \prob(Y \leq y|X = x).
        \end{equation*}
    \end{defn}
    \begin{rem}
        By definition:
        \begin{equation*}
            f_{Y|X}(y|x) = \frac{\prob(Y = y,X = x)}{\prob(X = x)} = \frac{\prob(Y = y,X = x)}{\sum_{v}\prob(X = x,Y = v)}.
        \end{equation*}
    \end{rem}
    \begin{rem}
        For any $x \in \mathbb{R}$, the conditional PMF $f_{Y|X}(y|x)$ is a probability mass function in $y$.
    \end{rem}
    \begin{rem}
        If $X$ and $Y$ are independent, then:
        \begin{equation*}
            f_{Y|X}(y|x) = f_{Y}(y), \qquad\text{for }x,y \in \mathbb{R}.
        \end{equation*}
    \end{rem}
    Conditional distributions retain the properties of the original distribution.
    \begin{lem}
        Given two discrete random variables $X$ and $Y$, conditional distributions have the following properties for $x,y \in \mathbb{R}$:
        \begin{enumerate}
            \item $F_{Y|X}(y|x) = \sum_{v \leq y}f_{Y|X}(v|x)$.
            \item $f_{Y|X}(y|x) = F_{Y|X}(y|x)-F_{Y|X}(y^{-}|x)$.
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item 
            \begin{equation*}
                \sum_{v \leq y}f_{Y|X}(v|x) = \sum_{v \leq y}\prob(Y = v|X = x) = \prob(Y \leq y|X = x) = F_{Y|X}(y|x).
            \end{equation*}
            \item 
            Using Lemma \ref{Chapter 4 (Lemma) Relationship between PMF and CDF}:
            \begin{align*}
                f_{Y|X}(y|x) &= \frac{1}{f_{X}(x)}\prob(X = x,Y = y)\\
                &= \frac{1}{f_{X}(x)}\left(\prob(X = x,Y \leq y)-\lim_{z \to y^{-}}\prob(X = x,Y \leq z)\right)\\
                &= \prob(Y \leq y|X = x)-\lim_{z \to y^{-}}\prob(Y \leq z|X = x)\\
                &= F_{Y|X}(y|x)-F_{Y|X}(y^{-}|x).
            \end{align*}
        \end{enumerate}
    \end{proofing}
    \newpage

\section{Convolution of Discrete Random Variables}
    Often, we consider the sum of two variables, such as the number of heads in $n$ tosses of a coin. However, more complex situations arise, especially when the summands are dependent. We aim to find a formula for describing the mass function of the sum $Z = X+Y$.
    \begin{thm}
        Given two jointly discrete random variables $X$ and $Y$, the probability of the sum of the two random variables is given by:
        \begin{equation*}
            \prob(X+Y = z) = \sum_{x}f_{X,Y}(x,z-x) = \sum_{y}f_{X,Y}(z-y,y), \qquad\text{for }z \in \mathbb{R}.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        We have the disjoint union:
        \begin{equation*}
            \{X+Y = z\} = \bigcup_{x}(\{X = x\}\cap\{Y = z-x\}).
        \end{equation*}
        At most countably many of its distributions have non-zero probability. Therefore:
        \begin{equation*}
            \prob(X+Y = z) = \sum_{x}\prob(X = x,Y = z-x) = \sum_{x}f_{X,Y}(x,z-x).
        \end{equation*}
    \end{proofing}
    \begin{eg}
        From Example \ref{Chapter 3 (Example) Toss coin twice}, let $X$ be the number of heads and $Y$ be the number of tails when tossing a coin twice. The JPMF is given by:
        \begin{equation*}
            f_{X,Y}(x, y) = \begin{cases}
                \frac{1}{4}, &(x, y) = (0,2),(1,1),(2,0),\\
                0, &\text{otherwise}.
            \end{cases}
        \end{equation*}
        Let $Z = X+Y$ be the total number of heads and tails. The PMF of $Z$ is given by:
        \begin{equation*}
            f_{Z}(z) = \sum_{x}f_{X,Y}(x,z-x) = \begin{cases}
                \frac{1}{4}, &z = 2,\\
                \frac{1}{2}, &z = 1,\\
                \frac{1}{4}, &z = 0,\\
                0, &\text{otherwise}.
            \end{cases}
        \end{equation*}
    \end{eg}
    \begin{defn}
        The \textbf{Convolution} $f_{X+Y}$ ($f_{X}*f_{Y}$) of the PMFs of two independent discrete random variables $X$ and $Y$ is the PMF of $X+Y$:
        \begin{equation*}
            f_{X+Y}(z) = \prob(X+Y = z) = \sum_{x}f_{X}(x)f_{Y}(z-x) = \sum_{y}f_{X}(z-y)f_{Y}(y), \qquad\text{for }z \in \mathbb{R}.
        \end{equation*}
    \end{defn}
    \begin{rem}
        The convolution operation is commutative and associative. That is, for any independent discrete random variables $X,Y,Z$:
        \begin{align*}
            f_{X}*f_{Y} &= f_{Y}*f_{X}, & (f_{X}*f_{Y})*f_{Z} &= f_{X}*(f_{Y}*f_{Z}).
        \end{align*}
    \end{rem}
    \newpage

\section{Examples of Discrete Random Variables}
    Here are some important examples of random variables that have a wide range of applications.
    \begin{defn}
        The \textbf{Parametric Distribution} of a discrete random variable is a distribution where the PMF depends on one or more parameters.
    \end{defn}
    The following examples illustrate some of the most useful distributions.
    \begin{eg}\named{Constant Variables}
        For a constant $c$, let $X$ be defined by $X(\omega) = c$ for all $\omega \in \Omega$. For all $B \in \mathcal{B}$:
        \begin{equation*}
            F_{X}(x) = \prob \circ X^{-1}(B) = \begin{cases}
                0, &B\cap\{c\} = \emptyset,\\
                1, &B\cap\{c\} = \{c\}.
            \end{cases}
        \end{equation*}
        $X$ is constant almost surely if there exists $c \in \mathbb{R}$ such that $\prob(X = c) = 1$.
    \end{eg}
    \begin{eg}\named{Bernoulli Distribution} $X \sim \Bern(p)$\\
        Let $A \in \mathcal{F}$ be a specific event. A Bernoulli trial is considered a success if $A$ occurs. Let $X$ be such that:
        \begin{align*}
            X(\omega) &= \mathbf{1}_{A}(\omega) = \begin{cases}
                1, &\omega \in A,\\
                0, &\omega \in A^{\complement}.
            \end{cases} & \prob(A) &= \prob(X = 1) = p, & \prob(A^{\complement}) &= \prob(X = 0) = 1-p.
        \end{align*}
    \end{eg}
    \begin{eg}
        Let $A \in \mathcal{F}$ and \textbf{Indicator Functions} $\mathbf{1}_{A}:\Omega \to \mathbb{R}$ such that, for all $B \in \mathcal{B}(\mathbb{R})$:
        \begin{align*}
            \mathbf{1}_{A}(\omega) &= \begin{cases}
                1, &\omega \in A,\\
                0, &\omega \in A^{\complement}.
            \end{cases} & \mathbf{1}_{A}^{-1}(B) &= \begin{cases}
                \emptyset, &B\cap\{0,1\} = \emptyset,\\
                A^{\complement}, & B\cap\{0,1\} = \{0\},\\
                A, &B\cap\{0,1\} = \{1\},\\
                \Omega, &B\cap\{0,1\} = \{0,1\}.
            \end{cases} & \prob \circ \mathbf{1}_{A}^{-1}(B) &= \begin{cases}
                0, &B\cap\{0,1\} = \emptyset,\\
                \prob(A^{\complement}), & B\cap\{0,1\} = \{0\},\\
                \prob(A), &B\cap\{0,1\} = \{1\},\\
                1, &B\cap\{0,1\} = \{0,1\}.
            \end{cases}
        \end{align*}
        Then $\mathbf{1}_{A}$ is a Bernoulli random variable taking values $1$ and $0$ with probabilities $\prob(A)$ and $\prob(A^{\complement})$, respectively.
    \end{eg}
    \begin{eg}\named{Binomial Distribution} $Y \sim \Bin(n,p)$\\
        Suppose we perform $n$ independent Bernoulli trials $X_{1},X_{2},\dots,X_{n}$. Let $Y = X_{1}+X_{2}+\cdots+X_{n}$ be the total number of successes.
        \begin{equation*}
            f_{Y}(k) = \prob(Y = k) = \prob\left(\sum_{i = 1}^{k}X_{i} = k\right) = \prob(\{\#\{i:X_{i} = 1\} = k\}).
        \end{equation*}
        We denote $A = \{\#\{i:X_{i} = 1\} = k\} = \bigcup_{\sigma}A_{\sigma}$, where $\sigma = (\sigma_{1},\sigma_{2},\dots,\sigma_{n})$ can be any sequence satisfying $\#\{i:\sigma_{i} = 1\} = k$, and $A_{\sigma}: = $ events that $(X_{1},X_{2},\dots,X_{n}) = (\sigma_{1},\sigma_{2},\dots,\sigma_{n})$. The events $A_{\sigma}$ are mutually exclusive. Hence,
        \begin{equation*}
        	\prob(A) = \sum_{\sigma}\prob(A_{\sigma}).
        \end{equation*}
        There are a total of $\binom{n}{k}$ different $\sigma$'s in the sum. By independence, we have:
        \begin{equation*}
            \prob(A_{\sigma}) = \prob(X_{1} = \sigma_{1},X_{2} = \sigma_{2},\dots,X_{n} = \sigma_{n}) = \prob(X_{1} = \sigma_{1})\prob(X_{2} = \sigma_{2})\cdots\prob(X_{n} = \sigma_{n}) = p^{k}(1-p)^{n-k}.
        \end{equation*}
        Hence, $f_{Y}(k) = \prob(A) = \binom{n}{k}p^{k}(1-p)^{n-k}$.
    \end{eg}
    \begin{eg}\named{Trinomial Distribution}
        Suppose we perform $n$ trials, each of which results in three outcomes $A$, $B$, and $C$, where $A$ occurs with probability $p$, $B$ with probability $q$, and $C$ with probability $1-p-q$. The probability of $r$ $A$'s, $w$ $B$'s, and $n-r-w$ $C$'s is:
        \begin{equation*}
            \prob(\#A = r, \#B = w, \#C = n-r-w) = \binom{n}{r,w,n-r-w}p^{r}q^{w}(1-p-q)^{n-r-w}.
        \end{equation*}
    \end{eg}
    \begin{rem}
        The multinomial distribution is a generalization of the binomial distribution. It describes the probabilities of counts for $k$ different outcomes in $n$ independent trials, where each outcome has a fixed probability.
    \end{rem}
    \newpage
    
    \begin{eg}\named{Geometric Distribution} $W \sim \Geom(p)$\\
        Suppose we keep performing independent Bernoulli trials until the first success occurs. Let $p$ be the probability of success, and let $W$ be the \textbf{waiting time} that elapses before the first success.
        \begin{align*}
            \prob(W > k) &= (1-p)^{k}, & \prob(W = k) &= \prob(W > k-1)-\prob(W > k) = p(1-p)^{k-1}.
        \end{align*}
    \end{eg}
    \begin{eg}(Alternative Geometric Distribution)
        Suppose we keep performing independent Bernoulli trials until the first success occurs. Let $p$ be the probability of success, and let $W'$ be the number of failures before the first success.
        \begin{align*}
            \prob(W' = k) &= p(1-p)^{k}, & \expect{W'} &= \frac{1-p}{p}, & \Var(W') &= \frac{1-p}{p^{2}}.
        \end{align*}
    \end{eg}
    \begin{rem}
        Conventionally, when we refer to the geometric distribution, we usually mean the one related to waiting time rather than the number of failures.
    \end{rem}
    \begin{eg}\named{Negative Binomial Distribution} $W_{r} \sim \NBin(r,p)$\\
        Similar to the examples of the geometric distribution, let $W_{r}$ be the waiting time for the $r$-th success. For $k \geq r$:
        \begin{equation*}
            f_{W_{r}}(k) = \prob(W_{r} = k) = \binom{k-1}{r-1}p^{r}(1-p)^{k-r}.
        \end{equation*}
    \end{eg}
    \begin{rem}
        $W_{r}$ is the sum of $r$ independent geometric variables.
    \end{rem}
    \begin{eg}\named{Hypergeometric Distribution} $X \sim \Hypergeometric(N,m,n)$\\
        Suppose that we have a set of $N$ balls. There are $m$ red balls and $N-m$ blue balls. We choose $n$ of these balls without replacement and define $X$ to be the number of red balls in our sample. Then:
        \begin{equation*}
            \prob(X = k) = \frac{\binom{m}{k}\binom{N-m}{n-k}}{\binom{N}{n}},
        \end{equation*}
        for $k = 0,1,\dots,\min(m,n)$.
    \end{eg}
    \begin{eg}\named{Poisson Distribution} $X \sim \Poisson(\lambda)$\\
        A \textbf{Poisson variable} is a discrete random variable with the Poisson PMF:
        \begin{equation*}
            f_{X}(k) = \frac{\lambda^{k}}{k!}e^{-\lambda}, \qquad\text{for }k = 0,1,2,\dots,
        \end{equation*}
        for some parameter $\lambda > 0$.
    \end{eg}
    \begin{rem}
        This is used to approximate a binomial random variable $\Bin(n,p)$ when $n$ is large, $p$ is small, and $np$ is moderate. Let $X \sim \Bin(n,p)$ and $\lambda = np$. For $k = 0,1,\dots,n$:
        \begin{align*}
            \prob(X = k) &= \binom{n}{k}p^{k}(1-p)^{n-k} = \frac{n!}{(n-k)!k!}\left(\frac{\lambda}{n}\right)^{k}\left(1-\frac{\lambda}{n}\right)^{n-k} = \frac{\lambda^{k}}{k!}\left(\frac{n!}{n^{k}(n-k)!}\right)\frac{\left(1-\frac{\lambda}{n}\right)^{n}}{\left(1-\frac{\lambda}{n}\right)^{k}}\\
            &\approx \frac{\lambda^{k}}{k!}(1)\left(\frac{e^{-\lambda}}{1}\right) = \frac{\lambda^{k}}{k!}e^{-\lambda}.
        \end{align*}
        Therefore, $X \sim \Poisson(\lambda)$.
    \end{rem}
    \newpage
    
    \begin{thm}
        \label{Chapter 4 (Theorem) Sum of Independent Poisson Variables}
        If $X \sim \Poisson(\lambda)$ and $Y \sim \Poisson(\mu)$ are independent, then:
        \begin{equation*}
            X+Y \sim \Poisson(\lambda+\mu).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        For any $k \geq 0$:
        \begin{align*}
            \prob(X+Y = k) &= \sum_{i = 0}^{k}\prob(X = i,Y = k-i) = \sum_{i = 0}^{k}\prob(X = i)\prob(Y = k-i)\\
            &= \sum_{i = 0}^{k}\left(\frac{\lambda^{i}}{i!}e^{-\lambda}\right)\left(\frac{\mu^{k-i}}{(k-i)!}e^{-\mu}\right) = \frac{1}{k!}e^{-(\lambda+\mu)}\sum_{i = 0}^{k}\frac{k!}{i!(k-i)!}\lambda^{i}\mu^{k-i}\\
            &= \frac{1}{k!}e^{-(\lambda+\mu)}\sum_{i = 0}^{k}\binom{k}{i}\lambda^{i}\mu^{k-i} = \frac{(\lambda+\mu)^{k}}{k!}e^{-(\lambda+\mu)}.
        \end{align*}
        Therefore, $X+Y \sim \Poisson(\lambda+\mu)$.
    \end{proofing}
    \begin{lem}
        If the number of occurrences of an event in unit time or space follows the Poisson distribution with rate $\lambda$, then the number of occurrences in $t$ units of time or space follows $\Poisson(\lambda t)$.
    \end{lem}
    \begin{proofing}
        Let $X_{1},X_{2},\dots,X_{t}$ be independent Poisson variables with parameter $\lambda$. Then, by Theorem \ref{Chapter 4 (Theorem) Sum of Independent Poisson Variables} and induction:
        \begin{equation*}
            X_{1}+X_{2}+\cdots+X_{t} \sim \Poisson(\lambda t).
        \end{equation*}
        Therefore, the number of occurrences in $t$ units of time or space follows $\Poisson(\lambda t)$.
    \end{proofing}
    \begin{rem}
        The Poisson distribution is often used to model the number of events occurring within a fixed interval of time or space, such as the number of phone calls received by a call center in an hour or the number of decay events from a radioactive source in a given time period.
    \end{rem}
    We have an interesting example concerning independence involving the Poisson distribution.
    \begin{eg}\named{Poisson Flips}
        A coin is tossed once, and heads turn up with probability $p$.\\
        Let $X$ and $Y$ be the numbers of heads and tails, respectively. $X$ and $Y$ are not independent since:
        \begin{align*}
            \prob(X = 1,Y = 1) &= 0, & \prob(X = 1)\prob(Y = 1) &= p(1-p) \neq 0.
        \end{align*}
        Suppose now that the coin is tossed $N$ times, where $N$ follows the Poisson distribution with parameter $\lambda$.\\
        In this case, the random variables $X$ and $Y$ are independent since:
        \begin{align*}
            \prob(X = x,Y = y) &= \prob(X = x,Y = y|N = x+y)\prob(N = x+y)\\
            &= \binom{x+y}{x}p^{x}(1-p)^{y}\frac{\lambda^{x+y}}{(x+y)!}e^{-\lambda} = \frac{(\lambda p)^{x}(\lambda(1-p))^{y}}{x!y!}e^{-\lambda},\\
            \prob(X = x)\prob(Y = y) &= \sum_{i \geq x}\prob(X = x|N = i)\prob(N = i)\sum_{j \geq y}\prob(Y = y|N = j)\prob(N = j)\\
            &= \sum_{i \geq x}\binom{i}{x}p^{x}(1-p)^{i-x}\frac{\lambda^{i}}{i!}e^{-\lambda}\sum_{j \geq y}\binom{j}{y}p^{j-y}(1-p)^{y}\frac{\lambda^{j}}{j!}e^{-\lambda}\\
            &= \frac{(\lambda p)^{x}}{x!}e^{-\lambda}\left(\sum_{i \geq x}\frac{(\lambda(1-p))^{i-x}}{(i-x)!}\right)\frac{(\lambda(1-p))^{y}}{y!}e^{-\lambda}\left(\sum_{j \geq y}\frac{(\lambda p)^{j-y}}{(j-y)!}\right)\\
            &= \frac{(\lambda p)^{x}}{x!}e^{-\lambda+\lambda(1-p)}\frac{(\lambda(1-p))^{y}}{y!}e^{-\lambda+\lambda p}\\
            &= \frac{(\lambda p)^{x}(\lambda(1-p))^{y}}{x!y!}e^{-\lambda} = \prob(X = x,Y = y).
        \end{align*}
    \end{eg}
    There is an important example that has a wide range of applications in real life. However, we will not discuss it here. You can find the example in Appendix \ref{Appendix A (Random Walk)}.

\chapter{Continuous Random Variables}
    \label{Chapter 5 (Continuous Random Variables)}
\section{Introduction to Continuous Random Variables}
    In this chapter, we discuss continuous random variables. We begin with the definition of a continuous random variable.
    \begin{defn}
        A random variable $X$ is \textbf{continuous} if its \textbf{distribution function} (CDF) $F_{X}$ can be written as:
        \begin{equation*}
            F_{X}(x) = \prob(X \leq x) = \int_{-\infty}^{x}f(u)\,du,
        \end{equation*}
        for some integrable probability density function (PDF) $f_{X}:\mathbb{R} \to [0,\infty)$.
    \end{defn}
    \begin{rem}
        The PDF $f_{X}$ is not uniquely prescribed since two integrable functions that take identical values except at some specific points have the same integral. However, if $F_{X}$ is \textbf{differentiable} at $u$, we set $f_{X}(u) = F_{X}'(u)$.
    \end{rem}
    \begin{rem}
        More generally, for an interval $B$, we have:
        \begin{equation*}
            \prob(X \in B) = \int_{B}f_{X}(x)\,dx.
        \end{equation*}
    \end{rem}
    Note that we use the same letter $f$ for mass functions and density functions since both perform similar tasks.
    \begin{rem}
        For $x \in \mathbb{R}$, the numerical value $f_{X}(x)$ is not a probability. However, we can consider:
        \begin{equation*}
            f_{X}(x)\,dx = \prob(x < X \leq x+dx),
        \end{equation*}
        as an element of probability.
    \end{rem}
    \begin{lem}
        \label{Chapter 5 (Lemma) Properties of PDF}
        If a continuous random variable $X$ has a density function $f_{X}$, then:
        \begin{enumerate}
            \item $\int_{-\infty}^{\infty} f_{X}(x)\,dx = 1$.
            \item $\prob(X = x) = 0$ for all $x \in \mathbb{R}$.
            \item $\prob(a \leq X \leq b) = \int_{a}^{b}f_{X}(x)\,dx$.
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item 
            \begin{equation*}
                \int_{-\infty}^{\infty} f_{X}(x)\,dx = \lim_{x \to \infty}F_{X}(x) = 1.
            \end{equation*}
            \item
            \begin{equation*}
                \prob(X = x) = \lim_{h \to 0^{+}}\int_{x-h}^{x}f_{X}(x)\,dx = F_{X}(x)-\lim_{h \to \infty}F(x-h) = F_{X}(x)-F_{X}(x) = 0.
            \end{equation*}
            \item
            \begin{equation*}
                \prob(a \leq X \leq b) = F(b)-F(a) = \int_{-\infty}^{b}f_{X}(x)\,dx-\int_{-\infty}^{a}f_{X}(x)\,dx = \int_{a}^{b}f_{X}(x)\,dx.
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    \newpage

    Similar to the discrete case, there is a joint distribution function for two random variables.
    \begin{defn}
        The \textbf{joint distribution function} (JCDF) of two continuous random variables $X$ and $Y$ is the function $F:\mathbb{R}^{2} \to [0,1]$ such that:
        \begin{equation*}
            F_{X,Y}(x, y) = \prob(X \leq x,Y \leq y).
        \end{equation*}
        Two continuous random variables $X$ and $Y$ are \textbf{jointly continuous} if they have a \textbf{joint density function} (JPDF) $f:\mathbb{R}^{2} \to [0,\infty)$ such that:
        \begin{align*}
            F_{X,Y}(x, y) &= \int_{-\infty}^{y}\int_{-\infty}^{x}f_{X,Y}(u, v)\,du\,dv, & f_{X,Y}(x, y) &= \pdv*{F_{X,Y}(x, y)}{x,y}.
        \end{align*}
    \end{defn}
    \begin{rem}
        More generally, for $B \in \mathcal{B}(\mathbb{R}^{2})$:
        \begin{equation*}
            \prob((X,Y) \in B) = \iint_{B}f_{X,Y}(x, y)\,dx\,dy.
        \end{equation*}
    \end{rem}
    We also recall the definition of the marginal distribution function.
    \begin{defn}
        Given two continuous random variables $X$ and $Y$, the \textbf{marginal probability density function} (Marginal PDF) of $X$ is:
        \begin{equation*}
            f_{X}(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, u)\,du.
        \end{equation*}
    \end{defn}
    The definition of independence also applies to continuous random variables.
    \begin{defn}
        Two continuous random variables $X$ and $Y$ are called \textbf{independent} if, for all $x,y \in \mathbb{R}$:
        \begin{equation*}
            F_{X,Y}(x, y) = F_{X}(x)F_{Y}(y).
        \end{equation*}
    \end{defn}
    \begin{thm}
        Two continuous random variables $X$ and $Y$ are independent if and only if, for all $x,y \in \mathbb{R}$:
        \begin{equation*}
            f_{X,Y}(x, y) = f_{X}(x)f_{Y}(y).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        If $f_{X,Y}(x, y) = f_{X}(x)f_{Y}(y)$, then:
        \begin{equation*}
            F_{X,Y}(x, y) = \int_{-\infty}^{y}\int_{-\infty}^{x}f_{X,Y}(u, v)\,du\,dv = \int_{-\infty}^{y}\int_{-\infty}^{x}f_{X}(u)f_{Y}(v)\,du\,dv = F_{X}(x)\int_{-\infty}^{y}f_{Y}(v)\,dv = F_{X}(x)F_{Y}(y).
        \end{equation*}
        If $X$ and $Y$ are independent, then:
        \begin{equation*}
            \int_{-\infty}^{y}\int_{-\infty}^{x}f_{X,Y}(u, v)\,du\,dv = \int_{-\infty}^{y}\int_{-\infty}^{x}f_{X}(u)f_{Y}(v)\,du\,dv.
        \end{equation*}
        By the Fundamental Theorem of Calculus, we have:
        \begin{equation*}
            f_{X,Y}(x, y) = \pdv*{}{x,y}\int_{-\infty}^{y}\int_{-\infty}^{x}f_{X,Y}(u, v)\,du\,dv = \pdv*{}{x,y}\int_{-\infty}^{y}\int_{-\infty}^{x}f_{X}(u)f_{Y}(v)\,du\,dv = f_{X}(x)f_{Y}(y).
        \end{equation*}
    \end{proofing}
    \begin{thm}
        Consider two continuous and independent random variables $X$ and $Y$. For any two functions $g$ and $h$, if $g(X)$ and $h(Y)$ are still continuous random variables, then $g(X)$ and $h(Y)$ are independent.
    \end{thm}
    \newpage

\section{Conditional Distribution of Continuous Random Variables}
    Recall the definition of the conditional distribution function of a discrete random variable $Y$ given $X = x$:
    \begin{equation*}
        F_{Y|X}(y|x) = \prob(Y \leq y|X = x) = \frac{\prob(Y \leq y,X = x)}{\prob(X = x)}.
    \end{equation*}
    However, for continuous random variables, $\prob(X = x) = 0$ for all $x$. We take a limiting point of view.\\
    Suppose the probability distribution function $f_{X}(x) > 0$:
    \begin{align*}
        F_{Y|X}(y|x) = \prob(Y \leq y|x \leq X \leq x+dx) &= \frac{\prob(Y \leq y,x \leq X \leq x+dx)}{\prob(x \leq X \leq x+dx)}\\
        &= \frac{\int_{-\infty}^{y}\int_{x}^{x+dx}f_{X,Y}(u, v)\,du\,dv}{\int_{x}^{x+dx}f_{X}(u)\,du}\\
        &\approx \frac{\int_{-\infty}^{y}f_{X,Y}(x, v)\,dx\,dv}{f_{X}(x)\,dx}\\
        &= \int_{-\infty}^{y}\frac{f_{X,Y}(x, v)}{f_{X}(x)}\,dv.
    \end{align*}
    \begin{defn}
        Suppose $X,Y:\Omega \to \mathbb{R}$ are two continuous random variables with PDF $f_{X}(x) > 0$ for some 
        $x \in \mathbb{R}$. The \textbf{conditional distribution function} (Conditional CDF) of $Y$ given $X = x$ for some $\mathbf{x} \in \mathbb{R}$ is defined by:
        \begin{equation*}
            F_{Y|X}(y|x) = \prob(Y \leq y|X = x) = \int_{-\infty}^{y}\frac{f_{X,Y}(x, v)}{f_{X}(x)}\,dv.
        \end{equation*}
        The \textbf{conditional probability density function} (Conditional PDF) of $Y$ given $X = x$ for some $\mathbf{x} \in \mathbb{R}$ is defined by:
        \begin{equation*}
            f_{Y|X}(y|x) = \pdv*{F_{Y|X}(y|x)}{y} = \frac{f_{X,Y}(x, y)}{f_{X}(x)}.
        \end{equation*}
    \end{defn}
    \begin{rem}
        For any $x,y \in \mathbb{R}$, since $f_{X}(x)$ can also be computed from $f_{X,Y}(x, y)$, we can simply compute:
        \begin{equation*}
            f_{Y|X}(y|x) = \frac{f_{X,Y}(x, y)}{\int_{-\infty}^{\infty} f_{X,Y}(x, y)\,dy}.
        \end{equation*}
    \end{rem}
    \begin{rem}
        More generally, for two continuous random variables $X$ and $Y$ with PDF $f_{X}(x) > 0$ for some $x \in \mathbb{R}$:
        \begin{equation*}
            \prob(Y \in A|X = x) = \int_{A}\frac{f_{X,Y}(x, v)}{f_{X}(x)}\,dv = \int_{A}f_{Y|X}(y|x)\,dy, \qquad\text{for }A \in \mathcal{B}(\mathbb{R}).
        \end{equation*}
    \end{rem}
    \begin{eg}
        \label{Chapter 5 (Example) JPDF to Conditional PDF Example}
        Assume that two jointly continuous random variables $X$ and $Y$ have a JPDF:
        \begin{equation*}
            f_{X,Y}(x, y) = \begin{cases}
                \frac{1}{x}, &0 \leq y \leq x \leq 1\\
                0, &\text{Otherwise}
            \end{cases} = \frac{1}{x}\mathbf{1}_{0 \leq y \leq x \leq 1}.
        \end{equation*}
        We want to compute $f_{X}(x)$ and $f_{Y|X}(y|x)$.
        For $x \in [0,1]$:
        \begin{equation*}
            f_{X}(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y)\,dy = \int_{-\infty}^{\infty}\frac{1}{x}\mathbf{1}_{0 \leq y \leq x \leq 1}\,dy = \int_{0}^{x}\frac{1}{x}\,dy = 1.
        \end{equation*}
        Therefore, $X \sim \U[0,1]$.\\
        For $0 \leq y \leq x$ and $0 \leq x \leq 1$:
        \begin{equation*}
            f_{Y|X}(y|x) = \frac{f_{X,Y}(x, y)}{f_{X}(x)} = \frac{1}{x}.
        \end{equation*}
        Therefore, $(Y|X = x) \sim \U[0,x]$.
    \end{eg}
    \newpage

    \begin{eg}
        We aim to find $\prob(X^{2}+Y^{2} \leq 1)$, where $X$ and $Y$ are two jointly continuous random variables with the JPDF given in Example \ref{Chapter 5 (Example) JPDF to Conditional PDF Example}. Let $Y \in A_{x} = \{y:|y| \leq \sqrt{1-x^{2}}\}$.
        \begin{align*}
            \prob(X^{2}+Y^{2} \leq 1|X = x) = \prob(|Y| \leq \sqrt{1-x^{2}}|X = x) &= \int_{A_{x}}f_{Y|X}(y|x)\,dy\\
            &= \int_{A_{x}\cap[0,1]}\frac{1}{x}\,dy\\
            &= \int_{0}^{\min\{x,\sqrt{1-x^{2}}\}}\frac{1}{x}\,dy\\
            &= \min\left\{1,\sqrt{\frac{1}{x^{2}}-1}\right\}.
        \end{align*}
        \begin{align*}
            \prob(X^{2}+Y^{2} \leq 1) = \iint_{x^{2}+y^{2} \leq 1}f_{X,Y}(x, y)\,dy\,dx &= \iint_{x^{2}+y^{2} \leq 1}f_{Y|X}(y|x)f_{X}(x)\,dy\,dx\\
            &= \int_{0}^{1}\min\left\{1,\sqrt{\frac{1}{x^{2}}-1}\right\}\,dx\\
            &= \int_{0}^{\frac{1}{\sqrt{2}}}\,dx+\int_{\frac{1}{\sqrt{2}}}^{1}\sqrt{\frac{1}{x^{2}}-1}\,dx\\
            \tag{$x = \sin\theta$}
            &= \frac{1}{\sqrt{2}}+\int_{\frac{\pi}{4}}^{\frac{\pi}{2}}\left(\frac{1}{\sin\theta}-\sin\theta\right)\,d\theta\\
            &= \left.\ln\left(\tan\frac{\theta}{2}\right)\right|_{\frac{\pi}{4}}^{\frac{\pi}{2}} = \ln(1)-\ln(\sqrt{2}-1) = \ln(1+\sqrt{2}).
        \end{align*}
    \end{eg}
    Similar to discrete random variables, we can find the distribution of $X+Y$ when $X$ and $Y$ are jointly continuous.
    \begin{thm}
        If two jointly continuous random variables $X$ and $Y$ have a JPDF $f_{X,Y}$, then $X+Y$ has a PDF:
        \begin{equation*}
            f_{X+Y}(z) = \int_{-\infty}^{\infty} f_{X,Y}(x,z-x)\,dx = \int_{-\infty}^{\infty} f_{X,Y}(z-y,y)\,dy, \qquad\text{for }z \in \mathbb{R}.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        \begin{align*}
            F_{X+Y}(z) = \prob(X+Y \leq z) &= \iint_{x+y \leq z}f_{X,Y}(x, y)\,dx\,dy\\
            &= \int_{-\infty}^{\infty}\int_{-\infty}^{z-y} f_{X,Y}(x, y)\,dx\,dy\\
            \tag{$v = x+y$}
            &= \int_{-\infty}^{\infty}\int_{-\infty}^{z} f_{X,Y}(v-y,y)\,dv\,dy\\
            &= \int_{-\infty}^{z}\int_{-\infty}^{\infty} f_{X,Y}(v-y,y)\,dy\,dv\\
            f_{X+Y}(z) = F_{X+Y}'(z) &= \int_{-\infty}^{\infty} f_{X,Y}(z-y,y)\,dy = \int_{-\infty}^{\infty} f_{X,Y}(x,z-x)\,dx.
        \end{align*}
    \end{proofing}
    \begin{defn}
        Given two independent continuous random variables $X$ and $Y$, the \textbf{convolution} $f_{X+Y}$ ($f_{X}*f_{Y}$) of the PDFs of $X$ and $Y$ is the PDF of $X+Y$:
        \begin{equation*}
            f_{X+Y}(z) = \int_{-\infty}^{\infty} f_{X}(z-y)f_{Y}(y)\,dy = \int_{-\infty}^{\infty} f_{X}(x)f_{Y}(z-x)\,dx, \qquad\text{for }z \in \mathbb{R}.
        \end{equation*}
    \end{defn}
    \newpage

\section{Examples of Continuous Random Variables}
    Similar to discrete random variables, we have some useful parametric distributions.
    \begin{defn}
        The \textbf{parametric distribution} of a continuous random variable is a distribution where the PDF depends on one or more parameters.
    \end{defn}
    \begin{eg}\named{Uniform Distribution} $X \sim \U[a, b]$\\
        A random variable $X$ is \textbf{uniform} on $[a, b]$ for $a < b$ if the CDF and PDF of $X$ are:
        \begin{align*}
            F_{X}(x) &= \begin{cases}
                0, &x \leq a\\
                \frac{x-a}{b-a}, &a < x \leq b\\
                1, &x > b
            \end{cases}, & f_{X}(x) &= \begin{cases}
                \frac{1}{b-a}, &a < x \leq b\\
                0, &\text{Otherwise}
            \end{cases} = \frac{1}{b-a}\mathbf{1}_{a < x \leq b}.
        \end{align*}
    \end{eg}
    \begin{eg}
        If $X \sim \U[0,1]$ and $Y \sim \U[0,1]$, and $X \independent Y$, then for all $t \in \mathbb{R}$:
        \begin{equation*}
            f_{X}(t) = f_{Y}(t) = \begin{cases}
                1, &0 \leq t \leq 1\\
                0, &\text{Otherwise}
            \end{cases}.
        \end{equation*}
        Therefore, for all $z \in \mathbb{R}$:
        \begin{align*}
            f_{X+Y}(z) = \int_{-\infty}^{\infty} f_{X}(z-y)f_{Y}(y)\,dy &= \int_{0}^{1}f_{X}(z-y)\,dy\\
            &= \int_{0}^{1}\mathbf{1}_{0 \leq z-y \leq 1}\,dy\\
            \tag{$z-1 \leq y \leq z$}
            &= \int_{\max\{0,z-1\}}^{\min\{1,z\}}\,dy\\
            &= \min\{1,z\}-\max\{0,z-1\} = \begin{cases}
                z, &0 \leq z \leq 1\\
                2-z, &1 \leq z \leq 2\\
                0, &\text{Otherwise}
            \end{cases}.
        \end{align*}
    \end{eg}
    \begin{eg}
        Assume that a plane is ruled by horizontal lines separated by $D$, and a needle of length $L \leq D$ is cast randomly on the plane. What is the probability that the needle intersects some lines?\\
        Let $X$ be the distance from the center of the needle to the nearest line, and $\Theta$ be the acute angle between the needle and the vertical line. Assume that $X \independent\Theta$. We have $X \sim \U\left[0,\frac{D}{2}\right]$ and $\Theta \sim \U\left[0,\frac{\pi}{2}\right]$.
        \begin{align*}
            f_{X,\Theta}(x,\theta) &= \begin{cases}
                \frac{4}{D\pi}, &0 \leq x \leq \frac{D}{2},0 \leq \theta \leq \frac{\pi}{2}\\
                0, &\text{Otherwise}
            \end{cases}\\
            \prob(\text{Intersection}) = \prob\left(\frac{L}{2}\cos\Theta \geq X\right) &= \iint_{\frac{L}{2}\cos\theta \geq x}\frac{4}{D\pi}\mathbf{1}_{0 \leq x \leq \frac{D}{2}}\mathbf{1}_{0 \leq \theta \leq \frac{\pi}{2}}\,dx\,d\theta = \int_{0}^{\frac{\pi}{2}}\int_{0}^{\frac{L}{2}\cos\theta}\frac{4}{D\pi}\,dx\,d\theta = \frac{2L}{D\pi}.
        \end{align*}
        Suppose that we throw the needle $n$ times.
        \begin{equation*}
            \frac{\#\{\text{Intersection}\}}{n} \approx \prob(\text{Intersection}) = \frac{2L}{D\pi}.
        \end{equation*}
        By measuring the number of intersections, we can estimate the value of $\pi$.
    \end{eg}
    \begin{eg} \named{Inverse Transform Sampling}
        Let $U \sim \U[0,1]$. For a continuous random variable $X$ with CDF $F_{X}$, we want to find a function $g:\mathbb{R} \to \mathbb{R}$ such that $g(U) \sim X$. Since $F_{X}$ is non-decreasing, we can define the \textbf{generalized inverse} of $F_{X}$ as:
        \begin{equation*}
            F_{X}^{-1}(y) = \inf\{x:F_{X}(x) \geq y\}, \qquad\text{for }y \in [0,1].
        \end{equation*}
        We set $g = F_{X}^{-1}$. For $x \in \mathbb{R}$:
        \begin{align*}
            \prob(F_{X}^{-1}(U) \leq x) &= \prob(U \leq F_{X}(x)) = F_{X}(x).
        \end{align*}
        Therefore, $F_{X}^{-1}(U) \sim X$.
        \end{eg}
    \newpage

    \begin{eg}\named{Exponential Distribution} $X \sim \Exp(\lambda)$\\
        A random variable $X$ is \textbf{exponentially distributed} with parameter $\lambda > 0$ if the CDF and PDF of $X$ are:
        \begin{align*}
            F_{X}(x) &= \begin{cases}
                1-e^{-\lambda x}, &x \geq 0\\
                0, &x < 0
            \end{cases}, & f_{X}(x) &= \begin{cases}
                \lambda e^{-\lambda x}, &x \geq 0\\
                0, &x < 0
            \end{cases} = \lambda e^{-\lambda x}\mathbf{1}_{x \geq 0}.
        \end{align*}
    \end{eg}
    \begin{thm}
        The exponential distribution has the memoryless property. This means that for all $s > 0$ and $t > 0$:
        \begin{equation*}
            \prob(X > s+t|X > s) = \prob(X > t).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Assume that $X \sim \Exp(\lambda)$.
        \begin{equation*}
            \prob(X > s+t|X > s) = \frac{\prob(\{X > s+t\}\cap\{X > s\})}{\prob(X > s)} = \frac{\prob(X > s+t)}{\prob(X > s)} = \frac{e^{-\lambda(s+t)}}{e^{-\lambda s}} = e^{-\lambda t} = \prob(X > t).
        \end{equation*}
    \end{proofing}
    \begin{eg}\named{Normal Distribution / Gaussian Distribution} $X \sim \N(\mu,\sigma^{2})$\\
        A random variable $X$ is \textbf{normally distributed} if it has two parameters $\mu$ and $\sigma^{2}$, and its PDF and CDF are:
        \begin{align*}
            f_{X}(x) &= \frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right), & F_{X}(x) &= \int_{-\infty}^{x}f_{X}(u)\,du.
        \end{align*}
        This distribution is one of the most important distributions.\\
        The random variable $X$ is \textbf{standard normal} if $\mu = 0$ and $\sigma^{2} = 1$ ($X \sim \N(0,1)$):
        \begin{align*}
            f_{X}(x) &= \phi(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^{2}}{2}}, & F_{X}(x) &= \Phi(x) = \int_{-\infty}^{x}\phi(u)\,du.
        \end{align*}
    \end{eg}
    \begin{cla}
        $\phi(x)$ is a probability density function.
    \end{cla}
    \begin{proofing}
        Let $I = \int_{-\infty}^{\infty}\phi(x)\,dx$.
        \begin{equation*}
            I^{2} = \int_{-\infty}^{\infty}\phi(x)\,dx\int_{-\infty}^{\infty}\phi(y)\,dy = \frac{1}{2\pi}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} e^{-\frac{x^{2}+y^{2}}{2}}\,dx\,dy.
        \end{equation*}
        Let $x = r\cos\theta$ and $y = r\sin\theta$, where $r \in [0,\infty)$ and $\theta \in [0,2\pi]$:
        \begin{equation*}
            I^{2} = \frac{1}{2\pi}\int_{0}^{2\pi}\int_{0}^{\infty}e^{-\frac{r^{2}}{2}}r\,dr\,d\theta = \frac{1}{2\pi}\int_{0}^{2\pi}\int_{0}^{\infty}e^{-\frac{r^{2}}{2}}\,d\left(\frac{r^{2}}{2}\right)\,d\theta = \frac{1}{2\pi}\int_{0}^{2\pi}\,d\theta = 1.
        \end{equation*}
        Since $\phi(x) > 0$, $I = 1$. Therefore, $\phi(x)$ is a probability density function.
    \end{proofing}
    These are some properties that are frequently used.
    \begin{lem}
        \label{Chapter 5 (Lemma) Properties of Normal Distribution}
        The normal distribution has the following properties:
        \begin{enumerate}
            \item Let $X \sim \N(0,1)$. If $Y = bX+a$ for some $a,b \in \mathbb{R}$ and $b \neq 0$, then $Y \sim \N(a,b^{2})$.
            \item Let $X \sim \N(a,b^{2})$ for some $a,b \in \mathbb{R}$ and $b \neq 0$. If $Y = \frac{X-a}{b}$, then $Y \sim \N(0,1)$.
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item Let $z = bx+a$.
            \begin{equation*}
                F_{Y}(y) = \prob(Y \leq y) = \prob\left(X \leq \frac{y-a}{b}\right) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\frac{y-a}{b}}e^{-\frac{x^{2}}{2}}\,dx = \frac{1}{\sqrt{2\pi b^{2}}}\int_{-\infty}^{y}e^{-\frac{(z-a)^{2}}{2b^{2}}}\,dz.
            \end{equation*}
            Therefore, $Y \sim \N(a,b^{2})$.
            \item Let $x = bz+a$.
            \begin{equation*}
                F_{Y}(y) = \prob(Y \leq y) = \prob(X \leq by+a) = \frac{1}{\sqrt{2\pi b^{2}}}\int_{-\infty}^{by+a}e^{-\frac{(x-a)^{2}}{2b^{2}}}\,dx = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{y}e^{-\frac{z^{2}}{2}}\,dz.
            \end{equation*}
            Therefore, $Y \sim \N(0,1)$.
        \end{enumerate}
    \end{proofing}
    \newpage

    \begin{lem}
        If $X \sim \N(\mu,\sigma^{2})$, then for all $s \leq t$:
        \begin{equation*}
            \prob(s \leq X \leq t) = \prob\left(\frac{s-\mu}{\sigma} \leq \frac{X-\mu}{\sigma} \leq \frac{t-\mu}{\sigma}\right) = \Phi\left(\frac{t-\mu}{\sigma}\right)-\Phi\left(\frac{s-\mu}{\sigma}\right).
        \end{equation*}
    \end{lem}
    \begin{proofing}
        Since $\sigma > 0$, $\frac{X-\mu}{\sigma} \sim \N(0,1)$ by Lemma \ref{Chapter 5 (Lemma) Properties of Normal Distribution}. Therefore:
        \begin{align*}
            \prob(s \leq X \leq t) &= \int_{s}^{t}\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right)\,dx\\
            \tag{$z = \frac{x-\mu}{\sigma}$}
            &= \int_{\frac{s-\mu}{\sigma}}^{\frac{t-\mu}{\sigma}}\frac{1}{\sqrt{2\pi}}e^{-\frac{z^{2}}{2}}\,dz = \Phi\left(\frac{t-\mu}{\sigma}\right)-\Phi\left(\frac{s-\mu}{\sigma}\right).
        \end{align*}
    \end{proofing}
    This is a very important theorem, as it states that the sum of normal distributions is still normal.
    \begin{thm}
        \label{Chapter 5 (Theorem) Additivity of Normal Distribution}
        If $X_{i} \sim \N(\mu_{i},\sigma_{i}^{2})$ for $i = 1,2,\dots,n$ and they are independent, then:
        \begin{equation*}
            \sum_{i = 1}^{n}X_{i} \sim \N\left(\sum_{i = 1}^{n}\mu_{i},\sum_{i = 1}^{n}\sigma_{i}^{2}\right).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        We first consider a special case where $X \sim \N(0,\sigma^{2})$, $Y \sim \N(0,1)$, and $X \independent Y$.
        \begin{align*}
            f_{X+Y}(z) &= \int_{-\infty}^{\infty} f_{X}(z-y)f_{Y}(y)\,dy\\
            &= \int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(z-y)^{2}}{2\sigma^{2}}\right)\left(\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{y^{2}}{2}\right)\right)\,dy\\
            &= \frac{1}{2\pi\sigma}\exp\left(-\frac{z^{2}}{2\sigma^{2}}\right)\int_{-\infty}^{\infty}\exp\left(-\frac{1}{2\sigma^{2}}(-2yz+y^{2}(1+\sigma^{2}))\right)\,dy\\
            &= \frac{1}{2\pi\sigma}\exp\left(-\frac{z^{2}}{2\sigma^{2}}+\frac{z^{2}}{2\sigma^{2}(1+\sigma^{2})}\right)\int_{-\infty}^{\infty}\exp\left(-\frac{1+\sigma^{2}}{2\sigma^{2}}\left(\frac{z^{2}}{(1+\sigma^{2})^{2}}-\frac{2yz}{1+\sigma^{2}}+y^{2}\right)\right)\,dy\\
            &= \frac{1}{\sqrt{2\pi}\sqrt{1+\sigma^{2}}}\exp\left(-\frac{z^{2}}{2\sigma^{2}}+\frac{z^{2}}{2\sigma^{2}(1+\sigma^{2})}\right)\int_{-\infty}^{\infty}\left(\frac{1}{\sqrt{2\pi}\frac{\sigma}{\sqrt{1+\sigma^{2}}}}\right)\exp\left(-\frac{\left(y-\frac{z}{1+\sigma^{2}}\right)^{2}}{2\left(\frac{\sigma}{\sqrt{1+\sigma^{2}}}\right)^{2}}\right)\,dy\\
            &= \frac{1}{\sqrt{2\pi}\sqrt{1+\sigma^{2}}}\exp\left(-\frac{z^{2}}{2(1+\sigma^{2})}\right).
        \end{align*}
        Therefore, $X+Y \sim \N(0,1+\sigma^{2})$. In the general case where $X_{1} \sim \N(\mu_{1},\sigma_{1}^{2})$, $X_{2} \sim \N(\mu_{2},\sigma_{2}^{2})$, and $X_{1} \independent X_{2}$:
        \begin{equation*}
            X_{1}+X_{2} = \sigma_{2}\left(\frac{X_{1}-\mu_{1}}{\sigma_{2}}+\frac{X_{2}-\mu_{2}}{\sigma_{2}}\right)+\mu_{1}+\mu_{2}.
        \end{equation*}
        We get $\frac{X_{1}-\mu_{1}}{\sigma_{2}} \sim \N\left(0,\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\right)$. Applying this to the special case, we find:
        \begin{equation*}
            \frac{X_{1}-\mu_{1}}{\sigma_{2}}+\frac{X_{2}-\mu_{2}}{\sigma_{2}} \sim \N\left(0,1+\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\right).
        \end{equation*}
        Therefore, $X_{1}+X_{2} \sim \N(\mu_{1}+\mu_{2},\sigma_{1}^{2}+\sigma_{2}^{2})$. By induction, if $X_{i} \sim \N(\mu_{i},\sigma_{i}^{2})$ for $i = 1,2,\dots,n$ and they are independent, then:
        \begin{equation*}
            \sum_{i = 1}^{n}X_{i} \sim \N\left(\sum_{i = 1}^{n}\mu_{i},\sum_{i = 1}^{n}\sigma_{i}^{2}\right).
        \end{equation*}
    \end{proofing}
    \newpage

    Combining two normal distributions into a joint distribution can be very useful.
    \begin{eg}\named{Standard Bivariate Normal Distribution}
        Two continuous random variables $X$ and $Y$ are \textbf{standard bivariate normal} if they have the JPDF:
        \begin{equation*}
            f_{X,Y}(x, y) = \frac{1}{2\pi\sqrt{1-\rho^{2}}}\exp\left(-\frac{x^{2}-2\rho xy+y^{2}}{2(1-\rho^{2})}\right),
        \end{equation*}
        where $\rho$ is a constant satisfying $-1 < \rho < 1$.
    \end{eg}
    \begin{rem}
        If $X \sim \N(0,1)$ and $Y \sim \N(0,1)$:
        \begin{align*}
            f_{Y}(y) &= \int_{-\infty}^{\infty} f_{X,Y}(x, y)\,dx = \frac{1}{2\pi\sqrt{1-\rho^{2}}}\int_{-\infty}^{\infty}\exp\left(-\frac{(x-\rho y)^{2}+(1-\rho^{2})y}{2(1-\rho^{2})}\right)\,dx\\
            &= \frac{1}{\sqrt{2\pi}}e^{-\frac{y^{2}}{2}}\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi(1-\rho^{2})}}e^{-\frac{(x-\rho y)^{2}}{2(1-\rho^{2})}}\,dx = \frac{1}{\sqrt{2\pi}}e^{-\frac{y^{2}}{2}}.
        \end{align*}
    \end{rem}
    \begin{rem}
        $\rho$ is called the \textbf{population correlation coefficient} between $X$ and $Y$. It will be discussed in Chapter \ref{Chapter 6 (Expectation)}.
    \end{rem}
    In most cases, normal random variables $X$ and $Y$ do not have a mean of $0$ and a variance of $1$. If we also include the mean and variance in the distribution, we obtain the following distribution.
    \begin{eg}\named{Bivariate Normal Distribution}
        Two continuous random variables $X$ and $Y$ are \textbf{bivariate normal} with means $\mu_{X}$ and $\mu_{Y}$, variances $\sigma_{X}^{2}$ and $\sigma_{Y}^{2}$, and correlation coefficient $\rho$ if the JPDF is given by:
        \begin{equation*}
            f_{X,Y}(x, y) = \frac{1}{2\pi\sigma_{X}\sigma_{Y}\sqrt{1-\rho^{2}}}\exp\left(-\frac{1}{2(1-\rho^{2})}\left(\left(\frac{x-\mu_{X}}{\sigma_{X}}\right)^{2}-2\rho\left(\frac{x-\mu_{X}}{\sigma_{X}}\right)\left(\frac{y-\mu_{Y}}{\sigma_{Y}}\right)+\left(\frac{y-\mu_{Y}}{\sigma_{Y}}\right)^{2}\right)\right).
        \end{equation*}
    \end{eg}
    \begin{eg}
        Assume that random variables $X \sim \N(0,1)$ and $Y \sim \N(0,1)$ are standard bivariate normal. For $-1 < \rho < 1$:
        \begin{equation*}
            f_{X,Y}(x, y) = \frac{1}{2\pi\sqrt{1-\rho^{2}}}\exp\left(-\frac{x^{2}-2\rho xy+y^{2}}{2(1-\rho^{2})}\right).
        \end{equation*}
        We want to find $f_{X|Y}(x|y)$.
        \begin{align*}
            f_{X|Y}(x|y) = \frac{f_{X,Y}(x, y)}{f_{Y}(y)} &= \sqrt{2\pi}e^{\frac{1}{2}y^{2}}f_{X,Y}(x, y)\\
            &= \frac{1}{\sqrt{2\pi}\sqrt{1-\rho^{2}}}e^{\frac{1}{2}y^{2}-\frac{y^{2}}{2(1-\rho^{2})}}\exp\left(-\frac{x^{2}-2\rho xy}{2(1-\rho^{2})}\right)\\
            &= \frac{1}{\sqrt{2\pi}{\sqrt{1-\rho^{2}}}}e^{\left(\frac{1}{2}-\frac{1}{2(1-\rho^{2})}-\frac{\rho^{2}}{2(1-\rho^{2})}\right)y^{2}}\exp\left(-\frac{(x-\rho y)^{2}}{2(1-\rho^{2})}\right)\\
            &= \frac{1}{\sqrt{2\pi}\sqrt{1-\rho^{2}}}\exp\left(-\frac{(x-\rho y)^{2}}{2(1-\rho^{2})}\right).
        \end{align*}
        Therefore, we have $(X|Y = y) \sim \N(\rho y,1-\rho^{2})$. As $\rho \to 1$, we have $X \to Y$. As $\rho \to -1$, we have $X \to -Y$.\\
        In general, there exists a random variable $Z \sim \N(0,1)$ such that:
        \begin{align*}
            X &= \rho Y+\sqrt{1-\rho^{2}}Z, & (X|Y = y) &= \rho y+\sqrt{1-\rho^{2}}Z, & \begin{pmatrix}
                X\\
                Y
            \end{pmatrix} &= \begin{pmatrix}
                \rho & \sqrt{1-\rho^{2}}\\
                1 & 0
            \end{pmatrix}\begin{pmatrix}
                Y\\
                Z
            \end{pmatrix}.
        \end{align*}
        This means that we can generate bivariate normal random variables $X$ and $Y$ from two independent standard normal random variables $Y$ and $Z$. More generally, for any orthogonal matrix $\mathbf{A}$ (i.e. $\mathbf{A}^{T}\mathbf{A} = \mathbf{I}$), if we let:
        \begin{equation*}
            \begin{pmatrix}
                W\\
                U
            \end{pmatrix} = \begin{pmatrix}
                \rho & \sqrt{1-\rho^{2}}\\
                1 & 0
            \end{pmatrix}\mathbf{A}\begin{pmatrix}
                Y\\
                Z
            \end{pmatrix},
        \end{equation*}
        then $W$ and $U$ will also be bivariate normal with $\rho$.
    \end{eg}
    \newpage

    There are some remarks that may be important to know.
    \begin{rem}
        $X$ and $Y$ are bivariate normal and uncorrelated if and only if $X$ and $Y$ are independent normal. We will discuss what uncorrelatedness means.
    \end{rem}
    \begin{rem}
        $X$ and $Y$ being jointly continuous and both normal does not imply that they are bivariate normal.
    \end{rem}
    \begin{eg}
        Consider the JPDF of random variables $X$ and $Y$:
        \begin{equation*}
            f_{X,Y}(x, y) = \begin{cases}
                \frac{1}{\pi}e^{-\frac{1}{2}(x^{2}+y^{2})}, &xy > 0\\
                0, &xy \leq 0
            \end{cases}.
        \end{equation*}
        As you can see, this is not a bivariate normal distribution.\\
        However, if you look at their marginal PDFs:
        \begin{align*}
            f_{X}(x) &= \int_{0}^{\infty}\frac{1}{\pi}e^{-\frac{1}{2}(x^{2}+y^{2})}\,dy = \frac{1}{2\pi}\int_{-\infty}^{\infty} e^{-\frac{1}{2}(x^{2}+y^{2})}\,dy = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^{2}}, & x &> 0,\\
            f_{X}(x) &= \int_{-\infty}^{0}\frac{1}{\pi}e^{-\frac{1}{2}(x^{2}+y^{2})}\,dy = \frac{1}{2\pi}\int_{-\infty}^{\infty} e^{-\frac{1}{2}(x^{2}+y^{2})}\,dy = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^{2}}, & x &< 0.
        \end{align*}
        This is the same as $f_{Y}(x)$.\\
        Therefore, $X$ and $Y$ being jointly continuous and both normal does not imply that they are bivariate normal.
    \end{eg}
    \begin{rem}
        Two random variables $X$ and $Y$ being jointly continuous and uncorrelated Gaussian does not imply that they are independent Gaussian.
    \end{rem}
    More generally, we can create a multivariate normal distribution from more than two random variables.
    \begin{eg}\named{Multivariate Normal Distribution} $\mathbf{X} \sim \N_{p}(\boldsymbol{\mu},\mathbf{\Sigma})$\\
        A random vector $\mathbf{X}$ with dimension $p$ is $p$-dimensional normal with $p\times 1$ mean vector $\boldsymbol{\mu}$ and $p\times p$ variance-covariance matrix $\mathbf{\Sigma}$ if:
        \begin{equation*}
            f(\mathbf{x}) = (2\pi)^{-\frac{p}{2}}\abs{\mathbf{\Sigma}}^{-\frac{1}{2}}e^{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T}\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})}.
        \end{equation*}
    \end{eg}
    \begin{rem}
        The elements $a_{ij}$ in the $i$-th row and $j$-th column of the variance-covariance matrix $\mathbf{\Sigma}$ are obtained by:
        \begin{equation*}
            a_{ij} = \cov(X_{i},X_{j}).
        \end{equation*}
        We will discuss how to calculate the covariance in the next chapter.
    \end{rem}
    \begin{eg}\named{Cauchy Distribution} $X \sim \Cauchy(\theta)$\\
        A random variable $X$ has a Cauchy distribution if it has the PDF:
        \begin{equation*}
            f_{X}(x) = \frac{1}{\pi(1+(x-\theta)^{2})}.
        \end{equation*}
    \end{eg}
    \begin{rem}
        If $X \sim \N(0,1)$ and $Y \sim \N(0,1)$, then $\frac{X}{Y} \sim \Cauchy(0)$.
    \end{rem}
    \newpage

    \begin{eg}\named{Gamma Distribution} $X \sim \Gam(\alpha,\lambda)$\\
        A random variable $X$ has a gamma distribution with parameters $\alpha$ and $\lambda$ if it has the PDF:
        \begin{equation*}
            f_{X}(x) = \begin{cases}
                \frac{1}{\Gamma(\alpha)}\lambda e^{-\lambda x}(\lambda x)^{\alpha-1}, &x \geq 0\\
                0, &x < 0
            \end{cases} = \frac{1}{\Gamma(\alpha)}\lambda e^{-\lambda x}(\lambda x)^{\alpha-1}\mathbf{1}_{x \geq 0},
        \end{equation*}
        where $\Gamma(\alpha)$ is called the \textbf{gamma function}, defined by:
        \begin{equation*}
            \Gamma(\alpha) = \int_{0}^{\infty}e^{-y}y^{\alpha-1}\,dy.
        \end{equation*}
        Note that $\Gamma(\alpha) = (\alpha-1)\Gamma(\alpha-1)$. If $\alpha$ is a positive integer, $\Gamma(\alpha) = (\alpha-1)!$.
    \end{eg}
    \begin{lem}
        When $\alpha = 1$, the gamma distribution becomes an exponential distribution.
    \end{lem}
    \begin{proofing}
        Let $X \sim \Gam(1,\lambda)$. The PDF is:
        \begin{equation*}
            f_{X}(x) = \begin{cases}
                \frac{1}{\Gamma(1)}\lambda e^{-\lambda x}(\lambda x)^{1-1}, &x \geq 0\\
                0, &x < 0
            \end{cases} = \begin{cases}
                \lambda e^{-\lambda x}, &x \geq 0\\
                0, &x < 0
            \end{cases}.
        \end{equation*}
    \end{proofing}
    \begin{eg}\named{Chi-Squared Distribution} $Y \sim \chi^{2}(n)$\\
        Assume that $X_{1},X_{2},\dots,X_{n}$ are independent standard normal random variables. Let $Y = \sum_{i = 1}^{n}X_{i}^{2}$. We say $Y$ has a $\chi^{2}$-distribution with parameter $n$ if it has the PDF:
        \begin{equation*}
            f_{Y}(x) = \begin{cases}
                \frac{1}{\Gamma(\frac{n}{2})}2^{-\frac{n}{2}}x^{\frac{n}{2}-1}e^{-\frac{x}{2}}, &x \geq 0\\
                0, &x < 0
            \end{cases} = \frac{1}{\Gamma(\frac{n}{2})}2^{-\frac{n}{2}}x^{\frac{n}{2}-1}e^{-\frac{x}{2}}\mathbf{1}_{x \geq 0}.
        \end{equation*}
    \end{eg}
    \begin{rem}
        $\Gamma(\frac{1}{2}) = \sqrt{\pi}$.
    \end{rem}
    \begin{lem}
        A random variable $\chi^{2}(n)$ is equivalent to $\Gam(\frac{n}{2},\frac{1}{2})$.
    \end{lem}
    \begin{proofing}
        Let $X \sim \Gam(\frac{n}{2},\frac{1}{2})$. Substituting $\alpha = \frac{n}{2}$ and $\lambda = \frac{1}{2}$ into the PDF of $X$, we have:
        \begin{equation*}
            f_{X}(x) = \begin{cases}
                \frac{1}{2\Gamma(\frac{n}{2})}e^{-\frac{x}{2}}\left(\frac{x}{2}\right)^{\frac{n}{2}-1}, &x \geq 0\\
                0, &x < 0
            \end{cases} = \begin{cases}
            \frac{1}{\Gamma(\frac{n}{2})}2^{-\frac{n}{2}}x^{\frac{n}{2}-1}e^{-\frac{x}{2}}, &x \geq 0\\
            0, &x < 0
            \end{cases}.
        \end{equation*}
        Therefore, $X \sim \chi^{2}(n)$.
    \end{proofing}
    \begin{lem}
        Given that $V \sim \chi^{2}(n_{1})$ and $W \sim \chi^{2}(n_{2})$, if $V$ and $W$ are independent, then $V+W \sim \chi^{2}(n_{1}+n_{2})$.
    \end{lem}
    \begin{proofing}
        Let $V = X_{1}^{2}+X_{2}^{2}+\dots+X_{n_{1}}^{2}$ and $W = Z_{1}^{2}+Z_{2}^{2}+\dots+Z_{n_{2}}^{2}$, where $X_{i},Z_{j} \sim \N(0,1)$ for all $i,j$.
        \begin{equation*}
            V+W = X_{1}^{2}+X_{2}^{2}+\dots+X_{n_{1}}^{2}+Z_{1}^{2}+Z_{2}^{2}+\dots+Z_{n_{2}}^{2} \sim \chi^{2}(n_{1}+n_{2}),
        \end{equation*}
    \end{proofing}
    \newpage

    We can derive further distributions from the chi-squared distribution.
    \begin{eg}\named{Student's t-Distribution} $W \sim t(n)$\\
        Given $Y \sim \chi^{2}(n)$ and $Z \sim \N(0,1)$, if $Y$ and $Z$ are independent, let:
        \begin{equation*}
            W = \frac{Z}{\sqrt{\frac{Y}{n}}}.
        \end{equation*}
        The random variable $W$ follows the $t$-distribution with $n$ degrees of freedom and has the PDF:
        \begin{equation*}
            f(w) = \frac{\Gamma\left(\frac{n+1}{2}\right)}{\sqrt{n\pi}\Gamma\left(\frac{n}{2}\right)}\left(1+\frac{w^{2}}{n}\right)^{-\frac{n+1}{2}}.
        \end{equation*}
    \end{eg}
    \begin{rem}
        If $W \sim t(1)$, then from the PDF:
        \begin{equation*}
            f(w) = \frac{\Gamma(1)}{\sqrt{\pi}\Gamma\left(\frac{1}{2}\right)}(1+w^{2})^{-1} = \frac{1}{\pi(1+w^{2})}.
        \end{equation*}
        This means that $W \sim \Cauchy(0)$.
    \end{rem}
    \begin{rem}
        Fixing $Y = y$ for some constant $y \neq 0$, we can easily find that $W \sim \N(0,\frac{n}{y})$.
    \end{rem}
    \begin{eg}\named{Beta Distribution} $X \sim \Beta(a, b)$\\
        A random variable $X$ has a beta distribution with parameters $a$ and $b$ if it has the PDF:
        \begin{equation*}
            f_{X}(x) = \begin{cases}
                \frac{1}{B(a, b)}x^{a-1}(1-x)^{b-1}, &0 < x < 1\\
                0, &\text{Otherwise}
            \end{cases} = \frac{1}{B(a, b)}x^{a-1}(1-x)^{b-1}\mathbf{1}_{0 < x < 1},
        \end{equation*}
        where $B(a, b)$ is called the \textbf{beta function}, defined as:
        \begin{equation*}
            B(a, b) = \int_{0}^{1}x^{a-1}(1-x)^{b-1}\,dx = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}.
        \end{equation*}
    \end{eg}
    \begin{eg}\named{F-Distribution} $F \sim F(r_{1},r_{2})$\\
        Assume that $X$ and $Y$ are independent random variables with $X \sim \chi^{2}(r_{1})$ and $Y \sim \chi^{2}(r_{2})$. Let:
        \begin{equation*}
            F = \frac{\frac{X}{r_{1}}}{\frac{Y}{r_{2}}}.
        \end{equation*}
        Then $F$ has an F-distribution with $r_{1}$ and $r_{2}$ degrees of freedom, and its PDF is:
        \begin{equation*}
            f_{F}(w) = \frac{\Gamma\left(\frac{r_{1}+r_{2}}{2}\right)}{\Gamma\left(\frac{r_{1}}{2}\right)\Gamma\left(\frac{r_{2}}{2}\right)}\left(\frac{r_{1}}{r_{2}}\right)^{\frac{r_{1}}{2}}w^{\frac{r_{1}}{2}-1}\left(1+\frac{r_{1}w}{r_{2}}\right)^{-\frac{r_{1}+r_{2}}{2}},
        \end{equation*}
        where $0 < w < \infty$.
    \end{eg}
    \begin{lem}
        If $U \sim F(r_{1},r_{2})$, then $\frac{1}{U} \sim F(r_{2},r_{1})$.
    \end{lem}
    \begin{proofing}
        By definition:
        \begin{equation*}
            U = \frac{\frac{X}{r_{1}}}{\frac{Y}{r_{2}}},
        \end{equation*}
        where $X \sim \chi^{2}(r_{1})$ and $Y \sim \chi^{2}(r_{2})$. Therefore:
        \begin{equation*}
            \frac{1}{U} = \frac{\frac{Y}{r_{2}}}{\frac{X}{r_{1}}} \sim F(r_{2},r_{1}).
        \end{equation*}
    \end{proofing}
    \newpage

\section{Functions of Continuous Random Variables}
    Given a continuous random variable $X$ and a function $g$ such that $g(X)$ is also a random variable, we have $\expect{g(X)} = \int_{-\infty}^{\infty} g(x)f_{X}(x)\,dx$. Therefore, we only need $f_{X}(x)$ to compute $\expect{g(X)}$. However, very often, we want to determine the distribution of $g(X)$.
    \begin{eg}
        Assume that $X$ is a continuous random variable with PDF $f_{X}(x)$. Let $Y = g(X)$ be a continuous random variable. How do we find the PDF $f_{Y}(y)$? We first work with $F_{Y}(y)$. Let $g^{-1}(A) = \{x \in \mathbb{R}:g(x) \in A\}$.
        \begin{align*}
            F_{Y}(y) &= \prob(Y \leq y) = \prob(g(X) \in (-\infty,y]) = \prob(X \in g^{-1}((-\infty,y])) = \int_{g^{-1}((-\infty,y])}f_{X}(x)\,dx,\\
            f_{Y}(y) &= \pdv*{\int_{g^{-1}((-\infty,y])}f_{X}(x)\,dx}{y}.
        \end{align*}
    \end{eg}
    \begin{eg}
        Let $X \sim \N(0,1)$. Let $Y = g(X) = X^{2}$. We want to find the PDF $f_{Y}(y)$.
        \begin{align*}
            F_{Y}(y) &= \prob(Y \leq y) = \prob(-\sqrt{y} \leq X \leq \sqrt{y}) = \Phi(\sqrt{y})-\Phi(-\sqrt{y}) = 2\Phi(\sqrt{y})-1,\\
            f_{Y}(y) &= F'(y) = 2\phi(\sqrt{y})\left(\frac{1}{2\sqrt{y}}\right) = \frac{1}{\sqrt{y}}\phi(\sqrt{y}) = \begin{cases}
                \frac{1}{\sqrt{2\pi y}}\exp\left(\frac{-y}{2}\right), &y > 0,\\
                0, &y < 0.
            \end{cases}
        \end{align*}
        We have $X^{2} \sim \chi^{2}(1)$. (This is a distribution.)
    \end{eg}
    \begin{thm}
        In the case where $g(x)$ is strictly monotonic (strictly increasing or strictly decreasing) and differentiable, let $Y = g(X)$. We have:
        \begin{equation*}
            f_{Y}(y) = \begin{cases}
                f_{X}(g^{-1}(y))\abs{\pdv*{g^{-1}(y)}{y}}, &\text{if }y = g(x)\text{ for some }x,\\
                0, &\text{Otherwise}.
            \end{cases}
        \end{equation*}
    \end{thm}
    \begin{proofing}
        If $g(x)$ is a strictly increasing function:
        \begin{align*}
            F_{Y}(y) &= \prob(g(X) \leq y) = \prob(X \leq g^{-1}(y)) = F_{X}(g^{-1}(y)),\\
            f_{Y}(y) &= F_{Y}'(y) = f_{X}(g^{-1}(y))\pdv*{g^{-1}(y)}{y} = f_{X}(g^{-1}(y))\abs{\pdv*{g^{-1}(y)}{y}}.
        \end{align*}
        If $g(x)$ is a strictly decreasing function:
        \begin{align*}
            F_{Y}(y) &= \prob(g(X) \leq y) = \prob(X \geq g^{-1}(y)) = 1-F_{X}(g^{-1}(y)),\\
            f_{Y}(y) &= F_{Y}'(y) = -f_{X}(g^{-1}(y))\pdv*{g^{-1}(y)}{y} = f_{X}(g^{-1}(y))\abs{\pdv*{g^{-1}(y)}{y}}.
        \end{align*}
    \end{proofing}
    We can consider the multivariable case.
    \begin{eg}
        Suppose two random variables $X$ and $Y$ are jointly continuous with JPDF $f_{X,Y}$. Given that $U = g(X,Y)$ and $V = h(X,Y)$, what is $f_{U,V}(u, v)$? To simplify the process, we need to make the following assumptions:
        \begin{enumerate}
            \item $X$ and $Y$ can be uniquely solved from $U$ and $V$. (There exists only one pair of functions $a$ and $b$ such that $X = a(U,V)$ and $Y = b(U,V)$.)
            \item The functions $g$ and $h$ are differentiable, and the Jacobian determinant:
            \begin{equation*}
                J(x, y) = \begin{vmatrix}
                    \pdv{g}{x} & \pdv{g}{y}\\
                    \pdv{h}{x} & \pdv{h}{y}
                \end{vmatrix} \neq 0.
            \end{equation*}
        \end{enumerate}
        Then:
        \begin{equation*}
            f_{U,V}(u, v) = \frac{1}{\abs{J(x, y)}}f_{X,Y}(x, y) = \begin{cases}
                \frac{1}{\abs{J(a(u, v),b(u, v))}}f_{X,Y}(a(u, v),b(u, v)), &(u, v) = (g(x, y),h(x, y))\text{ for some }x,y,\\
                0, &\text{Otherwise}.
            \end{cases}
        \end{equation*}
    \end{eg}
    \newpage

    \begin{eg}
        Given two jointly continuous random variables $X_{1}$ and $X_{2}$ with their JPDF $f_{X_{1},X_{2}}$.\\
        Let $Y_{1} = X_{1}+X_{2}$ and $Y_{2} = X_{1}-X_{2}$.
        \begin{align*}
            X_{1} &= \frac{Y_{1}+Y_{2}}{2} = a(Y_{1},Y_{2}), & X_{2} &= \frac{Y_{1}-Y_{2}}{2} = b(Y_{1},Y_{2}), & J(x_{1},x_{2}) &= \begin{vmatrix}
                1 & 1\\
                1 & -1
            \end{vmatrix} = -2.
        \end{align*}
        \begin{equation*}
            f_{Y_{1},Y_{2}}(y_{1},y_{2}) = \frac{1}{\abs{J(x_{1},x_{2})}}f_{X_{1},X_{2}}(x_{1},x_{2}) = \frac{1}{2}f_{X_{1},X_{2}}\left(\frac{y_{1}+y_{2}}{2},\frac{y_{1}-y_{2}}{2}\right).
        \end{equation*}
        More specifically, if $X_{1} \sim \N(0,1)$, $X_{2} \sim \N(0,1)$, and $X_{1} \independent X_{2}$:
        \begin{align*}
            f_{X_{1},X_{2}}(x_{1},x_{2}) &= \frac{1}{2\pi}e^{-\frac{1}{2}(x_{1}^{2}+x_{2}^{2})},\\
            f_{Y_{1},Y_{2}}(y_{1},y_{2}) &= \frac{1}{2}f_{X_{1},X_{2}}\left(\frac{y_{1}+y_{2}}{2},\frac{y_{1}-y_{2}}{2}\right)\\
            &= \frac{1}{4\pi}e^{-\frac{1}{2}\left(\left(\frac{1}{2}(y_{1}+y_{2})\right)^{2}+\left(\frac{1}{2}(y_{1}-y_{2})\right)^{2}\right)}\\
            &= \frac{1}{4\pi}e^{-\frac{1}{4}(y_{1}^{2}+y_{2}^{2})}.
        \end{align*}
        Therefore, $Y_{1} \independent Y_{2}$, and we have $Y_{1} \sim \N(0,2)$ and $Y_{2} \sim \N(0,2)$.
    \end{eg}
    \begin{eg}
        We do the same thing as the previous example, but instead, we have two independent random variables $X_{1} \sim \U[0,1]$ and $X_{2} \sim \U[0,1]$. For all $x_{1},x_{2} \in \mathbb{R}$:
        \begin{align*}
            f_{X_{1},X_{2}}(x_{1},x_{2}) &= \begin{cases}
                1, &x_{1},x_{2} \in [0,1],\\
                0, &\text{Otherwise}.
            \end{cases} = \mathbf{1}_{0 \leq x_{1} \leq 1,0 \leq x_{2} \leq 1},\\
            f_{Y_{1},Y_{2}}(y_{1},y_{2}) &= \frac{1}{2}f_{X_{1},X_{2}}\left(\frac{y_{1}+y_{2}}{2},\frac{y_{1}-y_{2}}{2}\right)\\
            &= \frac{1}{2}\mathbf{1}_{0 \leq y_{1}+y_{2} \leq 2,0 \leq y_{1}-y_{2} \leq 2}.
        \end{align*}
    \end{eg}

\chapter{Expectation}
    \label{Chapter 6 (Expectation)}
    \textit{In this chapter, if we are discussing more than one random variable, we assume that either all are discrete or all are continuous. In a practical sense, it is possible to have some discrete and others continuous, but we will not tackle this situation. It is relatively easy to prove all the theorems and lemmas in this case once you know how to prove them when all are discrete or all are continuous.}
\section{Introduction to Expectation}
    In real life, we often want to know the expected final result given the probabilities we calculated. The result is usually a theoretical approximation of the empirical average. Assume we have random variables $X_{1},X_{2},\dots,X_{N}$ which take values in $\{x_{1},x_{2},\dots,x_{n}\}$ with probability mass function $f_{X}(x)$. We get an empirical average:
    \begin{equation*}
        \mu = \frac{1}{N}\sum_{i = 1}^{N}X_{i} \approx \frac{1}{N}\sum_{i = 1}^{N}x_{i}Nf(x_{i}) = \sum_{i = 1}^{N}x_{i}f(x_{i}).
    \end{equation*}
    This gives us the formula for the expectation of a discrete random variable. However, for continuous random variables, the probability at every single point is $0$. To make sense of this, we use the probability density function to obtain the expectation:
    \begin{equation*}
        \mu = \int_{-\infty}^{\infty}xf(x)\,dx.
    \end{equation*}
    \begin{defn}
        The \textbf{mean value}, \textbf{expectation}, or \textbf{expected value} of a discrete random variable $X$ with PMF $f_{X}$ is defined as:
        \begin{equation*}
            \expect{X} = \expect(X): = \sum_{x:f_{X}(x) > 0}xf_{X}(x),
        \end{equation*}
        whenever this sum is absolutely convergent.\\
        The \textbf{expectation} of a continuous random variable $X$ with PDF $f_{X}$ is defined as:
        \begin{equation*}
            \expect{X} = \int_{-\infty}^{\infty}xf_{X}(x)\,dx,
        \end{equation*}
        whenever this integral exists.
    \end{defn}
    \begin{rem}
        Due to absolute convergence, we can usually define $\expect{X}$ only if $\expect\abs{X}$ exists.
    \end{rem}
    \begin{eg}
        Suppose a product is sold seasonally. Let $b$ be the net profit for each sold unit, $\ell$ be the net loss for each unsold unit, and $X$ be the number of products ordered by customers. If $y$ units are stocked, what is the expected profit $Q(y)$?
        \begin{equation*}
            Q(y) = \begin{cases}
                bX-(y-X)\ell, &X \leq y,\\
                by, &X > y.
            \end{cases}
        \end{equation*}
    \end{eg}
    \newpage

    \begin{thm}
        Given two random variables $X$ and $Y$, the expectation operator $\expect$ has the following properties:
        \begin{enumerate}
            \item For any $a \leq b$, if $a \leq X \leq b$, then $a \leq \expect{X} \leq b$.
            \item If $X \geq 0$, then $\expect{X} \geq 0$.
            \item If $a,b \in \mathbb{R}$, then $\expect(aX+bY) = a\expect{X}+b\expect{Y}$.
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item If $X$ is discrete, then:
            \begin{align*}
                \expect{X} &= \sum_{x}xf_{X}(x) \geq \sum_{x}af_{X}(x) = a, & \expect{X} &= \sum_{x}xf_{X}(x) \leq \sum_{x}bf_{X}(x) = b.
            \end{align*}
            If $X$ is continuous, then:
            \begin{align*}
                \expect{X} &= \int_{-\infty}^{\infty} xf_{X}(x) \geq \int_{-\infty}^{\infty} af_{X}(x) = a, & \expect{X} &= \int_{-\infty}^{\infty} xf_{X}(x) \leq \int_{-\infty}^{\infty} bf_{X}(x) = b.
            \end{align*}
            Therefore, we have $a \leq \expect{X} \leq b$.
            \item If $X$ is discrete, since $f_{X}(x) \geq 0$ for all $x \geq 0$, $\expect{X} = \sum_{x}xf_{X}(x) \geq 0$ if $X \geq 0$.\\
            If $X$ is continuous, since $f_{X}(x) \geq 0$ for all $x \geq 0$, $\expect{X} = \int_{0}^{\infty}xf_{X}(x)\,dx \geq 0$.
            \item When $X$ and $Y$ are discrete:
            \begin{align*}
                \expect(aX+bY) &= \sum_{x,y}(ax+by)f_{X,Y}(x, y)\\
                &= a\sum_{x}x\left(\sum_{y}f_{X,Y}(x, y)\right)+b\sum_{y}y\left(\sum_{x}f_{X,Y}(x, y)\right)\\
                &= a\sum_{x}xf_{X}(x)+b\sum_{y}yf_{Y}(y) = a\expect{X}+b\expect{Y}.
            \end{align*}
            When $X$ and $Y$ are continuous:
            \begin{align*}
                \expect(aX+bY) &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}(ax+by)f_{X,Y}(x, y)\,dy\,dx\\
                &= a\int_{-\infty}^{\infty} x\int_{-\infty}^{\infty} f_{X,Y}(x, y)\,dy\,dx+b\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} yf_{X,Y}(x, y)\,dy\,dx\\
                &= a\int_{-\infty}^{\infty} xf_{X}(x)\,dx+b\int_{-\infty}^{\infty} yf_{Y}(y)\,dy = a\expect{X}+b\expect{Y}.
            \end{align*}
        \end{enumerate}
    \end{proofing}
    Using induction, we can immediately obtain the following result.
    \begin{lem}\named{Linearity of Expectation} 
        More generally, for any sequence of random variables $\{X_{1},\dots,X_{n}\}$, we have:
        \begin{equation*}
            \expect\left(\sum_{i = 1}^{n}a_{i}X_{i}\right) = \sum_{i = 1}^{n}a_{i}\expect{X_{i}}.
        \end{equation*}
    \end{lem}
    \newpage

    \begin{thm}\named{Tail Sum Formula}
        \label{Chapter 6 (Theorem) Tail Sum Formula}
        If a discrete random variable $X$ has a PMF $f_{X}$ that satisfies $f_{X}(x) = 0$ when $x < 0$, then:
        \begin{equation*}
            \expect{X} = \sum_{k = 0}^{\infty}\prob(X > k).
        \end{equation*}
        If a continuous random variable $X$ has a PDF $f_{X}$ that satisfies $f_{X}(x) = 0$ when $x < 0$, then:
        \begin{equation*}
            \expect{X} = \int_{0}^{\infty}\prob(X > x)\,dx.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        For a discrete random variable $X$ with $f_{X}(x)$ for any $x < 0$:
        \begin{align*}
            \sum_{k = 0}^{\infty}\prob(X > k) &= \sum_{k = 1}^{\infty}\prob(X \geq k)\\
            &= \sum_{k = 1}^{\infty}\sum_{i = k}^{\infty}\prob(X = i)\\
            &= \sum_{k = 1}^{\infty}k\prob(X = k) = \expect{X}.
        \end{align*}
        For a continuous random variable $X$ with $f_{X}(x)$ for any $x < 0$:
        \begin{align*}
            \int_{0}^{\infty}\prob(X > x)\,dx &= \int_{0}^{\infty}\int_{x}^{\infty}f_{X}(y)\,dy\,dx\\
            &= \int_{0}^{\infty}\int_{0}^{y}f_{X}(y)\,dx\,dy\\
            &= \int_{0}^{\infty}yf_{X}(y)\,dy = \expect{X}.
        \end{align*}
    \end{proofing}
    The following lemma is a formula developed specifically for proving the next theorem.
    \begin{lem}
        \label{Chapter 6 (Lemma) Expectation as Integral of CDF}
        If a continuous random variable $X$ has a PDF $f_{X}$ with $f_{X}(x) = 0$ when $x > 0$, and a CDF $F_{X}$, then:
        \begin{equation*}
            \expect{X} = \int_{-\infty}^{0}-F_{X}(x)\,dx.
        \end{equation*}
    \end{lem}
    \begin{proofing}
        \begin{align*}
            \int_{-\infty}^{0}-F_{X}(x)\,dx &= \int_{-\infty}^{0}\int_{-\infty}^{x}-f_{X}(y)\,dy\,dx\\
            &= \int_{-\infty}^{0}\int_{y}^{0}-f_{X}(y)\,dx\,dy\\
            &= \int_{-\infty}^{0}yf_{X}(y)\,dy = \expect{X}.
        \end{align*}
    \end{proofing}
    \newpage

    \begin{thm}
        \label{Chapter 6 (Theorem) Expectation of Function of Random Variable}
        Given a function $g:\mathbb{R} \to \mathbb{R}$ and a random variable $X$:
        \begin{enumerate}
            \item If $X$ is discrete with a PMF $f_{X}(x)$, and $g(X)$ is still a discrete random variable, then:
            \begin{equation*}
                \expect{g(X)} = \sum_{x}g(x)f_{X}(x),
            \end{equation*}
            whenever this sum is absolutely convergent.
            \item If $X$ is continuous with a PDF $f_{X}(x)$, and $g(X)$ is still a continuous random variable, then:
            \begin{equation*}
                \expect{g(X)} = \int_{-\infty}^{\infty} g(x)f_{X}(x)\,dx,
            \end{equation*}
            whenever this integral exists.
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item Let $Y = g(X)$. We have:
            \begin{align*}
                \sum_{x}g(x)f_{X}(x) &= \sum_{y}\sum_{x:g(x) = y}g(x)f_{X}(x)\\
                &= \sum_{y}y\left(\sum_{x:g(x) = y}\prob(\{\omega \in \Omega:X(\omega) = x\})\right)\\
                &= \sum_{y}y\prob(\{\omega \in \Omega:g(X(\omega)) = g(y)\})\\
                &= \sum_{y}yf_{Y}(y) = \expect{Y} = \expect{g(X)}.
            \end{align*}
            \item Consider that $g(x) \geq 0$ for all $x$. Let $B = \{x:g(x) > y\}$. By Lemma \ref{Chapter 6 (Theorem) Tail Sum Formula}:
            \begin{align*}
                \expect{g(X)} &= \int_{0}^{\infty}\prob(g(X) > y)\,dy\\
                &= \int_{0}^{\infty}\int_{B}f_{X}(x)\,dx\,dy\\
                &= \int_{0}^{\infty}\int_{0}^{g(x)}f_{X}(x)\,dy\,dx\\
                &= \int_{0}^{\infty}g(x)f_{X}(x)\,dx.
            \end{align*}
            Now consider that $g(x) \leq 0$ for all $x$. Let $C = \{x:g(x) < z\}$. By Lemma \ref{Chapter 6 (Lemma) Expectation as Integral of CDF}:
            \begin{align*}
                \expect{g(X)} &= \int_{-\infty}^{0}-F_{g(X)}(z)\,dz\\
                &= \int_{-\infty}^{0}\int_{C}-f_{X}(x)\,dx\,dz\\
                &= \int_{-\infty}^{0}\int_{g(x)}^{0}-f_{X}(x)\,dz\,dx\\
                &= \int_{-\infty}^{0}g(x)f_{X}(x)\,dx.
            \end{align*}
            Combining both cases, if $g(X)$ is a random variable, then:
            \begin{equation*}
                \expect{g(X)} = \int_{0}^{\infty}g(x)f_{X}(x)\,dx+\int_{-\infty}^{0}g(x)f_{X}(x)\,dx = \int_{-\infty}^{\infty} g(x)f_{X}(x)\,dx.
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    \newpage

    \begin{thm}
        Given a function $g:\mathbb{R}^{2} \to \mathbb{R}$ and two random variables $X$ and $Y$:
        \begin{enumerate}
            \item If $X$ and $Y$ are jointly discrete with JPMF $f_{X,Y}(x, y)$, and $g(X,Y)$ is a discrete random variable, then:
            \begin{equation*}
                \expect{g(X,Y)} = \sum_{y}\sum_{x}g(x, y)f_{X,Y}(x, y).
            \end{equation*}
            \item If $X$ and $Y$ are jointly continuous with JPDF $f_{X,Y}(x, y)$, and $g(X,Y)$ is a continuous random variable, then:
            \begin{equation*}
                \expect{g(X,Y)} = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} g(x, y)f_{X,Y}(x, y)\,dx\,dy.
            \end{equation*}
        \end{enumerate} 
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item Let $Z = g(X,Y)$. We have:
            \begin{align*}
                \sum_{x,y}g(x, y)f_{X,Y}(x, y) &= \sum_{z}\sum_{x,y:g(x, y) = z}g(x, y)f_{X,Y}(x, y)\\
                &= \sum_{z}z\left(\sum_{x,y:g(x, y) = z}\prob((X,Y) = (x, y))\right)\\
                &= \sum_{z}z\prob(\{\omega \in \Omega:g(X,Y)(\omega) = z\})\\
                &= \sum_{z}zf_{Z}(z) = \expect{Z} = \expect{g(X,Y)}.
            \end{align*}
            \item \textit{We will not prove this case. Just note that it works similarly to the previous theorem.}
        \end{enumerate}
    \end{proofing}
    \begin{rem}
        We may generalize this to a random vector. 
    \end{rem}
    We have some special terms for specific expectations.
    \begin{defn}
        Let $k$ be a positive integer. We have special terms for the following expectations:
        \begin{enumerate}
            \item The \textbf{$k$-th moment} $m_{k}$ of $X$ is defined as:
            \begin{equation*}
                m_{k} = \expect(X^{k}).
            \end{equation*}
            \item The \textbf{$k$-th central moment} $\alpha_{k}$ is defined as:
            \begin{equation*}
                \alpha_{k} = \expect(X-\expect{X})^{k} = \expect(X-m_{1})^{k}.
            \end{equation*}
        \end{enumerate}
    \end{defn}
    \begin{rem}
        Not all random variables have $k$-th moments for all positive integers $k$.
    \end{rem}
    \begin{rem}
        We cannot use a finite number of moments to uniquely determine a distribution with $k$-th moments for all positive integers $k$.
    \end{rem}
    \begin{eg}
        Let $X \sim \N(\mu,\sigma^{2})$. We have:
        \begin{align*}
            m_{1} &= \expect{X} = \mu, & m_{2} &= \expect(X^{2}) = \sigma^{2}+\mu^{2}, & m_{3} &= \expect(X^{3}) = 3\mu\sigma^{2}+\mu^{3}, & m_{4} &= \expect(X^{4}) = 3\sigma^{4}+6\mu^{2}\sigma^{2}+\mu^{4}.
        \end{align*}
    \end{eg}
    \begin{eg}
        Let $Y \sim \Exp(\lambda)$. We have:
        \begin{align*}
            m_{1} &= \expect{Y} = \frac{1}{\lambda}, & m_{2} &= \expect(Y^{2}) = \frac{2}{\lambda^{2}}, & m_{3} &= \expect(Y^{3}) = \frac{6}{\lambda^{3}}, & m_{4} &= \expect(Y^{4}) = \frac{24}{\lambda^{4}}.
        \end{align*}
    \end{eg}
    \newpage

    \begin{defn}
        Given a random variable $X$:
        \begin{enumerate}
            \item The \textbf{mean} of $X$ is the $1$st moment, denoted by $\mu$, defined as:
            \begin{equation*}
                \mu = \expect{X}.
            \end{equation*}
            \item The \textbf{variance} of $X$ is the $2$nd central moment, denoted by $\Var(X)$, defined as:
            \begin{equation*}
                \Var(X) = \expect(X-\mu)^{2} = \expect(X^{2})-\mu^{2}.
            \end{equation*}
            \item The \textbf{standard deviation} of $X$, denoted by $\sigma$, is defined as:
            \begin{equation*}
                \sigma = \sqrt{\Var(X)}.
            \end{equation*}
        \end{enumerate}
    \end{defn}
    \begin{lem}
        If two random variables $X$ and $Y$ are independent, then:
        \begin{equation*}
            \expect(XY) = \expect{X}\expect{Y}.
        \end{equation*}
    \end{lem}
    \begin{proofing}
        If $X$ and $Y$ are both discrete:
        \begin{align*}
            \expect(XY) &= \sum_{x,y}xyf_{X,Y}(x, y)\\
            &= \sum_{x,y}xyf_{X}(x)f_{Y}(y)\\
            &= \sum_{x}xf_{X}(x)\sum_{y}yf_{Y}(y)\\
            &= \expect{X}\expect{Y}.
        \end{align*}
        If $X$ and $Y$ are both continuous:
        \begin{align*}
            \expect(XY) &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} xyf_{X,Y}(x, y)\,dy\,dx\\
            &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} xyf_{X}(x)f_{Y}(y)\,dy\,dx\\
            &= \int_{-\infty}^{\infty} xf_{X}(x)\,dx\int_{-\infty}^{\infty} yf_{Y}(y)\,dy\\
            &= \expect{X}\expect{Y}.
        \end{align*}
    \end{proofing}
    \begin{rem}
        The converse is not generally true.
    \end{rem}
    \newpage

    We may generalize this to a function of $X$ and $Y$. This is very important, as it implies that two resultant random variables, $g(X)$ and $h(Y)$, are "uncorrelated" as long as the two random variables from the domain, $X$ and $Y$, are independent.
    \begin{thm}
        Given two random variables $X$ and $Y$ and two functions $g,h:\mathbb{R} \to \mathbb{R}$ such that $g(X)$ and $h(Y)$ are still random variables. Let $X$ and $Y$ be independent. If $\expect(g(X)h(Y))$, $\expect{g(X)}$, and $\expect{h(Y)}$ exist, then:
        \begin{equation*}
            \expect(g(X)h(Y)) = \expect{g(X)}\expect{h(Y)}.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        If $X$ and $Y$ are both discrete:
        \begin{align*}
            \expect(g(X)h(Y)) &= \sum_{x,y}g(x)h(y)f_{X,Y}(x, y)\\
            &= \sum_{x,y}g(x)h(y)f_{X}(x)f_{Y}(y)\\
            &= \sum_{x}g(x)f_{X}(x)\sum_{y}h(y)f_{Y}(y)\\
            &= \expect{g(X)}\expect{h(Y)}.
        \end{align*}
        If $X$ and $Y$ are both continuous:
        \begin{align*}
            \expect(g(X)h(Y)) &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} g(x)h(y)f_{X,Y}(x, y)\,dy\,dx\\
            &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} g(x)h(y)f_{X}(x)f_{Y}(y)\,dy\,dx\\
            &= \int_{-\infty}^{\infty} g(x)f_{X}(x)\,dx\int_{-\infty}^{\infty} h(y)f_{Y}(y)\,dy\\
            &= \expect{g(X)}\expect{h(Y)}.
        \end{align*}
    \end{proofing}
    We can use the properties of expectations to deduce the properties of variance.
    \begin{thm}
        For random variables $X$ and $Y$:
        \begin{enumerate}
            \item $\Var(aX+b) = a^{2}\Var(X)$ for all $a,b \in \mathbb{R}$.
            \item $\Var(X+Y) = \Var(X)+\Var(Y)$ if $X$ and $Y$ are uncorrelated.
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item Using the linearity of $\expect$:
            \begin{align*}
                \Var(aX+b) &= \expect((aX+b-\expect(aX+b))^{2})\\
                &= \expect(a^{2}(X-\expect{X})^{2})\\
                &= a^{2}\expect((X-\expect{X})^{2})\\
                &= a^{2}\Var(X).
            \end{align*}
            \item When $X$ and $Y$ are uncorrelated:
            \begin{align*}
                \Var(X+Y) &= \expect((X+Y-\expect(X+Y))^{2})\\
                &= \expect((X-\expect{X})^{2}+2(XY-\expect{X}\expect{Y})+(Y-\expect{Y})^{2})\\
                &= \Var(X)+2(\expect(XY)-\expect(X)\expect(Y))+\Var(Y)\\
                &= \Var(X)+\Var(Y).
            \end{align*}
        \end{enumerate}
    \end{proofing}
    \newpage

\section{Conditional Expectation}
    Sometimes, it is not practical to find $\expect{X}$ itself. What if we want to find the expectation of $X$ given that another result occurred? Similar to conditional probability, we may also have conditional expectation.
    \begin{defn}
        Given two random variables $X$ and $Y$:
        \begin{enumerate}
            \item If $X$ and $Y$ are discrete, then the \textbf{conditional expectation} $\psi$ of $Y$ given $X = x$ for any $x$ is defined as:
            \begin{equation*}
                \psi(x) = \expect(Y|X = x) = \sum_{y}yf_{Y|X}(y|x).
            \end{equation*}
            \item If $X$ and $Y$ are continuous, then the \textbf{conditional expectation} $\psi$ of $Y$ given $X = x$ for any $x$ is defined as:
            \begin{equation*}
                \psi(x) = \expect(Y|X = x) = \int_{-\infty}^{\infty} yf_{Y|X}(y|x)\,dy.
            \end{equation*}
        \end{enumerate}
        The \textbf{conditional expectation} $\psi$ of $Y$ given $X$ is defined as:
        \begin{equation*}
            \psi(X) = \expect(Y|X).
        \end{equation*}
    \end{defn}
    \begin{eg}
        Assume we roll a fair die.
        \begin{align*}
            \Omega &= \{1,2,\dots,6\}, & Y(\omega) &= \omega, & X(\omega) &= \begin{cases}
                1, &\omega \in \{2,4,6\},\\
                0, &\omega \in \{1,3,5\}.
            \end{cases}
        \end{align*}
        We try to guess $Y$. If we do not have any information about $X$:
        \begin{equation*}
            \expect{Y} = \argmin_{e}\expect{(Y-e)^{2}} = 3.5.
        \end{equation*}
        If we know that $X = x$, we have two cases: $X = 1$ and $X = 0$:
        \begin{align*}
            f_{Y|X}(y|1) &= \frac{\prob(X = 1,Y = y)}{\prob(X = 1)} = \begin{cases}
                \frac{1}{3}, &y = 2,4,6,\\
                0, &y = 1,3,5.
            \end{cases}, & f_{Y|X}(y|0) &= \frac{\prob(X = 0,Y = y)}{\prob(X = 0)} = \begin{cases}
                0, &y = 2,4,6,\\
                \frac{1}{3}, &y = 1,3,5.
            \end{cases}\\
            \expect(Y|X = 1) &= \sum_{y}yf_{Y|X}(y|1) = \frac{2+4+6}{3} = 4, & \expect(Y|X = 0) &= \frac{1+3+5}{3} = 3.
        \end{align*}
        Finally, if we want to guess $Y$ based on the future information of $X$:
        \begin{equation*}
            \psi(X) = \expect(Y|X) = 4(\mathbf{1}_{X = 1})+3(\mathbf{1}_{X = 0}).
        \end{equation*}
    \end{eg}
    \begin{eg}
        If $Y = X$, then $\psi(X) = \expect(Y|X) = \expect(X|X) = X$.
    \end{eg}
    \begin{eg}
        If $X$ and $Y$ are independent, then $\psi(X) = \expect(Y|X) = \expect(Y)$.
    \end{eg}
    \newpage

    \begin{lem}
        Given two random variables $X$ and $Y$, the following properties hold:
        \begin{enumerate}
            \item $\expect(aY+bZ|X) = a\expect(Y|X)+b\expect(Z|X)$.
            \item If $Y \geq 0$, then $\expect(Y|X) \geq 0$.
            \item If $X$ and $Y$ are independent, then $\expect(Y|X) = \expect(Y)$.
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item If $X$, $Y$, and $Z$ are discrete, then for all $x$:
            \begin{align*}
                \expect(aY+bZ|X = x) &= \sum_{y,z}(ay+bz)\prob(Y = y,Z = z|X = x)\\
                &= a\sum_{y,z}y\prob(Y = y,Z = z|X = x)+b\sum_{y,z}z\prob(Y = y,Z = z|X = x)\\
                &= a\sum_{y}yf_{Y|X}(y|x)+b\sum_{z}zf_{Z|X}(z|x)\\
                &= a\expect(Y|X = x)+b\expect(Z|X = x).
            \end{align*}
            If $X$, $Y$, and $Z$ are continuous, then for all $x$:
            \begin{align*}
                \expect(aY+bZ|X = x) &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}(ay+bz)\frac{f_{Y,Z,X}(y,z,x)}{f_{X}(x)}\,dy\,dz\\
                &= a\int_{-\infty}^{\infty} y\int_{-\infty}^{\infty}\frac{f_{Y,Z,X}(y,z,x)}{f_{X}(x)}\,dz\,dy+b\int_{-\infty}^{\infty} z\int_{-\infty}^{\infty}\frac{f_{Y,Z,X}(y,z,x)}{f_{X}(x)}\,dy\,dz\\
                &= a\int_{-\infty}^{\infty} y\frac{f_{Y,X}(y,x)}{f_{X}(x)}\,dy+b\int_{-\infty}^{\infty} z\frac{f_{Z,X}(z,x)}{f_{X}(x)}\,dz\\
                &= a\expect(Y|X = x)+b\expect(Z|X = x).
            \end{align*}
            Therefore, $\expect(aY+bZ|X) = a\expect(Y|X)+b\expect(Z|X)$.
            \item If $X$ and $Y$ are discrete, then for all $x$:
            \begin{equation*}
                \expect(Y|X = x) = \sum_{y}yf_{Y|X}(y|x) \geq 0.
            \end{equation*}
            If $X$ and $Y$ are continuous, then for all $x$:
            \begin{equation*}
                \expect(Y|X = x) = \int_{-\infty}^{\infty} yf_{Y|X}(y|x)\,dy \geq 0.
            \end{equation*}
            Therefore, $\expect(Y|X) \geq 0$ if $Y \geq 0$.
            \item If $X$ and $Y$ are discrete, then for all $x$:
            \begin{equation*}
                \expect(Y|X = x) = \sum_{y}yf_{Y|X}(y|x) = \sum_{y}yf_{Y}(y) = \expect{Y}.
            \end{equation*}
            If $X$ and $Y$ are continuous, then for all $x$:
            \begin{equation*}
                \expect(Y|X = x) = \int_{-\infty}^{\infty} yf_{Y|X}(y|x)\,dy = \int_{-\infty}^{\infty} yf_{Y}(y)\,dy = \expect{Y}.
            \end{equation*}
            Therefore, if $X$ and $Y$ are independent, then $\expect(Y|X) = \expect{Y}$.
        \end{enumerate}
    \end{proofing}
    \newpage

    In fact, we can extend the definition of conditional expectation to $\sigma$-fields.
    \begin{defn}
        Given a random variable $Y$ and a $\sigma$-field $\mathcal{H}\subseteq\mathcal{F}$, $\expect(Y|\mathcal{H})$ is any random variable $Z$ satisfying the following properties:
        \begin{enumerate}
            \item $Z$ is $\mathcal{H}$-measurable. ($Z^{-1}(B) \in \mathcal{H}$ for all $B \in \mathcal{B}(\mathbb{R})$.)
            \item $\expect(Y\mathbf{1}_{A}) = \expect(Z\mathbf{1}_{A})$ for all $A \in \mathcal{H}$.
        \end{enumerate}
    \end{defn}
    \begin{rem}
        Under this definition:
        \begin{equation*}
            \expect(Y|X) = \expect(Y|\sigma(X)).
        \end{equation*}
    \end{rem}
    \begin{thm}\named{Law of Total Expectation}
        \label{Chapter 6 (Theorem) Law of Total Expectation}
        Given two random variables $X$ and $Y$, the conditional expectation $\psi(X) = \expect(Y|X)$ satisfies:
        \begin{equation*}
            \expect{\psi(X)} = \expect{Y}.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        We can apply Theorem \ref{Chapter 6 (Theorem) Expectation of Function of Random Variable}. If $X$ and $Y$ are discrete:
        \begin{align*}
            \expect{\psi(X)} &= \sum_{x}\psi(x)f_{X}(x)\\
            &= \sum_{x,y}yf_{Y|X}(y|x)f_{X}(x)\\
            &= \sum_{x,y}yf_{X,Y}(x, y)\\
            &= \sum_{y}yf_{Y}(y) = \expect{Y}.
        \end{align*}
        If $X$ and $Y$ are continuous:
        \begin{align*}
            \expect{\psi(X)} &= \int_{-\infty}^{\infty}\psi(x)f_{X}(x)\,dx\\
            &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} yf_{Y|X}(y|x)f_{X}(x)\,dy\,dx\\
            &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} yf_{X,Y}(x, y)\,dy\,dx\\
            &= \int_{-\infty}^{\infty} y\int_{-\infty}^{\infty} f_{X,Y}(x, y)\,dx\,dy\\
            &= \int_{-\infty}^{\infty} yf_{Y}(y)\,dy = \expect{Y}.
        \end{align*}
        The proof is similar if one of them is discrete and the other is continuous.
    \end{proofing}
    \begin{eg}
        A miner is trapped in a mine with doors, each leading to a tunnel. Tunnel 1 will help the miner reach safety after 3 hours. However, tunnels 2 and 3 will send the miner back after 5 and 7 hours, respectively. What is the expected amount of time the miner needs to reach safety? (Assume that the miner is memoryless.)\\
        Let $X$ be the amount of time to reach safety, and $Y$ be the door number he chooses first.
        \begin{align*}
            \expect{X} &= \expect(\expect{X|Y}) = \sum_{k = 1}^{3}\expect{X|Y = k}\prob(Y = k) = 3\left(\frac{1}{3}\right)+(\expect{X}+5)\left(\frac{1}{3}\right)+(\expect{X}+7)\left(\frac{1}{3}\right)\\
            \expect{X} &= 15.
        \end{align*}
    \end{eg}
    \newpage

    \begin{eg}
        Continuing from the previous example, what is the expected amount of time the miner needs to reach safety if he chooses door 2 first? Let $\widetilde{X}$ be the amount of time to reach safety if the miner chooses door 2 first.
        \begin{align*}
            \expect(X|Y = 2) &= \sum_{x}x f_{X|Y}(x|2)\\
            &= \sum_{x}x\frac{\prob(X = x, Y = 2)}{\prob(Y = 2)}\\
            &= \sum_{x}x\frac{\prob(\widetilde{X} = x-5, Y = 2)}{\prob(Y = 2)}\\
            &= \sum_{\widetilde{x}}(\widetilde{x}+5)\prob(\widetilde{X} = \widetilde{x})\\
            &= \expect{X}+5
        \end{align*}
    \end{eg}
    \begin{eg}
        A store has $N$ customers in a day, where $N$ is a random variable with $\expect{N} < \infty$. Each customer spends an amount $X_{i}$, where $X_{i}$'s are i.i.d. random variables with $\expect{X} < \infty$. Assume that $N$ and $X_{i}$'s are all independent and $\expect{X_{i}} = \expect{X}$. What is the expected total amount of money spent by all $N$ customers?
        \begin{align*}
            \expect\left(\sum_{i = 1}^{N}X_{i}\right) &= \expect\left(\expect\left(\left.\sum_{i = 1}^{N}X_{i}\right| N\right)\right)\\
            &= \sum_{n = 0}^{\infty}\expect\left(\left.\sum_{i = 1}^{N}X_{i}\right|N = n\right)\prob(N = n)\\
            &= \sum_{n = 0}^{\infty}\sum_{y}y\left(\frac{\prob\left(\sum_{i = 1}^{N}X_{i} = y, N = n\right)}{\prob(N = n)}\right)\prob(N = n)\\
            &= \sum_{n = 0}^{\infty}\sum_{y}y\prob\left(\sum_{i = 1}^{n}X_{i} = y\right)\prob(N = n)\\
            &= \sum_{n = 0}^{\infty}\expect\left(\sum_{i = 1}^{n}X_{i}\right)\prob(N = n)\\
            &= \sum_{n = 0}^{\infty}n\expect{X}\prob(N = n) = \expect{N}\expect{X}
        \end{align*}
    \end{eg}
    The following theorem is a generalization of the Law of Total Expectation.
    \begin{thm}
        \label{Chapter 6 (Theorem) Generalization of Law of Total Expectation}
        Given two random variables $X$ and $Y$, the conditional expectation $\psi(X) = \expect(Y|X)$ satisfies:
        \begin{equation*}
            \expect(\psi(X)g(X)) = \expect(Yg(X))
        \end{equation*}
        for any function $g$ for which both expectations exist.
    \end{thm}
    \begin{proofing}
        We can apply Theorem \ref{Chapter 6 (Theorem) Expectation of Function of Random Variable}. If $X$ and $Y$ are discrete,
        \begin{equation*}
            \expect(\psi(X)g(X)) = \sum_{x}\psi(x)g(x)f_{X}(x) = \sum_{x,y}yf_{Y|X}(y|x)g(x)f_{X}(x) = \sum_{x,y}yf_{X,Y}(x, y)g(x) = \expect(Yg(X))
        \end{equation*}
        If $X$ and $Y$ are continuous,
        \begin{align*}
            \expect(\psi(X)g(X)) &= \int_{-\infty}^{\infty}\psi(x)g(x)f_{X}(x)\,dx\\
            &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} yf_{Y|X}(y|x)f_{X}(x)g(x)\,dy\,dx\\
            &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} yf_{X,Y}(x, y)g(x)\,dy\,dx\\
            &= \expect(Yg(X))
        \end{align*}
    \end{proofing}
    \newpage

    If there is conditional expectation, there is also conditional variance.
    \begin{defn}
        Given two random variables $X$ and $Y$, the \textbf{conditional variance} is defined as:
        \begin{equation*}
            \Var(Y|X) = \expect((Y-\expect(Y|X))^{2}|X)
        \end{equation*}
    \end{defn}
    We can obtain the variance of a random variable based on conditional variance.
    \begin{thm}\named{Law of Total Variance}
        Given two random variables $X$ and $Y$, we have:
        \begin{equation*}
            \Var(Y) = \expect(\Var(Y|X))+\Var(\expect(Y|X))
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Using Theorem \ref{Chapter 6 (Theorem) Law of Total Expectation} and Theorem \ref{Chapter 6 (Theorem) Generalization of Law of Total Expectation},
        \begin{align*}
            \expect(\Var(Y|X))+\Var(\expect(Y|X)) &= \expect(\expect((Y-\expect(Y|X))^{2}|X))+\expect(\expect(Y|X))^{2}-(\expect(\expect(Y|X)))^{2}\\
            &= \expect(Y-\expect(Y|X))^{2}+\expect(\expect(Y|X))^{2}-(\expect{Y})^{2}\\
            &= \expect(Y^{2})-2\expect(Y\expect(Y|X))+\expect(\expect(Y|X))^{2}+\expect(\expect(Y|X))^{2}-(\expect{Y})^{2}\\
            &= \expect(Y^{2})-2\expect(\expect(Y|X))^{2}+2\expect(\expect(Y|X))^{2}-(\expect{Y})^{2}\\
            &= \expect(Y^{2})-(\expect{Y})^{2} = \Var(Y)
        \end{align*}
    \end{proofing}
    Sometimes, we want to find the tendency in the linear relationship between two random variables. This is called the covariance of two random variables.
    \begin{defn}
        The \textbf{covariance} of two random variables $X$ and $Y$ is:
        \begin{equation*}
            \cov(X,Y) = \expect((X-\expect{X})(Y-\expect{Y})) = \expect(XY)-\expect{X}\expect{Y}
        \end{equation*}
    \end{defn}
    \begin{rem}
        The magnitude of covariance is the geometric mean of the variances of two random variables.
    \end{rem}
    \begin{rem}
        The sign represents the linear relationship between the two random variables.
        \begin{enumerate}
            \item If the sign is positive, the two random variables show similar behavior.
            \item If the sign is negative, the two random variables show opposite behavior.
        \end{enumerate}
    \end{rem}
    \begin{lem}
        For any random variables $X$, $Y$, and $Z$, we have:
        \begin{enumerate}
            \item $\Var(X) = \cov(X,X)$.
            \item $\cov(X,Y) = \cov(Y,X)$.
            \item $\cov(X,Y+Z) = \cov(X,Y)+\cov(X,Z)$.
            \item If $X$ and $Y$ are uncorrelated, then $\cov(X,Y) = 0$.
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item \begin{equation*}
                \cov(X,X) = \expect((X-\expect{X})(X-\expect{X})) = \expect(X-\expect{X})^{2} = \Var(X)
            \end{equation*}
            \item \begin{equation*}
                \cov(X,Y) = \expect(XY)-\expect{X}\expect{Y} = \expect(YX)-\expect{Y}\expect{X} = \cov(Y,X)
            \end{equation*}
            \item \begin{equation*}
                \cov(X,Y+Z) = \expect(X(Y+Z))-\expect{X}\expect(Y+Z) = \expect(XY)-\expect{X}\expect{Y}+\expect(XZ)-\expect{X}\expect{Z} = \cov(X,Y)+\cov(X,Z)
            \end{equation*}
            \item If $X$ and $Y$ are independent, then
            \begin{equation*}
                \cov(X,Y) = \expect(XY)-\expect{X}\expect{Y} = \expect{X}\expect{Y}-\expect{X}\expect{Y} = 0
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    \newpage

    \begin{rem}
        In general, for any random variables $X_{1}, X_{2}, \cdots, X_{n}$,
        \begin{equation*}
            \Var(X_{1}+X_{2}+\cdots+X_{n}) = \sum_{i = 1}^{n}\Var(X_{i})+2\sum_{i < j}(\expect(X_{i}X_{j})-\expect{X_{i}}\expect{X_{j}}) = \sum_{i = 1}^{n}\Var(X_{i})+2\sum_{i < j}\cov(X_{i}, X_{j})
        \end{equation*}
    \end{rem}
    \begin{eg}
        If $X_{i}$ are independent and $\Var(X_{i}) = 1$ for all $i$, then:
        \begin{equation*}
            \Var\left(\sum_{i = 1}^{n}X_{i}\right) = \sum_{i = 1}^{n}\Var(X_{i}) = n
        \end{equation*}
        If $X_{i} = X$ for all $i$ and $\Var(X) = 1$, then:
        \begin{equation*}
            \Var\left(\sum_{i = 1}^{n}X_{i}\right) = \Var(nX) = n^{2}
        \end{equation*}
    \end{eg}
    We usually only care about the normalized covariance, which is called the correlation coefficient.
    \begin{defn}
        The \textbf{population correlation coefficient} between two random variables $X$ and $Y$, denoted by $\rho$, is given by:
        \begin{equation*}
            \rho = \frac{\cov(X, Y)}{\sqrt{\Var(X)\Var(Y)}}
        \end{equation*}
        We can determine the relationship between $X$ and $Y$ based on their correlation coefficient.
        \begin{enumerate}
            \item If $\rho > 0$, then $X$ and $Y$ are \textbf{positively correlated}.
            \item If $\rho < 0$, then $X$ and $Y$ are \textbf{negatively correlated}.
            \item If $\rho = 0$, then $X$ and $Y$ are \textbf{uncorrelated}.
        \end{enumerate}
    \end{defn}
    \begin{rem}
        The population correlation coefficient $\rho$ of random variables $X$ and $Y$ satisfies $-1 < \rho < 1$.
    \end{rem}
    \begin{rem}
        If $\rho$ is near $1$ or near $-1$, it indicates a strong linear relationship between $X$ and $Y$.
    \end{rem}
    \begin{rem}
        The constant $\rho$ used in the bivariate normal distribution is the population correlation coefficient.
    \end{rem}
    \begin{eg}
        If $X \sim \N(0, 1)$ and $Y \sim \N(0, 1)$,
        \begin{align*}
            \cov(X, Y) &= \expect(XY)-\expect{X}\expect{Y} = \expect(XY)\\
            &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\frac{y}{\sqrt{2\pi}}e^{-\frac{y^{2}}{2}}\frac{x}{\sqrt{2\pi(1-\rho^{2})}}e^{-\frac{(x-\rho y)^{2}}{2(1-\rho^{2})}}\,dx\,dy\\
            &= \int_{-\infty}^{\infty}\frac{y}{\sqrt{2\pi}}e^{-\frac{y^{2}}{2}}\rho y\,dy = \rho\int_{-\infty}^{\infty} y^{2}\phi(y)\,dy = \rho
        \end{align*}
    \end{eg}
    \begin{lem}
        Two random variables are uncorrelated if $\expect(XY) = \expect{X}\expect{Y}$.
    \end{lem}
    \begin{proofing}
        Based on the definition of the correlation coefficient, $\rho = 0$ if and only if $\cov(X, Y) = 0$. Therefore,
        \begin{equation*}
            \cov(X, Y) = \expect(XY)-\expect{X}\expect{Y} = 0 \iff \expect(XY) = \expect{X}\expect{Y}
        \end{equation*}
    \end{proofing}
    \newpage

    \begin{rem}
        If $X$ and $Y$ are independent, then they are uncorrelated. The converse is generally not true.
    \end{rem}
    \begin{eg}
        Let $X$ be such that $f_{X}(0) = f_{X}(1) = f_{X}(-1) = \frac{1}{3}$ and $Y = \mathbf{1}_{X = 0}$.
        \begin{equation*}
            \expect{X}\expect{Y} = 0 = \expect(XY)
        \end{equation*}
        However,
        \begin{align*}
            \prob(X = 0, Y = 0) &= 0 & \prob(X = 0) &= \frac{1}{3} & \prob(Y = 0) &= \frac{2}{3} & \prob(X = 0)\prob(Y = 0) &= \frac{2}{9} \neq 0
        \end{align*}
        Therefore, $X$ and $Y$ are uncorrelated, but they are not independent.
    \end{eg}
    When would the converse be true? It turns out it is true when $X$ and $Y$ are uncorrelated and bivariate normal. 
    \begin{thm}
        \label{Chapter 6 (Theorem) Bivariate normal and uncorrelated}
        Random variables $X \sim \N(\mu_{X}, \sigma_{X}^{2})$ and $Y \sim \N(\mu_{Y}, \sigma_{Y}^{2})$ are bivariate normal and uncorrelated if and only if $X$ and $Y$ are independent normal.
    \end{thm}
    \begin{proofing}
        Since $X$ and $Y$ are uncorrelated, $\cov(X, Y) = 0$ and thus the population correlation coefficient $\rho = 0$.\\
        Therefore, we have:
        \begin{align*}
            f_{X, Y}(x, y) &= \frac{1}{2\pi\sigma_{X}\sigma_{Y}}\exp\left(-\frac{1}{2}\left(\left(\frac{x-\mu_{X}}{\sigma_{X}}\right)^{2}+\left(\frac{y-\mu_{Y}}{\sigma_{Y}}\right)^{2}\right)\right)\\
            &= \left(\frac{1}{\sqrt{2\pi\sigma_{X}^{2}}}\exp\left(-\frac{(x-\mu_{X})^{2}}{2\sigma_{X}^{2}}\right)\right)\left(\frac{1}{\sqrt{2\pi\sigma_{Y}^{2}}}\exp\left(-\frac{(y-\mu_{Y})^{2}}{2\sigma_{Y}^{2}}\right)\right)\\
            &= f_{X}(x)f_{Y}(y)
        \end{align*}
        Therefore, $X$ and $Y$ are independent if $X$ and $Y$ are uncorrelated bivariate normal.\\
        If $X$ and $Y$ are independent normal, then we have:
        \begin{equation*}
            f_{X, Y}(x, y) = f_{X}(x)f_{Y}(y)
        \end{equation*}
        Therefore, $X$ and $Y$ are both uncorrelated and bivariate normal with $\rho = 0$.
    \end{proofing}
    \newpage

\section{Expectation and Variance of Distributions}
    In this section, we will primarily focus on finding the expectation and variance of distributions we have discussed.
    \begin{thm}
        Given a discrete variable $X$:
        \begin{enumerate}
            \item If $X \sim \Bern(p)$, then:
            \begin{align*}
                \expect{X} &= p, & \Var(X) &= p(1-p).
            \end{align*}
            \item If $X \sim \Bin(n,p)$, then:
            \begin{align*}
                \expect{X} &= np, & \Var(X) &= np(1-p).
            \end{align*}
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \item If $X \sim \Bern(p)$:
        \begin{align*}
            \expect{X} &= 0(1-p) + 1(p) = p, & \Var(X) &= \expect(X^{2}) - (\expect{X})^{2} = p - p^{2} = p(1-p).
        \end{align*}
        \item If $X \sim \Bin(n,p)$, then by definition, $X = Y_{1} + \cdots + Y_{n}$ where $Y_{i} \sim \Bern(p)$ and are independent. Therefore:
        \begin{align*}
            \expect{X} &= \sum_{i = 1}^{n}\expect{Y_{i}} = np, & \Var(X) &= \sum_{i = 1}^{n}\Var(Y_{i}) = np(1-p).
        \end{align*}
    \end{proofing}
    \begin{thm}
        If $X \sim \Geom(p)$, then:
        \begin{align*}
            \expect{X} &= \frac{1}{p}, & \Var(X) &= \frac{1-p}{p^{2}}.
        \end{align*}
    \end{thm}
    \begin{proofing}
        \begin{align*}
            \expect{X} &= \sum_{k = 1}^{\infty} kp(1-p)^{k-1} = p\sum_{k = 1}^{\infty} k(1-p)^{k-1} = \frac{p}{p^{2}} = \frac{1}{p},\\
            \Var(X) &= \expect{X} - (\expect{X})^{2} + \expect(X(X-1))\\
            &= \frac{1}{p} - \frac{1}{p^{2}} + \sum_{k = 2}^{\infty} k(k-1)p(1-p)^{k-1}\\
            &= \frac{1}{p} - \frac{1}{p^{2}} + \frac{2p(1-p)}{p^{3}} = \frac{1-p}{p^{2}}.
        \end{align*}
    \end{proofing}
    \begin{eg}
        Suppose that there are $N$ types of cards, and each time we obtain a card, it is equally likely to be any one of the $N$ types. We want to find the expected number of types of cards we can get if we obtain $n$ cards, and the expected number of cards needed to get all $N$ types. Let $X = X_{1} + X_{2} + \cdots + X_{N}$, where $X_{i} = 1$ if at least one type-$i$ card is among the $n$ cards, and otherwise $0$. The expected number of types of cards we can get after obtaining $n$ cards is:
        \begin{align*}
            \expect{X_{i}} &= \prob(X_{i} = 1) = 1 - \left(\frac{N-1}{N}\right)^{n}, & \expect{X} &= \sum_{i = 1}^{N}\expect{X_{i}} = N\left(1 - \left(\frac{N-1}{N}\right)^{n}\right).
        \end{align*}
        To find the expected number of cards needed to get all $N$ types, let $Y$ be the number of cards needed. We can write $Y = Y_{0} + Y_{1} + \cdots + Y_{N-1}$, where $Y_{i}$ is the number of additional cards needed to get a new type of card after we have already obtained $i$ types. Therefore, the expected number of cards needed to get all $N$ types is:
        \begin{align*}
            \tag{$Y_{i} \sim \Geom\left(\frac{N-i}{N}\right)$}
            \prob(Y_{i} = k) &= \left(\frac{i}{N}\right)^{k-1}\frac{N-i}{N},\\
            \expect{Y} &= \sum_{i = 0}^{N-1}\expect{Y_{i}} = \sum_{i = 0}^{N-1}\frac{N}{N-i} = N\left(\frac{1}{N} + \frac{1}{N-1} + \cdots + 1\right).
        \end{align*}
    \end{eg}
    \newpage

    \begin{thm}
        If $X \sim \NBin(r,p)$, then:
        \begin{align*}
            \expect{X} &= \frac{r}{p}, & \Var(X) &= \frac{r(1-p)}{p^{2}}.
        \end{align*}
    \end{thm}
    \begin{proofing}
        Assume that $X_{i} \sim \Geom(p)$ for all $i$. Since $X$ is the sum of $r$ independent geometric random variables, we get:
        \begin{align*}
            \expect{X} &= \sum_{k = 1}^{r}\expect{X_{k}} = \frac{r}{p}, & \Var(X) &= \sum_{k = 1}^{r}\Var(X_{k}) = \frac{r(1-p)}{p^{2}}.
        \end{align*}
    \end{proofing}
    \begin{thm}
        If $X \sim \Poisson(\lambda)$, then:
        \begin{align*}
            \expect{X} &= \lambda, & \Var(X) &= \lambda.
        \end{align*}
    \end{thm}
    \begin{proofing}
        \begin{align*}
            \expect{X} &= \sum_{k = 1}^{\infty}\frac{\lambda^{k}}{(k-1)!}e^{-\lambda} = \lambda\sum_{k = 1}^{\infty}\frac{\lambda^{k-1}}{(k-1)!}e^{-\lambda} = \lambda,\\
            \Var(X) = \expect(X(X-1)) + \expect{X} - (\expect{X})^{2} &= \lambda - \lambda^{2} + \sum_{k = 2}^{\infty}\frac{\lambda^{k}}{(k-2)!}e^{-\lambda}\\
            &= \lambda - \lambda^{2} + \lambda^{2}\sum_{k = 2}^{\infty}\frac{\lambda^{k-2}}{(k-2)!}e^{-\lambda}\\
            &= \lambda - \lambda^{2} + \lambda^{2} = \lambda.
        \end{align*}
    \end{proofing}
    \begin{thm}
        If $X \sim \Hypergeometric(N,m,n)$, then the expectation and variance are:
        \begin{align*}
            \expect{X} &= \frac{mn}{N}, & \Var(X) &= \frac{mn}{N}\left(\frac{(m-1)(n-1)}{N-1} + 1 - \frac{mn}{N}\right).
        \end{align*}
    \end{thm}
    We are not going to prove the variance. To prove the expectation, we imagine the following scenario.
    \begin{eg}
        There are $N$ balls in a box, of which $m$ are red and $N-m$ are blue. We randomly select $n$ balls from the box without replacement. Let $X$ be the number of red balls selected. We want to find $\expect{X}$. Let $X = X_{1} + X_{2} + \cdots + X_{m}$, where
        \begin{equation*}
            X_{i} = \begin{cases}
                1, &\text{if the $i$-th red ball is selected},\\
                0, &\text{otherwise}.
            \end{cases}, \qquad\text{for } i = 1, 2, \cdots, m.
        \end{equation*}
        Since each ball is equally likely to be selected, we have:
        \begin{equation*}
            \expect{X_{i}} = \frac{\binom{N-1}{n-1}}{\binom{N}{n}} = \frac{\frac{(N-1)!}{(n-1)!(N-n)!}}{\frac{N!}{n!(N-n)!}} = \frac{n}{N}.
        \end{equation*}
        Therefore, $\expect{X} = \frac{mn}{N}$.
    \end{eg}
    \newpage

    \begin{thm}
        If $X \sim \U[a, b]$, then:
        \begin{align*}
            \expect{X} &= \frac{1}{2}(a+b), & \Var(X) &= \frac{1}{12}(b-a)^{2}.
        \end{align*}
    \end{thm}
    \begin{proofing}
        \begin{align*}
            \expect{X} &= \int_{a}^{b}\frac{x}{b-a}\,dx\\
            &= \frac{1}{2}(a+b),\\
            \Var(X) &= -(\expect{X})^{2} + \expect(X^{2})\\
            &= -\frac{1}{4}(a+b)^{2} + \int_{a}^{b}\frac{x^{2}}{b-a}\,dx\\
            &= -\frac{1}{4}(a^{2} + 2ab + b^{2}) + \frac{1}{3}(a^{2} + ab + b^{2})\\
            &= \frac{1}{12}(a^{2} - 2ab + b^{2})\\
            &= \frac{1}{12}(b-a)^{2}.
        \end{align*}
    \end{proofing}
    \begin{thm}
        If $X \sim \Exp(\lambda)$, then:
        \begin{align*}
            \expect{X} &= \frac{1}{\lambda}, & \Var(X) &= \frac{1}{\lambda^{2}}.
        \end{align*}
    \end{thm}
    \begin{proofing}
        \begin{align*}
            \expect{X} &= \int_{0}^{\infty}x\lambda e^{-\lambda x}\,dx\\
            &= \left.-xe^{-\lambda x}\right|_{0}^{\infty} + \int_{0}^{\infty}e^{-\lambda x}\,dx\\
            &= \left.-\frac{1}{\lambda}e^{-\lambda x}\right|_{0}^{\infty}\\
            &= \frac{1}{\lambda},\\
            \Var(X) &= -(\expect{X})^{2} + \expect(X^{2})\\
            &= -\frac{1}{\lambda^{2}} + \int_{0}^{\infty}x^{2}\lambda e^{-\lambda x}\,dx\\
            &= -\frac{1}{\lambda^{2}} - \left.x^{2}e^{-\lambda x}\right|_{0}^{\infty} + \int_{0}^{\infty}2xe^{-\lambda x}\,dx\\
            &= -\frac{1}{\lambda^{2}} + \frac{2}{\lambda}\expect{X}\\
            &= -\frac{1}{\lambda^{2}} + \frac{2}{\lambda^{2}}\\
            &= \frac{1}{\lambda^{2}}.
        \end{align*}
    \end{proofing}
    \newpage

    \begin{thm}
        \label{Chapter 6 (Theorem) Expectation and Variance of normal distribution}
        If $X \sim \N(\mu,\sigma^{2})$, then:
        \begin{align*}
            \expect{X} &= \mu, & \Var(X) &= \sigma^{2}.
        \end{align*}
    \end{thm}
    \begin{proofing}
        \item Let $x = \sigma z + \mu$ for some $z$.
        \begin{align*}
            \expect{X} &= \frac{1}{\sqrt{2\pi\sigma^{2}}}\int_{-\infty}^{\infty} ye^{-\frac{(y-\mu)^{2}}{2\sigma^{2}}}\,dy\\
            &= \frac{1}{\sqrt{2\pi}}\left(\int_{-\infty}^{\infty}\sigma ze^{-\frac{z^{2}}{2}}\,dz + \int_{-\infty}^{\infty}\mu e^{-\frac{z^{2}}{2}}\,dz\right)\\
            &= \frac{\mu}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\frac{z^{2}}{2}}\,dz\\
            &= \mu,\\
            \Var(X) &= \frac{1}{\sqrt{2\pi\sigma^{2}}}\int_{-\infty}^{\infty}(y-\mu)^{2}e^{-\frac{(y-\mu)^{2}}{2\sigma^{2}}}\,dy\\
            &= \frac{\sigma^{2}}{\sqrt{2\pi}}\int_{-\infty}^{\infty}z^{2}e^{-\frac{z^{2}}{2}}\,dz\\
            &= \frac{-\sigma^{2}}{\sqrt{2\pi}}\int_{-\infty}^{\infty}z\,d\left(e^{-\frac{z^{2}}{2}}\right)\\
            &= \frac{\sigma^{2}}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\frac{z^{2}}{2}}\,dz\\
            &= \sigma^{2}.
        \end{align*}
    \end{proofing}
    \begin{thm}
        All Cauchy random variables $X \sim \Cauchy(\theta)$ do not have defined expectation and variance.
    \end{thm}
    \begin{proofing}
        We check if $\expect\abs{X}$ is infinite.
        \begin{equation*}
            \expect\abs{X} = \int_{-\infty}^{\infty}\frac{\abs{x}}{\pi(1+(x-\theta)^{2})}\,dx = 2\int_{0}^{\infty}\frac{x}{\pi(1+(x-\theta)^{2})}\,dx = \infty.
        \end{equation*}
        Therefore, expectation and thus variance do not exist.
    \end{proofing}
    \begin{thm}
        If $X \sim \chi^{2}(n)$, then $\expect{X} = n$ and $\Var(X) = 2n$.
    \end{thm}
    \begin{proofing}
        Since $X = Z_{1}^{2} + Z_{2}^{2} + \cdots + Z_{n}^{2}$ where $Z_{i} \sim \N(0,1)$ are independent, we have:
        \begin{align*}
            \expect{X} &= \sum_{i = 1}^{n}\expect(Z_{i}^{2}) = n, & \Var(X) &= \sum_{i = 1}^{n}\Var(Z_{i}^{2}) = 2n.
        \end{align*}
    \end{proofing}
    Solving the following expectations is out of our scope.
    \begin{thm}
        Given a continuous random variable $X$:
        \begin{enumerate}
            \item If $X \sim \Gam(a,\lambda)$, then $\expect{X} = \frac{\alpha}{\lambda}$ and $\Var(X) = \frac{\alpha}{\lambda^{2}}$.
            \item If $X \sim t(n)$, then:
            \begin{align*}
                \expect{X} &= \begin{cases}
                    0, & n > 1,\\
                    \text{undefined}, & \text{otherwise}.
                \end{cases} & \Var(X) &= \begin{cases}
                    \frac{n}{n-2}, & n > 2,\\
                    \infty, & 2 < n \leq 4,\\
                    \text{undefined}, & \text{otherwise}.
                \end{cases}
            \end{align*}
            \item If $X \sim \Beta(a, b)$, then $\expect{X} = \frac{a}{a+b}$ and $\Var(X) = \frac{ab}{(a+b)^{2}(a+b+1)}$.
        \end{enumerate}
    \end{thm}
    \newpage

\section{Combining Expectation from Discrete and Continuous Random Variables}
    Recall that the expectations are given respectively by:
    \begin{equation*}
        \expect{X} = \begin{cases}
            \sum x f_{X}(x), & X \text{ is discrete},\\
            \int x f_{X}(x)\,dx, & X \text{ is continuous}.
        \end{cases}
    \end{equation*}
    We want a notation that incorporates both these cases. Suppose that $X$ has a CDF $F_{X}$. We can rewrite the equations as:
    \begin{equation*}
        \expect{X} = \begin{cases}
            \sum x\,dF_{X}(x), & dF_{X}(x) = F_{X}(x) - \lim_{y \to x^{-}}F_{X}(y) = f_{X}(x),\\
            \int x\,dF_{X}(x), & dF_{X}(x) = \pdv{F_{X}}{x}\,dx = f_{X}(x)\,dx.
        \end{cases}
    \end{equation*}
    Instead of using the regular Riemann integral, which cannot handle the discrete case, we can use the Riemann-Stieltjes integral, which is a generalization of the Riemann integral:
    \begin{align*}
        \int_{a}^{b}g(x)\,dx &= \lim_{\max_{i}\abs{x_{i+1}-x_{i}}}\sum_{i}g(x_{i}^{*})(x_{i+1}-x_{i}),\\
        \int_{a}^{b}g(x)\,dF(x) &= \lim_{\max_{i}\abs{x_{i+1}-x_{i}}}\sum_{i}g(x_{i}^{*})(F(x_{i+1})-F(x_{i})),
    \end{align*}
    if the limit does not depend on the choice of $x_{i}^{*} \in [x_{i},x_{i+1})$.
    \begin{defn}
        The \textbf{expectation} of a random variable $X$ is given by:
        \begin{equation*}
            \expect{X} = \int x\,dF_{X}.
        \end{equation*}
    \end{defn}
    \begin{lem}
        If $g:\mathbb{R} \to \mathbb{R}$ such that $g(X)$ is also a random variable, then:
        \begin{equation*}
            \expect(g(X)) = \int g(x)\,dF_{X}.
        \end{equation*}
    \end{lem}
    \begin{rem}
        The notation $\int g(x)\,dF_{X}(x)$ does not imply the Riemann-Stieltjes integral.
    \end{rem}
    \begin{eg}
        If $g$ is regular (differentiable at every point, and every value in the domain maps to a value in the range), then:
        \begin{equation*}
            \sum_{i}g(x_{i}^{*})(F(x_{i+1}) - F(x_{i})) \approx \sum_{i}g(x_{i}^{*})f(x_{i}^{*})(x_{i+1} - x_{i}) \approx \int g(x)f(x)\,dx.
        \end{equation*}
    \end{eg}
    \begin{eg}
        In the irregular case, assume that the function $g$ is the Dirichlet function. That is:
        \begin{align*}
            \mathbf{1}_{\mathbb{Q}}(x) &= \begin{cases}
                1, & x \in \mathbb{Q},\\
                0, & x \not \in \mathbb{Q}.
            \end{cases} & \sum_{i}g(x_{i}^{*})(F(x_{i+1}) - F(x_{i})) = \sum_{i}g(x_{i}^{*})(x_{i+1} - x_{i}).
        \end{align*}
        Since the limit depends on the choice of $x_{i}^{*}$, the Riemann-Stieltjes integral of $\mathbf{1}_{\mathbb{Q}}(x)$ with respect to $F(x) = x$ is not well-defined. Therefore, $\expect{\mathbf{1}_{\mathbb{Q}}(X)}$ cannot be defined as a Riemann-Stieltjes integral.\\
        However, on the other hand:
        \begin{equation*}
            \expect{\mathbf{1}_{\mathbb{Q}}(X)} = \prob(\mathbf{1}_{\mathbb{Q}}(x) = 1) = \prob \circ X^{-1}(\mathbb{Q} \cap [0,1]) = 0.
        \end{equation*}
    \end{eg}

\addcontentsline{toc}{chapter}{Summary of Chapter 1-6}

\chapter*{Summary}
\section*{Definition}
    \begin{sdefn}
        Given a set with $n$ distinct elements:
        \begin{enumerate}
            \item A \textbf{permutation} of the set is an ordered arrangement of all elements of the set.
            \item If $k \leq n$, a \textbf{$k$-permutation} of the set is an ordered arrangement of $k$ elements of the set.
        \end{enumerate}
    \end{sdefn}
    \begin{sdefn}
        If $k \leq n$, a \textbf{$k$-combination} of a set with $n$ distinct elements is an unordered arrangement of $k$ elements of the set.
    \end{sdefn}
    \begin{sdefn}
        These are the basic objects of probability:
        \begin{enumerate}
            \item An \textbf{experiment} is an activity that produces distinct and well-defined possibilities called \textbf{outcomes}, denoted by $\omega$.
            \item A \textbf{sample space} is the set of all outcomes of an experiment, denoted by $\Omega$.
            \item An \textbf{event} is a subset of the sample space and is usually represented by $A, B, C, \cdots$.
            \item Outcomes are called \textbf{elementary events}.
        \end{enumerate}	
    \end{sdefn}
    \begin{sdefn}
        Given two events $A$ and $B$:
        \begin{enumerate}
            \item The \textbf{union} of $A$ and $B$ is an event $A \cup B = \{\omega \in \Omega : \omega \in A \text{ or } \omega \in B\}$.
            \item The \textbf{intersection} of $A$ and $B$ is an event $A \cap B = \{\omega \in \Omega : \omega \in A \text{ and } \omega \in B\}$.
            \item The \textbf{complement} of $A$ is an event containing all elements in the sample space $\Omega$ that are not in $A$. It is denoted by $A^{\complement}$.
            \item The \textbf{relative complement} of $B$ in $A$ is an event $A \setminus B = \{\omega \in \Omega : \omega \in A \text{ and } \omega \not \in B\}$.
            \item The \textbf{symmetric difference} of $A$ and $B$ is an event $A \Delta B = \{\omega \in \Omega : \omega \in A \cup B \text{ and } \omega \not \in A \cap B\}$.
        \end{enumerate}
    \end{sdefn}
    \begin{sdefn}
        For any two events $A$ and $B$, if all outcomes in $A$ are also in $B$, then we say $A$ is \textbf{contained} in $B$, written as $A \subset B$ or $B \supset A$.
    \end{sdefn}
    \begin{sdefn}
        Given a sequence of events $A_{1}, A_{2}, \cdots, A_{k}$:
        \begin{enumerate}
            \item For any $i$ and $j$, if $A_{i} \cap A_{j} = \emptyset$, then $A_{i}$ and $A_{j}$ are called \textbf{disjoint}.
            \item If $A_{i} \cap A_{j} = \emptyset$ for all $i$ and $j$, the sequence of events is called \textbf{mutually exclusive}.
            \item If $A_{1} \cup A_{2} \cup \cdots \cup A_{k} = \Omega$, the sequence of events is called \textbf{exhaustive}.
            \item If the sequence is both mutually exclusive and exhaustive, it is called a \textbf{partition}.
        \end{enumerate}
    \end{sdefn}
    \newpage

    \begin{sdefn}\named{Kolmogorov Axioms of Probability}
        Let $(\Omega, \mathcal{F}, \prob)$ be a probability space, with sample space $\Omega$, $\sigma$-field $\mathcal{F}$, and probability measure $\prob$:
        \begin{enumerate}
            \item The probability of an event is a non-negative real number. For all $E \in \mathcal{F}$:
            \begin{align*}
                \prob(E) &\in \mathbb{R}, & \prob(E) &\geq 0.
            \end{align*}
            \item The probability that at least one of the elementary events in the entire sample space will occur is $1$:
            \begin{equation*}
                \prob(\Omega) = 1.
            \end{equation*}
            \item Any countable sequence of disjoint events $E_{1}, E_{2}, \cdots$ satisfies:
            \begin{equation*}
                \prob\left(\bigcup_{i = 1}^{\infty}E_{i}\right) = \sum_{i = 1}^{\infty}\prob(E_{i}).
            \end{equation*}
        \end{enumerate}
        By this definition, we call $\prob(A)$ the \textbf{probability} of the event $A$.
    \end{sdefn}
    \begin{sdefn}
        A \textbf{$\sigma$-field} (\textbf{$\sigma$-algebra}) $\mathcal{F}$ is any collection of subsets of $\Omega$ that satisfies the following conditions:
        \begin{enumerate}
            \item If $A \in \mathcal{F}$, then $A^{\complement} \in \mathcal{F}$.
            \item If $A_{i} \in \mathcal{F}$ for all $i$, then $\bigcup_{i = 1}^{\infty}A_{i} \in \mathcal{F}$.
            \item $\emptyset \in \mathcal{F}$.
        \end{enumerate}
    \end{sdefn}
    \begin{sdefn}
        A \textbf{measurable space} $(\Omega, \mathcal{F})$ is a pair comprising a sample space $\Omega$ and a $\sigma$-field $\mathcal{F}$.
    \end{sdefn}
    \begin{sdefn}
        A \textbf{probability measure} $\prob: \mathcal{F} \to [0,1]$ is a measure on a measurable space $(\Omega, \mathcal{F})$ satisfying:
        \begin{enumerate}
            \item $\prob(\emptyset) = 0$.
            \item $\prob(\Omega) = 1$.
            \item If $A_{i} \in \mathcal{F}$ for all $i$ and they are disjoint, then $\prob\left(\bigcup_{i = 1}^{\infty}A_{i}\right) = \sum_{i = 1}^{\infty}\prob(A_{i})$.
        \end{enumerate}
    \end{sdefn}
    \begin{sdefn}
        A \textbf{probability space} $(\Omega, \mathcal{F}, \prob)$ is a triple comprising:
        \begin{enumerate}
            \item a sample space $\Omega$,
            \item a $\sigma$-field $\mathcal{F}$ of certain subsets of $\Omega$,
            \item a probability measure $\prob$ on $(\Omega, \mathcal{F})$.
        \end{enumerate}
    \end{sdefn}
    \begin{sdefn}
        We say a sequence of events $A_{n}$ \textbf{converges} and $\lim_{n \to \infty}A_{n}$ exists if:
        \begin{equation*}
            \limsup_{n \to \infty}A_{n} = \liminf_{n \to \infty}A_{n}.
        \end{equation*}
        Given a probability space $(\Omega, \mathcal{F}, \prob)$, let $A_{i} \in \mathcal{F}$ for all $i$ such that $A = \lim_{n \to \infty}A_{n}$ exists. Then:
        \begin{equation*}
            \lim_{n \to \infty}\prob(A_{n}) = \prob\left(\lim_{n \to \infty}A_{n}\right).
        \end{equation*}
    \end{sdefn}
    \begin{sdefn}
        An event $A$ is \textbf{null} if $\prob(A) = 0$.
    \end{sdefn}
    \begin{sdefn}
        An event $A$ occurs \textbf{almost surely} if $\prob(A) = 1$.
    \end{sdefn}
    \begin{sdefn}
        Given $\prob(B) > 0$, the \textbf{conditional probability} that $A$ occurs given that $B$ occurs is:
        \begin{equation*}
            \prob(A|B) = \frac{\prob(A \cap B)}{\prob(B)}.
        \end{equation*}
    \end{sdefn}
    \newpage

    \begin{sdefn}
        Events $A$ and $B$ are independent ($A \independent B$) if $\prob(A \cap B) = \prob(A)\prob(B)$.\\
        Given $A_{k}$ for all $k \in I$, if for all $i \neq j$:
        \begin{equation*}
            \prob(A_{i} \cap A_{j}) = \prob(A_{i})\prob(A_{j}),
        \end{equation*} 
        then they are \textbf{pairwise independent}.\\
        If additionally, for all subsets $J \subseteq I$:
        \begin{equation*}
            \prob\left(\bigcap_{i \in J}A_{i}\right) = \prod_{i \in J}\prob(A_{i}),
        \end{equation*}
        then they are \textbf{(mutually) independent}.
    \end{sdefn}
    \begin{sdefn}
        Let $A$ be a collection of subsets of $\Omega$. The \textbf{$\sigma$-field generated by $A$} is:
        \begin{equation*}
            \sigma(A) = \bigcap_{A \subseteq \mathcal{G}}\mathcal{G},
        \end{equation*}
        where $\mathcal{G}$ are also $\sigma$-fields. $\sigma(A)$ is the smallest $\sigma$-field containing $A$.
    \end{sdefn}
    \begin{sdefn}
        The \textbf{product space} of two probability spaces $(\Omega_{1}, \mathcal{F}_{1}, \prob_{1})$ and $(\Omega_{2}, \mathcal{F}_{2}, \prob_{2})$ is the probability space $(\Omega_{1} \times \Omega_{2}, \mathcal{G}, \prob_{12})$ comprising:
        \begin{enumerate}
            \item A collection of ordered pairs $\Omega_{1} \times \Omega_{2} = \{(\omega_{1}, \omega_{2}) : \omega_{1} \in \Omega_{1}, \omega_{2} \in \Omega_{2}\}$.
            \item A $\sigma$-algebra $\mathcal{G} = \sigma(\mathcal{F}_{1} \times \mathcal{F}_{2})$, where $\mathcal{F}_{1} \times \mathcal{F}_{2} = \{A_{1} \times A_{2} : A_{1} \in \mathcal{F}_{1}, A_{2} \in \mathcal{F}_{2}\}$.
            \item A probability measure $\prob_{12} : \mathcal{F}_{1} \times \mathcal{F}_{2} \to [0,1]$ given by:
            \begin{equation*}
                \prob_{12}(A_{1} \times A_{2}) = \prob_{1}(A_{1})\prob_{2}(A_{2}),
            \end{equation*}
            for $A_{1} \in \mathcal{F}_{1}, A_{2} \in \mathcal{F}_{2}$.
        \end{enumerate}
    \end{sdefn}
    \begin{sdefn}
        A \textbf{random variable} is a function $X : \Omega \to \mathbb{R}$ with the property that:
        \begin{equation*}
            X^{-1}((-\infty, x]) = \{\omega \in \Omega : X(\omega) \leq x\} \in \mathcal{F},
        \end{equation*}
        for any $x \in \mathbb{R}$. We say the function is \textbf{$\mathcal{F}$-measurable}.
    \end{sdefn}
    \begin{sdefn}
        A \textbf{Borel set} is a set that can be obtained by taking countable unions, intersections, or complements repeatedly.
    \end{sdefn}
    \begin{sdefn}
        The \textbf{Borel $\sigma$-field} $\mathcal{B}(\mathbb{R})$ of $\mathbb{R}$ is a $\sigma$-field generated by all open sets. It is a collection of Borel sets.
    \end{sdefn}
    \begin{sdefn}
        The \textbf{(cumulative) distribution function} (CDF) of a random variable $X$ is a function $F_{X} : \mathbb{R} \to [0,1]$ given by:
        \begin{equation*}
            F_{X}(x) = \prob(X \leq x) = \prob \circ X^{-1}((-\infty, x]).
        \end{equation*}
        In the \textbf{discrete} case, the \textbf{probability mass function} (PMF) of a discrete random variable $X$ is the function $f : \mathbb{R} \to [0,1]$ given by:
        \begin{align*}
            f_{X}(x) &= \prob(X = x) = \prob \circ X^{-1}(\{x\}), & F_{X}(x) &= \sum_{i : x_{i} \leq x}f(x_{i}), & f_{X}(x) &= F_{X}(x) - \lim_{y \to x^{-}}F_{X}(y).
        \end{align*}
        In the \textbf{continuous} case, the \textbf{probability density function} (PDF) of a continuous random variable $X$ is the function $f : \mathbb{R} \to [0,\infty)$ given by:
        \begin{align*}
            F_{X}(x) &= \int_{-\infty}^{x}f(u)\,du, & f_{X}(x) &= \pdv*{F_{X}(x)}{x}.
        \end{align*}
    \end{sdefn}
    \begin{sdefn}
        The \textbf{$q$-th quantile} of a random variable $X$ is defined as a number $z_{q}$ such that:
        \begin{equation*}
            \prob(X \leq z_{q}) = q.
        \end{equation*}
    \end{sdefn}
    \newpage

    \begin{sdefn}
        Let $X_{i} : \Omega \to \mathbb{R}$ for all $1 \leq i \leq n$ be random variables. A \textbf{random vector} $\mathbf{X} = (X_{1}, X_{2}, \cdots, X_{n}) : \Omega \to \mathbb{R}^{n}$ has the following properties:
        \begin{equation*}
            \mathbf{X}^{-1}(D) = \{\omega \in \Omega : \mathbf{X}(\omega) = (X_{1}(\omega), X_{2}(\omega), \cdots, X_{n}(\omega)) \in D\} \in \mathcal{F},
        \end{equation*}
        for all $D \in \mathcal{B}(\mathbb{R}^{n})$.\\
        We can also say $\mathbf{X}$ is a random vector if:
        \begin{equation*}
            X_{i}^{-1}(B) \in \mathcal{F},
        \end{equation*}
        for all $B \in \mathcal{B}(\mathbb{R})$ and $i$.
    \end{sdefn}
    \begin{sdefn}
        Given a random vector $(X, Y)$, the \textbf{joint distribution function} (JCDF) $F_{X,Y} : \mathbb{R}^{2} \to [0,1]$ is defined as:
        \begin{equation*}
            F_{X,Y}(x, y) = \prob(X \leq x, Y \leq y) = \prob \circ (X, Y)^{-1}((-\infty, x] \times (-\infty, y]);
        \end{equation*}
        In the discrete case, the \textbf{joint probability mass function} (JPMF) of \textbf{jointly discrete} random variables $X$ and $Y$ is the function $f_{X,Y} : \mathbb{R}^{2} \to [0,1]$ given by:
        \begin{align*}
            f_{X,Y}(x, y) &= \prob((X, Y) = (x, y)) = \prob \circ (X, Y)^{-1}(\{x, y\}), & F_{X,Y}(x, y) &= \sum_{u \leq x}\sum_{v \leq y}f(u, v)
        \end{align*}
        In the continuous case, the \textbf{joint probability density function} (JPDF) of \textbf{jointly continuous} random variables $X$ and $Y$ is the function $f_{X,Y} : \mathbb{R}^{2} \to [0, \infty)$ given by:
        \begin{align*}
            f_{X,Y}(x, y) &= \pdv*{F_{X,Y}(x, y)}{x, y}, & F_{X,Y}(x, y) &= \int_{-\infty}^{y}\int_{-\infty}^{x}f_{X,Y}(u, v)\,du\,dv
        \end{align*}
    \end{sdefn}
    \begin{sdefn}
        Let $X$ and $Y$ be random variables. The \textbf{marginal distribution function} (Marginal CDF) is given by:
        \begin{equation*}
            F_{X}(x) = \prob(X^{-1}((-\infty, x]) \cap Y^{-1}((-\infty, \infty))) = \lim_{y \to \infty}F_{X,Y}(x, y)
        \end{equation*}
        In the discrete case, the \textbf{marginal mass function} (Marginal PMF) is given by:
        \begin{equation*}
            f_{X}(x) = \sum_{y}f_{X,Y}(x, y)
        \end{equation*}
        In the continuous case, the \textbf{marginal density function} (Marginal PDF) is given by:
        \begin{equation*}
            f_{X}(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y)\,dy
        \end{equation*}
    \end{sdefn}
    \begin{sdefn}
        Given a random variable $X$, the \textbf{mean value}, \textbf{expectation}, or \textbf{expected value} of $X$ is given by:
        \begin{equation*}
            \expect{X} = \begin{cases}
                \sum_{x : f_{X}(x) > 0}xf_{X}(x), & X \text{ is discrete},\\
                \int_{-\infty}^{\infty} xf_{X}(x)\,dx, & X \text{ is continuous}
            \end{cases}
        \end{equation*}
        if it is absolutely convergent.
    \end{sdefn}
    \begin{sdefn}
        Given $k \in \mathbb{N}_{+}$ and a random variable $X$, the \textbf{$k$-th moment} $m_{k}$ is defined as:
        \begin{equation*}
            \expect(X^{k}) = \begin{cases}
                \sum_{x}x^{k}f_{X}(x), & X \text{ is discrete},\\
                \int_{-\infty}^{\infty} x^{k}f_{X}(x)\,dx, & X \text{ is continuous}
            \end{cases}
        \end{equation*}
        The \textbf{$k$-th central moment} $\alpha_{k}$ is defined as:
        \begin{equation*}
            \expect((X - \expect{X})^{k}) = \begin{cases}
                \sum_{x}(x - \expect{X})^{k}f_{X}(x), & X \text{ is discrete},\\
                \int_{-\infty}^{\infty}(x - \expect{X})^{k}f_{X}(x)\,dx, & X \text{ is continuous}
            \end{cases}
        \end{equation*}
        The \textbf{mean} $\mu$ is the $1$st moment $\mu = m_{1} = \expect{X}$.\\
        The \textbf{variance} is the $2$nd central moment $\alpha_{2} = \Var(X) = \expect((X - \expect{X})^{2}) = \expect(X^{2}) - (\expect{X})^{2}$.\\
        The \textbf{standard deviation} $\sigma$ is defined as $\sigma = \sqrt{\Var(X)}$.
    \end{sdefn}
    \begin{sdefn}
        Given two random variables $X$ and $Y$, the \textbf{conditional distribution function} (Conditional CDF) of $Y$ given $X = x$ for any $x$ is defined as:
        \begin{equation*}
            F_{Y|X}(y|x) = \prob(Y \leq y | X = x) = \begin{cases}
                \frac{\prob(Y \leq y, X = x)}{\prob(X = x)}, & X \text{ is discrete},\\
                \int_{-\infty}^{y}\frac{f_{X,Y}(x, v)}{f_{X}(x)}\,dv, & X \text{ is continuous}
            \end{cases}
        \end{equation*}
        In the discrete case, the \textbf{conditional mass function} (Conditional PMF) of $Y$ given $X = x$ is defined as:
        \begin{equation*}
            f_{Y|X}(y|x) = \begin{cases}
                \frac{\prob(Y = y, X = x)}{\prob(X = x)}, & X \text{ is discrete},\\
                \pdv*{F_{Y|X}(y|x)}{y} = \frac{f_{X,Y}(x, y)}{f_{X}(x)}, & X \text{ is continuous}
            \end{cases}
        \end{equation*}
    \end{sdefn}
    \begin{sdefn}
        Given two random variables $X$ and $Y$, and an event $X = x$ for some $X$, the \textbf{conditional expectation} of the random variable $Y$ is defined as:
        \begin{equation*}
            \psi(x) = \expect(Y | X = x) = \begin{cases}
                \sum_{y}yf_{Y|X}(y|x), & X \text{ and } Y \text{ are discrete},\\
                \int_{-\infty}^{\infty} yf_{Y|X}(y|x)\,dy, & X \text{ and } Y \text{ are continuous}
            \end{cases}
        \end{equation*}
        Given a random variable $X$, the conditional expectation of the random variable $Y$ is defined as:
        \begin{equation*}
            \psi(X) = \expect(Y | X) = \begin{cases}
                \sum_{x}\psi(x), & X \text{ and } Y \text{ are discrete},\\
                \int_{-\infty}^{\infty}\psi(x)\,dx, & X \text{ is continuous}
            \end{cases}
        \end{equation*}
    \end{sdefn}
    \begin{sdefn}
        Given $X \independent Y$, in the discrete case, the \textbf{convolution} $f_{X+Y}$ ($f_{X} * f_{Y}$) of the PMFs of random variables $X$ and $Y$ is the PMF of $X + Y$:
        \begin{equation*}
            f_{X+Y}(z) = \prob(X + Y = z) = \sum_{x}f_{X}(x)f_{Y}(z - x) = \sum_{y}f_{X}(z - y)f_{Y}(y)
        \end{equation*}
        In the continuous case, the \textbf{convolution} of the PDFs of random variables $X$ and $Y$ is the PDF of $X + Y$:
        \begin{equation*}
            f_{X+Y}(z) = \int_{-\infty}^{\infty} f_{X}(z - y)f_{Y}(y)\,dy = \int_{-\infty}^{\infty} f_{X}(x)f_{Y}(z - x)\,dx
        \end{equation*}
    \end{sdefn}
    \begin{sdefn}
        A \textbf{parametric distribution} of a random variable is a distribution where the PMF or PDF depends on one or more parameters.
    \end{sdefn}
    \newpage

\section*{Named Properties}
    \begin{spro}\named{Fundamental Principle of Counting}
        Suppose that $m_{i}$ represents the number of outcomes of the $i$-th event. The total number of outcomes of $n$ independent events is the product of the number of outcomes for each individual event:
        \begin{equation*}
            \prod_{i = 1}^{n}m_{i}.
        \end{equation*} 
    \end{spro}
    \begin{spro}\named{Pascal's Identity}
        Let $n$ and $k$ be integers with $0 < k < n$. Then:
        \begin{equation*}
            \binom{n}{k} = \binom{n-1}{k-1} + \binom{n-1}{k}.
        \end{equation*}
    \end{spro}
    \begin{spro}\named{Binomial Theorem}
        Let $n$ be a non-negative integer. We have:
        \begin{equation*}
            (x + y)^{n} = \sum_{k = 0}^{n}\binom{n}{k}x^{k}y^{n-k},
        \end{equation*} 
        where $\binom{n}{k}$ for all $k$ are called the \textbf{binomial coefficients}.
    \end{spro}
    \begin{spro}\named{Vandermonde's Identity}
        Let $m, n, r \in \mathbb{Z}$ with $0 \leq r \leq m$ and $0 \leq r \leq n$. We have:
        \begin{equation*}
            \binom{m+n}{r} = \sum_{k = 0}^{r}\binom{m}{r-k}\binom{n}{k}.
        \end{equation*}
    \end{spro}
    \begin{spro}\named{Multinomial Theorem}
        Let $n$ be a non-negative integer. We have:
        \begin{equation*}
            (x_{1} + x_{2} + \cdots + x_{k})^{n} = \sum_{(n_{1}, n_{2}, \cdots, n_{k}) : n_{1} + n_{2} + \cdots + n_{k} = n}\binom{n}{n_{1}, n_{2}, \cdots, n_{k}}x_{1}^{n_{1}}x_{2}^{n_{2}} \cdots x_{k}^{n_{k}},
        \end{equation*}
        where $(n_{1}, n_{2}, \cdots, n_{k})$ are all non-negative integer-valued vectors.
    \end{spro}
    \begin{spro}\named{Inclusion-Exclusion Formula}
        \begin{equation*}
            \prob\left(\bigcup_{i = 1}^{n}A_{i}\right) = \sum_{i}\prob(A_{i}) - \sum_{i < j}\prob(A_{i} \cap A_{j}) + \cdots + (-1)^{n+1}\prob(A_{1} \cap A_{2} \cap \cdots \cap A_{n}).
        \end{equation*}
    \end{spro}
    \begin{spro}\named{General Multiplication Rule}
        Let $A_{1}, A_{2}, \cdots, A_{n}$ be a sequence of events. We have:
        \begin{equation*}
            \prob\left(\bigcap_{i = 1}^{n}A_{i}\right) = \prob(A_{1})\prob(A_{2} | A_{1})\prob(A_{3} | A_{1} \cap A_{2}) \cdots \prob(A_{n} | A_{1} \cap A_{2} \cap \cdots \cap A_{n-1}).
        \end{equation*}
    \end{spro}
    \begin{spro}\named{Law of Total Probability}
        Let $\{B_{1}, B_{2}, \cdots, B_{n}\}$ be a partition of $\Omega$ ($B_{i} \cap B_{j} = \emptyset$ for all $i \neq j$ and $\bigcup_{i = 1}^{n}B_{i} = \Omega$).\\
        If $\prob(B_{i}) > 0$ for all $i$, then:
        \begin{equation*}
            \prob(A) = \sum_{i = 1}^{n}\prob(A | B_{i})\prob(B_{i}).
        \end{equation*}
    \end{spro}
    \begin{spro}\named{Bayes' Theorem}
        Suppose that a sequence of events $A_{1}, A_{2}, \cdots, A_{n}$ is a partition of the sample space. Assume further that $\prob(A_{i}) > 0$ for all $i$. Let $B$ be any event. Then, for any $i$:
        \begin{equation*}
            \prob(A_{i} | B) = \frac{\prob(B | A_{i})\prob(A_{i})}{\sum_{k = 1}^{n}\prob(B | A_{k})\prob(A_{k})}.
        \end{equation*}
    \end{spro}
    \newpage

    \begin{spro}\named{Law of Total Expectation}
        Let $\psi(X) = \expect(Y | X)$. The conditional expectation satisfies:
        \begin{equation*}
            \expect(\psi(X)) = \expect(\expect(Y | X)) = \expect(Y).
        \end{equation*}
    \end{spro}
    \begin{spro}\named{Tail Sum Formula}
        If a discrete random variable $X$ has a PMF $f_{X}$ with $f_{X}(x) = 0$ when $x < 0$, then:
        \begin{equation*}
            \expect{X} = \sum_{k = 0}^{\infty}\prob(X > k).
        \end{equation*}
        If a continuous random variable $X$ has a PDF $f_{X}$ with $f_{X}(x) = 0$ when $x < 0$, and a CDF $F_{X}$, then:
        \begin{equation*}
            \expect{X} = \int_{0}^{\infty}(1 - F_{X}(x))\,dx.
        \end{equation*}
    \end{spro}
    \newpage

\section*{Distributions}
    For discrete random variables:
    \begin{seg}\named{Bernoulli Distribution} $X \sim \Bern(p)$\\
        Suppose we perform one Bernoulli trial. Let $p$ be the probability of success, and $X$ be the number of successes.
        \begin{align*}
            F_{X}(x) &= \begin{cases}
                0, &x < 0,\\
                1-p, &0 \leq x < 1,\\
                1, &x \geq 1
            \end{cases} & f_{X}(x) &= \begin{cases}
                1-p, &x = 0,\\
                p, &x = 1,\\
                0, &\text{otherwise}
            \end{cases} & \expect{X} &= p, & \Var(X) &= p(1-p).
        \end{align*}
    \end{seg}
    \begin{seg}\named{Binomial Distribution} $Y \sim \Bin(n,p)$\\
        Suppose we perform $n$ independent Bernoulli trials. Let $p$ be the probability of success, and $Y = X_{1}+X_{2}+\cdots+X_{n}$ be the total number of successes.
        \begin{align*}
            f_{Y}(k) &= \binom{n}{k}p^{k}(1-p)^{n-k}, & F_{Y}(k) &= \sum_{i = 0}^{k}\binom{n}{i}p^{i}(1-p)^{n-i}, & \expect{Y} &= np, & \Var(Y) &= np(1-p).
        \end{align*}
    \end{seg}
    \begin{seg}\named{Trinomial Distribution}\\
        Suppose we perform $n$ trials with three outcomes: $A$, $B$, and $C$, where the probabilities of occurrence are $p$, $q$, and $1-p-q$, respectively. Let $X$ be the number of occurrences of $A$, and $Y$ be the number of occurrences of $B$.\\
        The probability of $x$ occurrences of $A$, $y$ occurrences of $B$, and $n-x-y$ occurrences of $C$ is:
        \begin{equation*}
            f_{X,Y}(x, y) = \binom{n}{x,y,n-x-y}p^{x}q^{y}(1-p-q)^{n-x-y}.
        \end{equation*}
    \end{seg}
    \begin{seg}\named{Geometric Distribution} $W \sim \Geom(p)$, $X \sim \Geom(p)$\\
        Suppose we keep performing independent Bernoulli trials until the first success occurs. Let $p$ be the probability of success.\\
        Let $W$ be the waiting time that elapses before the first success. For $k \geq 1$:
        \begin{align*}
            f_{W}(k) &= p(1-p)^{k-1}, & F_{W}(k) &= 1-(1-p)^{k}, & \expect{W} &= \frac{1}{p}, & \Var(W) &= \frac{1-p}{p^{2}}.
        \end{align*}
        The above is the conventional geometric distribution.\\
        Let $X$ be the number of failures before the first success. For $k \geq 0$:
        \begin{align*}
            f_{X}(k) &= p(1-p)^{k}, & F_{X}(k) &= 1-(1-p)^{k+1}, & \expect{X} &= \frac{1-p}{p}, & \Var(X) &= \frac{1-p}{p^{2}}.
        \end{align*}
    \end{seg}
    \begin{seg}\named{Negative Binomial Distribution} $W_{r} \sim \NBin(r,p)$, $X \sim \NBin(r,p)$\\
        Suppose we keep performing independent Bernoulli trials until the $r$-th success occurs. Let $p$ be the probability of success.\\
        Let $W_{r}$ be the waiting time that elapses before the $r$-th success. For any $k \geq r$:
        \begin{align*}
            f_{W_{r}}(k) &= \binom{k-1}{r-1}p^{r}(1-p)^{k-r}, & \expect{W_{r}} &= \frac{r}{p}, & \Var(W_{r}) &= \frac{r(1-p)}{p^{2}}.
        \end{align*}
        Let $X$ be the number of failures before the $r$-th success. For any $k \geq 0$:
        \begin{align*}
            f_{X}(k) &= \binom{k+r-1}{r-1}p^{r}(1-p)^{k}, & \expect{X} &= \frac{r(1-p)}{p}, & \Var(X) &= \frac{r(1-p)}{p^{2}}.
        \end{align*}
    \end{seg}
    \begin{seg}\named{Poisson Distribution} $X \sim \Poisson(\lambda)$\\
        Suppose we perform $n$ independent Bernoulli trials. Let $p$ be the probability of success, $\lambda = np$, and $X \sim \Bin(n,p)$. When $n$ is large, $p$ is small, and $np$ is moderate:
        \begin{align*}
            f_{X}(k) &= \binom{n}{k}p^{k}(1-p)^{n-k} \approx \frac{\lambda^{k}}{k!}e^{-\lambda}, & F_{X}(k) &= \sum_{i = 0}^{k}\frac{\lambda^{i}}{i!}e^{-\lambda}, & \expect{X} &= \lambda, & \Var(X) &= \lambda.
        \end{align*}
    \end{seg}
    \newpage

    \begin{seg}\named{Hypergeometric Distribution} $X \sim \Hypergeometric(N,m,n)$\\
        Suppose that we have a set of $N$ balls. There are $m$ red balls and $N-m$ blue balls. We choose $n$ of these balls without replacement. Let $X$ be the number of red balls in our sample. For $0 \leq k \leq \min(m,n)$:
        \begin{align*}
            f_{X}(k) &= \frac{\binom{m}{k}\binom{N-m}{n-k}}{\binom{N}{n}}, & \expect{X} &= \frac{mn}{N}, & \Var(X) &= \frac{mn}{N}\left(\frac{(m-1)(n-1)}{N-1}+1-\frac{mn}{N}\right).
        \end{align*}
    \end{seg}
    For continuous random variables:
    \begin{seg}\named{Uniform Distribution} $X \sim \U[a, b]$\\
        A random variable $X$ is uniform on $[a, b]$. Its PDF and CDF are:
        \begin{align*}
            f_{X}(x) &= \begin{cases}
                \frac{1}{b-a}, &a \leq x \leq b,\\
                0, &\text{otherwise}
            \end{cases} & F_{X}(x) &= \begin{cases}
                0, &x < a,\\
                \frac{x-a}{b-a}, &a \leq x \leq b,\\
                1, &x > b.
            \end{cases}
        \end{align*}    
    \end{seg}
    \begin{seg}\named{Exponential Distribution} $X \sim \Exp(\lambda)$\\
        A random variable $X$ is exponential with parameter $\lambda > 0$. Its PDF and CDF are:
        \begin{align*}
            f_{X}(x) &= \begin{cases}
                0, &x < 0,\\
                \lambda e^{-\lambda x}, &x \geq 0
            \end{cases} & F_{X}(x) &= \begin{cases}
                0, &x < 0,\\
                1-e^{-\lambda x}, &x \geq 0.
            \end{cases}
        \end{align*}
    \end{seg}
    \begin{seg}\named{Normal Distribution / Gaussian Distribution} $X \sim \N(\mu,\sigma^{2})$\\
        A random variable $X$ is normal if it has two parameters $\mu$ and $\sigma^{2}$. Its PDF and CDF are:
        \begin{align*}
            f_{X}(x) &= \frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right), & F_{X}(x) &= \int_{-\infty}^{x}f_{X}(u)\,du, & \expect{X} &= \mu, & \Var(X) &= \sigma^{2}.
        \end{align*}
        A random variable $X$ is standard normal if $\mu = 0$ and $\sigma^{2} = 1$ ($X \sim \N(0,1)$):
        \begin{align*}
            f_{X}(x) &= \phi(x) = \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x^{2}}{2}\right), & F_{X}(x) &= \Phi(x) = \int_{-\infty}^{x}\phi(u)\,du, & \expect{X} &= 0, & \Var(X) &= 1.
        \end{align*}
    \end{seg}
    \begin{seg}\named{Bivariate Normal Distribution}\\
        Two random variables $X$ and $Y$ are bivariate normal with means $\mu_{X}$ and $\mu_{Y}$, variances $\sigma_{X}^{2}$ and $\sigma_{Y}^{2}$, and population correlation coefficient $\rho$ if:
        \begin{equation*}
            f_{X,Y}(x, y) = \frac{1}{2\pi\sigma_{X}\sigma_{Y}\sqrt{1-\rho^{2}}}\exp\left(-\frac{1}{2(1-\rho^{2})}\left(\left(\frac{x-\mu_{X}}{\sigma_{X}}\right)^{2}-2\rho\left(\frac{x-\mu_{X}}{\sigma_{X}}\right)\left(\frac{y-\mu_{Y}}{\sigma_{Y}}\right)+\left(\frac{y-\mu_{Y}}{\sigma_{Y}}\right)^{2}\right)\right).
        \end{equation*}
        Two random variables $X$ and $Y$ are standard bivariate normal if $\mu_{X} = \mu_{Y} = 0$ and $\sigma_{X}^{2} = \sigma_{Y}^{2} = 1$:
        \begin{equation*}
            f_{X,Y}(x, y) = \frac{1}{2\pi\sqrt{1-\rho^{2}}}\exp\left(-\frac{x^{2}-2\rho xy+y^{2}}{2(1-\rho^{2})}\right).
        \end{equation*}
    \end{seg}
    \begin{seg}\named{Multivariate Normal Distribution} $\mathbf{X} \sim \N_{p}(\boldsymbol{\mu},\mathbf{\Sigma})$\\
        A random vector $\mathbf{X}$ with dimension $p$ is $p$-dimensional normal with $p\times 1$ mean vector $\boldsymbol{\mu}$ and $p\times p$ variance-covariance matrix $\mathbf{\Sigma}$ if:
        \begin{equation*}
            f(\mathbf{x}) = (2\pi)^{-\frac{p}{2}}\abs{\mathbf{\Sigma}}^{-\frac{1}{2}}e^{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T}\mathbf{\Sigma}(\mathbf{x}-\boldsymbol{\mu})}.
        \end{equation*}
    \end{seg}
    \begin{seg}\named{Cauchy Distribution} $X \sim \Cauchy(\theta)$\\
        A random variable $X$ has a Cauchy distribution with parameter $\theta$ if:
        \begin{align*}
            f_{X}(x) &= \frac{1}{\pi(1+(x-\theta)^{2})}, & \expect|X| &= \int_{-\infty}^{\infty}\frac{|x|}{\pi(1+(x-\theta)^{2})}\,dx = \infty.
        \end{align*}
    \end{seg}
    \newpage

    \begin{seg}\named{Gamma Distribution} $X \sim \Gam(\alpha,\lambda)$\\
        A random variable $X$ has a gamma distribution with parameters $\alpha$ and $\lambda$ if:
        \begin{align*}
            f_{X}(x) &= \begin{cases}
                0, &x < 0,\\
                \frac{1}{\Gamma(\alpha)}\lambda e^{-\lambda x}(\lambda x)^{\alpha-1}, &x \geq 0
            \end{cases}, & \Gamma(\alpha) &= \int_{0}^{\infty}e^{-y}y^{\alpha-1}\,dy, & \expect{X} &= \frac{\alpha}{\lambda}, & \Var(X) &= \frac{\alpha}{\lambda^{2}}.
        \end{align*}
        Here, $\Gamma(\alpha)$ is the gamma function, defined as $\Gamma(\alpha) = (\alpha-1)\Gamma(\alpha-1)$. If $\alpha$ is a positive integer, then $\Gamma(\alpha) = (\alpha-1)!$.
    \end{seg}
    \begin{seg}\named{Chi-Squared Distribution} $Y \sim \chi^{2}(n)$\\
        Assume that $X_{1}, X_{2}, \cdots, X_{n}$ are independent standard normal random variables. Let $Y = \sum_{i = 1}^{n}X_{i}^{2}$. The random variable $Y$ has a $\chi^{2}$-distribution with parameter $n$ if:
        \begin{align*}
            f_{Y}(x) &= \begin{cases}
                0, &x < 0,\\
                \frac{1}{\Gamma\left(\frac{n}{2}\right)}2^{-\frac{n}{2}}x^{\frac{n}{2}-1}e^{-\frac{x}{2}}, &x \geq 0
            \end{cases}, & \expect{Y} &= n, & \Var(Y) &= 2n.
        \end{align*}
    \end{seg}
    \begin{seg}\named{Student's t-Distribution} $W \sim t(n)$\\
        Given $Y \sim \chi^{2}(n)$ and $Z \sim \N(0,1)$, if $Y$ and $Z$ are independent, let:
        \begin{equation*}
            W = \frac{Z}{\sqrt{\frac{Y}{n}}}.
        \end{equation*}
        The random variable $W$ follows the $t$-distribution with $n$ degrees of freedom, and:
        \begin{align*}
            f(w) &= \frac{\Gamma\left(\frac{n+1}{2}\right)}{\sqrt{n\pi}\Gamma\left(\frac{n}{2}\right)}\left(1+\frac{w^{2}}{n}\right)^{-\frac{n+1}{2}}, & \expect{W} &= \begin{cases}
                \text{Undefined}, &n \leq 1,\\
                0, &n > 1
            \end{cases}, & \Var(W) &= \begin{cases}
                \text{Undefined}, &n \leq 1,\\
                \infty, &1 < n \leq 2,\\
                \frac{n}{n-2}, &n > 2
            \end{cases}.
        \end{align*}
        Here, $\Gamma(\alpha)$ is the gamma function.
    \end{seg}
    \begin{seg}\named{Beta Distribution} $X \sim \Beta(a, b)$\\
        A random variable $X$ has a beta distribution with parameters $a$ and $b$ if:
        \begin{align*}
            f_{X}(x) &= \begin{cases}
                \frac{1}{B(a, b)}x^{a-1}(1-x)^{b-1}, &0 < x < 1,\\
                0, &\text{otherwise}
            \end{cases}, & B(a, b) &= \int_{0}^{1}x^{a-1}(1-x)^{b-1}\,dx = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)},\\
            \expect{X} &= \frac{a}{a+b}, & \Var(X) &= \frac{ab}{(a+b)^{2}(a+b+1)}.
        \end{align*}
        Here, $B(a, b)$ is the beta function.
    \end{seg}
    \begin{seg}\named{F-Distribution} $F \sim F(r_{1},r_{2})$\\
        Assume that $X$ and $Y$ are independent random variables with $X \sim \chi^{2}(r_{1})$ and $Y \sim \chi^{2}(r_{2})$. Let:
        \begin{equation*}
            F = \frac{\frac{X}{r_{1}}}{\frac{Y}{r_{2}}}.
        \end{equation*}
        Then $F$ has an $F$-distribution with $r_{1}$ and $r_{2}$ degrees of freedom, and:
        \begin{equation*}
            f_{F}(w) = \frac{\Gamma\left(\frac{r_{1}+r_{2}}{2}\right)}{\Gamma\left(\frac{r_{1}}{2}\right)\Gamma\left(\frac{r_{2}}{2}\right)}\left(\frac{r_{1}}{r_{2}}\right)^{\frac{r_{1}}{2}}w^{\frac{r_{1}}{2}-1}\left(1+\frac{r_{1}w}{r_{2}}\right)^{-\frac{r_{1}+r_{2}}{2}},
        \end{equation*}
        where $0 < w < \infty$.
    \end{seg}

\chapter{Generating Function}
	\label{Chapter 7 (Generating Function)}
\section{Introduction to Generating Functions}
    A sequence of numbers $a = \{a_{i}:i = 0,1,2,\cdots\}$ may contain a lot of information. For example, the values of a PMF describe the distribution of a discrete random variable. A concise way of storing this information is to encapsulate the numbers in a generating function. 
    \begin{defn}
        For any sequence $\{a_{n}:n = 0,1,2,\cdots\}$, we define the generating function as:
        \begin{equation*}
            G_{a}(s) = \sum_{i = 0}^{\infty}a_{i}s^{i} = \lim_{N \to \infty}\sum_{i = 0}^{N}a_{i}s^{i},
        \end{equation*}
        for $s \in \mathbb{R}$ if the limit exists. 
    \end{defn}
    \begin{rem}
        We can observe that:
        \begin{equation*}
            a_{i} = \frac{G_{a}^{(i)}(0)}{i!}.
        \end{equation*}
    \end{rem}
    \begin{rem}
        Sometimes, we cannot interchange a countable sum with derivatives.
    \end{rem}
    \begin{eg}
        Let $b_{n}(x) = \frac{\sin{nx}}{n}$ such that $a_{1}(x) = b_{1}(x)$ and $a_{n}(x) = b_{n}(x)-b_{n-1}(x)$.
        \begin{align*}
            \tag{Squeeze Theorem}
            \sum_{n = 0}^{\infty}a_{n}(x) &= \lim_{n \to \infty}\sum_{i = 0}^{n}a_{n}(x) = \lim_{n \to \infty}\frac{\sin{nx}}{n} = 0,\\
            \lim_{n \to \infty}\pdv*{\sum_{i = 0}^{n}a_{i}(x)}{x} &= 0,\\
            \lim_{n \to \infty}\sum_{i = 0}^{n}\pdv*{a_{n}(x)}{x} &= \lim_{n \to \infty}\cos{nx}\quad \text{does not exist}.
        \end{align*}
    \end{eg}
    Convolutions are common in probability theory, and generating functions provide a useful tool for studying them.
    \begin{defn}
        Let $a = \{a_{i}:i \geq 0\}$ and $b = \{b_{i}:i \geq 0\}$ be two sequences of real numbers. The \textbf{convolution} $c = a*b = \{c_{i}:i \geq 0\}$ of $\{a_{i}\}$ and $\{b_{i}\}$ is defined as:
        \begin{equation*}
            c_{n} = \sum_{i = 0}^{n}a_{i}b_{n-i}.
        \end{equation*}
    \end{defn}
    \begin{eg}
        If $a_{n} = f_{X}(n)$ and $b_{n} = f_{Y}(n)$, then $c_{n} = f_{X+Y}(n)$.
    \end{eg}
    \begin{lem}
        If sequences $a$ and $b$ have generating functions $G_{a}(s)$ and $G_{b}(s)$ respectively, then:
        \begin{equation*}
            G_{c}(s) = G_{a}(s)G_{b}(s).
        \end{equation*}
    \end{lem}
    \begin{proofing}
        \begin{equation*}
            G_{c}(s) = \sum_{n = 0}^{\infty}c_{n}s^{n} = \sum_{n = 0}^{\infty}\sum_{i = 0}^{n}a_{i}b_{n-i}s^{i}s^{n-i} = \sum_{i = 0}^{\infty}a_{i}s^{i}\sum_{n = i}^{\infty}b_{n-i}s^{n-i} = \sum_{i = 0}^{\infty}a_{i}s^{i}\sum_{j = 0}^{\infty}b_{j}s^{j} = G_{a}(s)G_{b}(s).
        \end{equation*}
    \end{proofing}
    From the definition of a generating function, we see that it is a power series. We may want to determine whether the series is convergent.
    \begin{defn}
        The \textbf{radius of convergence} $R$ of a power series is the half-size of an interval such that the power series $f(s)$ is convergent. If $s \in (-R,R)$, then $f(s)$ is convergent. If $s \in [-R,R]^{\complement}$, then $f(s)$ is divergent.\\
        We can determine the radius of convergence by applying the root test:
        \begin{equation*}
            R = \frac{1}{\limsup_{n \to \infty}\sqrt[n]{\abs{a_{n}}}}.
        \end{equation*}
    \end{defn}
    \begin{rem}
        Additional tests are required to determine whether the power series converges at $s = -R$ and $s = R$.
    \end{rem}
    \begin{rem}
        Sometimes, it is difficult to compute $R$ using the root test. A convenient alternative is the ratio test. If the limit exists:
        \begin{equation*}
            R = \lim_{n \to \infty}\abs{\frac{a_{n}}{a_{n+1}}}.
        \end{equation*}
    \end{rem}
    Here are some properties of power series involving the radius of convergence. We will not prove them since the proofs are not essential.
    \begin{thm}
        If $R$ is the radius of convergence of $G_{a}(s) = \sum_{i = 0}^{\infty}a_{i}s^{i}$, then:
        \begin{enumerate}
            \item $G_{a}(s)$ converges absolutely for all $\abs{s} < R$ and diverges for all $\abs{s} > R$.
            \item $G_{a}(s)$ can be differentiated or integrated term by term for any fixed number of times if $\abs{s} < R$:
            \begin{equation*}
                \pdv*[order = {i}]{\sum_{n = 0}^{\infty}a_{n}s^{n}}{s} = \sum_{n = 0}^{\infty}\pdv*[order = {i}]{a_{n}s^{n}}{s}.
            \end{equation*}
            \item If $R > 0$ and $G_{a}(s) = G_{b}(s)$ for all $\abs{s} \leq R'$ for some $0 < R' \leq R$, then $a_{n} = b_{n}$ for all $n$.
        \end{enumerate}
    \end{thm}
    \begin{rem}
        For any sequence $\{a_{n}:n \geq 0\}$, if the radius of convergence of $G_{a}(s)$ is positive, then $\{a_{n}:n \geq 0\}$ is uniquely determined by $G_{a}(s)$ via:
        \begin{equation*}
            a_{n} = \frac{1}{n!}G_{a}^{(n)}(0).
        \end{equation*}
    \end{rem}
    Suppose that $X$ is a discrete random variable taking values in the non-negative integers. We can see how the generating function works in probability.
    \begin{defn}
        The \textbf{probability generating function} (PGF) of a non-negative random variable $X$ is:
        \begin{equation*}
            G_{X}(s) = \expect{s^{X}} = \sum_{i = 0}^{\infty}s^{i}f_{X}(i).
        \end{equation*}
    \end{defn}
    Using this, we can determine the distribution of a random variable with the following theorem.
    \begin{thm}
        Given two random variables $X$ and $Y$ with corresponding PGFs, if the two PGFs are the same, then $X$ and $Y$ have the same distribution.
    \end{thm}
    This is particularly useful for finding the distribution of a random variable.
    \begin{eg}
        Suppose that $X \independent Y$. Let $X \sim \Poisson(\lambda)$ and $Y \sim \Poisson(\mu)$. What is the distribution of $Z = X+Y$? Recall that $f_{Z} = f_{X}*f_{Y}$. We let $a_{n} = f_{X}(n)$ and $b_{n} = f_{Y}(n)$.
        \begin{align*}
            G_{X}(s) &= \sum_{i = 0}^{\infty}\frac{\lambda^{i}e^{-\lambda}}{i!}s^{i} = e^{\lambda(s-1)}, & G_{Y}(s) &= e^{\mu(s-1)}, & G_{Z}(s) &= e^{(\lambda+\mu)(s-1)}.
        \end{align*}
        We may conclude that $Z \sim \Poisson(\lambda+\mu)$.
    \end{eg}
    \newpage

    \begin{rem}
        If $a_{n} = f_{X}(n)$ for some random variable $X$, then $R \geq 1$ for $G_{X}(s) = G_{a}(s)$ since:
        \begin{equation*}
            \sum_{n = 0}^{\infty}f_{X}(n)s^{n}
        \end{equation*}
        converges when $s \in [-1,1]$.
    \end{rem}
    \begin{eg}
        Let $X \sim \Poisson(\lambda)$ and $a_{n} = f_{X}(n) = \frac{\lambda^{n}e^{-\lambda}}{n!}$. By the ratio test, as $n \to \infty$:
        \begin{equation*}
            \frac{a_{n}}{a_{n+1}} = \frac{n+1}{\lambda} \to \infty.
        \end{equation*}
        Therefore, $R = \infty$.
    \end{eg}
    \begin{eg}
        Let $X$ have a PMF $a_{n} = f_{X}(n) = \frac{c}{n^{2}}$. By the ratio test, as $n \to \infty$:
        \begin{equation*}
            \frac{a_{n}}{a_{n+1}} = \frac{(n+1)^{2}}{n} \to 1.
        \end{equation*}
        Therefore, $R = 1$.
    \end{eg}
    There is an important theorem regarding $s = 1$. Again, we will not prove it.
    \begin{thm}\named{Abel's Theorem}
        Suppose that $a_{n} \geq 0$ for all $n$. If $a$ has a generating function $G_{a}(s)$ and a radius of convergence $R = 1$, then if $\sum_{n = 0}^{\infty}a_{n}$ converges in $\mathbb{R}\cup\{\infty\}$, we have:
        \begin{equation*}
            \lim_{s \to 1^{-}}G_{a}(s) = \sum_{n = 0}^{\infty}a_{n}\lim_{s \to 1^{-}}s^{n} = \sum_{n = 0}^{\infty}a_{n}.
        \end{equation*}
    \end{thm}
    \begin{eg}
        If a random variable $X \sim \Bern(p)$ for some $p$, then:
        \begin{equation*}
            G_{X}(s) = ps^{1}+(1-p)s^{0} = 1-p+ps.
        \end{equation*}
    \end{eg}
    \begin{eg}
        If a random variable $X \sim \Bin(p)$ for some $n$ and $p$, then:
        \begin{equation*}
            G_{X}(s) = (1-p+ps)^{n}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        If a random variable $X \sim \Geom(p)$ for some $p$, then:
        \begin{equation*}
            G_{X}(s) = \sum_{n = 1}^{\infty}(1-p)^{n-1}ps^{n} = \frac{ps}{1-s(1-p)}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        If a random variable $X \sim \Poisson(\lambda)$ for some $\lambda$, then:
        \begin{equation*}
            G_{X}(s) = e^{\lambda(s-1)}.
        \end{equation*}
    \end{eg}
    \newpage

    We already know that by computing the derivatives of $G$ at $s = 0$, we can obtain the probability sequence. The following theorem shows that we can derive the moment sequence by computing the derivatives of $G$ at $s = 1$.
    \begin{thm}
        If a random variable $X$ has a PGF $G_{X}(s)$, then:
        \begin{enumerate}
            \item $\expect{X} = \lim_{s \to 1^{-}}G'(s) = G'(1)$,
            \item $\expect(X(X-1)\cdots(X-k+1)) = G^{(k)}(1)$,
            \item $\Var(X) = G''(1)+G'(1)-(G'(1))^{2}$.
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item By evaluating at $s = 1$:
            \begin{equation*}
                \left.\pdv*{G_{X}(s)}{s}\right|_{s = 1} = \left.\pdv*{\sum_{k = 0}^{\infty}f_{X}(k)s^{k}}{s}\right|_{s = 1} = \left.\sum_{k = 1}^{\infty}kf_{X}(k)s^{k-1}\right|_{s = 1} = \sum_{k = 1}^{\infty}kf_{X}(k) = \expect{X}.
            \end{equation*}
            \item Let $s < 1$:
            \begin{equation*}
                G^{(k)}(s) = \pdv*[order = {k}]{\sum_{n}f_{X}(n)s^{n}}{s} = \sum_{n}n(n-1)\cdots(n-k+1)s^{n-k}f_{X}(n) = \expect(s^{X-k}X(X-1)\cdots(X-k+1)).
            \end{equation*}
            By applying Abel's Theorem, we obtain:
            \begin{equation*}
                G^{(k)}(1) = \expect(X(X-1)\cdots(X-k+1)).
            \end{equation*}
            \item 
            \begin{equation*}
                \Var(X) = \expect(X^{2})-(\expect{X})^{2} = \expect(X(X-1))+\expect{X}-(\expect{X})^{2} = G''(1)+G'(1)-(G'(1))^{2}.
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    Interestingly, we can also use generating functions to deal with the sum of a random number of independent random variables.
    \begin{thm}
        Let $X_{1},X_{2},\cdots$ be a sequence of independent and identically distributed (i.i.d.) random variables with a common PGF $G_{X}(s)$, and let $N$ be a random variable independent of $X_{i}$ for all $i$, with PGF $G_{N}(s)$. If $T = X_{1}+X_{2}+\cdots+X_{N}$, then:
        \begin{equation*}
            G_{T}(s) = G_{N}(G_{X}(s)).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        \begin{align*}
            G_{T}(s) &= \expect{s^{T}},\\
            &= \expect(\expect(s^{T}|N)),\\
            &= \sum_{n}\expect(s^{T}|N = n)\prob(N = n),\\
            &= \sum_{n}\expect(s^{X_{1}+X_{2}+\cdots+X_{n}}|N = n)\prob(N = n),\\
            &= \sum_{n}(G_{X}(s))^{n}\prob(N = n),\\
            &= G_{N}(G_{X}(s)).
        \end{align*}
    \end{proofing}
    \begin{eg}
        The sum of a Poisson number of independent Bernoulli random variables is still Poisson.\\
        Let $G_{N}(t) = e^{\lambda(t-1)}$ and $G_{X}(s) = 1-p+ps$.
        \begin{equation*}
            G_{T}(s) = G_{N}(G_{X}(s)) = e^{\lambda(1-p+ps-1)} = e^{\lambda p(s-1)}.
        \end{equation*}
        Therefore, $T \sim \Poisson(\lambda p)$.
    \end{eg}
    \newpage

    When the JPMF exists, there will obviously be a joint PGF.
    \begin{defn}
        Let random variables $X_{1}$ and $X_{2}$ be non-negative integer-valued, jointly discrete with JPMF $f_{X_{1},X_{2}}$. The \textbf{joint probability generating function} (JPGF) is defined as:
        \begin{equation*}
            G_{X_{1},X_{2}}(s_{1},s_{2}) = \expect(s_{1}^{X_{1}}s_{2}^{X_{2}}) = \sum_{i = 0}^{\infty}\sum_{j = 0}^{\infty}s_{1}^{i}s_{2}^{j}f_{X_{1},X_{2}}(i, j).
        \end{equation*}
    \end{defn}
    \begin{rem}
        We can find that:
        \begin{equation*}
            f_{X_{1},X_{2}}(i, j) = \left.\left(\pdv*[order = {i}]{\pdv*[order = {j}]{\frac{G_{X_{1},X_{2}}(s_{1},s_{2})}{i!j!}}{s_{2}}}{s_{1}}\right)\right|_{(s_{1},s_{2}) = (0,0)}.
        \end{equation*}
    \end{rem}
    \begin{thm}
        Random variables $X$ and $Y$ are independent if and only if:
        \begin{equation*}
            G_{X,Y}(s, t) = G_{X}(s)G_{Y}(t).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        If $X \independent Y$:
        \begin{align*}
            G_{X,Y}(s, t) &= \sum_{i = 0}^{\infty}\sum_{j = 0}^{\infty}s^{i}t^{j}f_{X,Y}(i, j),\\
            &= \sum_{i = 0}^{\infty}s^{i}f_{X}(i)\sum_{j = 0}^{\infty}t^{j}f_{Y}(j),\\
            &= G_{X}(s)G_{Y}(t).
        \end{align*}
        If $G_{X,Y}(s, t) = G_{X}(s)G_{Y}(t)$, we consider the coefficient of terms $s^{i}t^{j}$ for all $i \geq 0$ and $j \geq 0$:
        \begin{align*}
            s^{i}t^{j}f_{X,Y}(i, j) &= s^{i}f_{X}(i)t^{j}f_{Y}(j),\\
            f_{X,Y}(i, j) &= f_{X}(i)f_{Y}(j).
        \end{align*}
        Therefore, $X \independent Y$.
    \end{proofing}
    \begin{thm}
        \label{Chapter 7 (Theorem) PGF for sum of random independent variables}
        If random variables $X$ and $Y$ are independent, then:
        \begin{equation*}
            G_{X+Y}(t) = G_{X}(t)G_{Y}(t).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        \begin{equation*}
            G_{X+Y}(t) = \expect(t^{X+Y}) = \expect(t^{X})\expect(t^{Y}) = G_{X}(t)G_{Y}(t).
        \end{equation*}
    \end{proofing}
    \begin{rem}
        The converse does not necessarily hold true.
    \end{rem}
    \newpage

\section{Applications of Generating Functions}
    The following example involves a simple random walk, which is discussed in Appendix \ref{Appendix A (Random Walk)}. Generating functions are particularly valuable when studying random walks. So far, we have only considered random variables $X$ taking finite values. In this application, we encounter variables that can take the value $+\infty$. For such variables $X$, $G_{X}(s)$ converges as long as $\abs{s} < 1$ and:
    \begin{equation*}
        \lim_{s \to 1^{-}}G_{X}(s) = \sum_{k}\prob(X = k) = 1-\prob(X = \infty).
    \end{equation*}
    \begin{defn}
        A random variable $X$ is \textbf{defective} if $\prob(X = \infty) > 0$.
    \end{defn}
    \begin{rem}
        It is no surprise that the expectation is infinite when the random variable is defective.
    \end{rem}
    With this generalization, we can start discussing random walks.
    \begin{eg}\named{Recurrence and Transience of Random Walk}
        \label{Chapter 7 (Example) Simple random walk recurrence and transience}
        Let $Y_{n}$ be the position of a particle after $n$ moves in a simple random walk on $\mathbb{Z}$, and let $X_{i}$ be independent and identically distributed random variables mentioned in Appendix \ref{Appendix A (Random Walk)}. For $n \geq 0$:
        \begin{align*}
            Y_{n} &= \sum_{i = 1}^{n}X_{i}, & Y_{0} &= 0, & \prob(X_{i} = 1) &= p, & \prob(X_{i} = -1) &= q = 1-p.
        \end{align*}
        Define the first return time to the origin as:
        \begin{equation*}
            T_{0} = \min\{i \geq 1:Y_{i} = 0\}.
        \end{equation*}
        We want to determine whether $T_{0}$ is a defective random variable. To do this, we need to calculate $\prob(T_{0} = \infty)$. Let $p_{0}(n)$ be the probability that the particle is at the origin after $n$ moves, and let $P_{0}$ be the generating function of $p_{0}$. Let $f_{0}(n)$ be the probability that the particle returns to the origin for the first time after $n$ moves, and let $F_{0}$ be the generating function of $f_{0}$.
        \begin{align*}
            p_{0}(n) &= \prob(Y_{n} = 0) = \begin{cases}
                \binom{n}{\frac{n}{2}}p^{\frac{n}{2}}q^{\frac{n}{2}}, &n\text{ is even},\\
                0, &n\text{ is odd}
            \end{cases},\\
            P_{0}(s) &= \lim_{N \to \infty}\sum_{n = 0}^{N}p_{0}(n)s^{n},\\
            f_{0}(n) &= \prob(Y_{1} \neq 0,Y_{2} \neq 0,\cdots,Y_{n-1} \neq 0,Y_{n} = 0) = \prob(T_{0} = n),\\
            F_{0}(s) &= \lim_{N \to \infty}\sum_{n = 1}^{N}f_{0}(n)s^{n}.
        \end{align*}
    \end{eg}
    \newpage

    \begin{thm}
        \label{Chapter 7 (Theorem) Simple random walk particle return generating function}
        From the definitions in Example \ref{Chapter 7 (Example) Simple random walk recurrence and transience}, we have:
        \begin{enumerate}
            \item $P_{0}(s) = 1+P_{0}(s)F_{0}(s)$,
            \item $P_{0}(s) = (1-4pqs^{2})^{-\frac{1}{2}}$,
            \item $F_{0}(s) = 1-(1-4pqs^{2})^{\frac{1}{2}}$.
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item By using the Law of Total Probability:
            \begin{align*}
                p_{0}(n) &= \sum_{i = 1}^{n}\prob(Y_{n} = 0|Y_{1} \neq 0,Y_{2} \neq 0,\cdots,Y_{i-1} \neq 0,Y_{i} = 0)f_{0}(i),\\
                \tag{Markov property in Lemma \ref{Appendix A (Lemma) Simple random walk properties}}
                &= \sum_{i = 1}^{n}\prob(Y_{n} = 0|Y_{i} = 0)f_{0}(i),\\
                \tag{Temporarily homogeneous property in Lemma \ref{Appendix A (Lemma) Simple random walk properties}}
                &= \sum_{i = 1}^{n}\prob(Y_{n-i} = 0)f_{0}(i),\\
                &= \sum_{i = 1}^{n}p_{0}(n-k)f_{0}(i),\\
                p_{0}(0) &= 1,\\
                P_{0}(s) &= \sum_{k = 0}^{\infty}p_{0}(k)s^{k} = 1+\sum_{k = 1}^{\infty}p_{0}(k)s^{k},\\
                &= 1+\sum_{k = 1}^{\infty}\sum_{i = 1}^{k}p_{0}(k-i)f_{0}(i)s^{k},\\
                &= 1+\sum_{i = 1}^{\infty}\sum_{k = i}^{\infty}p_{0}(k-i)s^{k-i}f_{0}(i)s^{i},\\
                &= 1+P_{0}(s)F_{0}(s).
            \end{align*}
            \item \textit{If you want to understand the proof, search "Central binomial coefficient" in Wikipedia.}\\
            We know that $Y_{n} = 0$ if $n$ is even. Therefore:
            \begin{align*}
                P_{0}(s) &= \lim_{N \to \infty}\sum_{n = 0}^{N}p_{0}(n)s^{n},\\
                &= \lim_{N \to \infty}\sum_{i = 0}^{N}\binom{2i}{i}p^{i}q^{i}s^{2i},\\
                \tag{$\binom{\frac{-1}{2}}{i}$ is a generalized binomial coefficient}
                &= \lim_{N \to \infty}\sum_{i = 1}^{N}(-1)^{i}4^{i}\binom{\frac{-1}{2}}{i}p^{i}q^{i}s^{2i},\\
                &= \frac{1}{\sqrt{1-4pqs^{2}}}.
            \end{align*}
            \item By applying (1) and (2), we can get:
            \begin{equation*}
                F_{0}(s) = \frac{P_{0}(s)-1}{P_{0}(s)} = 1-\sqrt{1-4pqs^{2}}.
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    \newpage

    From this theorem, we derive the following corollary.
    \begin{cor}
        The probability that the particle ever returns to the origin is:
        \begin{equation*}
            \sum_{n = 1}^{\infty}f_{0}(n) = F_{0}(1) = 1-\abs{p-q}.
        \end{equation*}
        The probability that the particle will never return to the origin is:
        \begin{equation*}
            \prob(T_{0} = \infty) = \abs{p-q}.
        \end{equation*}
    \end{cor}
    \begin{proofing}
        By using Theorem \ref{Chapter 7 (Theorem) Simple random walk particle return generating function}, since $p+q = 1$:
        \begin{align*}
            F_{0}(1) &= 1-(1-4pq)^{\frac{1}{2}},\\
            &= 1-(p^{2}-2pq+q^{2})^{\frac{1}{2}},\\
            &= 1-\abs{p-q}.
        \end{align*}
    \end{proofing}
    \begin{defn}
        A random walk is \textbf{recurrent} if it has at least one recurrent point, or equivalently:
        \begin{equation*}
            \prob(X < \infty) = 1.
        \end{equation*}
        A random walk is \textbf{transient} if it has no recurrent points, or equivalently:
        \begin{equation*}
            \prob(X = \infty) > 0.
        \end{equation*}
    \end{defn}
    \begin{rem}
        When $p = q = \frac{1}{2}$, $\prob(T_{0} = \infty) = 0$, and therefore the random walk is recurrent.
    \end{rem}
    \begin{rem}
        If $p \neq q$, then $\prob(T_{0} = \infty) \neq 0$, and so the random walk is transient.
    \end{rem}
    \begin{eg}
        Revisit Example \ref{Chapter 7 (Example) Simple random walk recurrence and transience}. When $p = q = \frac{1}{2}$:
        \begin{align*}
            F_{0}(s) &= 1-\sqrt{1-s^{2}}, & F_{0}'(s) &= \frac{s}{\sqrt{1-s^{2}}}, & \expect{T_{0}} &= \lim_{s \to 1^{-}}F_{0}'(s) = \infty.
        \end{align*}
        Therefore, even though the random walk is recurrent, the expected return time is infinite.
    \end{eg}
    \newpage

    We now move on to our next important application, the Branching Process.\\
    Many scientists have been interested in modeling reproduction in a population. Accurate models for evolution are extremely difficult to handle, but some non-trivial models are tractable. We will investigate one such model.
    \begin{eg}\named{Galton-Watson Process}
        Consider a population in which each individual in generation $n$ produces some number of offspring according to a fixed probability distribution, and these offspring form generation $n+1$. This process continues indefinitely. This is known as a \textbf{branching process} or \textbf{Galton-Watson process}.\\
        Let $Z_{n}$ be the size of generation $n$. Let $X_{i}^{(n)}$ be the number of offspring produced by the $i$-th individual in generation $n$. Then:
        \begin{equation*}
            Z_{n+1} = \begin{cases}
                X_{1}^{(n)}+X_{2}^{(n)}+\cdots+X_{Z_{n}}^{(n)}, &Z_{n} \geq 1,\\
                0, &Z_{n} = 0.
            \end{cases}
        \end{equation*}
        We make the following assumptions:
        \begin{enumerate}
            \item All individuals reproduce independently. ($X_{i}^{(k)}$'s are independent.)
            \item All individuals reproduce according to the same distribution. ($X_{i}^{(k)}$'s are identically distributed.)
        \end{enumerate}
        We also assume that $Z_{0} = 1$ (the population starts with one individual). We are interested in the distribution of $Z_{n}$ for all $n$.
    \end{eg}
    \begin{rem}
        $Z_{1} = X_{1}^{(0)}$
    \end{rem}
    \begin{thm}
        \label{Chapter 7 (Theorem) Galton-Watson process PGF n-fold iterate properties}
        Let $G_{n}(s) = \expect{s^{Z_{n}}}$ and $G(s) = G_{1}(s) = \expect{s^{Z_{1}}} = \expect{s^{X_{i}^{(m)}}}$ for all $i$ and $m$. Then:
        \begin{equation*}
            G_{n}(s) = G(G(\cdots(G(s))\cdots)) = G(G_{n-1}(s)) = G_{n-1}(G(s)),
        \end{equation*}
        is the $n$-fold iteration of $G$. This further implies:
        \begin{equation*}
            G_{m+n}(s) = G_{m}(G_{n}(s)) = G_{n}(G_{m}(s)).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        When $n = 2$:
        \begin{align*}
            G_{2}(s) &= \expect{s^{Z_{2}}},\\
            &= \expect{s^{X_{1}^{(1)}+X_{2}^{(1)}+\cdots+X_{Z_{1}}^{(1)}}},\\
            &= G_{Z_{1}}\left(G_{X_{1}^{(1)}}(s)\right),\\
            &= G(G(s)).
        \end{align*}
        When $n = m+1$ for some $m$:
        \begin{align*}
            G_{m+1}(s) &= \expect{s^{Z_{m+1}}},\\
            &= \expect{s^{X_{1}^{(m)}+X_{2}^{(m)}+\cdots+X_{Z_{m}}^{(m)}}},\\
            &= G_{Z_{m}}\left(G_{X_{1}^{(m)}}(s)\right),\\
            &= G_{m}(G(s)).
        \end{align*}
    \end{proofing}
    \newpage

    In principle, the above theorem describes the distribution of $Z_{n}$. However, computing $G_{n}(s)$ explicitly can be quite challenging. Fortunately, the moments of $Z_{n}$ can be computed more easily.
    \begin{lem}
        Let $\expect{Z_{1}} = \expect{X_{i}^{(m)}} = \mu$ and $\Var(Z_{1}) = \sigma^{2}$. Then:
        \begin{align*}
            \expect{Z_{n}} &= \mu^{n}, & \Var(Z_{n}) &= \begin{cases}
                n\sigma^{2}, &\mu = 1,\\
                \frac{\sigma^{2}(\mu^{n}-1)\mu^{n-1}}{\mu-1}, &\mu \neq 1.
            \end{cases}
        \end{align*}
    \end{lem}
    \begin{proofing}
        Using Theorem \ref{Chapter 7 (Theorem) Galton-Watson process PGF n-fold iterate properties}, we find:
        \begin{align*}
            \expect{Z_{2}} &= G_{2}'(1),\\
            &= G'(G(1))G'(1),\\
            &= G'(1)\mu,\\
            &= \mu^{2},\\
            \expect{Z_{n}} &= G_{n}'(1),\\
            &= G'(G_{n-1}(1))G_{n-1}'(1),\\
            &= G'(1)\mu^{n-1},\\
            &= \mu^{n}.\\
            G_{1}''(1) &= \sigma^{2}+(G'(1))^{2}-G'(1),\\
            &= \sigma^{2}+\mu^{2}-\mu,\\
            G_{2}''(1) &= G''(G(1))(G'(1))^{2}+G'(G(1))G''(1),\\
            &= G''(1)(\mu^{2}+\mu),\\
            G_{n}''(1) &= G''(G_{n-1}(1))(G_{n-1}'(1))^{2}+G'(G_{n-1}(1))G_{n-1}''(1),\\
            &= (\sigma^{2}+\mu^{2}-\mu)\mu^{2n-2}+\mu G_{n-1}''(1),\\
            &= \mu^{2n-2}(\sigma^{2}+\mu^{2}-\mu)+\mu^{2n-3}(\sigma^{2}+\mu^{2}-\mu)+\cdots+\mu^{n-1}(\sigma^{2}+\mu^{2}-\mu),\\
            &= \frac{\mu^{n-1}(\sigma^{2}+\mu^{2}-\mu)(\mu^{n}-1)}{\mu-1}.
        \end{align*}
        If $\mu = 1$:
        \begin{align*}
            \Var(Z_{n}) &= G_{n}''(1)+G_{n}'(1)-(G_{n}'(1))^{2},\\
            &= \sigma^{2}+G_{n-1}''(1)+1-1,\\
            &= n\sigma^{2}.
        \end{align*}
        If $\mu \neq 1$:
        \begin{align*}
            \Var(Z_{n}) &= G_{n}''(1)+G_{n}'(1)-(G_{n}'(1))^{2},\\
            &= \frac{\mu^{n-1}(\sigma^{2}+\mu^{2}-\mu)(\mu^{n}-1)}{\mu-1}+\mu^{n}-\mu^{2n},\\
            &= \frac{\mu^{n-1}\sigma^{2}(\mu^{n}-1)}{\mu-1}.
        \end{align*}
    \end{proofing}
    \begin{eg}
        We are particularly interested in the probability of ultimate extinction, i.e., the event that the population eventually dies out. Note that if $Z_{n} = 0$ for some $n$, then $Z_{m} = 0$ for all $m > n$. Therefore:
        \begin{align*}
            \{\text{ultimate extinction}\} &= \bigcup_{n}\{Z_{n} = 0\} = \lim_{n \to \infty}\{Z_{n} = 0\},\\
            \prob(\text{ultimate extinction}) &= \prob\left(\lim_{n \to \infty}\{Z_{n} = 0\}\right) = \lim_{n \to \infty}\prob(Z_{n} = 0) = \lim_{n \to \infty}G_{n}(0).
        \end{align*}
        Let $\eta_{n} = G_{n}(0)$ and $\eta = \lim_{n \to \infty}\eta_{n}$.
    \end{eg}
    \newpage

    \begin{thm}
        The probability of ultimate extinction $\eta$ is the smallest non-negative root of the equation:
        \begin{equation*}
            s = G(s).
        \end{equation*}
        Furthermore:
        \begin{enumerate}
            \item If $\mu \leq 1$, then $\eta = 1$.
            \item If $\mu > 1$, then $\eta < 1$ and $G'(\eta) \leq 1$.
            \item If $\mu = 1$ and $\sigma^{2} > 0$, then $\eta = 1$.
            \item If $\mu = 1$ and $\sigma^{2} = 0$, then $\eta = 0$.
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{equation*}
            \eta_{n} = G_{n}(0) = G(G_{n-1}(0)) = G(\eta_{n-1}).
        \end{equation*}
        Since $G(s)$ is continuous and non-decreasing on $[0,1]$, $\eta_{n}$ is a non-decreasing sequence bounded above by 1. Therefore, $\eta_{n}$ converges to some limit $\eta \leq 1$.
        \begin{equation*}
            \eta = \lim_{n \to \infty}\eta_{n} = \lim_{n \to \infty}G(\mu_{n-1}) = G\left(\lim_{n \to \infty}\eta_{n-1}\right) = G(\eta).
        \end{equation*}
        Thus, $\eta$ is a non-negative root of the equation $s = G(s)$.\\
        Let $\psi$ be any non-negative root of the equation $s = G(s)$. We will show that $\eta \leq \psi$. Since $G(s)$ is non-decreasing on $[0,1]$ and $\psi \geq 0$:
        \begin{align*}
            \eta_{1} &= G(0) \leq G(\psi) = \psi, & \eta_{2} &= G(\eta_{1}) \leq G(\psi) = \psi.
        \end{align*}
        By induction, we find that $\eta_{n} \leq \psi$ for all $n$. Taking the limit as $n \to \infty$, we obtain $\eta \leq \psi$.\\
        Next, we analyze the properties of $G(s)$ on $[0,1]$:
        \begin{equation*}
            G''(s) = \sum_{i = 2}^{\infty}i(i-1)s^{i-2}\prob(Z_{1} = i) \geq 0.
        \end{equation*}
        Thus, $G(s)$ is convex on $[0,1]$. Since $G(1) = 1$, the line $y = s$ intersects the curve $y = G(s)$ at $s = 1$.\\
        If $\mu = G'(1) \leq 1$, then $G(s)$ lies above the line $y = s$ for all $s \in [0,1)$, and so the only intersection is at $s = 1$. Thus, $\eta = 1$.\\
        If $\mu = G'(1) > 1$, then $G(s)$ intersects the line $y = s$ at some $s = k \in (0,1)$. Since $G(s)$ is convex, there can be only one such intersection. Thus, $\eta = k < 1$.\\
        Furthermore, since $G(s)$ is convex and intersects the line $y = s$ at $s = \eta$ and $s = 1$, we have:
        \begin{equation*}
            \sigma^{2} = G''(1)+G'(1)-(G'(1))^{2} = G''(1).
        \end{equation*}
        If $\mu = 1$ and $\sigma^{2} > 0$, then $G''(1) > 0$. Therefore, $G(s)$ is strictly convex, and the line $y = s$ intersects the curve $y = G(s)$ only at $s = 1$. Thus, $\eta = 1$.\\
        If $\mu = 1$ and $\sigma^{2} = 0$, then $G''(1) = 0$. Therefore, $G(s)$ is linear, and since $G(1) = 1$ and $G'(1) = 1$, we have $G(s) = s$. Thus, $\eta = 0$.
    \end{proofing}
    \newpage

\section{Moment Generating Function and Characteristic Function}
    Recall that both discrete and continuous distributions can be unified into one framework. We can redefine the PGF accordingly.
    \begin{defn}
        The \textbf{probability generating function} of a random variable $X$ is given by:
        \begin{equation*}
            \expect{s^{X}} = \int s^{x}\,dF_{X}.
        \end{equation*}
    \end{defn}
    For more general random variables $X$, it is convenient to substitute $s = e^{t}$. This leads to the following definition.
    \begin{defn}
        The \textbf{moment generating function} (MGF) of a random variable $X$ is the function $M:\mathbb{R} \to [0,\infty)$ defined as:
        \begin{equation*}
            M_{X}(t) = \expect(e^{tX}) = \int e^{tx}\,dF_{X}.
        \end{equation*}
    \end{defn}
    \begin{rem}
        The definition of the MGF is obtained by replacing $s$ with $e^{t}$ in the PGF. While the MGF is more convenient for computing moments, it is less useful for deriving distributions.
    \end{rem}
    \begin{rem}
        MGFs are closely related to Laplace transforms.
    \end{rem}
    \begin{defn}
        The \textbf{joint moment generating function} (JMGF) of two random variables $X$ and $Y$ is defined as:
        \begin{equation*}
            M_{X,Y}(s, t) = \expect(e^{sX+tY}).
        \end{equation*}
    \end{defn}
    The following lemma can be derived easily.
    \begin{lem}
        Given the MGF $M_{X}(t)$ of a random variable $X$:
        \begin{enumerate}
            \item For any $k \geq 0$:
            \begin{equation*}
                \expect{X^{k}} = M_{X}^{(k)}(0).
            \end{equation*}
            \item The function $M$ can be expanded using Taylor's theorem within its radius of convergence:
            \begin{equation*}
                M_{X}(t) = \sum_{i = 0}^{\infty}\frac{\expect{X^{i}}}{i!}t^{i}.
            \end{equation*}
            \item If $X$ and $Y$ are independent, then:
            \begin{equation*}
                M_{X+Y}(t) = M_{X}(t)M_{Y}(t).
            \end{equation*}
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item 
            \begin{equation*}
                M^{(k)}(0) = \left.\pdv*[order = {k}]{\int e^{tx}\,dF_{X}(x)}{t}\right|_{t = 0} = \left.\int x^{k}e^{tx}\,dF_{X}(x)\right|_{t = 0} = \int x^{k}\,dF_{X}(x) = \expect{X^{k}}.
            \end{equation*}
            \item This follows directly from (1) and Taylor's theorem.
            \item Substitute $s = e^{t}$ into Theorem \ref{Chapter 7 (Theorem) PGF for sum of random independent variables}.
        \end{enumerate}
    \end{proofing}
    \begin{lem}
        If $X_{1},X_{2},\cdots,X_{n}$ are independent, then:
        \begin{equation*}
            M_{X_{1},X_{2},\cdots,X_{n}}(t_{1},t_{2},\cdots,t_{n}) = M_{X_{1}}(t_{1})M_{X_{2}}(t_{2})\cdots M_{X_{n}}(t_{n}).
        \end{equation*}
    \end{lem}
    \begin{proofing}
        By independence:
        \begin{equation*}
            M_{X_{1},X_{2},\cdots,X_{n}}(t_{1},t_{2},\cdots,t_{n}) = \expect(e^{t_{1}X_{1}+t_{2}X_{2}+\cdots+t_{n}X_{n}}) = \expect(e^{t_{1}X_{1}})\expect(e^{t_{2}X_{2}})\cdots\expect(e^{t_{n}X_{n}}) = M_{X_{1}}(t_{1})M_{X_{2}}(t_{2})\cdots M_{X_{n}}(t_{n}).
        \end{equation*}
    \end{proofing}
    \newpage
    
    \begin{rem}
        $M_{X}(0) = 1$ for all random variables $X$.
    \end{rem}
    \begin{eg}
        Let $X \sim \Bern(p)$. We have:
        \begin{equation*}
            M_{X}(t) = \expect(e^{tX}) = q+pe^{t}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $X \sim \Bin(n,p)$. We have:
        \begin{equation*}
            M_{X}(t) = (q+pe^{t})^{n}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $X \sim \Geom(p)$. We have:
        \begin{align*}
            f_{X}(x) &= p(1-p)^{x-1}, & M_{X}(t) &= \sum_{k = 1}^{\infty}e^{tk}p(1-p)^{k-1} = \frac{pe^{t}}{1-e^{t}(1-p)},\\
            f_{X}(x) &= p(1-p)^{x}, & M_{X}(t) &= \sum_{k = 0}^{\infty}e^{tk}p(1-p)^{k} = \frac{p}{1-e^{t}(1-p)}.
        \end{align*}
    \end{eg}
    \begin{eg}
        Let $X \sim \NBin(r,p)$. We have:
        \begin{align*}
            f_{X}(x) &= \binom{x-1}{r-1}p^{r}(1-p)^{x-r}, & M_{X}(t) &= \left(\frac{pe^{t}}{1-e^{t}(1-p)}\right)^{n},\\
            f_{X}(x) &= \binom{x+r-1}{r-1}p^{r}(1-p)^{x}, & M_{X}(t) &= \left(\frac{p}{1-e^{t}(1-p)}\right)^{n}.
        \end{align*}
    \end{eg}
    \begin{eg}
        Let $X \sim \Poisson(\lambda)$. We have:
        \begin{equation*}
            M_{X}(t) = \sum_{k = 0}^{\infty}\frac{\lambda^{k}e^{tk-\lambda}}{k!} = e^{\lambda(e^{t}-1)}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $X \sim \U[a, b]$ for some $a < b$. We have:
        \begin{equation*}
            M_{X}(t) = \begin{cases}
                \int_{a}^{b}\frac{e^{tx}}{b-a}\,dx = \frac{e^{tb}-e^{ta}}{t(b-a)}, &t \neq 0,\\
                1, &t = 0.
            \end{cases}
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $X \sim \Exp(\lambda)$. For $t < \lambda$, we have:
        \begin{equation*}
            M_{X}(t) = \int_{0}^{\infty}\lambda e^{x(t-\lambda)}\,dx = \frac{\lambda}{\lambda-t}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $X \sim \N(\mu,\sigma^{2})$. We have:
        \begin{align*}
            M_{X}(t) &= \frac{1}{\sqrt{2\pi\sigma^{2}}}\int_{-\infty}^{\infty}e^{tx}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right)\,dx,\\
            &= \frac{1}{\sqrt{2\pi\sigma^{2}}}\int_{-\infty}^{\infty}\exp\left(-\frac{x^{2}-2(\mu-\sigma^{2}t)x+\mu^{2}}{2\sigma^{2}}\right)\,dx,\\
            &= \frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(\frac{(\mu-\sigma^{2}t)^{2}-\mu^{2}}{2\sigma^{2}}\right)\int_{-\infty}^{\infty}\exp\left(-\frac{(x-(\mu-\sigma^{2}t))^{2}}{2\sigma^{2}}\right)\,dx,\\
            &= \exp\left(\frac{-2\mu\sigma^{2}t+\sigma^{4}t^{2}}{2\sigma^{2}}\right),\\
            &= \exp\left(\frac{1}{2}\sigma^{2}t^{2}-\mu t\right).
        \end{align*}	
    \end{eg}
    \newpage

    \begin{eg}
        Let $X \sim \Cauchy(0)$.
        \begin{align*}
            f_{X}(x) &= \frac{1}{\pi(1+x^{2})}, & M_{X}(t) &= \frac{1}{\pi}\int_{-\infty}^{\infty}\frac{e^{tx}}{1+x^{2}}\,dx.
        \end{align*}
        $M_{X}(t)$ exists only at $t = 0$. We get $M_{X}(0) = 1$.
    \end{eg}
    \begin{eg}
        Let $X \sim \Gam(\alpha,\lambda)$. If $t < \lambda$, we have:
        \begin{equation*}
            M_{X}(t) = \left(1-\frac{t}{\lambda}\right)^{-\alpha}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $X \sim \chi^{2}(k)$. If $t < \frac{1}{2}$, we have:
        \begin{equation*}
            M_{X}(t) = (1-2t)^{-\frac{k}{2}}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $X \sim \Beta(\alpha,\beta)$. We have:
        \begin{equation*}
            M_{X}(t) = 1+\sum_{k = 1}^{\infty}\left(\prod_{r = 0}^{k-1}\frac{\alpha+r}{\alpha+\beta+r}\right)\frac{t^{k}}{k!}.
        \end{equation*}
    \end{eg}
    \begin{rem}
        Not all distributions have MGFs.
    \end{rem}
    \begin{eg}
        The MGF of $X \sim t(n)$ is undefined.
    \end{eg}
    Moment generating functions provide a useful technique, but the integrals used to define them may not always converge. There is another class of functions for which convergence is guaranteed.
    \begin{defn}
        The \textbf{characteristic function} (CF) of a random variable $X$ is the function $\phi_{X}:\mathbb{R} \to \mathbb{C}$ defined as:
        \begin{align*}
            \phi_{X}(t) &= \expect{e^{itX}} = \int e^{itx}\,dF_{X}(x) = \expect{\cos(tX)}+i\expect{\sin(tX)}, & i &= \sqrt{-1}.
        \end{align*}
    \end{defn}
    \begin{rem}
        $\phi_{X}(t)$ is essentially a Fourier transform.
    \end{rem}
    \begin{lem}
        The CF $\phi_{X}$ of a random variable $X$ has the following properties:
        \begin{enumerate}
            \item $\phi_{X}(0) = 1$. Moreover, $\abs{\phi_{X}(t)} \leq 1$ for all $t$.
            \item $\phi_{X}(t)$ is uniformly continuous.
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item For all $t$:
            \begin{align*}
                \phi_{X}(0) &= \int\,dF_{X}(x) = 1,\\
                \abs{\phi_{X}(t)} &= \abs{\int(\cos(tx)+i\sin(tx))\,dF_{X}(x)} \leq \int\abs{\cos(tx)+i\sin(tx)}\,dF_{X}(x) = \int\,dF_{X}(x) = 1.
            \end{align*}
            \item
            \begin{equation*}
                \sup_{t}\abs{\phi_{X}(t+c)-\phi_{X}(t)} = \sup_{t}\abs{\int(e^{i(t+c)x}-e^{itx})\,dF_{X}(x)} \leq \sup_{t}\left(\int\abs{e^{itx}}\abs{e^{icx}-1}\,dF_{X}(x)\right).
            \end{equation*}
            As $c \to 0$, the supremum approaches $0$. Therefore, $\phi_{X}(t)$ is uniformly continuous.
        \end{enumerate}
    \end{proofing}
    \newpage

    \begin{thm}
        The characteristic function $\phi_{X}$ of a random variable $X$ has the following properties regarding derivatives and moments:
        \begin{enumerate}
            \item If $\phi_{X}^{(k)}(0)$ exists, then:
            \begin{equation*}
                \begin{cases}
                    \expect\abs{X}^{k} < \infty, &k \text{ is even},\\
                    \expect\abs{X}^{k-1} < \infty, &k \text{ is odd}.
                \end{cases}
            \end{equation*}
            \item If $\expect\abs{X}^{k} < \infty$, then $\phi_{X}^{(k)}(0)$ exists. Moreover:
            \begin{equation*}
                \phi_{X}(t) = \sum_{j = 0}^{k}\frac{\phi_{X}^{(j)}(0)}{j!}t^{j}+o(t^{k}) = \sum_{j = 0}^{k}\frac{\expect{X^{j}}}{j!}(it)^{j}+o(t^{k}).
            \end{equation*}
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        Using Taylor's theorem:
        \begin{equation*}
            \phi_{X}(t) = \sum_{j = 0}^{k}\frac{\phi_{X}^{(j)}(0)}{j!}t^{j}+o(t^{k}) = \sum_{j = 0}^{k}\frac{\expect{X^{j}}}{j!}(it)^{j}+o(t^{k}).
        \end{equation*}
        \begin{enumerate}
            \item 
            \begin{equation*}
                \phi_{X}^{(k)}(0) = i^{k}\expect{X^{k}}.
            \end{equation*}
            If $k$ is even, we have $\phi_{X}^{(k)}(0) = (-1)^{\frac{k}{2}}\expect{X^{k}} = (-1)^{\frac{k}{2}}\expect\abs{X}^{k}$, which exists. Therefore, $\expect\abs{X}^{k} < \infty$.\\
            If $k$ is odd, note that $\phi_{X}^{(k-1)}(0)$ exists if $\phi_{X}^{(k)}(0)$ exists.\\
            Thus, with $\phi_{X}^{(k-1)}(0) = (-1)^{\frac{k-1}{2}}\expect{X^{k-1}} = (-1)^{\frac{k-1}{2}}\expect\abs{X}^{k-1}$, we conclude $\expect\abs{X}^{k-1} < \infty$.
            \item Using the formula in (1):
            \begin{equation*}
                \frac{\phi_{X}^{(k)}(0)}{i^{k}} = \expect{X^{k}} \leq \expect\abs{X}^{k} < \infty.
            \end{equation*}
            Therefore, $\phi_{X}^{(k)}(0)$ exists. The formula follows directly from Taylor's theorem.
        \end{enumerate}
    \end{proofing}
    \begin{thm}
        If $X \independent Y$, then:
        \begin{equation*}
            \phi_{X+Y}(t) = \phi_{X}(t)\phi_{Y}(t).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        \begin{equation*}
            \phi_{X+Y}(t) = \expect(e^{it(X+Y)}) = \expect(e^{itX})\expect(e^{itY}) = \phi_{X}(t)\phi_{Y}(t).
        \end{equation*}
    \end{proofing}
    \newpage

    The concept of a joint characteristic function is defined as follows:
    \begin{defn}
        The \textbf{joint characteristic function} (JCF) $\phi_{X,Y}$ of two random variables $X$ and $Y$ is given by:
        \begin{equation*}
            \phi_{X,Y}(s, t) = \expect(e^{i(sX+tY)}).
        \end{equation*}
    \end{defn}
    This leads to another method for proving the independence of two random variables:
    \begin{thm}
        \label{Chapter 7 (Theorem) Independence via CF}
        Two random variables $X$ and $Y$ are independent if and only if, for all $s$ and $t$:
        \begin{equation*}
            \phi_{X,Y}(s, t) = \phi_{X}(s)\phi_{Y}(t).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        If $X \independent Y$:
        \begin{equation*}
            \phi_{X,Y}(s, t) = \expect(e^{i(sX+tY)}) = \expect(e^{isX})\expect(e^{itY}) = \phi_{X}(s)\phi_{Y}(t).
        \end{equation*}
        To prove the converse, additional theorems are required (see Example \ref{Chapter 7 (Example) Proof of Independence via CF}).
    \end{proofing}
    \begin{eg}
        Let $X \sim \Bern(p)$. We have:
        \begin{equation*}
            \phi_{X}(t) = \expect(e^{itX}) = q+pe^{it}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $X \sim \Bin(n,p)$. We have:
        \begin{equation*}
            \phi_{X}(t) = (q+pe^{it})^{n}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $X \sim \Exp(1)$. We have:
        \begin{equation*}
            \phi_{X}(t) = \int e^{(it-1)x}\,dx = \frac{1}{1-it}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $X \sim \Cauchy$. We have:
        \begin{equation*}
            \phi_{X}(t) = e^{-\abs{t}}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $X \sim \N(\mu,\sigma^{2})$. Using the fact that, for any $u \in \mathbb{C}$ (not just in $\mathbb{R}$):
        \begin{equation*}
            \frac{1}{\sqrt{2\pi\sigma^{2}}}\int_{-\infty}^{\infty}\exp\left(-\frac{(x-u)^{2}}{2\sigma^{2}}\right)\,dx = 1,
        \end{equation*}
        we have:
        \begin{align*}
            \phi_{X}(t) &= \frac{1}{\sqrt{2\pi\sigma^{2}}}\int_{-\infty}^{\infty}e^{itx}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right)\,dx,\\
            &= \frac{1}{\sqrt{2\pi\sigma^{2}}}\int_{-\infty}^{\infty}\exp\left(-\frac{x^{2}-(2\mu+2\sigma^{2}it)x+\mu^{2}}{2\sigma^{2}}\right)\,dx,\\
            &= \frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(\frac{(\mu+\sigma^{2}it)^{2}-\mu^{2}}{2\sigma^{2}}\right)\int_{-\infty}^{\infty}\exp\left(-\frac{(x-(\mu+\sigma^{2}it))^{2}}{2\sigma^{2}}\right)\,dx,\\
            &= \exp\left(\frac{\mu^{2}+2\sigma^{2}i\mu t-\sigma^{4}t^{2}-\mu^{2}}{2\sigma^{2}}\right),\\
            &= \exp\left(i\mu t-\frac{1}{2}\sigma^{2}t^{2}\right).
        \end{align*}
    \end{eg}
    \begin{rem}
        The \textbf{cumulant generating function} is defined as $\log\phi_{X}(t)$. The normal distribution is the only distribution we have encountered whose cumulant generating function has a finite number of terms:
        \begin{equation*}
            \log\phi_{X}(t) = i\mu t-\frac{1}{2}\sigma^{2}t^{2}.
        \end{equation*}
    \end{rem}
    \newpage

\section{Inversion and Continuity Theorems}
    Characteristic functions are useful in two major ways. One of them is that the characteristic function of a random variable can be used to generate the probability density function of that random variable.
    \begin{thm}\named{Fourier Inverse Transform for Continuous Case}
        If a random variable $X$ is continuous with a PDF $f_{X}$ and a CF $\phi_{X}$, then:
        \begin{equation*}
            f_{X}(x) = \frac{1}{2\pi}\int_{-\infty}^{\infty} e^{-itx}\phi_{X}(t)\,dt,
        \end{equation*}
        at all points $x$ where $f_{X}$ is differentiable.\\
        If $X$ has a CDF $F_{X}$, then:
        \begin{equation*}
            F_{X}(b)-F_{X}(a) = \frac{1}{2\pi}\int_{-\infty}^{\infty}\int_{a}^{b}e^{-itx}\phi_{X}(t)\,dx\,dt.
        \end{equation*}
    \end{thm}
    \begin{proofing}[Proof (Non-Rigorous)]
        Let:
        \begin{align*}
            I(x) &= \frac{1}{2\pi}\int_{-\infty}^{\infty} e^{itx}\phi_{X}(t)\,dt = \frac{1}{2\pi}\int_{-\infty}^{\infty} e^{-itx}\int_{-\infty}^{\infty} e^{ity}f_{X}(y)\,dy\,dt,\\
            I_{\varepsilon}(x) &= \frac{1}{2\pi}\int_{-\infty}^{\infty} e^{-itx}\int_{-\infty}^{\infty} e^{ity}f_{X}(y)\,dy\,e^{-\frac{1}{2}\varepsilon^{2}t^{2}}\,dt.
        \end{align*}
        We want to show that $I_{\varepsilon}(x) \to I(x)$ as $\varepsilon \to 0$.
        \begin{align*}
            I_{\varepsilon}(x) &= \frac{1}{2\pi}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} e^{-\frac{1}{2}\varepsilon^{2}t^{2}+i(y-x)t}f_{X}(y)\,dt\,dy,\\
            &= \frac{1}{\sqrt{2\pi\varepsilon^{2}}}\left(\frac{1}{\sqrt{2\pi\frac{1}{\varepsilon^{2}}}}\right)\int_{-\infty}^{\infty}\exp\left(-\frac{(y-x)^{2}}{2\varepsilon^{2}}\right)f_{X}(y)\int_{-\infty}^{\infty}\exp\left(-\frac{-\left(t-i\frac{y-x}{\varepsilon}\right)^{2}}{2\left(\frac{1}{\varepsilon^{2}}\right)}\right)\,dt\,dy,\\
            &= \frac{1}{\sqrt{2\pi\varepsilon^{2}}}\int_{-\infty}^{\infty}\exp\left(-\frac{(y-x)^{2}}{2\varepsilon^{2}}\right)f_{X}(y)\,dy.
        \end{align*}
        Let $Z \sim \N(0,1)$ and $Z_{\varepsilon} = \varepsilon Z$. $I_{\varepsilon}(x)$ is the PDF of $\varepsilon Z+X$. Therefore, we can conclude that $f_{\varepsilon Z+X}(x) \to f_{X}(x)$ as $\varepsilon \to 0$.
    \end{proofing}
    \begin{thm}\named{Inversion Theorem}
        If a random variable $X$ has a CDF $F_{X}$ and a CF $\phi_{X}$, we define $\overline{F}_{X}:\mathbb{R} \to [0,1]$ by:
        \begin{equation*}
            \overline{F}_{X}(x) = \frac{1}{2}\left(F_{X}(x)+F_{X}(x^{-})\right).
        \end{equation*}
        Then, for all $a \leq b$:
        \begin{equation*}
            \overline{F}_{X}(b)-\overline{F}_{X}(a) = \int_{-\infty}^{\infty}\frac{e^{-iat}-e^{-ibt}}{2\pi it}\phi_{X}(t)\,dt.
        \end{equation*}
    \end{thm}
    \begin{rem}
        The function $\overline{F}_{X}$ represents the average of the limits from both directions.
    \end{rem}
    \begin{eg}
        \label{Chapter 7 (Example) Proof of Independence via CF}
        Using the Inversion Theorem, we can now prove Theorem \ref{Chapter 7 (Theorem) Independence via CF}.\\
        Given two random variables $X$ and $Y$, we first extend the Fourier Inverse Transform to the multivariable case.\\
        If $\phi_{X,Y}(s, t) = \phi_{X}(s)\phi_{Y}(t)$, then for any $a \leq b$ and $c \leq d$:
        \begin{align*}
            \overline{F}_{X,Y}(b,d)-\overline{F}_{X,Y}(b,c)-\overline{F}_{X,Y}(a,d)+\overline{F}_{X,Y}(a,c) &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\frac{(e^{-ias}-e^{-ibs})(e^{-ict}-e^{-idt})}{-4\pi^{2}t^{2}}\phi_{X}(s)\phi_{Y}(t)\,ds\,dt,\\
            &= (\overline{F}_{X}(b)-\overline{F}_{X}(a))\int_{-\infty}^{\infty}\frac{e^{-ict}-e^{-idt}}{2\pi it}\phi_{Y}(t)\,dt,\\
            &= (\overline{F}_{X}(b)-\overline{F}_{X}(a))(\overline{F}_{Y}(d)-\overline{F}_{Y}(c)),\\
            &= \overline{F}_{X}(b)\overline{F}_{Y}(d)-\overline{F}_{X}(b)\overline{F}_{Y}(c)-\overline{F}_{X}(a)\overline{F}_{Y}(d)+\overline{F}_{X}(a)\overline{F}_{Y}(c).
        \end{align*}
        From the definition of independent random variables, we prove that $X \independent Y$ if $\phi_{X,Y}(s, t) = \phi_{X}(s)\phi_{Y}(t)$.
    \end{eg}
    \newpage

    Another application is to evaluate the convergence of a sequence of cumulative distribution functions.
    \begin{defn}\named{Convergence of Distribution Function Sequence [Weak Convergence]}
        A sequence of CDFs $F_{1},F_{2},\cdots$ \textbf{converges} to a CDF $F$, written as $F_{n} \to F$, if at each point $x$ where $F$ is continuous:
        \begin{equation*}
            F_{n}(x) \to F(x).
        \end{equation*}
    \end{defn}
    \begin{eg}
        Assume we have two sequences of CDFs:
        \begin{align*}
            F_{n}(x) &= \begin{cases}
                0, &x < \frac{1}{n},\\
                1, &x \geq \frac{1}{n},
            \end{cases} & G_{n}(x) &= \begin{cases}
                0, &x < \frac{-1}{n},\\
                1, &x \geq \frac{-1}{n}.
            \end{cases}
        \end{align*}
        As $n \to \infty$, we get:
        \begin{align*}
            F(x) &= \begin{cases}
                0, &x \leq 0,\\
                1, &x > 0,
            \end{cases} & G(x) &= \begin{cases}
                0, &x < 0,\\
                1, &x \geq 0.
            \end{cases}
        \end{align*}
        This is problematic because $F(x)$ in this case is not a distribution function since it is not right-continuous.\\
        Therefore, it is necessary to define convergence so that both sequences $\{F_{n}\}$ and $\{G_{n}\}$ have the same limit.
    \end{eg}
    We can modify the definition slightly to state that each distribution function in the sequence represents a different random variable.
    \begin{defn}\named{Convergence in Distribution for Random Variables}
        Let $X,X_{1},X_{2},\cdots$ be a family of random variables with CDFs $F,F_{1},F_{2},\cdots$. We say $X_{n} \to X$, written as $X_{n} \xrightarrow{D} X$ or $X_{n}\Rightarrow X$, if $F_{n} \to F$.
    \end{defn}
    \begin{rem}
        For this convergence definition, we do not consider the closeness of $X_{n}$ and $X$ as functions of $\omega$.
    \end{rem}
    \begin{rem}
        Sometimes, we also write $X_{n}\Rightarrow F$ or $X_{n} \xrightarrow{D} F$.
    \end{rem}
    Using this definition, a sequence of characteristic functions can be used to determine whether the sequence of cumulative distribution functions converges.
    \begin{thm}\named{L\'evy Continuity Theorem}
        Suppose that $F_{1},F_{2},\cdots$ is a sequence of CDFs with CFs $\phi_{1},\phi_{2},\cdots$. Then:
        \begin{enumerate}
            \item If $F_{n} \to F$ for some CDF $F$ with CF $\phi$, then $\phi_{n} \to \phi$ pointwise.
            \item If $\phi_{n} \to \phi$ pointwise for some CF $\phi$, and $\phi$ is continuous at $0$ ($t = 0$), then $\phi$ is the CF of some CDF $F$, and $F_{n} \to F$.
        \end{enumerate}
    \end{thm}
    \begin{rem}
        In L\'evy Continuity Theorem (2), the statement that $\phi$ is continuous at $0$ can be replaced by any of the following statements:
        \begin{enumerate}
            \item $\phi(t)$ is a continuous function of $t$.
            \item $\phi(t)$ is a CF of some CDF.
            \item The sequence $\{F_{n}\}_{n = 1}^{\infty}$ is tight, i.e., for all $\epsilon > 0$, there exists $M_{\epsilon} > 0$ such that
            \begin{equation*}
                \sup_{n}(F_{n}(-M_{\epsilon})+1-F_{n}(M_{\epsilon})) \leq \epsilon.
            \end{equation*}
        \end{enumerate}
    \end{rem}
    \begin{eg}
        Let $X_{n} \sim \N(0,\frac{1}{n^{2}})$, and let $\phi_{n}$ be the CF of $X_{n}$. Then
        \begin{equation*}
            \phi_{n}(t) = \exp\left(-\frac{t^{2}}{2n^{2}}\right) \to \phi(t) = 1.
        \end{equation*}
        Since $\phi(t)$ is continuous at $0$, by L\'evy Continuity Theorem (2), we conclude that $X_{n} \xrightarrow{D} 0$.
    \end{eg}
    \newpage

    \begin{eg}
        Let $X_{n} \sim \N(0,n^{2})$, and let $\phi_{n}$ be the CF of $X_{n}$. Then
        \begin{equation*}
            \phi_{n}(t) = \exp\left(-\frac{1}{2}n^{2}t^{2}\right) \to \phi(t) = \begin{cases}
                0, &t \neq 0\\
                1, &t = 0
            \end{cases}.
        \end{equation*}
    \end{eg}
    We have a more general definition of convergence.
    \begin{defn}\named{Vague convergence}
        Given a sequence of CDFs $F_{1},F_{2},\cdots$, suppose that $F_{n}(x) \to G(x)$ at all continuity points of $G$, but $G$ may not be a CDF. Then we say $F_{n} \to G$ \textbf{vaguely}, written as $F_{n} \xrightarrow{v} G$.
    \end{defn}
    \begin{eg}
        If
        \begin{align*}
            F_{n}(x) &= \begin{cases}
                0, &x < \frac{1}{n}\\
                \frac{1}{2}, &\frac{1}{n} \leq x < n\\
                1, &x \geq n
            \end{cases} & G(x) &= \begin{cases}
                0, &x < 0\\
                \frac{1}{2}, &x \geq 0
            \end{cases}
        \end{align*}
        We can see that $F_{n} \xrightarrow{v} G$ as $n \to \infty$, and $G$ is not a CDF.
    \end{eg}
    \newpage

\section{Two limit theorems}
    In this section, we introduce two fundamental theorems in probability theory: the Law of Large Numbers and the Central Limit Theorem.
    \begin{thm}\named{Weak Law of Large Numbers} [WLLN]
        Let $X_{1},X_{2},\cdots$ be i.i.d. random variables. Assume that $\expect\abs{X_{1}} < \infty$ and $\expect{X_{1}} = \mu$. We have:
        \begin{equation*}
            \frac{1}{n}\sum_{i = 1}^{n}X_{i} \xrightarrow{D} \mu.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        We recall the Taylor expansion of $\phi_{\xi}(s)$ at $0$. If $\expect\abs{\xi}^{k} < \infty$ and $s$ is small, then
        \begin{equation*}
            \phi_{\zeta}(s) = \sum_{j = 0}^{k}\frac{\expect{\xi^{j}}}{j!}(is)^{j}+o(s^{k}).
        \end{equation*}
        For any $t \in \mathbb{R}$, let $\phi_{X_{1}}(s) = \expect{e^{isX_{1}}}$.
        \begin{align*}
            \phi_{n}(t) &= \expect\left(\exp\left(\frac{it}{n}\sum_{i = 1}^{n}X_{i}\right)\right)\\
            &= \expect\left(\prod_{i = 1}^{n}\exp\left(\frac{itX_{i}}{n}\right)\right)\\
            &= \left(\expect\left(\exp\left(\frac{itX_{1}}{n}\right)\right)\right)^{n}\\
            &= \left(\phi_{X_{1}}\left(\frac{t}{n}\right)\right)^{n}\\
            &= \left(1+\frac{it}{n}\expect{X_{1}}+o\left(\frac{t}{n}\right)\right)^{n}\\
            &= \left(1+\frac{i\mu t}{n}+o\left(\frac{t}{n}\right)\right)^{n} \to e^{i\mu t}.
        \end{align*}
        By L\'evy continuity theorem, we get that $\frac{1}{n}\sum_{i = 1}^{n}X_{i} \xrightarrow{D} \mu$.
    \end{proofing}
    \begin{thm}\named{Central Limit Theorem} [CLT]
        Let $X_{1},X_{2},\cdots$ be i.i.d. random variables with $\expect\abs{X_{1}}^{2} < \infty$ and $\expect{X_{1}} = \mu$, $\Var(X_{1}) = \sigma^{2}$. Then
        \begin{equation*}
            \frac{1}{\sigma}\sqrt{n}\left(\frac{1}{n}\sum_{i = 1}^{n}X_{i}-\mu\right) = \frac{\sum_{i = 1}^{n}X_{i}-n\mu}{\sqrt{n}\sigma} \xrightarrow{D} \N(0,1).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Let $Y_{i} = \frac{X_{i}-\mu}{\sigma}$. We have $\expect{Y_{i}} = 0$ and $\Var(Y_{i}) = 1$.
        \begin{align*}
            \frac{\sum_{i = 1}^{n}X_{i}-n\mu}{\sqrt{n}\sigma} &= \sum_{i = 1}^{n}\frac{1}{\sqrt{n}}\frac{X_{i}-\mu}{\sigma} = \sum_{i = 1}^{n}\frac{Y_{i}}{\sqrt{n}}\\
            \phi_{n}(t) &= \expect\left(\exp\left(it\sum_{\ell = 1}^{n}\frac{Y_{\ell}}{\sqrt{n}}\right)\right)\\
            &= \left(\expect\left(\exp\left(\frac{itY_{1}}{\sqrt{n}}\right)\right)\right)^{n}\\
            &= \left(\phi_{Y_{1}}\left(\frac{t}{\sqrt{n}}\right)\right)^{n}\\
            \tag{Taylor expansion}
            &= \left(1+\frac{it}{\sqrt{n}}\expect{Y_{1}}+\frac{1}{2}\left(\frac{it}{\sqrt{n}}\right)^{2}\expect{Y_{1}^{2}}+o\left(\frac{t^{2}}{n}\right)\right)^{n}\\
            &= \left(1-\frac{t^{2}}{2n}+o\left(\frac{t^{2}}{n}\right)\right)^{n} \to e^{-\frac{1}{2}t^{2}}.
        \end{align*}
        By L\'evy continuity theorem, $\frac{\sum_{i = 1}^{n}X_{i}-n\mu}{\sqrt{n}\sigma} \xrightarrow{D} \N(0,1)$.
    \end{proofing}
    \newpage

    The Central Limit Theorem can be generalized in several directions, one of which concerns independent random variables instead of i.i.d. random variables.
    \begin{thm}
        Let $X_{1},X_{2},\cdots$ be independent random variables satisfying $\expect{X_{i}} = 0$, $\Var(X_{i}) = \sigma_{i}^{2}$, $\expect\abs{X_{i}}^{3} < \infty$, and such that
        \begin{equation*}
            \tag{*}
            \frac{1}{(\sigma(n))^{3}}\sum_{i = 1}^{n}\expect\abs{X_{i}^{3}} \to 0 \text{ as }n \to \infty,
        \end{equation*}
        where $(\sigma(n))^{2} = \Var\left(\sum_{i = 1}^{n}X_{i}\right) = \sum_{i = 1}^{n}\sigma_{i}^{2}$. Then
        \begin{equation*}
            \frac{1}{\sigma(n)}\sum_{i = 1}^{n}X_{i} \xrightarrow{D} \N(0,1).
        \end{equation*}
    \end{thm}
    \begin{rem}
        The condition (*) means that none of the random variables $X_{i}$ can dominate the sum.
        \begin{equation*}
            \frac{1}{(\sigma(n))^{3}}\sum_{i = 1}^{n}\abs{X_{i}}^{3}\lesssim\frac{1}{\sigma(n)}\max_{i = 1,2,\cdots,n}\abs{X_{i}}\left(\frac{1}{(\sigma(n))^{2}}\right)\sum_{i = 1}^{n}(X_{i})^{2} \approx \frac{1}{\sigma(n)}\max_{i = 1,2,\cdots,n}\abs{X_{i}} \to 0.
        \end{equation*}
    \end{rem}
    This theorem is a special case of the Central Limit Theorem that focuses on the sum of Bernoulli random variables.
    \begin{thm}\named{De Moivre-Laplace Limit Theorem}
        Suppose that $X \sim \Bin(n,p)$. Then for any $a < b$, as $n \to \infty$,
        \begin{equation*}
            \prob\left(a < \frac{X-np}{\sqrt{np(1-p)}} \leq b\right) \to \Phi(b)-\Phi(a).
        \end{equation*}	
    \end{thm}
    \begin{proofing}
        Our goal is to transform the PMF of the Binomial random variable into the PDF of the standard normal distribution. Let $q = 1-p$. For $0 \leq k \leq n$, by applying Stirling's formula,
        \begin{align*}
            \binom{n}{k}p^{k}q^{n-k} &= \frac{n!}{k!(n-k)!}p^{k}q^{n-k}\\
            \tag{Using Stirling's formula: $n! \sim \sqrt{2\pi n}\left(\frac{n}{e}\right)^{n}$}
            &\sim \sqrt{\frac{n}{2\pi k(n-k)}}\frac{n^{n}}{k^{k}(n-k)^{n-k}}p^{k}q^{n-k} = \sqrt{\frac{n}{2\pi k(n-k)}}\left(\frac{np}{k}\right)^{k}\left(\frac{nq}{n-k}\right)^{n-k}\\
            \tag{$\frac{k}{n} \to p$}
            &\sim \frac{1}{\sqrt{2\pi npq}}\exp\left(-k\ln\left(\frac{k}{np}\right)+(k-n)\ln\left(\frac{n-k}{nq}\right)\right).
        \end{align*}
        With $\expect{X} = np$ and $\Var(X) = npq$, for any integer $k$ chosen between $0$ and $n$, there exists an arbitrary finite point $c$ such that $k = np+c\sqrt{npq}$. Using the Taylor series expansion $\ln(1+x) = x-\frac{x^{2}}{2}+o(x^{2})$, we get:
        \begin{align*}
            \binom{n}{k}p^{k}q^{n-k} &\sim \frac{1}{\sqrt{2\pi npq}}\exp\left((-np-c\sqrt{npq})\ln\left(\frac{np+c\sqrt{npq}}{np}\right)+(np+c\sqrt{npq}-n)\ln\left(\frac{n-np-c\sqrt{npq}}{nq}\right)\right)\\
            &= \frac{1}{\sqrt{2\pi npq}}\exp\left((-np-c\sqrt{npq})\ln\left(1+c\sqrt{\frac{q}{np}}\right)+(c\sqrt{npq}-nq)\ln\left(1-c\sqrt{\frac{p}{nq}}\right)\right)\\
            &= \frac{1}{\sqrt{2\pi npq}}\exp\left((-np-c\sqrt{npq})\left(c\sqrt{\frac{q}{np}}-\frac{c^{2}q}{2np}+o(n^{-1})\right)+(c\sqrt{npq}-nq)\left(-c\sqrt{\frac{p}{nq}}-\frac{c^{2}p}{2nq}+o(n^{-1})\right)\right)\\
            &= \frac{1}{\sqrt{2\pi npq}}\exp\left((-c\sqrt{npq}-c^{2}q+\frac{1}{2}c^{2}q+o(1))+(-c^{2}p+c\sqrt{npq}+\frac{1}{2}c^{2}p+o(1))\right)\\
            &\sim \frac{1}{\sqrt{2\pi npq}}\exp\left(-\frac{1}{2}c^{2}\right) = \frac{1}{\sqrt{2\pi npq}}\exp\left(-\frac{(k-np)^{2}}{2npq}\right).
        \end{align*}
        Therefore, as $n \to \infty$, $\frac{X-np}{\sqrt{np(1-p)}} \xrightarrow{D} \N(0,1)$, and the theorem is proven.
    \end{proofing}
    \begin{rem}
        Let $X_{1},X_{2},\cdots,X_{n}$ be a random sample of a population $X \sim \Bern(p)$. We have:
        \begin{equation*}
            \prob\left(a < \frac{\overline{X}-p}{\sqrt{\frac{p(1-p)}{n}}} \leq b\right) \to \Phi(b)-\Phi(a).
        \end{equation*}
    \end{rem}
    \newpage

\section{Sampling}
    In many cases, we do not know the actual distribution of the population. We can only predict the distribution based on the samples we obtain. This section is closer to statistics than probability, so we will not delve deeply into it.
    \begin{defn}
        A set of random variables $\{X_{1},X_{2},\cdots,X_{n}\}$ is called a \textbf{random sample} of a random variable $X$ with PMF or PDF $f_{X}(x)$ and CDF $F_{X}(x)$ if they are independent and identically distributed (i.i.d.).
        \begin{enumerate}
            \item The \textbf{sample mean} of $X$, denoted by $\overline{X}$, is defined as:
            \begin{equation*}
                \overline{X} = \frac{1}{n}\sum_{i = 1}^{n}X_{i}.
            \end{equation*}
            \item The \textbf{sample variance} of $X$, denoted by $S_{n-1}^{2}$, is defined as:
            \begin{equation*}
                S_{n-1}^{2} = \frac{1}{n-1}\sum_{i = 1}^{n}(X_{i}-\overline{X})^{2}.
            \end{equation*}
        \end{enumerate}
    \end{defn}
    \begin{rem}
        Notice that the denominator is $n-1$.
    \end{rem}
    \begin{thm}
        \label{Chapter 7 (Theorem) Expectation and variance of sample mean}
        Given a sample mean $\overline{X}$ of a random variable $X$, we have $\expect{\overline{X}} = \mu$ and $\Var(\overline{X}) = \frac{\sigma^{2}}{n}$.
    \end{thm}
    \begin{proofing}
        \begin{align*}
            \expect{\overline{X}} &= \expect\left(\frac{1}{n}\sum_{k = 1}^{n}X_{k}\right) = \frac{1}{n}\sum_{k = 1}^{n}\expect{X_{k}} = \frac{1}{n}\sum_{k = 1}^{n}\mu = \mu,\\
            \Var(\overline{X}) &= \frac{1}{n^{2}}\sum_{k = 1}^{n}\Var(X_{k}) = \frac{n\sigma^{2}}{n^{2}} = \frac{1}{n}\sigma^{2}.
        \end{align*}
    \end{proofing}
    \begin{thm}
        Given a sample variance $S_{n-1}^{2}$ of a random variable $X$, we have $\expect{S_{n-1}^{2}} = \sigma^{2}$.
    \end{thm}
    \begin{proofing}
        \begin{align*}
            \expect{S_{n-1}^{2}} &= \frac{1}{n-1}\sum_{i = 1}^{n}\expect(X_{i}-\overline{X})^{2}\\
            &= \frac{1}{n-1}\sum_{i = 1}^{n}\left(\expect(X_{i}-\mu)^{2}+\expect(\overline{X}-\mu)^{2}-2\expect((X_{i}-\mu)(\overline{X}-\mu))\right)\\
            &= \frac{1}{n-1}\sum_{i = 1}^{n}\left(\Var(X_{i})+\Var(\overline{X})-2\cov(X_{i},\overline{X})\right)\\
            &= \frac{n\sigma^{2}}{n-1}+\frac{\sigma^{2}}{n-1}-\frac{2}{n-1}\sum_{i = 1}^{n}\cov\left(X_{i},\frac{1}{n}\sum_{j = 1}^{n}X_{j}\right)\\
            &= \frac{n\sigma^{2}}{n-1}+\frac{\sigma^{2}}{n-1}-\frac{2}{n(n-1)}\sum_{i = 1}^{n}\sum_{j = 1}^{n}\cov(X_{i},X_{j})\\
            &= \frac{n\sigma^{2}}{n-1}+\frac{\sigma^{2}}{n-1}-\frac{2\sigma^{2}}{n-1} = \sigma^{2}.
        \end{align*}
    \end{proofing}
    \newpage

    By using the CLT, we can estimate $\mu_{X}$ if we know the value of $\sigma_{X}^{2}$.
    \begin{thm}
        \label{Chapter 7 (Theorem) Sampling distribution with sample mean}
        Let $X_{1},X_{2},\cdots,X_{n}$ be a random sample from the population $X \sim \N(\mu_{X},\sigma_{X}^{2})$. We have:
        \begin{equation*}
            \overline{X} \sim \N\left(\mu_{X},\frac{\sigma_{X}^{2}}{n}\right).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        By Theorem \ref{Chapter 5 (Theorem) Additivity of Normal Distribution} and the properties of the Normal distribution,
        \begin{equation*}
            X_{1}+X_{2}+\cdots+X_{n} \sim \N(n\mu_{X},n\sigma_{X}^{2}).
        \end{equation*}
        By Lemma \ref{Chapter 5 (Lemma) Properties of Normal Distribution}, we have:
        \begin{equation*}
            \overline{X} = \frac{1}{n}(X_{1}+X_{2}+\cdots+X_{n}) \sim \N\left(\mu_{X},\frac{\sigma_{X}^{2}}{n}\right).
        \end{equation*}
    \end{proofing}
    What if we want to estimate $\sigma_{X}^{2}$ using $\mu_{X}$?
    \begin{thm}
        \label{Chapter 7 (Theorem) Sampling distribution with known mean}
        Let $X_{1},X_{2},\cdots,X_{n}$ be a random sample from the population $X \sim \N(\mu_{X},\sigma_{X}^{2})$. Then we have:
        \begin{equation*}
            \sum_{i = 1}^{n}\left(\frac{X_{i}-\mu_{X}}{\sigma_{X}}\right)^{2} \sim \chi^{2}(n).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Using the properties of the Normal distribution, for any $i = 1,\cdots,n$, we have:
        \begin{equation*}
            \frac{X_{i}-\mu_{X}}{\sigma_{X}} \sim \N(0,1).
        \end{equation*}
        Therefore, by the definition of the $\chi^{2}$-distribution,
        \begin{equation*}
            \sum_{i = 1}^{n}\left(\frac{X_{i}-\mu_{X}}{\sigma_{X}}\right)^{2} \sim \chi^{2}(n).
        \end{equation*}
    \end{proofing}
    \newpage

    How do we find $\sigma_{X}$ if $\mu_{X}$ is unknown? We can use the following theorem.
    \begin{thm}
        \label{Chapter 7 (Theorem) Sampling distribution with sample variance}
        Let $X_{1},X_{2},\cdots,X_{n}$ be a random sample from the population $X \sim \N(\mu_{X},\sigma_{X}^{2})$. Then we have:
        \begin{enumerate}
            \item $\overline{X}$ and $S_{n-1}^{2}$ are independent.
            \item \begin{equation*}
                \frac{(n-1)S_{n-1}^{2}}{\sigma_{X}^{2}} \sim \chi^{2}(n-1).
            \end{equation*}
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item It suffices to prove that $\overline{X}$ and $X_{i}-\overline{X}$ are independent for any $i = 1,2,\cdots,n$.\\
            We know that $\overline{X} \sim \N(\mu_{X},\frac{\sigma_{X}^{2}}{n})$ and $X_{i}-\overline{X} \sim \N(0,\frac{(n+1)\sigma_{X}^{2}}{n})$.
            \begin{equation*}
                \cov(\overline{X},X_{i}-\overline{X}) = 0.
            \end{equation*}
            \item We may find that:
            \begin{align*}
                \frac{(n-1)S_{n-1}^{2}}{\sigma_{X}^{2}} = \sum_{i = 1}^{n}\frac{(X_{i}-\overline{X})^{2}}{\sigma_{X}^{2}} = \sum_{i = 1}^{n}\frac{((X_{i}-\mu_{X})+(\mu_{X}-\overline{X}))^{2}}{\sigma_{X}^{2}} &= \sum_{i = 1}^{n}\frac{(X_{i}-\mu_{X})^{2}}{\sigma_{X}^{2}}-\frac{n(\overline{X}-\mu_{X})^{2}}{\sigma_{X}^{2}}\\
                &= \sum_{i = 1}^{n}\left(\frac{X_{i}-\mu_{X}}{\sigma_{X}}\right)^{2}-\left(\frac{\overline{X}-\mu_{X}}{\frac{\sigma_{X}}{\sqrt{n}}}\right)^{2}.
            \end{align*}
            We know that $\frac{X_{i}-\mu_{X}}{\sigma_{X}} \sim \N(0,1)$. 
            Let:
            \begin{align*}
                U &= \frac{(n-1)S_{n-1}^{2}}{\sigma_{X}^{2}}, & V &= \left(\frac{X_{1}-\mu_{X}}{\sigma_{X}}\right)^{2}.
            \end{align*}
            Using Theorem \ref{Chapter 7 (Theorem) Sampling distribution with known mean} and by definition, we have:
            \begin{align*}
                U+V &\sim \chi^{2}(n), & V &\sim \chi^{2}(1).
            \end{align*} 
            We can use the MGF to prove the theorem. Note that for any $i = 1,2,\cdots,n$, $\overline{X}$ and $X_{i}-\overline{X}$ are independent.
            \begin{equation*}
                M_{U}(t) = \frac{M_{U+V}(t)}{M_{V}(t)} = (1-2t)^{-\frac{n-1}{2}} \implies U = \frac{(n-1)S_{n-1}^{2}}{\sigma_{X}^{2}} \sim \chi^{2}(n-1).
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    \begin{thm}
        Let $X_{1},X_{2},\cdots,X_{n}$ be a random sample from the population $X \sim \N(\mu_{X},\sigma_{X}^{2})$. We have:
        \begin{equation*}
            \frac{\overline{X}-\mu_{X}}{\frac{S_{n-1}}{\sqrt{n}}} \sim t(n-1).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        By Theorems \ref{Chapter 7 (Theorem) Sampling distribution with sample mean} and \ref{Chapter 7 (Theorem) Sampling distribution with sample variance}, we let:
        \begin{align*}
            U &= \frac{\overline{X}-\mu_{X}}{\frac{\sigma_{X}}{\sqrt{n}}} \sim \N(0,1), & V &= \frac{(n-1)S_{n-1}^{2}}{\sigma_{X}^{2}} \sim \chi^{2}(n-1).
        \end{align*}
        By the definition of the $t$-distribution, we have:
        \begin{equation*}
            \frac{\overline{X}-\mu_{X}}{\frac{S_{n-1}}{\sqrt{n}}} = \frac{U}{\sqrt{\frac{V}{n-1}}} \sim t(n-1).
        \end{equation*}
    \end{proofing}

\chapter{Convergence of random variables}
    \label{Chapter 8 (Convergence of random variables)}
    We discussed convergence in distribution in Chapter \ref{Chapter 7 (Generating Function)}. However, this is not the only important mode of convergence for random variables. In this chapter, we introduce other modes of convergence.
\section{Modes of convergence}
    Several modes of convergence for a sequence of random variables will be discussed.\\
    Let us recall the convergence modes for real functions. Let $f,f_{1},f_{2},\cdots:[0,1] \to \mathbb{R}$.
    \begin{enumerate}
        \item \textbf{Pointwise convergence}\\
        We say $f_{n} \to f$ pointwise if, for all $x \in [0,1]$,
        \begin{equation*}
            f_{n}(x) \to f(x) \text{ as } n \to \infty.
        \end{equation*}
        \item \textbf{Convergence in norm $\norm{\cdot}$}\\
        We say $f_{n} \to f$ in norm $\norm{\cdot}$ if
        \begin{equation*}
            \norm{f_{n}-f} \to 0 \text{ as } n \to \infty.
        \end{equation*}
        \item \textbf{Convergence in Lebesgue (uniform) measure}\\
        We say $f_{n} \to f$ in uniform measure $\mu$ if, for all $\varepsilon > 0$,
        \begin{equation*}
            \mu\left(\{x \in [0,1]:\abs{f_{n}(x)-f(x)} > \varepsilon\}\right) \to 0 \text{ as } n \to \infty.
        \end{equation*}
    \end{enumerate}
    These definitions can be used to define convergence modes for random variables.
    \begin{defn}\named{Almost sure convergence}
        We say $X_{n} \to X$ \textbf{almost surely}, written as $X_{n} \xrightarrow{\text{a.s.}} X$, if
        \begin{align*}
            \prob(\{\omega \in \Omega:X_{n}(\omega) \to X(\omega) \text{ as } n \to \infty\}) &= 1, & \text{or} & & \prob(\{\omega \in \Omega:X_{n}(\omega) \not\to X(\omega) \text{ as } n \to \infty\}) &= 0.
        \end{align*}
    \end{defn}
    \begin{rem}
        $X_{n} \xrightarrow{\text{a.s.}} X$ is an adaptation of pointwise convergence for functions.
    \end{rem}
    \begin{rem}
        Almost sure convergence is often referred to as:
        \begin{enumerate}
            \item $X_{n} \to X$ almost everywhere ($X_{n}\xrightarrow{\text{a.e.}}X$).
            \item $X_{n} \to X$ with probability $1$ ($X_{n} \to X$ w.p. $1$).
        \end{enumerate}
    \end{rem}
    \begin{defn}\named{Convergence in $r$-th mean}
        Let $r \geq 1$. We say $X_{n} \to X$ \textbf{in $r$-th mean}, written as $X_{n} \xrightarrow{r} X$, if
        \begin{equation*}
            \expect\abs{X_{n}-X}^{r} \to 0 \text{ as } n \to \infty.
        \end{equation*}
    \end{defn}
    \begin{eg}
        If $r = 1$, we say $X_{n} \to X$ in mean or expectation. If $r = 2$, we say $X_{n} \to X$ in mean square.
    \end{eg}
    \begin{defn}\named{Convergence in probability}
        We say $X_{n} \to X$ \textbf{in probability}, written as $X_{n} \xrightarrow{\prob} X$, if, for all $\varepsilon > 0$,
        \begin{equation*}
            \prob(\abs{X_{n}-X} > \varepsilon) \to 0 \text{ as } n \to \infty.
        \end{equation*}
    \end{defn}
    \newpage

    \begin{defn}\named{Convergence in distribution}
        We say that $X_{n} \to X$ \textbf{in distribution}, written as $X_{n} \xrightarrow{D} X$, if at every continuity point of $\prob(X \leq x)$,
        \begin{equation*}
            F_{n}(x) = \prob(X_{n} \leq x) \to \prob(X \leq x) = F(x) \text{ as } n \to \infty.
        \end{equation*}
    \end{defn}
    Before exploring the relationships between different convergence modes, we first introduce some formulas.
    \begin{lem}\named{Markov's inequality}
        If $X$ is any random variable with a finite mean, then for all $a > 0$,
        \begin{equation*}
            \prob(\abs{X} \geq a) \leq \frac{\expect\abs{X}}{a}.
        \end{equation*}
    \end{lem}
    \begin{proofing}
        \begin{equation*}
            \prob(\abs{X} \geq a) = \expect(\mathbf{1}_{\abs{X} \geq a}) \leq \expect\left(\frac{\abs{X}}{a}\mathbf{1}_{\abs{X} > a}\right) \leq \frac{\expect\abs{X}}{a}.
        \end{equation*}
    \end{proofing}
    \begin{rem}
        For any non-negative function $\varphi$ that is increasing on $[0,\infty)$,
        \begin{equation*}
            \prob(\abs{X} \geq a) = \prob(\varphi(\abs{X}) \geq \varphi(a)) \leq \frac{\expect(\varphi(\abs{X}))}{\varphi(a)}.
        \end{equation*}
    \end{rem}
    The following inequality requires H\"older's inequality (in Appendix $C$) for its proof. Therefore, we will not prove it here.
    \begin{lem}\named{Lyapunov's inequality}
        Let $Z$ be any random variable. For all $r \geq s > 0$,
        \begin{equation*}
            (\expect\abs{Z}^{s})^{\frac{1}{s}} \leq (\expect\abs{Z}^{r})^{\frac{1}{r}}.
        \end{equation*}
    \end{lem}
    We also need to understand how to obtain almost sure convergence.
    \begin{lem}
        \label{Chapter 8 (Lemma) Obtaining almost sure convergence}
        Let
        \begin{align*}
            A_{n}(\varepsilon) &= \{\omega \in \Omega:\abs{X_{n}(\omega)-X(\omega)} > \varepsilon\}, & B_{m}(\varepsilon) &= \bigcup_{n = m}^{\infty}A_{n}(\varepsilon).
        \end{align*}
        We have:
        \begin{enumerate}
            \item $X_{n} \xrightarrow{\text{a.s.}} X$ if and only if $\lim_{m \to \infty}\prob(B_{m}(\varepsilon)) = 0$ for all $\varepsilon > 0$.
            \item $X_{n} \xrightarrow{\text{a.s.}} X$ if $\sum_{n = 1}^{\infty}\prob(A_{n}(\varepsilon)) < \infty$ for all $\varepsilon > 0$.
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item We denote $C = \{\omega \in \Omega:X_{n}(\omega) \to X(\omega) \text{ as } n \to \infty\}$.\\
            If $\omega \in C$, it means that for all $\varepsilon > 0$, there exists $n_{0} > 0$ such that $\abs{X_{n}(\omega)-X(\omega)} \leq \varepsilon$ for all $n \geq n_{0}$.\\
            This also implies that for all $\varepsilon > 0$, $\abs{X_{n}(\omega)-X(\omega)} > \varepsilon$ for finitely many $n$.\\
            If $\omega \in C^{\complement}$, it means that for all $\varepsilon > 0$, $\abs{X_{n}(\omega)-X(\omega)} > \varepsilon$ for infinitely many $n$ ($\omega \in \bigcap_{m = 1}^{\infty}\bigcup_{n = m}^{\infty}A_{n}(\varepsilon)$).\\
            Therefore,
            \begin{equation*}
                C^{\complement} = \bigcup_{\varepsilon > 0}\bigcap_{m = 1}^{\infty}\bigcup_{n = m}^{\infty}A_{n}(\varepsilon).
            \end{equation*}
            If $\prob(C^{\complement}) = 0$, then for all $\varepsilon > 0$,
            \begin{align*}
                \prob\left(\bigcap_{m = 1}^{\infty}\bigcup_{n = m}^{\infty}A_{n}(\varepsilon)\right) &= 0, & \implies & \prob(C^{\complement}) = \prob\left(\bigcup_{\varepsilon > 0}\bigcap_{m = 1}^{\infty}\bigcup_{n = m}^{\infty}A_{n}(\varepsilon)\right) = \prob\left(\bigcup_{k = 1}^{\infty}\bigcap_{m = 1}^{\infty}\bigcup_{n = m}^{\infty}A_{n}\left(\frac{1}{k}\right)\right) = 0.
            \end{align*}
            Therefore, $X_{n} \xrightarrow{\text{a.s.}} X$ if and only if $\lim_{m \to \infty}\prob(B_{m}(\varepsilon)) = 0$ for all $\varepsilon > 0$.
            \item From (1), for all $\varepsilon > 0$,
            \begin{equation*}
                \sum_{n = 1}^{\infty}\prob(A_{n}(\varepsilon)) < \infty \implies \lim_{m \to \infty}\sum_{n = m}^{\infty}\prob(A_{n}(\varepsilon)) = 0 \implies \lim_{m \to \infty}\prob(B_{m}(\varepsilon)) = 0 \implies (X_{n} \xrightarrow{\text{a.s.}} X).
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    \newpage

    Now we can explore the relationships between different convergence modes.
    Roughly speaking, convergence in distribution is the weakest among all convergence modes, as it only concerns the distribution of $X_{n}$.
    \begin{thm}
        \label{Chapter 8 (Theorem) implications of different convergence modes}
        The following implications hold:
        \begin{enumerate}
            \item \begin{enumerate}
                \item $(X_{n} \xrightarrow{\text{a.s.}} X) \implies (X_{n} \xrightarrow{\prob} X)$.
                \item $(X_{n} \xrightarrow{r} X) \implies (X_{n} \xrightarrow{\prob} X)$.
                \item $(X_{n} \xrightarrow{\prob} X) \implies (X_{n} \xrightarrow{D} X)$.
            \end{enumerate}
            \item If $r \geq s \geq 1$, then $(X_{n} \xrightarrow{r} X) \implies (X_{n} \xrightarrow{s} X)$.
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item \begin{enumerate}
                \item From Lemma \ref{Chapter 8 (Lemma) Obtaining almost sure convergence}, for all $\varepsilon > 0$,
                \begin{equation*}
                    \prob(A_{m}(\varepsilon)) \leq \prob\left(\bigcup_{n = m}^{\infty}A_{n}(\varepsilon)\right) = \prob(B_{m}(\varepsilon)) \to 0.
                \end{equation*}
                Therefore, $(X_{n} \xrightarrow{\text{a.s.}} X) \implies (X_{n} \xrightarrow{\prob} X)$.
                \item From Markov's inequality, since $r \geq 1$,
                \begin{equation*}
                    0 \leq \prob(\abs{X-X_{n}} > \varepsilon) = \prob(\abs{X-X_{n}}^{r} > \varepsilon^{r}) \leq \frac{\expect\abs{X_{n}-X}^{r}}{\varepsilon^{r}}.
                \end{equation*}
                Therefore, if $X_{n} \xrightarrow{r} X$, then $\expect\abs{X_{n}-X}^{r} \to 0$. We have $\prob(\abs{X-X_{n}} > \varepsilon) \to 0$, and thus $X_{n} \xrightarrow{\prob} X$.
                \item 
                \begin{equation*}
                    \prob(X_{n} \leq x) = \prob(X_{n} \leq x,X \leq x+\varepsilon)+\prob(X_{n} \leq x,X > x+\varepsilon) \leq \prob(X \leq x+\varepsilon)+\prob(\abs{X_{n}-X} > \varepsilon).
                \end{equation*}
                \begin{align*}
                    \tag{$y = x-\varepsilon$}
                    \prob(X \leq y) &\leq \prob(X_{n} \leq y+\varepsilon)+\prob(\abs{X_{n}-X} > \varepsilon), & \prob(X_{n} \leq x) &\geq \prob(X \leq x-\varepsilon)-\prob(\abs{X_{n}-X} > \varepsilon).
                \end{align*}
                Since $X_{n} \xrightarrow{\prob} X$, $\prob(\abs{X_{n}-X} > \varepsilon) \to 0$ for all $\varepsilon > 0$. Therefore,
                \begin{equation*}
                    \prob(X \leq x-\varepsilon) \leq \liminf_{n \to \infty}\prob(X_{n} \leq x) \leq \limsup_{n \to \infty}\prob(X_{n} \leq x) \leq \prob(X \leq x+\varepsilon).
                \end{equation*}
                By letting $\varepsilon \to 0$,
                \begin{equation*}
                    \prob(X \leq x) \leq \liminf_{n \to \infty}\prob(X_{n} \leq x) \leq \limsup_{n \to \infty}\prob(X_{n} \leq x) \leq \prob(X \leq x).
                \end{equation*}
                Therefore, $\lim_{n \to \infty}\prob(X_{n} \leq x) = \prob(X \leq x)$, and thus $X_{n} \xrightarrow{D} X$.
            \end{enumerate}
            \item Since $X_{n} \xrightarrow{r} X$, $\expect\abs{X_{n}-X} \to 0$ as $n \to \infty$. By Lyapunov's inequality, if $r \geq s$,
            \begin{equation*}
                \expect\abs{X_{n}-X}^{s} \leq (\expect\abs{X_{n}-X}^{r})^{\frac{s}{r}} \to 0.
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    \newpage

    \begin{rem}
        The converses of the implications in Theorem \ref{Chapter 8 (Theorem) implications of different convergence modes} do not hold in general.
    \end{rem}
    \begin{eg}
        Let $\Omega = \{H,T\}$ and $\prob(H) = \prob(T) = \frac{1}{2}$. Define
        \begin{align*}
            X_{2m}(\omega) &= \begin{cases}
                1, &\omega = H,\\
                0, &\omega = T,
            \end{cases} & X_{2m+1}(\omega) = \begin{cases}
                0, &\omega = H,\\
                1, &\omega = T.
            \end{cases}
        \end{align*}
        Since $F(x) = F_{n}(x)$ for all $n$, we have $X_{n} \xrightarrow{D} X$. However, for $\varepsilon \in [0,1]$,
        \begin{equation*}
            \prob(\abs{X_{n}-X} > \varepsilon) \centernot\to 0 \qquad \text{as } n \to \infty.
        \end{equation*}
        Therefore,
        \begin{equation*}
            (X_{n} \xrightarrow{D} X) \centernot\implies (X_{n} \xrightarrow{\prob} X).
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $r = 1$ and
        \begin{align*}
            X_{n} &= \begin{cases}
                n, &\text{probability } = \frac{1}{n},\\
                0, &\text{probability } = 1-\frac{1}{n},
            \end{cases} & X &= 0.
        \end{align*}
        As $n \to \infty$, we have:
        \begin{align*}
            \prob(\abs{X_{n}-X} > \varepsilon) &= \frac{1}{n} \to 0, & \expect\abs{X_{n}-X} &= n\left(\frac{1}{n}\right) = 1 \centernot\to 0.
        \end{align*}
        Therefore,
        \begin{equation*}
            (X_{n} \xrightarrow{\prob} X) \centernot\implies (X_{n} \xrightarrow{r} X).
        \end{equation*}
    \end{eg}
    \begin{eg}
        \label{Chapter 8 (Example) Convergence in probability does not imply almost sure convergence}
        Let $\Omega = [0,1]$, $\mathcal{F} = \mathcal{B}([0,1])$, and $\prob$ be uniform. 
        Let $I_{i}$ be such that:
        \begin{equation*}
            I_{\frac{1}{2}m(m-1)+1}, I_{\frac{1}{2}m(m-1)+2}, \cdots, I_{\frac{1}{2}m(m-1)+m}
        \end{equation*} 
        are $m$ disjoint intervals that partition $[0,1]$ with length $\frac{1}{m}$ for all $m = 1,2,\cdots$. We have $I_{1} = [0,1]$, $I_{2}\cup I_{3} = [0,1]$, $\cdots$. Define
        \begin{align*}
            X_{n}(\omega) &= \mathbf{1}_{I_{n}(\omega)} = \begin{cases}
                1, &\omega \in I_{n},\\
                0, &\omega \in I_{n}^{\complement},
            \end{cases} & X(\omega) &= 0 \text{ for all } \omega \in \Omega.
        \end{align*}
        For all $\varepsilon \in [0,1]$,
        \begin{equation*}
            \prob(\abs{X_{n}-X} > \varepsilon) = \prob(I_{n}) = \frac{1}{n} \to 0 \qquad \text{as } n \to \infty.
        \end{equation*}
        Therefore, $X_{n} \xrightarrow{\prob} X$. However, for any $\omega \in \Omega$, there exists a subsequence $X_{k}(\omega)$ such that $\omega \in I_{k}$ for all $k$. Thus,
        \begin{equation*}
            X_{k}(\omega) = \mathbf{1}_{I_{k}(\omega)} = 1.
        \end{equation*}
        Therefore, $X_{n}(\omega) \not\to X(\omega)$ for all $\omega \in \Omega$. We have:
        \begin{equation*}
            (X_{n} \xrightarrow{\prob} X) \centernot\implies (X_{n} \xrightarrow{\text{a.s.}} X).
        \end{equation*}
    \end{eg}
    \newpage

    \begin{eg}
        If $r \geq s \geq 1$, let
        \begin{align*}
            X_{n} &= \begin{cases}
                n, &\text{probability } = n^{-\left(\frac{r+s}{2}\right)},\\
                0, &\text{probability } = 1-n^{-\left(\frac{r+s}{2}\right)},
            \end{cases} & X &= 0.
        \end{align*}
        As $n \to \infty$, we have:
        \begin{align*}
            \expect\abs{X_{n}-X}^{s} &= n^{s}\left(n^{-\left(\frac{r+s}{2}\right)}\right) = n^{\frac{s-r}{2}} \to 0, & \expect\abs{X_{n}-X}^{r} &= n^{r}\left(n^{-\left(\frac{r+s}{2}\right)}\right) = n^{\frac{r-s}{2}} \to \infty.
        \end{align*}
        Therefore, if $r \geq s \geq 1$,
        \begin{equation*}
            (X_{n} \xrightarrow{s} X) \centernot\implies (X_{n} \xrightarrow{r} X).
        \end{equation*}
    \end{eg}
    \begin{eg}
        Consider
        \begin{equation*}
            X_{n} = \begin{cases}
                n^{3}, &\text{Probability } = n^{-2},\\
                0, &\text{Probability } = 1-n^{-2}.
            \end{cases}
        \end{equation*}
        By applying Lemma \ref{Chapter 8 (Lemma) Obtaining almost sure convergence}, for some $\varepsilon > 0$,
        \begin{align*}
            \prob(\abs{X_{n}(\omega)-X(\omega)} > \varepsilon) &= \frac{1}{n^{2}}, & \sum_{n = 1}^{\infty}\prob(\abs{X_{n}(\omega)-X(\omega)} > \varepsilon) &< \infty.
        \end{align*}
        Therefore, $X_{n} \xrightarrow{\text{a.s.}} X$. However, in mean, as $n \to \infty$, we have
        \begin{equation*}
            \expect\abs{X_{n}-X} = n^{3}\left(\frac{1}{n^{2}}\right) = n \to \infty.
        \end{equation*}
        Therefore, the sequence does not converge in mean. We have:
        \begin{equation*}
            (X_{n} \xrightarrow{\text{a.s.}} X) \centernot\implies (X_{n} \xrightarrow{r} X).
        \end{equation*}
    \end{eg}
    \begin{eg}
        Consider
        \begin{equation*}
            X_{n} = \begin{cases}
                1, &\text{Probability } = n^{-1},\\
                0, &\text{Probability } = 1-n^{-1}.
            \end{cases}
        \end{equation*}
        As $n \to \infty$, we have:
        \begin{equation*}
            \expect\abs{X_{n}-X} = 1\left(\frac{1}{n}\right) = \frac{1}{n} \to 0.
        \end{equation*}
        Therefore, $X_{n} \xrightarrow{1} X$. However, for any $m > 1$, by applying Lemma \ref{Chapter 8 (Lemma) Obtaining almost sure convergence}, for some $\varepsilon \in (0,1)$,
        \begin{align*}
            \prob(B_{m}(\varepsilon)) &= 1-\lim_{r \to \infty}\prob(X_{n} = 0 \text{ for all } n \text{ such that } m \leq n \leq r)\\
            &= 1-\lim_{r \to \infty}\prod_{i = m}^{r}\frac{i-1}{i}\\
            &= 1-\lim_{r \to \infty}\frac{m-1}{r} \to 1 \neq 0.
        \end{align*}
        Therefore, the sequence does not converge almost surely. We have:
        \begin{equation*}
            (X_{n} \xrightarrow{1} X) \centernot\implies (X_{n} \xrightarrow{\text{a.s.}} X).
        \end{equation*}
    \end{eg}
    \newpage

    Some partial converses of the implications in Theorem \ref{Chapter 8 (Theorem) implications of different convergence modes} do hold.
    \begin{thm}
        \label{Chapter 8 (Theorem) Partial converse statements}
        The following implications hold:
        \begin{enumerate}
            \item If $X_{n} \xrightarrow{D} c$, where $c$ is a constant, then $X_{n} \xrightarrow{\prob} c$.
            \item If $X_{n} \xrightarrow{\prob} X$ and $\prob(\abs{X_{n}} \leq k) = 1$ for all $n$ with some fixed constant $k > 0$, then $X_{n} \xrightarrow{r} X$ for all $r \geq 1$.
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item Since $X_{n} \xrightarrow{D} X$, $\prob(X_{n} \leq x) \to \prob(c \leq x)$ as $n \to \infty$. For all $\varepsilon > 0$,
            \begin{equation*}
                \prob(\abs{X_{n}-c} \geq \varepsilon) = \prob(X_{n} \leq c-\varepsilon)+\prob(X_{n} \geq c+\varepsilon) = \prob(X_{n} \leq c-\varepsilon)+1-\prob(X_{n} < c+\varepsilon).
            \end{equation*}
            We find that $\prob(X_{n} \leq c-\varepsilon) \to \prob(c \leq c-\varepsilon) = 0$. For $\prob(X_{n} < c+\varepsilon)$,
            \begin{equation*}
                \prob\left(X_{n} \leq c+\frac{\varepsilon}{2}\right) \leq \prob(X_{n} < c+\varepsilon) \leq \prob(X_{n} \leq c+2\varepsilon).
            \end{equation*}
            \begin{align*}
                \prob\left(X_{n} \leq c+\frac{\varepsilon}{2}\right) &\to \prob\left(c \leq c+\frac{\varepsilon}{2}\right) = 1, & \prob(X_{n} \leq c+2\varepsilon) &\to \prob(c \leq c+2\varepsilon) = 1.
            \end{align*}
            Therefore, $\prob(X_{n} < c+\varepsilon) \to 1$. We have
            \begin{equation*}
                \prob(\abs{X_{n}-c} \geq \varepsilon) \to 0+1-1 = 0.
            \end{equation*}
            Hence, $X_{n} \xrightarrow{\prob} c$.
            \item Since $X_{n} \xrightarrow{\prob} X$, $X_{n} \xrightarrow{D} X$. We have $\prob(\abs{X_{n}} \leq k) \to \prob(\abs{X} \leq k) = 1$.\\
            Therefore, for all $\varepsilon > 0$, if $\abs{X_{n}-X} \leq \varepsilon$, $\abs{X_{n}-X} \leq \abs{X_{n}}+\abs{X} \leq 2k$.
            \begin{align*}
                \expect\abs{X_{n}-X}^{r} &= \expect\left(\abs{X_{n}-X}^{r}\mathbf{1}_{\abs{X_{n}-X} \leq \varepsilon}\right)+\expect\left(\abs{X_{n}-X}^{r}\mathbf{1}_{\abs{X_{n}-X} > \varepsilon}\right)\\
                &\leq \varepsilon^{r}\expect\left(\mathbf{1}_{\abs{X_{n}-X} \leq \varepsilon}\right)+(2k)^{r}\expect\left(\mathbf{1}_{\abs{X_{n}-X} > \varepsilon}\right)\\
                &\leq \varepsilon^{r}+((2k)^{r}-\varepsilon^{r})\prob(\abs{X_{n}-X} > \varepsilon).
            \end{align*}
            Since $X_{n} \xrightarrow{\prob} X$, as $n \to \infty$, $\expect\abs{X_{n}-X}^{r} \to \varepsilon^{r}$. If we send $\varepsilon \to 0$, $\expect\abs{X_{n}-X}^{r} \to 0$ and therefore $X_{n} \xrightarrow{r} X$.
        \end{enumerate}
    \end{proofing}
    Note that any sequence $\{X_{n}\}$ that satisfies $X_{n} \xrightarrow{\prob} X$ always has a subsequence that converges almost surely to $X$.
    \begin{thm}
        \label{Chapter 8 (Theorem) Probability convergence implies almost sure convergence for a subsequence}
        If $X_{n} \xrightarrow{\prob} X$, then there exists a subsequence $X_{n_{1}},X_{n_{2}},\cdots$ such that
        \begin{equation*}
            X_{n_{i}} \xrightarrow{\text{a.s.}} X.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Since $X_{n} \xrightarrow{\prob} X$, for all $\varepsilon > 0$,
        \begin{equation*}
            \prob(\abs{X_{n}-X} > \varepsilon) \to 0 \text{ as } n \to \infty.
        \end{equation*}
        We can find a subsequence $X_{n_{1}},X_{n_{2}},\cdots$ such that for all $i \geq 1$,
        \begin{equation*}
            \prob(\abs{X_{n_{i}}-X} > i^{-1}) \leq i^{-2}.
        \end{equation*}
        For any fixed $\varepsilon > 0$, we have
        \begin{equation*}
            \sum_{i > \varepsilon^{-1}}\prob(\abs{X_{n_{i}}-X} > \varepsilon) \leq \sum_{i > \varepsilon^{-1}}\prob(\abs{X_{n_{i}}-X} > i^{-1}) \leq \sum_{i}i^{-2} < \infty.
        \end{equation*}
        By Lemma \ref{Chapter 8 (Lemma) Obtaining almost sure convergence}, we get that $X_{n_{i}} \xrightarrow{\text{a.s.}} X$ as $i \to \infty$.
    \end{proofing}
    \newpage

    So far, we have only discussed convergence of a single sequence of random variables. We now consider the convergence of two sequences of random variables.
    \begin{thm}\named{Slutsky's Theorem}
        If $X_{n} \xrightarrow{D} X$ and $Y_{n} \xrightarrow{\prob} c$, for a constant $c$, then:
        \begin{enumerate}
            \item $X_{n}+Y_{n} \xrightarrow{D} X+c$.
            \item $X_{n}Y_{n} \xrightarrow{D} Xc$.
            \item $\frac{X_{n}}{Y_{n}} \xrightarrow{D} \frac{X}{c}$ if $c \neq 0$.
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item Suppose that $c > 0$ and pick $\delta$ such that $0 < \delta < c$. We can find $N$ such that $\prob(\abs{Y_{n}-c} > \delta) < \delta$ for $n \geq N$. For all $x$, we have:
            \begin{align*}
                \prob(X_{n}+Y_{n} \leq x) &\leq \prob(X_{n}+Y_{n} \leq x,\abs{Y_{n}-c} \leq \delta)+\prob(\abs{Y_{n}-c} > \delta) \leq \prob(X_{n} \leq x-c+\delta)+\delta,\\
                \prob(X_{n}+Y_{n} > x) &\leq \prob(X_{n}+Y_{n} > x,\abs{Y_{n}-c} \leq \delta)+\delta \leq \prob(X_{n} > x-c-\delta)+\delta.
            \end{align*}
            By sending $n \to \infty$ and $\delta \to 0$, we find that $\prob(X_{n}+Y_{n} \leq x) \to \prob(X+c \leq x)$ when $c > 0$.\\
            A similar argument can be used to prove that $\prob(X_{n}+Y_{n} \leq x) \to \prob(X+c \leq x)$ when $c < 0$.\\
            Suppose that $c = 0$. Choose an arbitrarily small number $\delta > 0$ and a number $N$ such that $\prob(\abs{Y_{n}} > \delta) < \delta$ for $n \geq N$. For all $x$, we have:
            \begin{align*}
                \prob(X_{n}+Y_{n} \leq x) &\leq \prob(X_{n}+Y_{n} \leq x,\abs{Y_{n}} \leq \delta)+\prob(\abs{Y_{n}} > \delta) \leq \prob(X_{n} \leq x+\delta)+\delta,\\
                \prob(X_{n}+Y_{n} > x) &\leq \prob(X_{n}+Y_{n} \leq x,\abs{Y_{n}} \leq \delta)+\prob(\abs{Y_{n}} > \delta) \leq \prob(X_{n} \leq x-\delta)+\delta.
            \end{align*}
            By sending $n \to \infty$ and $\delta \to 0$, we find that $\prob(X_{n}+Y_{n} \leq x) \to \prob(X+c \leq x)$ when $c = 0$.\\
            Therefore, $X_{n}+Y_{n} \xrightarrow{D} X+c$.
            \item Suppose that $c > 0$ and pick $\delta$ such that $0 < \delta < c$. We can find $N$ such that $\prob(\abs{Y_{n}-c} > \delta) < \delta$ for $n \geq N$. For all $x$, we have:
            \begin{align*}
                \prob(X_{n}Y_{n} \leq x) &\leq \prob(X_{n}Y_{n} \leq x,\abs{Y_{n}-c} \leq \delta)+\prob(\abs{Y_{n}-c} > \delta) \leq \prob\left(X_{n} \leq \frac{x}{c-\delta}\right)+\delta,\\
                \prob(X_{n}Y_{n} > x) &\leq \prob(X_{n}Y_{n} > x,\abs{Y_{n}-c} \leq \delta)+\delta \leq \prob\left(X_{n} > \frac{x}{c+\delta}\right)+\delta.
            \end{align*}
            By sending $n \to \infty$ and $\delta \to 0$, we find that $\prob(X_{n}Y_{n} \leq x) \to \prob(Xc \leq x)$ when $c > 0$.\\
            A similar argument can be used to prove that $\prob(X_{n}Y_{n} \leq x) \to \prob(Xc \leq x)$ when $c < 0$.\\
            Suppose that $c = 0$. Choose an arbitrarily small number $\delta > 0$ and a number $N$ such that $\prob(\abs{Y_{n}} > \delta) < \delta$ for $n \geq N$. For all $x$, we have:
            \begin{align*}
                \prob(X_{n}Y_{n} \leq x) &\leq \prob(X_{n}Y_{n} \leq x,\abs{Y_{n}} \leq \delta)+\prob(\abs{Y_{n}} > \delta) \leq \prob(-\delta X_{n} \leq x)+\delta,\\
                \prob(X_{n}Y_{n} > x) &\leq \prob(X_{n}Y_{n} > x,\abs{Y_{n}} \leq \delta)+\prob(\abs{Y_{n}} > \delta) \leq \prob(\delta X_{n} > x)+\delta.
            \end{align*}
            By sending $n \to \infty$ and $\delta \to 0$, we find that $\prob(X_{n}Y_{n} \leq x) \to \prob(0 \leq x)$ when $c = 0$.\\
            Therefore, $X_{n}Y_{n} \xrightarrow{D} Xc$.
            \item It suffices to prove that $Y_{n}^{-1} \xrightarrow{\prob} c^{-1}$ if $Y_{n} \xrightarrow{\prob} c$, or equivalently by Theorem \ref{Chapter 8 (Theorem) Partial converse statements}, $Y_{n} \xrightarrow{D} c$.\\
            If $Y_{n} \xrightarrow{D} c$, then $\prob(Y_{n} \leq x) \to \prob(c \leq x)$ as $n \to \infty$ for all $x$. When $x \geq 0$, as $n \to \infty$,
            \begin{equation*}
                \prob\left(\frac{1}{Y_{n}} \leq x\right) = \prob(Y_{n} < 0)+\prob\left(Y_{n} \geq \frac{1}{x}\right) \to \prob(c < 0)+\prob\left(c \geq \frac{1}{x}\right) = \prob\left(\frac{1}{c} \leq x\right).
            \end{equation*}
            When $x < 0$, as $n \to \infty$,
            \begin{equation*}
                \prob\left(\frac{1}{Y_{n}} \leq x\right) = \prob\left(\frac{1}{x} \leq Y_{n} < 0\right) \to \prob\left(\frac{1}{x} \leq c < 0\right) = \prob\left(\frac{1}{c} \leq x\right).
            \end{equation*}
            Therefore, $\frac{1}{Y_{n}} \xrightarrow{D} \frac{1}{c}$ and thus $\frac{1}{Y_{n}} \xrightarrow{\prob} \frac{1}{c}$.\
            By using (2), $\frac{X_{n}}{Y_{n}} \xrightarrow{D} \frac{X}{c}$.
        \end{enumerate}
    \end{proofing}
    \newpage

    \begin{thm}\named{Continuous Mapping Theorem}
        Given a sequence of random variables $\{X_{n}\}$ and a random variable $X$, let $f$ be a function with the set of discontinuity points $D_{f}$ such that $\prob(X \in D_{f}) = 0$. Then:
        \begin{enumerate}
            \item If $X_{n} \xrightarrow{D} X$, then $f(X_{n}) \xrightarrow{D} f(X)$.
            \item If $X_{n} \xrightarrow{\prob} X$, then $f(X_{n}) \xrightarrow{\prob} f(X)$.
            \item If $X_{n} \xrightarrow{\text{a.s.}} X$, then $f(X_{n}) \xrightarrow{\text{a.s.}} f(X)$.
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item \textit{Current knowledge in these notes does not suffice to prove (1). Search for "Weak convergence of measures" on Wikipedia to start.}
            \item Fix an arbitrary $\varepsilon > 0$. For any $\delta > 0$, consider a set $B_{\delta}$ defined as:
            \begin{equation*}
                B_{\delta} = \{x:x \not \in D_{f}\text{ and there exists }y\text{ such that }\abs{x-y} < \delta\text{ and }\abs{f(x)-f(y)} > \varepsilon\}.
            \end{equation*}
            Suppose that $\abs{f(X)-f(X_{n})} > \varepsilon$. This means either $\abs{X-X_{n}} \geq \delta$, $X \in D_{f}$, or $X \in B_{\delta}$. Therefore,
            \begin{equation*}
                \prob(\abs{f(X_{n})-f(X)} > \varepsilon) \leq \prob(\abs{X_{n}-X} \geq \delta)+\prob(X \in D_{f})+\prob(X \in B_{\delta}).
            \end{equation*}
            Since $X_{n} \xrightarrow{\prob} X$, $\prob(\abs{X_{n}-X} \geq \delta) \to 0$ as $n \to \infty$ for all $\delta > 0$.\\ 
            Since $\prob(X \in D_{f}) = 0$ by assumption and $\prob(X \in B_{\delta}) \to 0$ as $\delta \to 0$, by first sending $n \to \infty$ and then $\delta \to 0$, we get:
            \begin{equation*}
                \prob(\abs{f(X_{n})-f(X)} > \varepsilon) \to 0.
            \end{equation*}
            Generalizing to all $\varepsilon > 0$, we get $f(X_{n}) \xrightarrow{\prob} f(X)$.
            \item By the definition of almost sure convergence, we want to show that:
            \begin{align*}
                \prob(\{\omega \in \Omega:f(X_{n}(\omega)) \to f(X(\omega))\text{ as }n \to \infty\}) &\geq \prob(\{\omega \in \Omega:f(X_{n}) \to f(X)\text{ as }n \to \infty\text{ and }X \not \in D_{f}\})\\
                &\geq \prob(\{\omega \in \Omega:X_{n}(\omega) \to X(\omega)\text{ as }n \to \infty\text{ and }X \not \in D_{f}\}).
            \end{align*}
            Since $\prob(X \in D_{f}) = 0$ by assumption, and $X_{n} \xrightarrow{\text{a.s.}} X$,
            \begin{equation*}
                \prob(\{\omega \in \Omega:X_{n} \to X\text{ as }n \to \infty\text{ and }X \not \in D_{f}\}) = \prob(\{\omega \in \Omega:X_{n}(\omega) \to X(\omega)\text{ as }n \to \infty\}) = 1.
            \end{equation*}
            Therefore, $\prob(\{\omega \in \Omega:f(X_{n}(\omega)) \to f(X(\omega))\text{ as }n \to \infty\}) = 1$, and thus $f(X_{n}) \xrightarrow{\text{a.s.}} f(X)$.
        \end{enumerate}
    \end{proofing}
    \newpage

\section{Different Versions and Applications of the Weak Law of Large Numbers}
    Recall the Weak Law of Large Numbers (WLLN) for i.i.d. random variables with finite mean and variance, which states that the sample average converges in probability to the population mean.
    \begin{thm}\named{WLLN for i.i.d. Case}
        Let $X_{1},X_{2},\cdots$ be i.i.d. random variables with $\expect(X_{i}) = \mu$ and $\Var(X_{i}) = \sigma^{2} < \infty$. Then:
        \begin{equation*}
            \frac{1}{n}\sum_{i = 1}^{n}X_{i} \xrightarrow{\prob} \mu.
        \end{equation*}
    \end{thm}
    There are many applications of the WLLN. We present one such application here.
    \begin{eg}\named{Bernstein Approximation}
        Let $f:[0,1] \to \mathbb{R}$ be a continuous function. For each $n = 1,2,\cdots$, define
        \begin{equation*}
            \tag{Bernstein Polynomial}
            f_{n}(x) = \sum_{m = 0}^{n}\binom{n}{m}x^{m}(1-x)^{n-m}f\left(\frac{m}{n}\right).
        \end{equation*}
        Then,
        \begin{equation*}
            \sup_{x \in [0,1]}\abs{f_{n}(x)-f(x)} \to 0 \qquad\text{as } n \to \infty.
        \end{equation*}
    \end{eg}
    \begin{proofing}
        For each fixed $x \in [0,1]$, let $Y_{n,x} \sim \Bin(n,x)$. Then,
        \begin{align*}
            \prob(Y_{n,x} = m) &= \binom{n}{m}x^{m}(1-x)^{n-m},\\
            f_{n}(x) &= \sum_{m = 0}^{n}\prob(Y_{n,x} = m)f\left(\frac{m}{n}\right) = \expect\left(f\left(\frac{Y_{n,x}}{n}\right)\right).
        \end{align*}
        By the WLLN and the Continuous Mapping Theorem, as $n \to \infty$,
        \begin{equation*}
            \frac{Y_{n,x}}{n} \xrightarrow{\prob} x \implies f\left(\frac{Y_{n,x}}{n}\right) \xrightarrow{\prob} f(x).
        \end{equation*}
        Since $f$ is continuous on the compact set $[0,1]$, it is uniformly continuous and bounded. For all $\varepsilon > 0$, there exists a $\delta_{\varepsilon} > 0$ such that for all $x,y \in [0,1]$,
        \begin{equation*}
            \abs{x-y} \leq \delta_{\varepsilon} \implies \abs{f(x)-f(y)} \leq \varepsilon.
        \end{equation*}
        Let $M > 0$ be such that for all $x \in [0,1]$, $\abs{f(x)} \leq M$. Then,
        \begin{align*}
            \abs{\expect\left(f\left(\frac{Y_{n,x}}{n}\right)\right)-f(x)} &\leq \expect\abs{f\left(\frac{Y_{n,x}}{n}\right)-f(x)}\\
            &= \expect\left(\abs{f\left(\frac{Y_{n,x}}{n}\right)-f(x)}\mathbf{1}_{\abs{\frac{Y_{n,x}}{n}-x} \leq \delta_{\varepsilon}}\right)+\expect\left(\abs{f\left(\frac{Y_{n,x}}{n}\right)-f(x)}\mathbf{1}_{\abs{\frac{Y_{n,x}}{n}-x} > \delta_{\varepsilon}}\right)\\
            &\leq \varepsilon+2M\prob\left(\abs{\frac{Y_{n,x}-nx}{n}} > {\delta_{\varepsilon}}\right).\\
            \sup_{x \in [0,1]}\abs{\expect\left(f\left(\frac{Y_{n,x}}{n}\right)\right)-f(x)} &= \varepsilon+2M\sup_{x \in [0,1]}\prob\left(\abs{\frac{Y_{n,x}-nx}{n}} > {\delta_{\varepsilon}}\right)\\
            \tag{Markov's Inequality}
            &\leq \varepsilon+\frac{2M}{n\delta_{\varepsilon}}\sup_{x \in [0,1]}\expect\abs{Y_{n,x}-nx}\\
            \tag{Lyapunov's Inequality}
            &\leq \varepsilon+\frac{2M}{n\delta_{\varepsilon}}\sup_{x \in [0,1]}\sqrt{\expect(Y_{n,x}-nx)^{2}}\\
            \tag{$\expect{Y_{n,x}} = nx$}
            &\leq \varepsilon+\frac{2M}{n\delta_{\varepsilon}}\sup_{x \in [0,1]}\sqrt{\Var(Y_{n,x})} = \varepsilon+\frac{2M}{n\delta_{\varepsilon}}\sup_{x \in [0,1]}\sqrt{nx(1-x)}\\
            &\leq \varepsilon+\frac{M}{\delta_{\varepsilon}\sqrt{n}}.\\
            \limsup_{n \to \infty}\sup_{x \in [0,1]}\abs{\expect\left(f\left(\frac{Y_{n,x}}{n}\right)\right)-f(x)} &\leq \varepsilon \to 0.
        \end{align*}
        Therefore, $\sup_{x \in [0,1]}\abs{f_{n}(x)-f(x)} \to 0$ as $n \to \infty$.
    \end{proofing}
    \newpage

    \begin{eg}\named{Borel's Geometric Concentration}
        Let $\mu_{n}$ be the uniform distribution on $[-1,1]^{n}$.\\
        Let $\mathcal{H} = \{\mathbf{x} \in [-1,1]^{n}:\left < \mathbf{x},(1,\cdots,1)\right > = 0\}$ be a hyperplane orthogonal to a principal diagonal in $\mathbb{R}^{n}$.\\
        For $r > 0$, let $\mathcal{H}_{r} = \{\mathbf{x} \in [-1,1]^{n}:\dist(\mathbf{x},\mathcal{H}) \leq r\}$ be the $r$-neighbourhood of $\mathcal{H}$. For all $\varepsilon > 0$,
        \begin{equation*}
            \mu_{n}(\mathcal{H}_{\varepsilon\sqrt{n}}) \to 1 \qquad\text{as } n \to \infty.
        \end{equation*}
    \end{eg}
    \begin{proofing}
        We can prove this by letting $X_{1},X_{2},\cdots \sim \U[-1,1]$ be i.i.d. random variables with $\expect{X_{i}} = 0$. 
        Let $\mathbf{X} = (X_{1},\cdots,X_{n})$. Then, $\mathbf{X} \sim \mu_{n}$. 
        For all $B\subseteq[-1,1]^{n}$, $\mu_{n}(B) = \prob(\mathbf{X} \in B)$.
        Therefore,
        \begin{align*}
            \mu_{n}(\mathcal{H}_{\varepsilon\sqrt{n}}) &= \prob(\mathbf{X} \in \mathcal{H}_{\varepsilon\sqrt{n}})\\
            &= \prob(\dist(\mathbf{X},\mathcal{H}) \leq \varepsilon\sqrt{n})\\
            &= \prob\left(\frac{\abs{\left < X,(1,\cdots,1)\right > }}{\norm{(1,\cdots,1)}_{2}} \leq \varepsilon\sqrt{n}\right)\\
            &= \prob\left(\abs{\frac{\sum_{i = 1}^{n}X_{i}}{n}} \leq \varepsilon\right)\\
            &= 1-\prob\left(\abs{\frac{\sum_{i = 1}^{n}X_{i}}{n}} > \varepsilon\right)\\
            \tag{WLLN}
            &\to 1.
        \end{align*}
    \end{proofing}
    Another version of the WLLN is the $L^{2}$-WLLN, which states that the sample average converges in $L^{2}$ to the population mean under weaker conditions.
    \begin{thm}\named{$L^{2}$-WLLN}
        Let $X_{1},X_{2},\cdots$ be uncorrelated random variables with $\expect(X_{i}) = \mu$ and $\Var(X_{i}) \leq c < \infty$ for all $i$. Then:
        \begin{equation*}
            \frac{1}{n}\sum_{i = 1}^{n}X_{i} \xrightarrow{2} \mu.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        \begin{equation*}
            \expect\left(\frac{1}{n}\sum_{i = 1}^{n}X_{i}-\mu\right)^{2} = \frac{1}{n^{2}}\expect\left(\sum_{i = 1}^{n}X_{i}-\expect\left(\sum_{i = 1}^{n}X_{i}\right)\right)^{2} = \frac{1}{n^{2}}\Var\left(\sum_{i = 1}^{n}X_{i}\right) = \frac{1}{n^{2}}\sum_{i = 1}^{n}\Var(X_{i}) \leq \frac{c}{n} \to 0.
        \end{equation*}
        Therefore, $\frac{1}{n}\sum_{i = 1}^{n} \xrightarrow{2} \mu$.
    \end{proofing}
    \begin{rem}
        By Theorem \ref{Chapter 8 (Theorem) implications of different convergence modes}, convergence in $r$-th mean implies convergence in probability. Therefore, by the $L^{2}$-WLLN, we also have:
        \begin{equation*}
            \left(\frac{1}{n}\sum_{i = 1}^{n}X_{i} \xrightarrow{2} \mu\right) \implies \left(\frac{1}{n}\sum_{i = 1}^{n}X_{i} \xrightarrow{\prob} \mu\right).
        \end{equation*}
    \end{rem}
    \begin{rem}
        In the $L^{2}$-WLLN, we do not require the random variables to be identically distributed. We only require them to be uncorrelated and have the same mean and bounded variance.
    \end{rem}
    \begin{rem}
        The $L^{2}$-WLLN can be generalized to the case where $\Var(X_{i})$ are not uniformly bounded. If $\Var(X_{i}) < \infty$ for all $i$ and $\frac{1}{n^{2}}\sum_{i = 1}^{n}\Var(X_{i}) \to 0$, then we still have $\frac{1}{n}\sum_{i = 1}^{n}X_{i} \xrightarrow{2} \mu$.
    \end{rem}
    \newpage

    A triangular array is a collection of random variables $\{X_{n,j}\}_{1 \leq j \leq n < \infty}$ such that for each fixed $n$, $X_{n,1},X_{n,2},\cdots,X_{n,n}$ are independent random variables. The WLLN can be generalized to triangular arrays.
    \begin{thm}\named{WLLN for Triangular Array}
        Let $\{X_{n,j}\}_{1 \leq j \leq n < \infty}$ be a triangular array. Define $Y_{n} = \sum_{i = 1}^{n}X_{n,i}$, $\mu_{n} = \expect{Y_{n}}$, and $\sigma_{n}^{2} = \Var(Y_{n})$. Suppose that for some sequence $b_{n}$,
        \begin{equation*}
            \frac{\sigma_{n}^{2}}{b_{n}^{2}} = \expect\left(\frac{Y_{n}-\mu_{n}}{b_{n}}\right)^{2} \to 0.
        \end{equation*}
        Then we have:
        \begin{equation*}
            \frac{Y_{n}-\mu_{n}}{b_{n}} \xrightarrow{\prob} 0.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        \begin{equation*}
            \expect\left(\frac{Y_{n}-\mu_{n}}{b_{n}}\right)^{2} = \frac{\Var(Y_{n})}{b_{n}^{2}} \to 0.
        \end{equation*}
        Therefore, $\frac{Y_{n}-\mu_{n}}{b_{n}} \xrightarrow{2} 0$, and thus $\frac{Y_{n}-\mu_{n}}{b_{n}} \xrightarrow{\prob} 0$.
    \end{proofing}
    \begin{rem}
        We should choose $b_{n}$ to be no larger than $\expect{Y_{n}}$ if possible.
    \end{rem}
    \begin{eg}\named{Coupon Collector's Problem} 
        There are $n$ different types of coupons. Each time we pick a coupon, it is equally likely to be any one of the $n$ types, independent of previous picks. Let $\tau_{k}^{n}$ be the number of picks needed to get $k$ distinct types of coupons. We want to find the asymptotic behaviour of $\tau_{n}^{n}$ as $n \to \infty$.\\
        Let $X_{n,k}$ be the number of picks needed to get the $k$-th new type of coupon after we have already collected $k-1$ distinct types. Then, $X_{n,1},X_{n,2},\cdots,X_{n,n}$ are independent random variables.\\
        Note that $X_{n,1} = 1$ since we always get a new type of coupon on the first pick. Also, for $2 \leq k \leq n$, we have:
        \begin{equation*}
            \tau_{n}^{n} = \sum_{k = 1}^{n}X_{n,k}.
        \end{equation*}
        For $2 \leq k \leq n$ and $\ell = 1,2,\cdots$, we have:
        \begin{equation*}
            \prob(X_{n,k} = \ell) = \left(\frac{k-1}{n}\right)^{\ell-1}\left(1-\frac{k-1}{n}\right) \implies X_{n,k} \sim \Geom\left(1-\frac{k-1}{n}\right).
        \end{equation*}
        Therefore,
        \begin{align*}
            \expect{X_{n,k}} &= \left(1-\frac{k-1}{n}\right)^{-1}, & \Var(X_{n,k}) &= \left(1-\frac{k-1}{n}\right)^{-2}-\left(1-\frac{k-1}{n}\right)^{-1}.
        \end{align*}
        We can see that:
        \begin{align*}
            \expect{\tau_{n}^{n}} &= \sum_{k = 1}^{n}\expect{X_{n,k}} = \sum_{k = 1}^{n}\left(1-\frac{k-1}{n}\right)^{-1} = n\sum_{m = 1}^{n}\frac{1}{m} \sim n\log n,\\
            \Var(\tau_{n}^{n}) &= \sum_{k = 1}^{n}\Var(X_{n,k}) = \sum_{k = 1}^{n}\left(\left(1-\frac{k-1}{n}\right)^{-2}-\left(1-\frac{k-1}{n}\right)^{-1}\right)\\
            &\leq \sum_{k = 1}^{n}\left(1-\frac{k-1}{n}\right)^{-2} = n^{2}\sum_{m = 1}^{n}\frac{1}{m^{2}} \sim \frac{\pi^{2}}{6}n^{2}.
        \end{align*}
        By using the WLLN for triangular array, let $b_{n} = n\log n$:
        \begin{equation*}
            \frac{\Var(\tau_{n}^{n})}{b_{n}^{2}} \leq \frac{\frac{\pi^{2}}{6}n^{2}}{(n\log n)^{2}} = \frac{\pi^{2}}{6\log^{2} n} \to 0.
        \end{equation*}
        Therefore, $\frac{\tau_{n}^{n}-\expect{\tau_{n}^{n}}}{n\log n} \xrightarrow{\prob} 0$, or equivalently,
        \begin{equation*}
            \frac{\tau_{n}^{n}}{n\log n} \xrightarrow{\prob} 1.
        \end{equation*}
    \end{eg}
    \newpage

    \begin{eg}\named{Empty Bins Problem}
        We throw $r$ balls into $n$ bins independently and uniformly at random.\\
        Let $N_{n}$ be the number of empty bins after throwing the $r$ balls. 
        We want to find the asymptotic behaviour of $N_{n}$ as $n \to \infty$ and $\frac{r}{n} \to c$ for some constant $c > 0$.\\
        For $1 \leq i \leq n$, let $A_{i}$ be the event that the $i$-th bin is empty after throwing the $r$ balls. Then,
        \begin{equation*}
            N_{n} = \sum_{i = 1}^{n}\mathbf{1}_{A_{i}}.
        \end{equation*}
        Note that $\mathbf{1}_{A_{i}}$ are identically distributed but not independent. However, they are pairwise negatively correlated since for $i \neq j$,
        \begin{align*}
            \frac{\expect{N_{n}}}{n} &= \frac{1}{n}\sum_{i = 1}^{n}\expect(\mathbf{1}_{A_{i}})\\
            &= \prob(A_{1})\\
            &= \left(1-\frac{1}{n}\right)^{r}\\
            &\sim e^{-c},\\
            \Var(N_{n}) &= \expect(N_{n}-\expect{N_{n}})^{2}\\
            &= \expect\left(\sum_{i = 1}^{n}(\mathbf{1}_{A_{i}}-\expect{\mathbf{1}_{A_{i}}})\right)^{2}\\
            &= \sum_{i = 1}^{n}\expect\left(\mathbf{1}_{A_{i}}-\expect{\mathbf{1}_{A_{i}}}\right)^{2}+2\sum_{i \neq j}\cov(\mathbf{1}_{A_{i}},\mathbf{1}_{A_{j}})\\
            &= \sum_{i = 1}^{n}\Var(\mathbf{1}_{A_{i}})+2\sum_{i \neq j}\left(\expect(\mathbf{1}_{A_{i}}\mathbf{1}_{A_{j}})-\expect{\mathbf{1}_{A_{i}}}\expect{\mathbf{1}_{A_{j}}}\right)\\
            &= n\prob(A_{1})(1-\prob(A_{1}))+n(n-1)(\prob(A_{1}\cap A_{2})-\prob(A_{1})\prob(A_{2}))\\
            &= n\left(1-\frac{1}{n}\right)^{r}\left(1-\left(1-\frac{1}{n}\right)^{r}\right)+n(n-1)\left(1-\frac{2}{n}\right)^{r}-n(n-1)\left(1-\frac{1}{n}\right)^{2r}\\
            &\sim n\left(e^{-c}-e^{-2c}\right)+n^{2}\left(e^{-2c}-e^{-2c}\right) = o(n^{2}).
        \end{align*}
        By using the WLLN for triangular array, let $b_{n} = n$:
        \begin{equation*}
            \frac{\Var(N_{n})}{b_{n}^{2}} = \frac{\Var(N_{n})}{n^{2}} \to 0.
        \end{equation*}
        Therefore, $\frac{N_{n}-\expect{N_{n}}}{n} \xrightarrow{\prob} 0$, or equivalently,
        \begin{equation*}
            \frac{N_{n}}{n} \xrightarrow{\prob} e^{-c}.
        \end{equation*}
    \end{eg}
    \newpage

\section{Borel-Cantelli Lemmas}
    Let $A_{1},A_{2},\cdots$ be a sequence of events in $(\Omega,\mathcal{F})$. We are particularly interested in:
    \begin{equation*}
        \limsup_{n \to \infty}A_{n} = \{A_{n}\text{ i.o.}\} = \bigcap_{m}\bigcup_{n = m}^{\infty} A_{n}.
    \end{equation*}
    \begin{thm}\named{Borel-Cantelli Lemmas}
        For any sequence of events $A_{n} \in \mathcal{F}$:
        \begin{enumerate}
            \item (BCI) If $\sum_{n = 1}^{\infty}\prob(A_{n}) < \infty$, then:
            \begin{equation*}
                \prob(A_{n}\text{ i.o.}) = 0.
            \end{equation*}
            \item (BCII) If $\sum_{n = 1}^{\infty}\prob(A_{n}) = \infty$ and $A_{n}$ are independent, then:
            \begin{equation*}
                \prob(A_{n}\text{ i.o.}) = 1.
            \end{equation*}
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item If $\sum_{n = 1}^{\infty}\prob(A_{n}) < \infty$:
            \begin{equation*}
                \prob(A_{n}\text{ i.o.}) = \lim_{m \to \infty}\prob\left(\bigcup_{n = m}^{\infty}A_{n}\right) \leq \lim_{m \to \infty}\sum_{n = m}^{\infty}\prob(A_{n}) = 0.
            \end{equation*}
            \item If $\sum_{n = 1}^{\infty}\prob(A_{n}) = \infty$ and $A_{n}$ are independent, we have:
            \begin{align*}
                \prob\left(\bigcup_{m = 1}^{\infty}\bigcap_{n = m}^{\infty}A_{n}^{\complement}\right) &= \lim_{m \to \infty}\prob\left(\bigcap_{n = m}^{\infty}A_{n}^{\complement}\right) = \lim_{m \to \infty}\lim_{r \to \infty}\prob\left(\bigcap_{n = m}^{r}A_{n}^{\complement}\right) = \lim_{m \to \infty}\lim_{r \to \infty}\prod_{n = m}^{r}\prob(A_{n}^{\complement}) = \lim_{m \to \infty}\prod_{n = m}^{\infty}(1-\prob(A_{n})),\\
                \tag{$1-x \leq e^{-x}$ if $x \geq 0$}
                &\leq \lim_{m \to \infty}\prod_{n = m}^{\infty}e^{-\prob(A_{n})} = \lim_{m \to \infty}\exp\left(-\sum_{n = m}^{\infty}\prob(A_{n})\right) = 0,\\
                \prob(A_{n}\text{ i.o.}) &= \prob\left(\bigcap_{m = 1}^{\infty}\bigcup_{n = m}^{\infty}A_{n}\right) = 1-\prob\left(\bigcup_{m = 1}^{\infty}\bigcap_{n = m}^{\infty}A_{n}^{\complement}\right) = 1.
            \end{align*}
        \end{enumerate}
    \end{proofing}
    \begin{rem}
        BCII can be considered a partial converse of BCI.
    \end{rem}
    \begin{rem}
        "i.o." stands for "infinitely often," while "f.o." stands for "finitely often."
    \end{rem}
    We will now explore how the Borel-Cantelli Lemmas can be applied in various scenarios.
    \begin{eg}\named{Infinite Monkey Problem}
        Suppose there are $N$ letters in the alphabet. A monkey is hitting keys on a typewriter at random, with each letter being equally likely to be any one of the $N$ letters, independent of previous hits. Given a specific string $S$ of length $m$, such as "monkey," we want to determine the probability that the monkey will type the string $S$ infinitely many times.\\
        Let $E_{k}$ be the event that the monkey types the string $S$ starting from the $k$-th letter. Then, $E_{1},E_{2},\cdots$ are not independent since if the monkey types $S$ starting from the $k$-th letter, it will continue to type $S$ for the next $m-1$ letters. However, $E_{1},E_{m+1},E_{2m+1},\cdots$ are independent. Note that $\prob(E_{k}) = N^{-m}$ for all $k$.\\
        Since $\sum_{k = 0}^{\infty}\prob(E_{mk+1}) = \sum_{k = 0}^{\infty}N^{-m} = \infty$, by BCII:
        \begin{equation*}
            \prob(E_{mk+1}\text{ i.o.}) = 1.
        \end{equation*}
        Therefore, the monkey will type the string $S$ infinitely many times with probability 1.
    \end{eg}
    \newpage

    Recall that if $X_{n} \xrightarrow{\prob} X$, there exists a non-random increasing sequence of integers $n_{1},n_{2},\cdots$ such that $X_{n_{i}} \xrightarrow{\text{a.s.}} X$ as $i \to \infty$.\\
    We now present a theorem that provides a necessary and sufficient condition for convergence in probability.
    \begin{thm}
        $X_{n} \xrightarrow{\prob} X$ if and only if, for all subsequences $X_{n(m)}$, there exists a further subsequence:
        \begin{equation*}
            X_{n(m_{k})} \xrightarrow{\text{a.s.}} X.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        \begin{figure}[h!]
            \begin{subfigure}[b]{0.05\textwidth}
                ($\Longrightarrow$)
            \end{subfigure}
            \begin{subfigure}[t]{0.9\textwidth}
                For all $\varepsilon > 0$, let $\varepsilon_{k} = \min\{\varepsilon,1/k\}$. Since $X_{n} \xrightarrow{\prob} X$, for all $k$, there exists $n(m_{k})$ such that:
                \begin{equation*}
                    \prob(\abs{X_{n(m_{k})}-X} > \varepsilon_{k}) < \frac{1}{2^{k}}.
                \end{equation*}
                Since $\sum_{k = 1}^{\infty}\prob(\abs{X_{n(m_{k})}-X} > \varepsilon_{k}) < \infty$, by BCI:
                \begin{align*}
                    \prob(\abs{X_{n(m_{k})}-X} > \varepsilon_{k}\text{ i.o.}) &= 0, & \prob(\abs{X_{n(m_{k})}-X} > \varepsilon_{k}\text{ f.o.}) &= 1.
                \end{align*}
                For all $\varepsilon > 0$, there exists $k$ such that $\varepsilon_{k} \leq \varepsilon$. We have:
                \begin{equation*}
                    \{\abs{X_{n(m_{k})}-X} > \varepsilon\}\subseteq\{\abs{X_{n(m_{k})}-X} > \varepsilon_{k}\}.
                \end{equation*}
                Therefore,
                \begin{equation*}
                    \prob(\abs{X_{n(m_{k})}-X} > \varepsilon\text{ i.o.}) = 0 \implies \prob(\abs{X_{n(m_{k})}-X} \leq \varepsilon\text{ f.o.}) = 1.
                \end{equation*}
            \end{subfigure}
        \end{figure}
        \begin{figure}[h!]
            \begin{subfigure}[b]{0.05\textwidth}
                ($\Longleftarrow$)
            \end{subfigure}
            \begin{subfigure}[t]{0.9\textwidth}
                Let $a_{n} = \prob(\abs{X_{n}-X} > \varepsilon)$ for some $\varepsilon > 0$. Assume that $a_{n} \not\to 0$.\\
                There exists a subsequence $a_{n(m)}$ such that $a_{n(m)} \to a > 0$.\\
                This implies that for some $\delta > 0$ and all $m$ sufficiently large, $a_{n(m)} > \delta$ and $\prob(\abs{X_{n(m)}-X} > \varepsilon) > \delta$.\\
                Therefore, there does not exist a further subsequence $X_{n(m_{k})}$ such that $X_{n(m_{k})} \xrightarrow{\text{a.s.}} X$. This contradicts our assumption. Hence, $a_{n} \to 0$. Note that:
                \begin{equation*}
                    a_{n} = \prob(\abs{X_{n}-X} > \varepsilon) \leq 1.
                \end{equation*}
                Since $a_{n} \to 0$, we have $X_{n} \xrightarrow{\prob} X$.
            \end{subfigure}
        \end{figure}
    \end{proofing}
    We now present a theorem with conditions similar to the Law of Large Numbers. However, note that $\expect\abs{X_{1}} = \infty$ in this case.
    \begin{thm}
        If $X_{1},X_{2},\cdots$ are i.i.d. random variables with $\expect\abs{X_{i}} = \infty$, then:
        \begin{align*}
            \prob(\abs{X_{n}} \geq n\text{ i.o.}) &= 1, & \prob\left(\lim_{n \to \infty}\frac{1}{n}\sum_{i = 1}^{n}X_{i}\text{ exists in }(-\infty,\infty)\right) &= 0.
        \end{align*}
    \end{thm}
    \begin{proofing}
        \begin{equation*}
            \expect\abs{X_{1}} = \int_{0}^{\infty}\prob(\abs{X_{1}} > t)\,dt \leq \sum_{n = 0}^{\infty}\prob(\abs{X_{1}} > n).
        \end{equation*}
        Since $\expect\abs{X_{1}} = \infty$, we have $\sum_{n = 1}^{\infty}\prob(\abs{X_{n}} \geq n) = \infty$. By BCII, $\prob(\abs{X_{n}} \geq n\text{ i.o.}) = 1$.\\
        Let $Y_{n} = \sum_{i = 1}^{n}X_{i}$ and $C = \{\omega \in \Omega:\frac{Y_{n}(\omega)}{n}\text{ exists in }(-\infty,\infty)\}$. Assume that $\prob(C) > 0$.\\
        For all $\omega \in C$, we have:
        \begin{equation*}
            \frac{Y_{n}(\omega)}{n}-\frac{Y_{n+1}(\omega)}{n+1} = \frac{Y_{n}(\omega)}{n(n+1)}-\frac{X_{n+1}(\omega)}{n+1}.
        \end{equation*}
        Since $\frac{Y_{n}}{n}$ converges, we have $\frac{Y_{n}(\omega)}{n(n+1)} \to 0$. Therefore, $\frac{X_{n+1}(\omega)}{n+1} \to 0$, or equivalently, $X_{n+1}(\omega) = o(n+1)$.\\
        This implies that there exists $N(\omega)$ such that for all $n \geq N(\omega)$, $\abs{X_{n}(\omega)} < n$. Therefore, $\omega \not \in \{\abs{X_{n}} \geq n\text{ i.o.}\}$.\\
        We have shown that $C\subseteq\{\abs{X_{n}} < n\text{ f.o.}\}$. Since $\prob(\abs{X_{n}} \geq n\text{ i.o.}) = 1$, we have $\prob(C) = 0$.
    \end{proofing}
    \newpage

    We now present a theorem that weakens the independence condition in BCII to pairwise independence.
    \begin{thm}
        If $A_{1},A_{2},\cdots$ are pairwise independent and $\sum_{n = 1}^{\infty}\prob(A_{n}) = \infty$, then as $n \to \infty$:
        \begin{equation*}
            \frac{\sum_{m = 1}^{n}\mathbf{1}_{A_{m}}}{\sum_{m = 1}^{n}\prob(A_{m})} \xrightarrow{\text{a.s.}} 1.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Let $Y_{n} = \sum_{m = 1}^{n}\mathbf{1}_{A_{m}}$. Note that $\expect{Y_{n}} = \sum_{m = 1}^{n}\prob(A_{m}) \to \infty$ as $n \to \infty$.\\
        Also, $\Var(Y_{n}) = \sum_{m = 1}^{n}\Var(\mathbf{1}_{A_{m}}) = \sum_{m = 1}^{n}\prob(A_{m})(1-\prob(A_{m})) \leq \sum_{m = 1}^{n}\prob(A_{m})$.\\
        By Markov's inequality, for all $\varepsilon > 0$:
        \begin{equation*}
            \prob\left(\abs{\frac{Y_{n}-\expect{Y_{n}}}{\expect{Y_{n}}}} > \varepsilon\right) \leq \frac{\Var(Y_{n})}{\varepsilon^{2}(\expect{Y_{n}})^{2}} \leq \frac{1}{\varepsilon^{2}\expect{Y_{n}}} \to 0.
        \end{equation*}
        Therefore, we get that $\frac{Y_{n}-\expect{Y_{n}}}{\expect{Y_{n}}} \xrightarrow{\prob} 0$.\\
        Let $n_{k} = \min\{n:\expect{Y_{n}} \geq k^{2}\}$. Note that $n_{k}$ is an increasing sequence of integers.\\
        Since $\expect{Y_{n}} \to \infty$ as $n \to \infty$, we have $n_{k} \to \infty$ as $k \to \infty$. Also, $\expect{Y_{n_{k}}} \geq k^{2}$ and $\expect{Y_{n_{k}-1}} < k^{2}$.\\
        By Markov's inequality, for all $\varepsilon > 0$:
        \begin{equation*}
            \sum_{k = 1}^{\infty}\prob\left(\abs{\frac{Y_{n_{k}}-\expect{Y_{n_{k}}}}{\expect{Y_{n_{k}}}}} > \varepsilon\right) \leq \sum_{k = 1}^{\infty}\frac{1}{\varepsilon^{2}\expect{Y_{n_{k}}}} \leq \sum_{k = 1}^{\infty}\frac{1}{\varepsilon^{2}k^{2}} < \infty.
        \end{equation*}
        By BCI, we have:
        \begin{align*}
            \prob\left(\abs{\frac{Y_{n_{k}}-\expect{Y_{n_{k}}}}{\expect{Y_{n_{k}}}}} > \varepsilon\text{ i.o.}\right) &= 0, & \prob\left(\abs{\frac{Y_{n_{k}}-\expect{Y_{n_{k}}}}{\expect{Y_{n_{k}}}}} \leq \varepsilon\text{ f.o.}\right) &= 1.
        \end{align*}
        Let $C = \left\{\omega \in \Omega:\frac{Y_{n_{k}}(\omega)}{\expect{Y_{n_{k}}}} \to 1\text{ as }k \to \infty\right\}$. Then, $\prob(C) = 1$. For any $\omega \in C$ and $n_{k} \leq n < n_{k+1}$, we have:
        \begin{equation*}
            \frac{Y_{n_{k}}(\omega)}{\expect{Y_{n_{k}}}}\cdot\frac{\expect{Y_{n_{k}}}}{\expect{Y_{n_{k}+1}}} \leq \frac{Y_{n}(\omega)}{\expect{Y_{n}}} \leq \frac{Y_{n_{k+1}}(\omega)}{\expect{Y_{n_{k+1}}}}\cdot\frac{\expect{Y_{n_{k+1}}}}{\expect{Y_{n_{k}+1}}}.
        \end{equation*}
        Since $\frac{\expect{Y_{n_{k}}}}{\expect{Y_{n_{k}+1}}} \to 1$ and $\frac{\expect{Y_{n_{k+1}}}}{\expect{Y_{n_{k}+1}}} \to 1$ as $k \to \infty$, we have:
        \begin{equation*}
            \frac{Y_{n}(\omega)}{\expect{Y_{n}}} \to 1\text{ as }n \to \infty.
        \end{equation*}
        Therefore, $C\subseteq\left\{\omega \in \Omega:\frac{Y_{n}(\omega)}{\expect{Y_{n}}} \to 1\text{ as }n \to \infty\right\}$. We have:
        \begin{equation*}
            \prob\left(\frac{Y_{n}}{\expect{Y_{n}}} \to 1\text{ as }n \to \infty\right) = 1 \Longleftrightarrow \frac{\sum_{m = 1}^{n}\mathbf{1}_{A_{m}}}{\sum_{m = 1}^{n}\prob(A_{m})} \xrightarrow{\text{a.s.}} 1.
        \end{equation*}
    \end{proofing}
    If the events $A_{1}, A_{2},\cdots$ in the Borel-Cantelli Lemmas are independent, then $\prob(A)$ is either $0$ or $1$ depending on whether $\sum\prob(A_{n})$ converges. We now present a more general version of this result.
    \begin{thm}\named{Borel Zero-One Law}
        Let $A_{1},A_{2},\cdots \in \mathcal{F}$ and $\mathcal{A} = \sigma(A_{1},A_{2},\cdots)$. Suppose that:
        \begin{enumerate}
            \item $A \in \mathcal{A}$.
            \item $A$ is independent of any finite collection of $A_{1},A_{2},\cdots$.
        \end{enumerate}
        Then $\prob(A) = 0$ or $1$.
    \end{thm}
    \begin{proofing}[Proof (Non-rigorous)]
        Suppose that $A_{1},A_{2},\cdots$ are independent. Let $A = \limsup_{n}A_{n} = \bigcap_{m = 1}^{\infty}\bigcup_{n = m}^{\infty}A_{n}$.\\
        Let $B_{m} = \bigcup_{n = m}^{\infty}A_{n}$.  Since $B_{1}\supseteq B_{2}\supseteq\cdots$, we have $A = \lim_{m \to \infty}B_{m}$.\\
        For all $k$, $B_{k} \in \sigma(A_{k},A_{k+1},\cdots)$, which is independent of $\sigma(A_{1},A_{2},\cdots,A_{k-1})$.\\
        Therefore, $B_{k}$ is independent of any $A_{i} \in \sigma(A_{1},A_{2},\cdots,A_{k-1})$.\\
        Setting $k \to \infty$, we have that $B_{k} \to A$ is independent of all elements in $\mathcal{A}$, which also includes itself.\\
        Therefore, $\prob(A) = \prob(A\cap A) = \prob(A)\prob(A)$, which implies that $\prob(A) = 0$ or $1$.
    \end{proofing}
    \newpage

    We now explore more about sequences of random variables. Let $X_{1},X_{2},\cdots$ be independent random variables.\\
    For any subcollection $X_{i_{1}},X_{i_{2}},\cdots,X_{i_{k}}$, we can write $\sigma(X_{i_{1}},X_{i_{2}},\cdots,X_{i_{k}})$ as the smallest $\sigma$-field with respect to which each $X_{i_{j}}$ is measurable.
    \begin{defn}
        Let $\mathcal{H}_{n} = \sigma(X_{n+1},X_{n+2},\cdots)$. We have $\mathcal{H}_{n}\supseteq\mathcal{H}_{n+1}\supseteq\cdots$. The \textbf{tail $\sigma$-field} is defined as:
        \begin{equation*}
            \mathcal{H}_{\infty} = \bigcap_{n}\mathcal{H}_{n}.
        \end{equation*}
        An event $E \in \mathcal{F}$ is called a \textbf{tail event} if $E \in \mathcal{H}_{\infty}$.
    \end{defn}
    \begin{rem}
        $E \in \mathcal{H}_{\infty}$ means that $E$ is independent of any finite collection of $X_{i}$'s.
    \end{rem}
    \begin{eg}
        $\{\limsup_{n \to \infty}X_{n} = \infty\}$ and $\{\liminf_{n \to \infty}X_{n} = -\infty\}$ are tail events.
    \end{eg}
    \begin{eg}
        $\{\sum_{n}X_{n}\text{ converges}\}$ is a tail event.
    \end{eg}
    \begin{eg}
        $\{\sum_{n}X_{n}\text{ converges to }1\}$ is not a tail event.
    \end{eg}
    We now present Kolmogorov's Zero-One Law, which is a special case of the Borel Zero-One Law.
    \begin{thm}\named{Kolmogorov's Zero-One Law}
        If $H \in \mathcal{H}_{\infty}$, then $\prob(H) = 0$ or $1$.
    \end{thm}
    \begin{proofing}
        Since $H \in \mathcal{H}_{\infty}$, $H$ is independent of any finite collection of $X_{i}$'s. By the Borel Zero-One Law, $\prob(H) = 0$ or $1$.
    \end{proofing}
    We now define a tail function.
    \begin{defn}
        We define a \textbf{tail function} to be $Y:\Omega \to \mathbb{R}\cup\{-\infty,\infty\}$, such that $Y$ is a measurable function with respect to the tail $\sigma$-field $\mathcal{H}_{\infty}$. In other words, for all $y \in \mathbb{R}\cup\{-\infty,\infty\}$, $\{Y \leq y\} \in \mathcal{H}_{\infty}$.
    \end{defn}
    \begin{rem}
        $Y$ is a function of the "tail" of the sequence $X_{1},X_{2},\cdots$.
    \end{rem}
    \begin{rem}
        $Y$ is a tail function if and only if $Y$ is independent of any finite collection of $X_{i}$'s.
    \end{rem}
    \begin{eg}
        Let $Y = \limsup_{n \to \infty}X_{n}$. Then, $Y$ is a tail function.\\
        For any $y \in \mathbb{R}$:
        \begin{equation*}
            \{Y \leq y\} = \bigcup_{m = 1}^{\infty}\bigcap_{n = m}^{\infty}\{X_{n} \leq y\} \in \mathcal{H}_{\infty}.
        \end{equation*}
        Therefore, $\{Y \leq y\}$ is a tail event.
    \end{eg}
    \begin{thm}
        \label{Chapter 8 (Theorem) Probability of Tail Function is 0 or 1 in a Single Value}
        If $Y$ is a tail function of independent random variables $X_{1},X_{2},\cdots$, then there exists $-\infty \leq k \leq \infty$ such that:
        \begin{equation*}
            \prob(Y = k) = 1.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        For all $y \in \mathbb{R}\cup\{-\infty,\infty\}$, $\{Y \leq y\} \in \mathcal{H}_{\infty}$. By Kolmogorov's Zero-One Law, $\prob(Y \leq y) = 0$ or $1$.\\
        Let $k = \inf\{y:\prob(Y \leq y) = 1\}$. We have:
        \begin{align*}
            \prob(Y < k) &= \prob\left(\bigcup_{m = 1}^{\infty}\left\{Y \leq k-\frac{1}{m}\right\}\right) = \lim_{m \to \infty}\prob\left(Y \leq k-\frac{1}{m}\right) = 0, & \prob(Y > k) &= 1-\prob(Y \leq k) = 0.
        \end{align*}
        Therefore, $\prob(Y = k) = 1$.
    \end{proofing}
    \newpage

    We now apply the above theorem to the sequence of independent and identically distributed (i.i.d.) random variables $X_{1},X_{2},\cdots$. Recall that if $X_{1},X_{2},\cdots$ are i.i.d. with $\expect\abs{X_{1}} < \infty$, then:
    \begin{equation*}
        \prob\left(\lim_{n \to \infty}\frac{1}{n}\sum_{i = 1}^{n}X_{i} = \expect{X_{1}}\right) = 1.
    \end{equation*}
    If $\expect\abs{X_{1}} = \infty$, then:
    \begin{equation*}
        \prob\left(\lim_{n \to \infty}\frac{1}{n}\sum_{i = 1}^{n}X_{i}\text{ exists}\right) = 0.
    \end{equation*}
    We now present a theorem that does not require the random variables to be identically distributed.
    \begin{thm}
        If $X_{1},X_{2},\cdots$ are independent random variables, then:
        \begin{equation*}
            \prob\left(\lim_{n \to \infty}\frac{1}{n}\sum_{i = 1}^{n}X_{i}\text{ exists}\right) = 0\text{ or }1.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Let:
        \begin{align*}
            Z_{1}(\omega) &= \limsup_{n \to \infty}\frac{1}{n}\sum_{i = 1}^{n}X_{i}(\omega), & Z_{2}(\omega) &= \liminf_{n \to \infty}\frac{1}{n}\sum_{i = 1}^{n}X_{i}(\omega).
        \end{align*}
        Note that $\lim_{n \to \infty}\frac{1}{n}\sum_{i = 1}^{n}X_{i}(\omega)$ exists if and only if $Z_{1}(\omega) = Z_{2}(\omega)$. For all $k$, we have:
        \begin{align*}
            Z_{1}(\omega) &= \limsup_{n \to \infty}\frac{1}{n}\left(\sum_{i = 1}^{k}X_{i}(\omega)+\sum_{i = k+1}^{n}X_{i}(\omega)\right) = \limsup_{n \to \infty}\frac{1}{n}\sum_{i = k+1}^{n}X_{i}(\omega),\\
            Z_{2}(\omega) &= \liminf_{n \to \infty}\frac{1}{n}\left(\sum_{i = 1}^{k}X_{i}(\omega)+\sum_{i = k+1}^{n}X_{i}(\omega)\right) = \liminf_{n \to \infty}\frac{1}{n}\sum_{i = k+1}^{n}X_{i}(\omega).
        \end{align*}
        Therefore, $Z_{1}$ and $Z_{2}$ do not depend on any finite collection of $X_{i}$'s.\\
        This implies that $Z_{1}$ and $Z_{2}$ are tail functions of $X_{i}$'s. By Theorem \ref{Chapter 8 (Theorem) Probability of Tail Function is 0 or 1 in a Single Value}, there exist $-\infty \leq c_{1},c_{2} \leq \infty$ such that:
        \begin{align*}
            \prob(Z_{1} = c_{1}) &= 1, & \prob(Z_{2} = c_{2}) &= 1.
        \end{align*}
        Therefore:
        \begin{equation*}
            \prob\left(\lim_{n \to \infty}\frac{1}{n}\sum_{i = 1}^{n}X_{i}\text{ exists}\right) = \prob(Z_{1} = Z_{2}) = \begin{cases}
                1, & c_{1} = c_{2},\\
                0, & c_{1} \neq c_{2}.
            \end{cases}
        \end{equation*}
    \end{proofing}
    \newpage

    \begin{eg}\named{Random power series}
        Let $X_{1},X_{2},\cdots$ be i.i.d. exponential random variables with parameter $\lambda = 1$. 
        Consider the random power series:
        \begin{equation*}
            \sum_{n = 1}^{\infty}X_{n}(\omega)z^{n}.
        \end{equation*}
        For each $\omega \in \Omega$, the radius of convergence is given by:
        \begin{equation*}
            R(\omega) = \frac{1}{\limsup_{n \to \infty}\abs{X_{n}(\omega)}^{\frac{1}{n}}}.
        \end{equation*}
        Note that $R$ is a tail function of $X_{1},X_{2},\cdots$. By Theorem \ref{Chapter 8 (Theorem) Probability of Tail Function is 0 or 1 in a Single Value}, there exists $C$ such that $\prob(R = C) = 1$ ($R = C$ almost surely). We will show that $C = 1$. Note that:
        \begin{equation*}
            \prob\left(\limsup_{n \to \infty}\abs{X_{n}}^{\frac{1}{n}} = \frac{1}{C}\right) = 1.
        \end{equation*}
        For all $\varepsilon > 0$, we have:
        \begin{align*}
            \prob\left(\limsup_{n \to \infty}\abs{X_{n}}^{\frac{1}{n}} \leq 1+\varepsilon\right) &= 1, & \prob\left(\limsup_{n \to \infty}\abs{X_{n}}^{\frac{1}{n}} \geq  1-\varepsilon\right) &= 1.
        \end{align*}
        We first prove the first equation. Note that:
        \begin{equation*}
            \sum_{n = 1}^{\infty}\prob\left(\abs{X_{n}}^{\frac{1}{n}} > 1+\varepsilon\right) = \sum_{n = 1}^{\infty}\prob(\abs{X_{n}} > (1+\varepsilon)^{n}) = \sum_{n = 1}^{\infty}e^{-(1+\varepsilon)^{n}} < \infty.
        \end{equation*}
        Therefore, by BCI:
        \begin{equation*}
            \prob(\abs{X_{n}}^{\frac{1}{n}} > 1+\varepsilon\text{ i.o.}) = 0 \implies \prob\left(\limsup_{n \to \infty}\abs{X_{n}}^{\frac{1}{n}} \leq 1+\varepsilon\right) = 1.
        \end{equation*}
        We now prove the second equation. Note that:
        \begin{equation*}
            \sum_{n = 1}^{\infty}\prob\left(\abs{X_{n}}^{\frac{1}{n}} > 1-\varepsilon\right) = \sum_{n = 1}^{\infty}\prob(\abs{X_{n}} > (1-\varepsilon)^{n}) = \sum_{n = 1}^{\infty}e^{-(1-\varepsilon)^{n}} = \infty.
        \end{equation*}
        Therefore, by BCII:
        \begin{equation*}
            \prob(\abs{X_{n}}^{\frac{1}{n}} > 1-\varepsilon\text{ i.o.}) = 1 \implies \prob\left(\limsup_{n \to \infty}\abs{X_{n}}^{\frac{1}{n}} \geq 1-\varepsilon\right) = 1.
        \end{equation*}
        Since the above two equations hold for all $\varepsilon > 0$, we have:
        \begin{equation*}
            \prob\left(\limsup_{n \to \infty}\abs{X_{n}}^{\frac{1}{n}} = 1\right) = 1.
        \end{equation*}
        Therefore, $C = 1$.
    \end{eg}
    \newpage

\section{Strong Law of Large Numbers}
    We now present the Law of Large Numbers. The Weak Law of Large Numbers (WLLN) states that the sample average converges in probability to the expected value. Let $X_{1},X_{2},\cdots$ be a sequence of i.i.d. random variables with $\expect(X_{1}) = \mu$.
    As $n \to \infty$:
    \begin{align*}
        \frac{1}{n}\sum_{i = 1}^{n}X_{i}& \xrightarrow{D} \mu, & \frac{1}{n}\sum_{i = 1}^{n}X_{i} &\xrightarrow{\prob} \mu.
    \end{align*}
    By its name, the Weak Law of Large Numbers (WLLN) indeed has a stronger version, called the Strong Law of Large Numbers. We prove one of the versions of SLLN.
    \begin{thm}\named{Strong Law of Large Numbers} [SLLN]
        Let $X_{1},X_{2},\cdots$ be i.i.d. random variables with $\expect{X_{1}} = \mu$ and $\expect\abs{X_{1}} < \infty$. We have:
        \begin{equation*}
            \frac{1}{n}\sum_{i = 1}^{n}X_{i} \xrightarrow{\text{a.s.}} \mu.
        \end{equation*}
    \end{thm}
    Note that the proof for SLLN is more complicated than that for WLLN. We will not prove this version of SLLN here. Instead, we will prove two simpler versions of SLLN with stronger conditions. We first prove SLLN with the fourth moment condition.
    \begin{thm}\named{SLLN with $\expect{X_{i}^{4}} < \infty$}
        Let $X_{1},X_{2},\cdots$ be i.i.d. random variables with $\expect{X_{1}} = 0$ and $\expect{X_{1}^{4}} < \infty$. We have:
        \begin{equation*}
            \frac{1}{n}\sum_{i = 1}^{n}X_{i} \xrightarrow{\text{a.s.}} 0.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Let $Y_{n} = \sum_{i = 1}^{n}X_{i}$. We will use the fourth moment condition to show that $\frac{Y_{n}}{n} \xrightarrow{\text{a.s.}} 0$. Note that:
        \begin{equation*}
            \expect{Y_{n}^{4}} = \sum_{i,j,k,\ell = 1}^{n}\expect(X_{i}X_{j}X_{k}X_{\ell}).
        \end{equation*}
        Since $X_{i}$ are i.i.d. with $\expect{X_{i}} = 0$, if any of $i,j,k,\ell$ appears only once, then $\expect(X_{i}X_{j}X_{k}X_{\ell}) = 0$.\\
        Therefore, the only non-zero terms in the above summation are those with pairs of indices. There are two cases:
        \begin{enumerate}
            \item $i = j,k = \ell$ or $i = k,j = \ell$ or $i = \ell,j = k$ with $i \neq k$. There are $3n(n-1)$ such terms, each equal to $\expect{X_{1}^{2}}\expect{X_{1}^{2}}$.
            \item $i = j = k = \ell$. There are $n$ such terms, each equal to $\expect{X_{1}^{4}}$.
        \end{enumerate}
        Therefore,
        \begin{equation*}
            \expect{Y_{n}^{4}} = 3n(n-1)(\expect{X_{1}^{2}})^{2}+n\expect{X_{1}^{4}} = O(n^{2}).
        \end{equation*}
        By Markov's inequality, for all $\varepsilon > 0$:
        \begin{align*}
            \prob\left(\abs{\frac{1}{n}\sum_{i = 1}^{n}X_{i}} \geq \varepsilon\right) &\leq \frac{1}{n^{4}\varepsilon^{4}}\expect\left(\sum_{i = 1}^{n}X_{i}\right)^{4} = O\left(\frac{1}{n^{2}}\right).
        \end{align*}
        Therefore, for all $\varepsilon > 0$:
        \begin{equation*}
            \sum_{n = 1}^{\infty}\prob\left(\abs{\frac{1}{n}\sum_{i = 1}^{n}X_{i}} \geq \varepsilon\right) < \infty.
        \end{equation*}
        By BCI, for all $\varepsilon > 0$:
        \begin{equation*}
            \prob\left(\abs{\frac{1}{n}\sum_{i = 1}^{n}X_{i}} \geq \varepsilon\text{ i.o.}\right) = 0.
        \end{equation*}
        Therefore, 
        \begin{equation*}
            \frac{1}{n}\sum_{i = 1}^{n}X_{i} \xrightarrow{\text{a.s.}} 0.
        \end{equation*}
    \end{proofing}
    \newpage

    \begin{thm}\named{SLLN with $\expect{X_{1}^{2}} < \infty$} Let $X_{1},X_{2},\cdots$ be i.i.d. random variables with $\expect{X_{1}^{2}} < \infty$ and $\expect{X_{i}} = \mu$. We have:
        \begin{align*}
            \frac{1}{n}\sum_{i = 1}^{n}X_{i} &\xrightarrow{2} \mu, & \frac{1}{n}\sum_{i = 1}^{n}X_{i} &\xrightarrow{\text{a.s.}} \mu.
        \end{align*}
    \end{thm}
    \begin{proofing}
        Let $Y_{n} = \sum_{i = 1}^{n}X_{i}$. We first prove the convergence in mean square. Note that:
        \begin{equation*}
            \expect\left(\frac{Y_{n}}{n}-\mu\right)^{2} = \frac{\expect(Y_{n}-n\mu)^{2}}{n^{2}} = \frac{\Var(Y_{n})}{n^{2}} = \frac{\Var(X_{1})}{n}.
        \end{equation*}
        Therefore, as $n \to \infty$:
        \begin{equation*}
            \frac{Y_{n}}{n} \xrightarrow{2} \mu.
        \end{equation*}
        We now prove the almost sure convergence. 
        Since convergence in mean square implies convergence in probability, by Theorem \ref{Chapter 8 (Theorem) Probability convergence implies almost sure convergence for a subsequence}, there exists a subsequence $n_{1} < n_{2} < \cdots$ such that $\frac{Y_{n_{i}}}{n_{i}} \xrightarrow{\text{a.s.}} \mu$.\\
        For simplicity, we choose $n_{i} = i^{2}$. By Markov's inequality, for all $\varepsilon > 0$:
        \begin{equation*}
            \sum_{i = 1}^{\infty}\prob\left(\abs{\frac{Y_{i^{2}}}{i^{2}}-\mu} > \varepsilon\right) \leq \sum_{i = 1}^{\infty}\frac{\Var(X_{1})}{i^{2}\varepsilon^{2}} < \infty.
        \end{equation*}
        By BCI, for all $\varepsilon > 0$:
        \begin{equation*}
            \prob\left(\abs{\frac{Y_{i^{2}}}{i^{2}}-\mu} > \varepsilon\text{ i.o.}\right) = 0.
        \end{equation*}
        Therefore, $\frac{Y_{i^{2}}}{i^{2}} \xrightarrow{\text{a.s.}} \mu$. We need to show that $\frac{Y_{n}}{n} \xrightarrow{\text{a.s.}} \mu$. Note that for any $n$, there exists $i$ such that $i^{2} \leq n \leq (i+1)^{2}$, and thus when $X_{i}$ are non-negative:
        \begin{equation*}
            \frac{Y_{i^{2}}}{(i+1)^{2}} \leq \frac{Y_{n}}{n} \leq \frac{Y_{(i+1)^{2}}}{i^{2}}.
        \end{equation*}
        Since $\frac{Y_{i^{2}}}{i^{2}} \xrightarrow{\text{a.s.}} \mu$, we have:
        \begin{align*}
            \frac{Y_{i^{2}}}{(i+1)^{2}} = \frac{Y_{i^{2}}}{i^{2}}\cdot\frac{i^{2}}{(i+1)^{2}} &\xrightarrow{\text{a.s.}} \mu, & \frac{Y_{(i+1)^{2}}}{i^{2}} = \frac{Y_{(i+1)^{2}}}{(i+1)^{2}}\cdot\frac{(i+1)^{2}}{i^{2}} &\xrightarrow{\text{a.s.}} \mu.
        \end{align*}
        By the Squeeze Theorem, we conclude that whenever $X_{i}$ are non-negative, as $n \to \infty$:
        \begin{equation*}
            \frac{Y_{n}}{n} \xrightarrow{\text{a.s.}} \mu.
        \end{equation*}
        For general $X_{i}$, we can write $X_{n} = X_{n}^{+}-X_{n}^{-}$ where:
        \begin{align*}
            X_{n}^{+}(\omega) &= \max\{X_{n}(\omega),0\}, & X_{n}^{-}(\omega) &= -\min\{X_{n}(\omega),0\}.
        \end{align*}
        Both $X_{n}^{+}$ and $X_{n}^{-}$ are non-negative random variables.\\
        Furthermore, $X_{1}^{+},X_{2}^{+},\cdots$ are i.i.d. with $\expect{X_{1}^{+}} < \infty$, and $X_{1}^{-},X_{2}^{-},\cdots$ are i.i.d. with $\expect{X_{1}^{-}} < \infty$. Note that:
        \begin{equation*}
            \expect(X_{1}^{+})^{2} \leq \expect(X_{1}^{+})^{2}+2\expect{X_{1}^{+}}\expect{X_{1}^{-}}+\expect(X_{1}^{-})^{2} = \expect(X_{1}^{+}-X_{1}^{-})^{2} = \expect{X_{1}^{2}} < \infty.
        \end{equation*}
        Similarly, $\expect(X_{1}^{-})^{2} < \infty$. By the above result for non-negative random variables, we have:
        \begin{equation*}
            \frac{1}{n}\sum_{i = 1}^{n}X_{i}^{+} \xrightarrow{\text{a.s.}} \expect{X_{1}^{+}},\quad \frac{1}{n}\sum_{i = 1}^{n}X_{i}^{-} \xrightarrow{\text{a.s.}} \expect{X_{1}^{-}}.
        \end{equation*}
        Therefore, as $n \to \infty$:
        \begin{equation*}
            \frac{1}{n}\sum_{i = 1}^{n}X_{i} = \frac{1}{n}\sum_{i = 1}^{n}X_{i}^{+}-\frac{1}{n}\sum_{i = 1}^{n}X_{i}^{-} \xrightarrow{\text{a.s.}} \expect{X_{1}^{+}}-\expect{X_{1}^{-}} = \expect{X_{1}} = \mu.
        \end{equation*}
    \end{proofing}
    \newpage

    Why do we need SLLN? There are many applications that specifically require SLLN.
    \begin{eg}\named{Renewal Theory} 
        Consider a bulb that has a random lifetime. When the bulb burns out, it is immediately replaced by a new identical bulb with another random lifetime. This process continues indefinitely.\\
        Let $X_{1},X_{2},\cdots$ be the lifetimes of the bulbs. Let $T_{n} = X_{1}+X_{2}+\cdots+X_{n}$ be the time that the $n$-th bulb burns out. Let $N_{t} = \max\{n:T_{n} \leq t\}$ be the number of bulbs that have burned out by time $t$. Note that
        \begin{equation*}
            T_{N_{t}} \leq t < T_{N_{t}+1}.
        \end{equation*}
        We want to find the average lifetime of the bulbs, which is given by $\frac{t}{N_{t}}$.\\
        We assume that $X_{1},X_{2},\cdots$ are i.i.d. random variables with $0 < X_{i} < \infty$ and $\expect{X_{1}} < \infty$. Let $\expect{X_{1}} = \mu$. 
        As $t \to \infty$:
        \begin{equation*}
            \frac{t}{N_{t}} \xrightarrow{\text{a.s.}} \mu.
        \end{equation*}
    \end{eg}
    \begin{proofing}
        Since $X_{i} > 0$, $T_{n}$ is an increasing sequence. Therefore, $N_{t}$ is also an increasing sequence. Note that:
        \begin{equation*}
            \frac{T_{N_{t}}}{N_{t}} \leq \frac{t}{N_{t}} < \frac{T_{N_{t}+1}}{N_{t}+1}\cdot\frac{N_{t}+1}{N_{t}}.
        \end{equation*}
        By SLLN, as $n \to \infty$:
        \begin{equation*}
            \frac{T_{n}}{n} = \frac{X_{1}+X_{2}+\cdots+X_{n}}{n} \xrightarrow{\text{a.s.}} \mu.
        \end{equation*} 
        Therefore, as $t \to \infty$:
        \begin{align*}
            \frac{T_{N_{t}}}{N_{t}} &\xrightarrow{\text{a.s.}} \mu, & \frac{T_{N_{t}+1}}{N_{t}+1} &\xrightarrow{\text{a.s.}} \mu.
        \end{align*}
        Since $T_{N_{t}} \leq t < T_{N_{t}+1}$, as $t \to \infty$, $N_{t} \to \infty$ almost surely. Therefore, $\frac{N_{t}+1}{N_{t}} \xrightarrow{\text{a.s.}} 1$.\\
        By the Squeeze Theorem, as $t \to \infty$:
        \begin{equation*}
            \frac{t}{N_{t}} \xrightarrow{\text{a.s.}} \mu.
        \end{equation*}
    \end{proofing}
    \begin{rem}
        If $X_{n} \xrightarrow{\prob} X_{\infty}$, then $N_{m} \xrightarrow{\text{a.s.}} \infty$ as $m \to \infty$.
        To prove this, it is not necessary that $X_{N_{m}} \xrightarrow{\text{a.s.}} X_{\infty}$ or $X_{N_{m}} \xrightarrow{\prob} X_{\infty}$.
    \end{rem}
    \begin{eg}
        Recall the Example \ref{Chapter 8 (Example) Convergence in probability does not imply almost sure convergence}. Let $\Omega = [0,1]$. For each $m \geq 1$, we have a partition of $\Omega$ into $m$ subintervals:
        \begin{equation*}
            Y_{m,k} = \mathbf{1}_{I_{m,k}} = \begin{cases}
                1, &\omega \in \left[\frac{k-1}{m},\frac{k}{m}\right],\\
                0, &\text{Otherwise}.
            \end{cases}
        \end{equation*}
        Note that for each fixed $m$, $Y_{m,1},Y_{m,2},\cdots,Y_{m,m}$ are mutually exclusive and exhaustive.\\
        We define a sequence of random variables $X_{1},X_{2},\cdots$ by arranging $Y_{m,k}$ in a triangular array, i.e.:
        \begin{align*}
            X_{1} &= Y_{1,1},\\
            X_{2} &= Y_{2,1}, & X_{3} &= Y_{2,2},\\
            X_{4} &= Y_{3,1}, & X_{5} &= Y_{3,2}, & X_{6} &= Y_{3,3},\\
            &\vdots & &\vdots & &\vdots
        \end{align*}
        We have that for each fixed $m$, $Y_{m,1},Y_{m,2},\cdots,Y_{m,m}$ are mutually independent.\\
        Therefore, $X_{1},X_{2},\cdots$ are independent. Based on the example, let $X_{\infty} = 0$, we have that $X_{n} \xrightarrow{\prob} 0$.\\
        For each $\omega \in [0,1]$, there exists a unique $k_{m}(\omega)$ such that $Y_{m,k_{m}(\omega)}(\omega) = 1$. Let $N_{m}(\omega) = \frac{(m-1)m}{2}+k_{m}(\omega)$.\\
        Therefore, $N_{m} \xrightarrow{\text{a.s.}} \infty$ as $m \to \infty$. However, for all $\omega \in [0,1]$ and $m \geq 1$, $X_{N_{m}(\omega)}(\omega) = 1$.\\
        Therefore, $X_{N_{m}} \not\rightarrow X_{\infty}$ almost surely or in probability.
    \end{eg}
    \newpage

    We now present another important application of SLLN, the Glivenko-Cantelli Theorem. This theorem states that the empirical distribution function converges uniformly to the true distribution function almost surely. It is also called the Fundamental Theorem of Statistics.
    \begin{thm}\named{Glivenko-Cantelli Theorem}
        Let $X \sim F$, where $F$ is an unknown distribution function.
        Let $X_{1},X_{2},\cdots,X_{N}$ be i.i.d. random variables with the same distribution as $X$.\\
        Let $F_{N}(x)$ be the empirical distribution function based on $X_{1},X_{2},\cdots,X_{N}$, which is also a distribution function of a histogram with $N$ samples.
        \begin{align*}
            F_{N}(x) &= \frac{1}{N}\sum_{i = 1}^{N}\mathbf{1}_{X_{i} \leq x}, & F(x) &= \prob(X \leq x).
        \end{align*}
        We have:
        \begin{equation*}
            \sup_{x}\abs{F_{N}(x)-F(x)} \xrightarrow{\text{a.s.}} 0.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        We only prove the case when $F(x)$ is continuous.\\
        For any $m \geq 1$, there exist $x_{0} = -\infty < x_{1} < x_{2} < \cdots < x_{m} = +\infty$ such that for all $1 \leq i \leq m$:
        \begin{equation*}
            F(x_{i})-F(x_{i-1}) = \frac{1}{m}.
        \end{equation*}
        Note that such $x_{i}$ exist because $F(x)$ is continuous and non-decreasing with $0 \leq F(x) \leq 1$.\\
        For any $x \in \mathbb{R}$, there exists $i$ such that $x_{i-1} \leq x \leq x_{i}$. Since $F_{N}(x)$ and $F(x)$ are non-decreasing, we have:
        \begin{align*}
            F_{N}(x)-F(x) &\leq F_{N}(x_{i})-F(x_{i-1}) = F_{N}(x_{i})-F(x_{i})+\frac{1}{m},\\
            F_{N}(x)-F(x) &\geq F_{N}(x_{i-1})-F(x_{i}) = F_{N}(x_{i-1})-F(x_{i-1})-\frac{1}{m}.
        \end{align*}
        Therefore,
        \begin{equation*}
            \abs{F_{N}(x)-F(x)} \leq \sup_{i}\abs{F_{N}(x_{i})-F(x_{i})}+\frac{1}{m} \implies \sup_{x}\abs{F_{N}(x)-F(x)} \leq \sup_{i}\abs{F_{N}(x_{i})-F(x_{i})}+\frac{1}{m}.
        \end{equation*}
        By SLLN, for each fixed $x$:
        \begin{align*}
            F_{N}(x;\omega) &= \frac{1}{N}\sum_{i = 1}^{N}\mathbf{1}_{X_{i}(\omega) \leq x} \xrightarrow{\text{a.s.}} F(x), & \prob(F_{N}(x) \to F(x)\text{ as }N \to \infty) &= 1.
        \end{align*}
        Let $C_{x} = \{\omega \in \Omega:F_{N}(x;\omega) \to F(x)\text{ as }N \to \infty\}$.
        For each fixed $m$, if $\omega \in \bigcap_{i = 1}^{m}C_{x_{i}}$:
        \begin{align*}
            \sup_{i}\abs{F_{N}(x_{i};\omega)-F(x_{i})} &\to 0\text{ as }N \to \infty,\\
            \limsup_{N}\sup_{x}\abs{F_{N}(x;\omega)-F(x)} &\leq \frac{1}{m}.
        \end{align*}
        Since the above holds for all $m \geq 1$:
        \begin{equation*}
            \limsup_{N}\sup_{x}\abs{F_{N}(x;\omega)-F(x)} = 0 \implies \sup_{x}\abs{F_{N}(x;\omega)-F(x)} \to 0\text{ as }N \to \infty.
        \end{equation*}
        Note that $\prob(C_{x_{i}}) = 1$ for all $1 \leq i \leq m$. Therefore:
        \begin{equation*}
            \prob\left(\bigcap_{i = 1}^{m}C_{x_{i}}\right) = 1 \implies \prob\left(\sup_{x}\abs{F_{N}(x)-F(x)} \to 0\text{ as }N \to \infty\right) = 1.
        \end{equation*}
    \end{proofing}
    Of course, there are still many examples that we haven't explored (including some mentioned during the lectures that I am too lazy to include here). We also skipped many proofs in some of the theorems. It is up to you to explore further, either in other courses or in the future world of mathematics.

\appendix
\renewcommand{\thechapter}{\Alph{chapter}}

\chapter{Random Walk}
    \label{Appendix A (Random Walk)}
    \begin{eg}\named{Simple Random Walk}
        A particle starts at position $a \in \mathbb{Z}$. At each time step, it moves one unit to the right with probability $p$ and one unit to the left with probability $q = 1-p$. The moves are independent of each other. Let $Y_{n}$ be the position of the particle at time $n$. We have:
        \begin{equation*}
            Y_{n} = a+\sum_{i = 1}^{n}X_{i},
        \end{equation*}
        where $X_{i}$ are i.i.d. random variables with:
        \begin{equation*}
            \prob(X_{i} = 1) = p, \quad \prob(X_{i} = -1) = q.
        \end{equation*}
        Such a process $\{Y_{n},n \geq 0\}$ is called a \textbf{simple random walk} on $\mathbb{Z}$. If $p = q = \frac{1}{2}$, the random walk is called a \textbf{symmetric simple random walk}.
    \end{eg}
    \begin{lem}
        \label{Appendix A (Lemma) Simple random walk properties}
        The simple random walk $\{Y_{n},n \geq 0\}$ has the following properties:
        \begin{enumerate}
            \item It is \textbf{spatially homogeneous}: $\prob(Y_{n} = j|Y_{0} = a) = \prob(Y_{n} = j+b|Y_{0} = a+b)$.
            \item It is \textbf{temporally homogeneous}: $\prob(Y_{n} = j|Y_{0} = a) = \prob(Y_{m+n} = j|Y_{m} = a)$, $m,n \geq 0$.
            \item It has the \textbf{Markov property}: $\prob(Y_{m+n} = j|Y_{m} = a,Y_{m-1} = a_{m-1},\cdots,Y_{0} = a_{0}) = \prob(Y_{m+n} = j|Y_{m} = a)$.
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item 
            \begin{equation*}
                \prob(Y_{n} = j|Y_{0} = a) = \prob\left(\sum_{i = 1}^{n}X_{i} = j-a\right) = \prob\left(\sum_{i = 1}^{n}X_{i} = j+b-(a+b)\right) = \prob(Y_{n} = j+b|Y_{0} = a+b).
            \end{equation*}
            \item 
            \begin{equation*}
                \prob(Y_{n} = j|Y_{0} = a) = \prob\left(\sum_{i = 1}^{n}X_{i} = j-a\right) = \prob\left(\sum_{i = m+1}^{m+n}X_{i} = j-a\right) = \prob(Y_{m+n} = j|Y_{m} = a).
            \end{equation*}
            \item If we know $Y_{m}$, then the distribution of $Y_{m+n}$ depends only on $X_{m+1},X_{m+2},\cdots,X_{m+n}$, and $Y_{0},Y_{1},\cdots,Y_{m-1}$ do not influence the dependency.
            Therefore,
            \begin{equation*}
                \prob(Y_{m+n} = j|Y_{m} = a,Y_{m-1} = a_{m-1},\cdots,Y_{0} = a_{0}) = \prob(Y_{m+n} = j|Y_{m} = a).
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    \newpage

    \begin{eg}\named{Sample Path Counting}
        \label{Appendix A (Example) Sample Path Counting}
        A sample path of the random walk is a sequence $(s_{0},s_{1},\cdots,s_{n})$ with $s_{0} = a$ and $\abs{s_{i+1}-s_{i}} = 1$ for all $0 \leq i \leq n-1$. The number of such paths is $2^{n}$.\\
        Each path corresponds to a unique outcome in the sample space.\\
        Let $M_{n}^{r}(a, b)$ be the number of paths from $(0,a)$ to $(n,b)$ with $r$ rightward steps and $\ell = n-r$ leftward steps. We have:
        \begin{equation*}
            M_{n}^{r}(a, b) = \begin{cases}
                \binom{n}{r}, & r-\ell = b-a,\\
                0, & \text{Otherwise}.
            \end{cases}
        \end{equation*}
        Since $r+\ell = n$ and $r-\ell = b-a$, we have $r = \frac{1}{2}(n+b-a)$ and $\ell = \frac{1}{2}(n-b+a)$.\\
        Therefore, if $n+b-a$ is even and $\abs{b-a} \leq n$, then:
        \begin{equation*}
            \prob(Y_{n} = b) = M_{n}^{\frac{1}{2}(n+b-a)}(a, b)p^{\frac{1}{2}(n+b-a)}q^{\frac{1}{2}(n-b+a)} = \binom{n}{\frac{1}{2}(n+b-a)}p^{\frac{1}{2}(n+b-a)}q^{\frac{1}{2}(n-b+a)}.
        \end{equation*}
        Otherwise, $\prob(Y_{n} = b) = 0$.
    \end{eg}
    \begin{thm}\named{Reflection Principle}
        Let $N_{n}(a, b)$ be the number of paths from $(0,a)$ to $(n,b)$, and let $N_{n}^{0}(a, b)$ be the number of such paths that intersect the $x$-axis. We have:
        \begin{equation*}
            N_{n}^{0}(a, b) = N_{n}(-a,b).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Let $k$ be the first time that the path intersects the $x$-axis, i.e. $k = \min\{i:s_{i} = 0\}$.\\
        We reflect the path from time $k$ onwards about the $x$-axis. The new path starts from $(0,-a)$ and ends at $(n,b)$.\\
        Conversely, given a path from $(0,-a)$ to $(n,b)$, we can find the first time that the path intersects the $x$-axis and reflect the path from that time onwards about the $x$-axis to obtain a path from $(0,a)$ to $(n,b)$ that intersects the $x$-axis. Therefore, there is a one-to-one correspondence between paths from $(0,a)$ to $(n,b)$ that intersect the $x$-axis and paths from $(0,-a)$ to $(n,b)$. Hence, $N_{n}^{0}(a, b) = N_{n}(-a,b)$.
    \end{proofing}
    \begin{lem}
        \label{Appendix A (Lemma) Number of paths calculation}
        The number of paths from $(0,a)$ to $(n,b)$ is given by:
        \begin{equation*}
            N_{n}(a, b) = \binom{n}{\frac{1}{2}(n+b-a)}.
        \end{equation*}
    \end{lem}
    \begin{proofing}
        Let $\alpha$ be the number of positive steps and $\beta$ be the number of negative steps. We have $\alpha+\beta = n$ and $\alpha-\beta = b-a$.\\
        Therefore, $\alpha = \frac{1}{2}(n+b-a)$ and $\beta = \frac{1}{2}(n-b+a)$.\\
        The number of paths from $(0,a)$ to $(n,b)$ is the number of ways to choose $\alpha$ positive steps from $n$ steps. Therefore:
        \begin{equation*}
            N_{n}(a, b) = \binom{n}{\alpha} = \binom{n}{\frac{1}{2}(n+b-a)}.
        \end{equation*}
    \end{proofing}
    \begin{eg}
        Let $Y_{0},Y_{1},\cdots,Y_{n}$ be a simple random walk. We want to know the probability that the path does not revisit the starting point in the time interval $[1,n]$.\\
        Without loss of generality, we assume that $Y_{0} = 0$. We want to find $\prob(Y_{1}Y_{2}\cdots Y_{n} \neq 0,Y_{n} = b)$.\\
        Note that if $n+b$ is odd or $\abs{b} > n$, then $\prob(Y_{1}Y_{2}\cdots Y_{n} \neq 0,Y_{n} = b) = 0$. Assume that $n+b$ is even and $\abs{b} \leq n$.\\
        Let $N_{n}(0,b)$ be the number of paths from $(0,0)$ to $(n,b)$, and let $N_{n}^{0}(0,b)$ be the number of such paths that intersect the $x$-axis. If $b > 0$, the first step must be $(1,1)$. By the Reflection Principle and Lemma \ref{Appendix A (Lemma) Number of paths calculation}:
        \begin{align*}
            N_{n-1}(1,b)-N_{n-1}^{0}(1,b) &= N_{n-1}(1,b)-N_{n-1}(-1,b)\\
            &= \binom{n-1}{\frac{1}{2}(n-1+b-1)}-\binom{n-1}{\frac{1}{2}(n-1+b+1)}\\
            &= \binom{n-1}{\frac{1}{2}(n+b)-1}-\binom{n-1}{\frac{1}{2}(n+b)} = \frac{b}{n}\binom{n}{\frac{1}{2}(n+b)}.
        \end{align*}
        Therefore, by Lemma \ref{Appendix A (Lemma) Number of paths calculation}:
        \begin{equation*}
            \prob(Y_{1}Y_{2}\cdots Y_{n} \neq 0,Y_{n} = b) = \frac{N_{n-1}(1,b)-N_{n-1}^{0}(1,b)}{N_{n}(0,b)}\prob(Y_{n} = b) = \frac{b}{n}\prob(Y_{n} = b).
        \end{equation*}
    \end{eg}
    \newpage

    \begin{eg}
        Let $Y_{0},Y_{1},\cdots,Y_{n}$ be a simple random walk. We want to find the distribution of the maximum position reached by the random walk in the time interval $[0,n]$:
        \begin{equation*}
            M_{n} = \max\{Y_{0},Y_{1},\cdots,Y_{n}\}.
        \end{equation*}
        Suppose that $Y_{0} = 0$ so that $M_{n} \geq 0$. If $b \geq r$, then $M_{n} \geq r$ whenever $Y_{n} = b$. Therefore, for $b \geq r$:
        \begin{equation*}
            \prob(M_{n} \geq r,Y_{n} = b) = \prob(Y_{n} = b).
        \end{equation*}
        It follows that for $r \geq 0$:
        \begin{equation*}
            \prob(M_{n} \geq r) = \sum_{b = r}^{\infty}\prob(Y_{n} = b).
        \end{equation*}
    \end{eg}
    \begin{thm}
        Suppose that $Y_{0} = 0$. For $r \geq 1$ and $b < r$:
        \begin{equation*}
            \prob(M_{n} \geq r,Y_{n} = b) = \left(\frac{q}{p}\right)^{r-b}\prob(Y_{n} = 2r-b).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Let $N_{n}^{r}(0,b)$ be the number of paths from $(0,0)$ to $(n,b)$ that intersect the line $y = r$. 
        For a path $\pi$, let $(i_{\pi},r)$ be the earliest intersection point of $\pi$ and the line $y = r$. We reflect the path $\pi$ from time $i_{\pi}$ onwards about the line $y = r$. The new path starts from $(0,0)$ and ends at $(n,2r-b)$.\\
        Conversely, given a path from $(0,0)$ to $(n,2r-b)$, we can find the earliest intersection point of the path and the line $y = r$ and reflect the path from that time onwards about the line $y = r$ to obtain a path from $(0,0)$ to $(n,b)$ that intersects the line $y = r$. 
        Therefore, there is a one-to-one correspondence between paths from $(0,0)$ to $(n,b)$ that intersect the line $y = r$ and paths from $(0,0)$ to $(n,2r-b)$. Hence, $N_{n}^{r}(0,b) = N_{n}(0,2r-b)$.\\
        By Example \ref{Appendix A (Example) Sample Path Counting} and Lemma \ref{Appendix A (Lemma) Number of paths calculation}:
        \begin{align*}
            \prob(M_{n} \geq r,Y_{n} = b) &= \frac{N_{n}^{r}(0,b)}{N_{n}(0,b)}\prob(Y_{n} = b) = \frac{N_{n}(0,2r-b)}{N_{n}(0,b)}\prob(Y_{n} = b)\\
            &= \frac{\binom{n}{\frac{1}{2}(n+2r-b)}}{\binom{n}{\frac{1}{2}(n+b)}}\binom{n}{\frac{1}{2}(n+b)}p^{\frac{1}{2}(n+b)}q^{\frac{1}{2}(n-b)} = \binom{n}{\frac{1}{2}(n+2r-b)}p^{\frac{1}{2}(n+b)}q^{\frac{1}{2}(n-b)}\\
            &= \left(\frac{q}{p}\right)^{r-b}\binom{n}{\frac{1}{2}(n+2r-b)}p^{\frac{1}{2}(n+2r-b)}q^{\frac{1}{2}(n-2r+b)} = \left(\frac{q}{p}\right)^{r-b}\prob(Y_{n} = 2r-b).
        \end{align*}
    \end{proofing}
    \begin{rem}
        Combining the two cases, for $r \geq 1$:
        \begin{equation*}
            \prob(M_{n} \geq r) = \sum_{b = r}^{\infty}\prob(Y_{n} = b)+\sum_{b = -\infty}^{r-1}\left(\frac{q}{p}\right)^{r-b}\prob(Y_{n} = 2r-b).
        \end{equation*}
        If $p = q = \frac{1}{2}$, then for $r \geq 1$:
        \begin{equation*}
            \prob(M_{n} \geq r) = \sum_{b = r}^{\infty}\prob(Y_{n} = b)+\sum_{b = -\infty}^{r-1}\prob(Y_{n} = 2r-b) = \sum_{b = r}^{\infty}\prob(Y_{n} = b)+\sum_{c = r+1}^{\infty}\prob(Y_{n} = c) = 2\sum_{b = r}^{\infty}\prob(Y_{n} = b).
        \end{equation*}
    \end{rem}


\chapter{Terminologies in Other Fields of Mathematics}
    \begin{defn}
        The \textbf{supremum} of a subset $S$ is the lowest upper bound $x$ such that for all $a \in S$, $x \geq a$. It is written as:
        \begin{equation*}
            x = \sup S.
        \end{equation*}
    \end{defn}
    \begin{defn}
        The \textbf{infimum} of a subset $S$ is the highest lower bound $x$ such that for all $b \in S$, $x \leq b$. It is written as:
        \begin{equation*}
            x = \inf S.
        \end{equation*}
    \end{defn}
    \begin{defn}
        The \textbf{limit superior} and \textbf{limit inferior} of a sequence $x_{1},x_{2},\cdots$ are defined as:
        \begin{align*}
            \limsup_{n \to \infty}x_{n} &= \lim_{n \to \infty}\sup_{m \geq n}x_{m}, & \liminf_{n \to \infty}x_{n} &=  \lim_{n \to \infty}\inf_{m \geq n}x_{m}.
        \end{align*}
    \end{defn}
    \begin{defn}
        An infinite series $\sum_{n = 0}^{\infty}a_{n}$ is \textbf{absolutely convergent} if, for some real number $L$:
        \begin{equation*}
            \sum_{n = 0}^{\infty}\abs{a_{n}} = L.
        \end{equation*}
        Any grouping or rearranging of an absolutely convergent infinite series does not change the result of the series.\\
        An infinite series is \textbf{conditionally convergent} if it converges but does not satisfy the condition for absolute convergence.
    \end{defn}
    \begin{defn}\named{Monotonicity}
        A \textbf{monotonic} function is a function that is either entirely non-increasing or entirely non-decreasing. A \textbf{strictly monotonic} function is a function that is either entirely strictly increasing or entirely strictly decreasing.
    \end{defn}
    \begin{defn}
        The \textbf{arguments of the maxima} are the input points at which a function's output is maximized. It is defined as:
        \begin{equation*}
            \argmax_{x \in S}f(x) = \{x \in S:f(x) \geq f(s)\text{ for all }s \in S\}.
        \end{equation*}
    \end{defn}
    \begin{defn}
        The \textbf{arguments of the minima} are the input points at which a function's output is minimized. It is defined as:
        \begin{equation*}
            \argmin_{x \in S}f(x) = \{x \in S:f(x) \leq f(s)\text{ for all }s \in S\}.
        \end{equation*}
    \end{defn}
    \begin{defn}\named{Linearity}
        A \textbf{linear} function is a function $f$ that satisfies the following two properties:
        \begin{enumerate}
            \item $f(x+y) = f(x)+f(y)$.
            \item $f(ax) = af(x)$ for all $a$.
        \end{enumerate}
    \end{defn}
    \newpage

    \begin{defn}
        A \textbf{regular} function is a function $f$ that is:
        \begin{enumerate}
            \item Single-valued (any value in the domain maps to exactly one value).
            \item Analytic ($f$ can be written as a convergent power series).
        \end{enumerate}
    \end{defn}
    \begin{defn}
        Let $V$ be the space of all real functions on $[0,1]$. A mapping $\norm{\cdot}:V \to \mathbb{R}$ is a \textbf{norm} of a function $f$ if:
        \begin{enumerate}
            \item $\norm{f} \geq 0$ for all $f \in V$.
            \item If $\norm{f} = 0$, then $f = 0$.
            \item $\norm{af} = \abs{a}\norm{f}$ for all $f \in V$ and $a \in \mathbb{R}$.
            \item (Triangle inequality) $\norm{f+g} \leq \norm{f}+\norm{g}$ for all $f,g \in V$.
        \end{enumerate}
        The $L_{p}$ norm for $p \geq 1$ is defined as:
        \begin{equation*}
            \norm{f}_{p} = \left(\int_{0}^{1}\abs{f(x)}^{p}\,dx\right)^{\frac{1}{p}}.
        \end{equation*}
        The \textbf{infinity norm} of a function $f \in V$ is defined as:
        \begin{equation*}
            \norm{f}_{\infty} = \max_{0 \leq x \leq 1}\abs{f(x)}.
        \end{equation*}
    \end{defn}
    \begin{defn}
        Let $f$ and $g$ be real-valued functions.
        \begin{enumerate}
            \item We write $f(x) = O(g(x))$ if and only if:
            \begin{equation*}
                \limsup_{x \to \infty}\frac{f(x)}{g(x)} < \infty.
            \end{equation*}
            This is called \textbf{big O notation}.
            \item We write $f(x) = o(g(x))$ if and only if:
            \begin{equation*}
                \lim_{x \to \infty}\frac{f(x)}{g(x)} = 0.
            \end{equation*}
            This is called \textbf{small o notation}.
            \item We write $f(x) = \Omega(g(x))$ if and only if:
            \begin{equation*}
                \liminf_{x \to \infty}\frac{f(x)}{g(x)} > 0.
            \end{equation*}
            This is called \textbf{big Omega notation}.
            \item We write $f(x) = \omega(g(x))$ if and only if:
            \begin{equation*}
                \lim_{x \to \infty}\frac{f(x)}{g(x)} = \infty.
            \end{equation*}
            This is called \textbf{small omega notation}.
            \item Functions $f$ and $g$ are \textbf{asymptotically equivalent} ($f \sim g$) if and only if:
            \begin{equation*}
                \lim_{x \to \infty}\frac{f(x)}{g(x)} = 1.
            \end{equation*}
            \item If $f(x) = O(g(x))$ and $g(x) = O(f(x))$, then we write:
            \begin{equation*}
                f(x)\asymp g(x).
            \end{equation*}
        \end{enumerate}
    \end{defn}

\chapter{Some Useful Inequalities}
    \begin{thm}\named{Triangle Inequality}
        Let $X$ and $Y$ be random variables. Then:
        \begin{equation*}
            \abs{X+Y} \leq \abs{X}+\abs{Y}.
        \end{equation*}
    \end{thm}
    \begin{thm}\named{Reverse Triangle Inequality}
        Let $X$ and $Y$ be random variables. Then:
        \begin{equation*}
            \abs{X-Y} \geq \abs{\abs{X}-\abs{Y}}.
        \end{equation*}
    \end{thm}
    \begin{thm}\named{Cauchy-Schwarz Inequality}
        Let $X$ and $Y$ be random variables. Then:
        \begin{equation*}
            \abs{\expect(XY)}^{2} \leq \expect(X^{2})\expect(Y^{2}).
        \end{equation*}
    \end{thm}
    \begin{thm}\named{Covariance Inequality}
        Let $X$ and $Y$ be random variables. Then:
        \begin{equation*}
            \abs{\cov(X,Y)}^{2} \leq \Var(X)\Var(Y).
        \end{equation*}
    \end{thm}
    \begin{thm}\named{Markov's Inequality}
        Let $X$ be a random variable with a finite mean. Then, for all $k > 0$ and any non-negative function $\gamma$ that is increasing on $[0,\infty)$:
        \begin{equation*}
            \prob(\abs{X} \geq k) \leq \frac{\expect(\gamma(\abs{X}))}{\gamma(k)}.
        \end{equation*}
    \end{thm}
    \begin{thm}\named{Chebyshev's Inequality}
        Let $X$ be a random variable with $\expect{X} = \mu$ and $\Var(X) = \sigma^{2}$. Then, for all $k > 0$:
        \begin{equation*}
            \prob(\abs{X-\mu} \geq k\sigma) \leq \frac{1}{k^{2}}.
        \end{equation*}
    \end{thm}
    \begin{thm}\named{H\"older's Inequality}
        Let $X$ and $Y$ be random variables. For any $p > 1$, let $q = \frac{p}{p-1}$. Then:
        \begin{equation*}
            \expect\abs{XY} \leq (\expect\abs{X}^{p})^{\frac{1}{p}}(\expect\abs{Y}^{q})^{\frac{1}{q}}.
        \end{equation*}
    \end{thm}
    \begin{thm}\named{Lyapunov's Inequality}
        Let $X$ be a random variable. For all $0 < s \leq r$:
        \begin{equation*}
            (\expect\abs{X}^{s})^{\frac{1}{s}} \leq (\expect\abs{X}^{r})^{\frac{1}{r}}.
        \end{equation*}
    \end{thm}
    \begin{thm}\named{Minkowski Inequality}
        Let $X$ and $Y$ be random variables. For any $r \geq 1$:
        \begin{equation*}
            (\expect\abs{X+Y}^{r})^{\frac{1}{r}} \leq (\expect\abs{X}^{r})^{\frac{1}{r}}+(\expect\abs{Y}^{r})^{\frac{1}{r}}.
        \end{equation*}
    \end{thm}
    \begin{thm}\named{Jensen's Inequality}
        Let $X$ be a random variable and $\gamma$ be a convex function. Then:
        \begin{equation*}
            \gamma(\expect{X}) \leq \expect(\gamma(X)).
        \end{equation*}
    \end{thm}
    For better memorization:\
    Triangle inequality $\implies$ Reverse triangle inequality.\\
    Markov's inequality $\implies$ Chebyshev's inequality.\\
    H\"older's inequality $\implies$ Cauchy-Schwarz inequality $\implies$ Covariance inequality.

\addcontentsline{toc}{chapter}{Change Log}
\chapter*{Change Log}
\begin{description}[style = nextline]
	\item[1.0]
	Create the notes
	\item[1.1]
	Add the definition "Parametric distribution" in Chapter 5.5 "Examples of continuous random variables"
	\item[1.2]
	Add the Student's t-distribution and the properties of chi-squared distribution\\
	Add theorems that relate sample mean and sample variance with distributions\\
	Create a new Chapter 7.8 "Sampling"\\
	Add a remark in De Moivre-Laplace Limit Theorem\\
	Fix some typos
	\item[1.3]
	Modify the wording of some theorems\\
	Add the definition of random sample and combine it with sample mean and sample variance\\
	Add the definitions related to asymptotic notations\\
	Change how Change Log is produced
	\item[1.4] Add a Lemma linking Gamma distribution and Chi-squared distribution\\
	Add Slutsky's Theorem\\
	Add multivariate normal distribution\\
	Modify the appearance of random vectors
	\item[2.0]
	Combining all expectation related topic into a separate chapter\\
	Greatly modify the ordering of topics\\
	Add MGF of some of the distribution taught\\
	Add a theorem that allows estimating population variance from population mean\\
	Finish the proof of a theorem that allows estimating population variance from sample variance\\
	Add a theorem that correlates uncorrelated bivariate normal and independent normal\\
	Add a lemma regarding the properties of covariance\\
	Add definition of conditional variance and Law of total variance\\
	Modify notations for regular convergence
	\item[2.1]
	Use the updated document template\\
	Create a new Chapter 3.5 "Marginal Distribution of Random Variables"\\
	Add more examples and remarks\\
	Reorder items for better logical flow\\
	Rewrite definitions, theorems and lemmas\\
	Fix typos with the help of AI\\
\end{description}

\end{document}