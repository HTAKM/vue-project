\documentclass{huhtakm-template-book}
\usepackage{tikz}
\DeclareMathOperator{\adj}{adj}
\title{Vectors and Matrix (Elementary)}
\author{
	HU-HTAKM\\
	Website: \url{https://htakm.github.io/htakm_test/}
	}
\date{
	Last major change: August 29, 2024\\
	Last small update (fixed typo): August 29, 2024
}
\begin{document}
\maketitle
This is just all the knowledge about vectors and matrices that you may have learnt in high school mathematics.
\tableofcontents
\chapter{Introduction of vectors}
In daily life, many quantities are described by \textbf{magnitude} alone, such as length, time, and temperature. These are called \textbf{scalars}.\\
However, some quantities are described by both \textbf{magnitude} and \textbf{direction}. For example, the acceleration of an object.\\
For simplicity, we visualize vectors in two dimensions.
\begin{defn}
    A \textbf{vector} is represented by a directed line segment. Arrowhead represents the direction, while the length represents the magnitude. The vector is denoted by $\overrightarrow{AB}$ ($A$ to $B$), $\mathbf{a}$, $\vec{a}$.\\
    Magnitude (length) of a vector $\mathbf{a}$ is denoted by $\abs{\vec{a}}$.\\
    We say that two vectors $\vec{a}$ and $\vec{b}$ are equal if they have the same magnitude and direction.
\end{defn}
In 2D, it is represented as the following:
\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \draw[thin,black!20] (0,0) grid (5,5);
        \draw[<->] (0,0) -- (5,0) node[right]{$x$};
        \draw[<->] (0,0) -- (0,5) node[above]{$y$};
        \draw[line width=1.5pt, blue, -stealth] (1,1) -- node [above, sloped] {$\vec{a}$}(3,2);
        \draw[line width=1.5pt, red, -stealth] (1,4) node[anchor=east]{$A$} -- node[above,sloped]{$\overrightarrow{AB}$}(4,3) node[anchor=west]{$B$};
    \end{tikzpicture}
    \caption{Vector}
    \label{fig:vector}
\end{figure}
\begin{defn}
    \textbf{Negative vector} of a vector $\vec{u}$ is the vector with equal magnitude but opposite direction. It is denoted by $-\vec{u}$.
\end{defn}
\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \draw[line width=1.5pt, blue, -stealth] (0,0) -- node [above, sloped] {$\vec{u}$}(3,2);
        \draw[line width=1.5pt, red, -stealth] (4,2) -- node [above, sloped] {$-\vec{u}$}(1,0);
        \draw[line width=1.5pt, blue, -stealth] (4,0) -- node [above, sloped] {$\overrightarrow{AB}$}(7,2);
        \draw[line width=1.5pt, red, -stealth] (8,2) -- node [below, sloped] {$-\overrightarrow{AB}=\overrightarrow{BA}$}(5,0);
    \end{tikzpicture}
    \caption{A vector and its corresponding negative vector}
    \label{fig:negative vector}
\end{figure}
\begin{defn}
    A \textbf{zero vector} is a vector with zero magnitude and no direction. It is denoted by $\vec{0}$.
\end{defn}
\begin{defn}
    An \textbf{unit vector} is a vector with magnitude 1. It is denoted by $\hat{u}$.
\end{defn}
\begin{lem}
    $\hat{u}=\frac{\vec{u}}{\abs{\vec{u}}}$ is unit vector with the same direction as $\vec{u}$.
\end{lem}
\begin{eg}
    The most common unit vectors are the unit vectors along the axis.\\
    $\hat{i}$ corresponds to the unit vector along the $x$-axis.\\
    $\hat{j}$ corresponds to the unit vector along the $y$-axis.\\
    $\hat{k}$ corresponds to the unit vector along the $z$-axis.
\end{eg}
\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \draw[thin,black!20] (-1,-1) grid (2,2);
        \draw[<->] (-1,0) -- (2,0) node[right]{$x$};
        \draw[<->] (0,-1) -- (0,2) node[above]{$y$};
        \draw[line width=1.5pt, blue, -stealth] (0,0) -- node [above, midway] {$\hat{i}$}(1,0);
        \draw[line width=1.5pt, red, -stealth] (0,0) -- node [left, midway] {$\hat{j}$}(0,1);
    \end{tikzpicture}
    \caption{The unit vectors along the $x$-axis and $y$-axis}
    \label{fig:unit vector}
\end{figure}
\begin{defn}
    The addition of two vectors are also a vector and is defined as below.
\end{defn}
\begin{figure}[h]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \begin{tikzpicture}
            \draw (0,0) node[anchor=north east]{$A$};
            \draw (3,0) node[anchor=north west]{$B$};
            \draw (4,2) node[anchor=south west]{$C$};
            \draw[line width=1.5pt, blue, -stealth] (0,0) -- node [above, midway] {$\overrightarrow{AB}$}(3,0);
            \draw[line width=1.5pt, blue, -stealth] (3,0) -- node [right, midway] {$\overrightarrow{BC}$} (4,2);
            \draw[line width=1.5pt, red, -stealth] (0,0) -- node [above, sloped] {$\overrightarrow{AC}=\overrightarrow{AB}+\overrightarrow{BC}$}(4,2);
        \end{tikzpicture}
        \caption{Triangle Law of Addition}
        \label{fig:triangle addition of vector}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \begin{tikzpicture}
            \draw (0,0) node[anchor=north east]{$A$};
            \draw (3,0) node[anchor=north west]{$B$};
            \draw (1,2) node[anchor=south east]{$C$};
            \draw (4,2) node[anchor=south west]{$D$};
            \draw[line width=1.5pt, blue, -stealth] (0,0) -- node [above, midway]{$\overrightarrow{AB}$}(3,0);
            \draw[line width=1.5pt, blue, -stealth] (0,0) -- node[left, midway]{$\overrightarrow{AC}$} (1,2);
            \draw[line width=1.5pt, red, -stealth] (0,0) -- node[above, sloped]{$\overrightarrow{AD}=\overrightarrow{AB}+\overrightarrow{AC}$} (4,2);
            \draw[dashed] (1,2) -- (4,2) -- (3,0);
        \end{tikzpicture}
        \caption{Parallelogram Law of Addition}
        \label{fig:parallelogram addition of vector}
    \end{subfigure}
\end{figure}
\begin{defn}
    The subtraction of two vectors $\vec{u}$ and $\vec{v}$ is $\vec{u}-\vec{v}=\vec{u}+(-\vec{v})$.
\end{defn}
\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \draw (0,0) node[anchor=north east]{$A$};
        \draw (3,0) node[anchor=north west]{$B$};
        \draw (1,2) node[anchor=south east]{$C$};
        \draw[line width=1.5pt, blue, -stealth] (0,0) -- node [above, midway]{$\overrightarrow{AB}$}(3,0);
        \draw[line width=1.5pt, blue, -stealth] (0,0) -- node [left, midway]{$\overrightarrow{AC}$} (1,2);
        \draw[line width=1.5pt, red, -stealth] (3,0) -- node [above, sloped]{$\overrightarrow{BC}=\overrightarrow{AC}-\overrightarrow{AB}$} (1,2);
    \end{tikzpicture}
    \caption{Subtraction}
    \label{fig:subtraction of vector}
\end{figure}
\begin{defn}
    The scalar multiplication of a vector $\vec{u}$ is defined by:
    \begin{enumerate}
        \item If $\lambda>0$, then $\lambda\vec{u}$ is a vector with magnitude $\lambda\abs{u}$ and same direction as $\vec{u}$.
        \item If $\lambda<0$, then $\lambda\vec{u}$ is a vector with magnitude $-\lambda\abs{u}$ and opposite direction as $\vec{u}$.
        \item If $\lambda=0$, then $\lambda\vec{u}$ is a zero vector.
    \end{enumerate}
\end{defn}
\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \draw[line width=1.5pt, blue, -stealth] (0,0) -- node [above, sloped]{$\vec{u}$} (1,1);
        \draw[line width=1.5pt, red, -stealth] (1,0) -- node [above, sloped]{$2\vec{u}$} (3,2);
        \draw[line width=1.5pt, red, -stealth] (0,1) -- node [above, sloped]{$(-1)\vec{u}$} (-1,0);
    \end{tikzpicture}
    \caption{Scalar multiplication of vector}
    \label{fig:scalar multiplication of vector}
\end{figure}
\newpage
\begin{lem}
    For any vectors $\vec{u}$, $\vec{v}$, $\vec{w}$ and any real number $\lambda$, $\mu$,
    \begin{enumerate}
        \item $\vec{u}+\vec{v}=\vec{v}+\vec{u}$
        \item $\vec{u}+(\vec{v}+\vec{w})=(\vec{u}+\vec{v})+\vec{w}$
        \item $\vec{u}+\vec{0}=\vec{u}$
        \item $\lambda(\mu\vec{a})=(\lambda\mu)\vec{a}$
        \item $\lambda(\vec{u}\pm\vec{v})=\lambda\vec{u}\pm\lambda\vec{v}$
        \item $(\lambda\pm\mu)\vec{u}=\lambda\vec{u}\pm\mu\vec{u}$
    \end{enumerate}
\end{lem}
\begin{thm}
    If $\vec{u}$ and $\vec{v}$ are non-zero and not parallel, then
    \begin{enumerate}
        \item If $\lambda\vec{u}+\mu\vec{v}=\vec{0}$, then $\lambda=\mu=0$
        \item If $\lambda_{1}\vec{u}+\mu_{1}\vec{v}=\lambda_{2}\vec{u}+\mu_{2}\vec{v}$, then $\lambda_{1}=\lambda_{2}$ and $\mu_{1}=\mu_{2}$.
    \end{enumerate}
\end{thm}
\begin{rem}
    Knowing the properties, we can find that we can represent any vectors in 2D space in terms of $\hat{i}$ and $\hat{j}$, and any vectors in 3D space in terms of $\hat{i}$, $\hat{j}$ and $\hat{k}$.
\end{rem}
We have now finished introducing the basics of vectors. We can use the vectors to do all sorts of stuff.
\begin{defn}
    A \textbf{position vector} is a vector that starts at origin $O$.
\end{defn}
\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \draw (0,0) node[anchor=north west]{$O$};
        \draw[thin,black!20] (-3,-3) grid (3,3);
        \draw[<->] (-3,0) -- (3,0) node[right]{$x$};
        \draw[<->] (0,-3) -- (0,3) node[above]{$y$};
        \draw[line width=1.5pt, blue, -stealth] (0,0) -- node [below, sloped]{$\overrightarrow{OA}$}(1,2) node[anchor=south west]{$A$};
        \draw[line width=1.5pt, red, -stealth] (0,0) -- node[below,sloped]{$\overrightarrow{OB}$} (-2,-1) node[anchor=north east]{$B$};
    \end{tikzpicture}
    \caption{Position Vectors}
    \label{fig:position vector}
\end{figure}
\begin{thm}(Section formula)
    If $P$ is a point of $AB$ such that $AP:PB=r:s$, then $\vec{p}=\frac{s\vec{a}+r\vec{b}}{s+r}$ where $\vec{a}$, $\vec{b}$ and $\vec{p}$ are the position vector of $A$, $B$ and $P$ respectively.
\end{thm}
\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \draw (0,2) node[anchor=south east]{$A$};
        \draw (2,2) node[anchor=south]{$P$};
        \draw (3,2) node[anchor=south west]{$B$};
        \draw (1,0) node[anchor=north]{$O$};
        \draw[line width=1.5pt, blue, -stealth] (1,0) -- node[left, midway]{$\vec{a}$}(0,2);
        \draw[line width=1.5pt, blue, -stealth] (1,0) -- node[right, midway]{$\vec{b}$} (3,2);
        \draw[line width=1.5pt, red, -stealth] (1,0) -- node[left, midway] {$\vec{p}$}(2,2);
        \draw (0,2) -- node[above, sloped]{$r$} (2,2) -- node[above, sloped]{$s$} (3,2);
    \end{tikzpicture}
    \caption{Section formula}
    \label{fig:section formula}
\end{figure}

\chapter{Introduction to matrix}
In real life, we may process data using a rectangular array of real numbers.
\begin{defn}
    A \textbf{matrix} is a rectangular array of real numbers arranged in the form:
    \begin{equation*}
        \begin{pmatrix}
            a_{11} & a_{12} & \hdots & a_{1n}\\
            a_{21} & a_{22} & \hdots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{m1} & a_{m2} & \hdots & a_{mn}
        \end{pmatrix}
    \end{equation*}
    The \textbf{dimension} of this matrix is $m\times n$. Numbers inside the matrix are \textbf{elements} of the matrix. It is denoted by $\mathbf{A}$.
\end{defn}
In some advanced mathematics, we use vectors to define matrix as in the form:
\begin{equation*}
    \begin{pmatrix}
        \vert & \vert & & \vert\\
        \vec{a}_{1} & \vec{a}_{2} & \hdots & \vec{a}_{n}\\
        \vert & \vert & & \vert
    \end{pmatrix}
\end{equation*}
where $\vec{a}_{1},\vec{a}_{2},\cdots,\vec{a}_{n}$ are $m$-dimensional vectors.
\begin{defn}
    A \textbf{zero matrix} is a matrix where all the elements are zero.
    \begin{equation*}
        \begin{pmatrix}
            0 & 0 & \hdots & 0\\
            0 & 0 & \hdots & 0\\
            \vdots & \vdots & \ddots & \vdots\\
            0 & 0 & \hdots & 0
        \end{pmatrix}
    \end{equation*}
    A $m\times n$ zero matrix is denoted as $\mathbf{0}_{m\times n}$ or $\mathbf{0}$.
\end{defn}
\begin{defn}
    A \textbf{row matrix} is a matrix with only 1 row. A \textbf{column matrix} is a matrix with only 1 column.
\end{defn}
\begin{eg}
    $\begin{pmatrix}
        1 & 2
    \end{pmatrix}$ is a row matrix.
\end{eg}
\begin{eg}
    $\begin{pmatrix}
        3\\
        4
    \end{pmatrix}$ is a column matrix.
\end{eg}
\begin{defn}
    A \textbf{square matrix} with order $n$ is a matrix with dimension $n\times n$. The elements $a_{11},a_{22},\cdots,a_{nn}$ are the principal diagonal elements.
\end{defn}
\begin{defn}
    A \textbf{diagonal matrix} is a square matrix where elements that are not principal diagonal elements are zero.
\end{defn}
\begin{eg}
    $\begin{pmatrix}
        4 & 0\\
        0 & 1
    \end{pmatrix}$ is a diagonal matrix.
\end{eg}
\newpage
\begin{defn}
    An \textbf{identity matrix} is a diagonal matrix were principal diagonal elements are all 1.
    \begin{equation*}
        \begin{pmatrix}
            1 & 0 & \hdots & 0\\
            0 & 1 & \ddots & \vdots\\
            \vdots & \ddots & \ddots & 0\\
            0 & \hdots & 0 & 1
        \end{pmatrix}
    \end{equation*}
    A $n\times n$ identity matrix is denoted as $\mathbf{I}_{n}$ or $\mathbf{I}$.
\end{defn}
\begin{defn}
    For two $m\times n$ matrices $\mathbf{A}$ with elements $a_{ij}$ and $\mathbf{B}$ with elements $b_{ij}$, addition and subtraction of $\mathbf{A}$ and $\mathbf{B}$ is defined by
    \begin{equation*}
        \mathbf{A}\pm \mathbf{B}=\begin{pmatrix}
            a_{11}\pm b_{11} & a_{12}\pm b_{12} & \hdots & a_{1n}\pm b_{1n}\\
            a_{21}\pm b_{21} & a_{22}\pm b_{22} & \hdots & a_{2n}\pm b_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{m1}\pm b_{m1} & a_{m2}\pm b_{m2} & \hdots & a_{mn}\pm b_{mn}
        \end{pmatrix}
    \end{equation*}
\end{defn}
\begin{defn}
    For a $m\times n$ matrix $\mathbf{A}$ with elements $a_{ij}$ and a scalar $k$, scalar multiplication is defined by:
    \begin{equation*}
        k\mathbf{A}=\begin{pmatrix}
            ka_{11} & ka_{12} & \hdots & ka_{1n}\\
            ka_{21} & ka_{22} & \hdots & ka_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            ka_{m1} & ka_{m2} & \hdots & ka_{mn}
        \end{pmatrix}
    \end{equation*}
\end{defn}
\begin{lem}
    For $m\times n$ matrices $\mathbf{A},\mathbf{B},\mathbf{C}$ and scalars $\lambda,\mu$,
    \begin{enumerate}
        \item $\mathbf{A}+\mathbf{B}=\mathbf{B}+\mathbf{A}$
        \item $(\mathbf{A}+\mathbf{B})+\mathbf{C}=\mathbf{A}+(\mathbf{B}+\mathbf{C})$
        \item $\mathbf{A}+\mathbf{0}=\mathbf{0}+\mathbf{A}=\mathbf{A}$
        \item $\mathbf{A}+(-\mathbf{A})=\mathbf{0}$
        \item $(\lambda\pm\mu)\mathbf{A}=\lambda\mathbf{A}\pm\mu\mathbf{A}$
        \item $\lambda(\mathbf{A}\pm\mathbf{B})=\lambda\mathbf{A}\pm\lambda\mathbf{B}$
        \item $\lambda(\mu\mathbf{A})=(\lambda\mu)\mathbf{A}$
        \item $0\mathbf{A}=\mathbf{0}$
        \item $\lambda\mathbf{0}=\mathbf{0}$
    \end{enumerate}
\end{lem}
There is an important scalar that can be computed from square matrices. It is called determinant.
\begin{defn}
    For any square matrix $\mathbf{A}$, the \textbf{determinant} of $\mathbf{A}$ is a real number that is denoted by $\abs{\mathbf{A}}$ or $\det{\mathbf{A}}$.
    \begin{equation*}
        \abs{\mathbf{A}}=\begin{vmatrix}
            a_{11} & a_{12} & \hdots & a_{1n}\\
            a_{21} & a_{22} & \hdots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{n1} & a_{n2} & \hdots & a_{nn}
        \end{vmatrix}
    \end{equation*}
\end{defn}
How do we calculate determinant?
\begin{defn}
    For a $2\times 2$ matrix,
    \begin{equation*}
        \begin{vmatrix}
        a_{11} & a_{12}\\
        a_{21} & a_{22}
    \end{vmatrix}=a_{11}a_{22}-a_{12}a_{21}
    \end{equation*}
\end{defn}
\newpage
\begin{defn}
    For a $3\times 3$ matrix,
    \begin{equation*}
        \begin{vmatrix}
            a_{11} & a_{12} & a_{13}\\
            a_{21} & a_{22} & a_{23}\\
            a_{31} & a_{32} & a_{33}
        \end{vmatrix}=a_{11}a_{22}a_{33}+a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-a_{13}a_{22}a_{31}-a_{12}a_{21}a_{33}-a_{11}a_{23}a_{32}
    \end{equation*}
\end{defn}
\begin{defn}
    Given a matrix $\mathbf{A}$ with elements $a_{ij}$. \textbf{Minor} of the element $a_{ij}$, denoted by $M_{ij}$, is a determinant of order $(n-1)$ which is obtained by removing the $i$-th row and $j$-th column of $\abs{\mathbf{A}}$.\\
    \textbf{Cofactor} of the element $a_{ij}$, denoted by $A_{ij}$, is defined by $A_{ij}=(-1)^{i+j}M_{ij}$.
\end{defn}
\begin{eg}
    Given a matrix $A=\begin{pmatrix}
        1 & 2 & 3\\
        4 & 5 & 6\\
        7 & 8 & 9
    \end{pmatrix}$.
    \begin{align*}
        M_{12}&=\begin{vmatrix}
            4 & 6\\
            7 & 9
        \end{vmatrix} & A_{12}&=-\begin{vmatrix}
            4 & 6\\
            7 & 9
        \end{vmatrix}\\
        M_{33}&=\begin{vmatrix}
            1 & 2\\
            4 & 5
        \end{vmatrix} & A_{33}&=+\begin{vmatrix}
            1 & 2\\
            4 & 5
        \end{vmatrix}
    \end{align*}
\end{eg}
\begin{lem}
    For any identity matrix $\mathbf{I}$, $\abs{\mathbf{I}}=1$.
\end{lem}
\begin{thm}(Cofactor expansion)
    We can expand the determinant using cofactors along $i$-th row or $j$-th column.
    \begin{equation*}
        \begin{vmatrix}
            a_{11} & a_{12} & \hdots & a_{1n}\\
            a_{21} & a_{22} & \hdots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{n1} & a_{n2} & \hdots & a_{nn}
        \end{vmatrix}=a_{i1}A_{i1}+a_{i2}A_{i2}+\cdots+a_{in}A_{in}=a_{1j}A_{1j}+a_{2j}A_{2j}+\cdots+a_{nj}A_{nj}
    \end{equation*}
\end{thm}
It is sometimes very tedious to calculate determinant directly. Luckily, we can perform row and column operations on the matrix so that we can simplify the process.
\begin{defn}
    By definition, we can perform addition and scalar multiplication to determinant.
    \begin{align*}
        \begin{vmatrix}
            a_{11} & a_{12}+b_{1} & \hdots & a_{1n}\\
            a_{21} & a_{22}+b_{2} & \hdots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{n1} & a_{n2}+b_{n} & \hdots & a_{nn}
        \end{vmatrix}&=\begin{vmatrix}
            a_{11} & a_{12} & \hdots & a_{1n}\\
            a_{21} & a_{22} & \hdots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{n1} & a_{n2} & \hdots & a_{nn}
        \end{vmatrix}+\begin{vmatrix}
            a_{11} & b_{1} & \hdots & a_{1n}\\
            a_{21} & b_{2} & \hdots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{n1} & b_{n} & \hdots & a_{nn}
        \end{vmatrix}\\
        \begin{vmatrix}
            a_{11} & a_{12} & \hdots & a_{1n}\\
            a_{21}+b_{1} & a_{22}+b_{2} & \hdots & a_{2n}+b_{n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{n1} & a_{n2} & \hdots & a_{nn}
        \end{vmatrix}&=\begin{vmatrix}
            a_{11} & a_{12} & \hdots & a_{1n}\\
            a_{21} & a_{22} & \hdots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{n1} & a_{n2} & \hdots & a_{nn}
        \end{vmatrix}+\begin{vmatrix}
            a_{11} & a_{12} & \hdots & a_{1n}\\
            b_{1} & b_{2} & \hdots & b_{n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{n1} & a_{n2} & \hdots & a_{nn}
        \end{vmatrix}\\
        \begin{vmatrix}
            a_{11} & \lambda a_{12} & \hdots & a_{1n}\\
            a_{21} & \lambda a_{22} & \hdots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{n1} & \lambda a_{n2} & \hdots & a_{nn}
        \end{vmatrix}&=\lambda \begin{vmatrix}
            a_{11} & a_{12} & \hdots & a_{1n}\\
            a_{21} & a_{22} & \hdots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{n1} & a_{n2} & \hdots & a_{nn}
        \end{vmatrix}\\
        \begin{vmatrix}
            a_{11} & a_{12} & \hdots & a_{1n}\\
            \mu a_{21} & \mu a_{22} & \hdots & \mu a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{n1} & a_{n2} & \hdots & a_{nn}
        \end{vmatrix}&=\mu \begin{vmatrix}
            a_{11} & a_{12} & \hdots & a_{1n}\\
            a_{21} & a_{22} & \hdots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{n1} & a_{n2} & \hdots & a_{nn}
        \end{vmatrix}
    \end{align*}
    We can also change the order.
    \begin{equation*}
        -\begin{vmatrix}
            a_{12} & a_{11} & \hdots & a_{1n}\\
            a_{22} & a_{21} & \hdots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{n2} & a_{n1} & \hdots & a_{nn}
        \end{vmatrix}=\begin{vmatrix}
            a_{11} & a_{12} & \hdots & a_{1n}\\
            a_{21} & a_{22} & \hdots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{n1} & a_{n2} & \hdots & a_{nn}
        \end{vmatrix}=-\begin{vmatrix}
            a_{21} & a_{22} & \hdots & a_{2n}\\
            a_{11} & a_{12} & \hdots & a_{1n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{n1} & a_{n2} & \hdots & a_{nn}
        \end{vmatrix}
    \end{equation*}
\end{defn}
\newpage
\begin{lem}
    We can add multiples of rows and columns into another rows and columns respectively.
    \begin{equation*}
        \begin{vmatrix}
            a_{11} & a_{12} & \hdots & a_{1n}\\
            a_{21} & a_{22} & \hdots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{n1} & a_{n2} & \hdots & a_{nn}
        \end{vmatrix}=\begin{vmatrix}
            a_{11}-\lambda a_{1j} & a_{12} & \hdots & a_{1n}\\
            a_{21}-\lambda a_{2j} & a_{22} & \hdots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{n1}-\lambda a_{nj} & a_{n2} & \hdots & a_{nn}
        \end{vmatrix}=\begin{vmatrix}
            a_{11}-\mu a_{i1} & a_{12}-\mu a_{i2} & \hdots & a_{1n}-\mu a_{in}\\
            a_{21} & a_{22} & \hdots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{n1} & a_{n2} & \hdots & a_{nn}
        \end{vmatrix}
    \end{equation*}
\end{lem}
\begin{eg}
    \begin{equation*}
        \begin{vmatrix}
            2 & 5 & 10\\
            10 & 3 & 4\\
            2 & 5 & 10
        \end{vmatrix}=\begin{vmatrix}
            2 & 5 & 10\\
            10 & 3 & 4\\
            0 & 0 & 0
        \end{vmatrix}=0
    \end{equation*}
\end{eg}

\chapter{Product of vectors and matrices}
When we calculate the products of vectors and matrices, it is quite different with our scalar multiplication. In this chapter, we will purely discuss them and their applications. Let's start with vector.
\begin{defn}
    Given two vectors $\vec{u}$ and $\vec{v}$. \textbf{Dot product} (Scalar product) of $\vec{u}$ and $\vec{v}$ is defined by:
    \begin{equation*}
        \vec{u}\cdot\vec{v}=\abs{\vec{u}}\abs{\vec{v}}\cos\theta
    \end{equation*}
    where $\theta$ is the angle between two vectors.
\end{defn}
\begin{lem}
    For any vectors $\vec{u},\vec{v},\vec{w}$ and any scalar $\lambda$,
    \begin{enumerate}
        \item $\vec{u}\cdot\vec{0}=\vec{0}\cdot\vec{u}=0$
        \item $\vec{u}\cdot\vec{u}=\abs{\vec{u}}^{2}\geq 0$
        \item $\vec{u}\cdot\vec{v}=\vec{v}\cdot\vec{u}$
        \item $\vec{u}\cdot\vec{u}=0$ if and only if $\vec{u}=\vec{0}$
        \item $\lambda(\vec{u}\cdot\vec{v})=(\lambda\vec{u})\cdot\vec{v}=\vec{u}\cdot(\lambda\vec{v})$
        \item $\vec{u}\cdot(\vec{v}+\vec{w})=\vec{u}\cdot\vec{v}+\vec{u}\cdot\vec{w}$
        \item $\abs{\vec{u}\cdot\vec{v}}\leq\abs{\vec{u}}\abs{\vec{v}}$
        \item $\abs{\vec{u}-\vec{v}}^{2}=\abs{u}^{2}+\abs{v}^{2}-2(\vec{u}\cdot\vec{v})$
    \end{enumerate}
\end{lem}
\begin{thm}
    For any two non-zero vectors $\vec{u}$ and $\vec{v}$, $\vec{u}\cdot\vec{v}=0$ if and only if $\vec{u}\perp\vec{v}$.
\end{thm}
\begin{lem}
    In the 3D coordinate system,
    \begin{align*}
        \hat{i}\cdot\hat{i}=\hat{j}\cdot\hat{j}=\hat{k}\cdot\hat{k}&=1 & \hat{i}\cdot\hat{j}=\hat{j}\cdot\hat{k}=\hat{k}\cdot\hat{i}&=0
    \end{align*}
    Given two vectors $\vec{u}=u_{1}\hat{i}+u_{2}\hat{j}+u_{3}\hat{k}$ and $\vec{v}=v_{1}\hat{i}+v_{2}\hat{j}+v_{3}\hat{k}$. We can define dot product of $\vec{u}$ and $\vec{v}$ by:
    \begin{equation*}
        \vec{u}\cdot\vec{v}=u_{1}v_{1}+u_{2}v_{2}+u_{3}v_{3}
    \end{equation*}
\end{lem}
\newpage
Interestingly, from the definition, we can project a vector into another vector.
\begin{defn}
    Given two non-zero vectors $\vec{u}$ and $\vec{v}$. \textbf{Projection} of $\vec{u}$ onto $\vec{v}$ is a vector $\vec{p}$ defined by:
    \begin{equation*}
        \vec{p}=\frac{\vec{u}\cdot\vec{v}}{\vec{v}\cdot\vec{v}}\vec{v}
    \end{equation*}
    Magnitude of $\vec{p}$ is $\abs{\vec{p}}=\frac{\abs{\vec{u}\cdot\vec{v}}}{\abs{\vec{v}}}$.
\end{defn}
\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \begin{tikzpicture}
            \draw[line width=1.5pt, blue, -stealth] (0,0.25) -- node [above, sloped]{$\vec{u}$}(1,2);
            \draw[line width=1.5pt, blue, -stealth] (0,0.25) -- node[above, sloped]{$\vec{v}$} (3,0.25);
            \draw[line width=1.5pt, red, -stealth] (0,0) -- node[below, midway]{$\vec{p}$} (1,0);
            \draw[dashed] (1,2) -- (1,0.25);
        \end{tikzpicture}
        \caption{Projection with equal direction}
        \label{fig:positive projection}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \begin{tikzpicture}
            \draw[line width=1.5pt, blue, -stealth] (0,0.25) -- node [above, sloped]{$\vec{u}$}(-1,2);
            \draw[line width=1.5pt, blue, -stealth] (0,0.25) -- node[above, sloped]{$\vec{v}$} (3,0.25);
            \draw[line width=1.5pt, red, -stealth] (0,0) -- node[below, midway]{$\vec{p}$} (-1,0);
            \draw[dashed] (-1,2) -- (-1,0.25) -- (0,0.25);
        \end{tikzpicture}
        \caption{Projection with opposite direction}
        \label{fig:negative projection}
    \end{subfigure}
\end{figure}
We have another type of product for vectors.
\begin{defn}
    Given two vectors $\vec{u}$ and $\vec{v}$. \textbf{Cross product} (Vector product) of $\vec{u}$ and $\vec{v}$ is defined by:
    \begin{equation*}
        \vec{u}\times\vec{v}=(\abs{\vec{u}}\abs{\vec{v}}\sin\theta)\hat{n}
    \end{equation*}
    where $\theta$ is the angle between $\vec{u}$ and $\vec{v}$, and $\hat{n}$ is the unit vector perpendicular to both $\vec{u}$ and $\vec{v}$ and its direction is determined by right-hand rule.
\end{defn}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth, alt={Right-Hand Rule}]{2.png}
    \caption{Right Hand Rule}
    \label{fig:right-hand rule}
\end{figure}
\begin{lem}
    For any vectors $\vec{u},\vec{v},\vec{w}$ and any scalar $\lambda$,
    \begin{enumerate}
        \item $\vec{u}\times\vec{0}=\vec{0}\times\vec{u}=\vec{0}$
        \item $\vec{u}\times\vec{u}=\vec{0}$
        \item $\vec{u}\times\vec{v}=-(\vec{v}\times\vec{u})$
        \item $(\lambda\vec{u})\times\vec{v}=\vec{u}\times(\lambda\vec{v})=\lambda(\vec{u}\times\vec{v})$
        \item $\vec{u}\times(\vec{v}+\vec{w})=\vec{u}\times\vec{v}+\vec{u}\times\vec{w}$
        \item $(\vec{u}+\vec{v})\times\vec{w}=\vec{u}\times\vec{w}+\vec{v}\times\vec{w}$
        \item $\abs{\vec{u}\times\vec{v}}^{2}=\abs{\vec{u}}^{2}\abs{\vec{v}}^{2}-(\vec{u}\cdot\vec{v})^{2}$
    \end{enumerate}
\end{lem}
\newpage
We can make use of the determinant learnt to calculate the cross product.
\begin{lem}
    In the 3D coordinate system,
    \begin{align*}
        \hat{i}\times\hat{i}=\hat{j}\times\hat{j}=\hat{k}\times\hat{k}&=0 & \hat{i}\times\hat{j}&=\hat{k} & \hat{j}\times\hat{k}&=\hat{i} & \hat{k}\times\hat{i}&=\hat{j}
    \end{align*}
    Given two vectors $\vec{u}=u_{1}\hat{i}+u_{2}\hat{j}+u_{3}\hat{k}$ and $\vec{u}=v_{1}\hat{i}+v_{2}\hat{j}+v_{3}\hat{k}$. We can define cross product of $\vec{u}$ and $\vec{v}$ by:
    \begin{equation*}
        \vec{u}\times\vec{v}=\begin{vmatrix}
            \hat{i} & \hat{j} & \hat{k}\\
            u_{1} & u_{2} & u_{3}\\
            v_{1} & v_{2} & v_{3}
        \end{vmatrix}
    \end{equation*}
\end{lem}
Now that we have introduced both products about vectors. What are the applications? The most simple one is to calculate the area and volume.
\begin{thm}
    Area of the parallelogram formed by two vectors $\vec{u}$ and $\vec{v}$ is $\abs{\vec{u}\times\vec{v}}$.
\end{thm}
\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \draw[line width=1.5pt, blue, -stealth] (0,0) -- node [above, midway]{$\vec{u}$}(3,0);
        \draw[line width=1.5pt, blue, -stealth] (0,0) -- node[left, midway]{$\vec{v}$} (1,2);
        \draw (1,2) -- (4,2) -- (3,0);
    \end{tikzpicture}
    \caption{Parallelogram formed by two vectors}
    \label{fig:parallelogram}
\end{figure}
\begin{lem}
    Area of the triangle formed by two vectors $\vec{u}$ and $\vec{v}$ is $\frac{1}{2}\abs{\vec{u}\times\vec{v}}$.
\end{lem}
\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \draw[line width=1.5pt, blue, -stealth] (0,0) -- node [above, midway]{$\vec{u}$}(3,0);
        \draw[line width=1.5pt, blue, -stealth] (0,0) -- node [left, midway]{$\vec{v}$} (1,2);
        \draw (3,0) -- (1,2);
    \end{tikzpicture}
    \caption{Triangle formed by two vectors}
    \label{fig:triangle}
\end{figure}
\begin{thm}
    Volume of the parallelepiped formed by three vectors $\vec{u}$, $\vec{v}$ and $\vec{w}$ is $\abs{\vec{u}\cdot(\vec{v}\times\vec{w})}$.
\end{thm}
\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \draw[line width=1.5pt, blue, -stealth] (0,0) -- node [above, midway]{$\vec{u}$} (3,0);
        \draw[line width=1.5pt, blue, -stealth] (0,0) -- node [above, midway]{$\vec{v}$} (1,0.5);
        \draw[line width=1.5pt, blue, -stealth] (0,0) -- node[left, midway]{$\vec{w}$} (1,1.5);
        \draw (4,0.5) -- (1,0.5) -- (2,2) -- (5,2) -- (4,0.5) -- (3,0) -- (4,1.5) -- (5,2);
        \draw (4,1.5) -- (1,1.5) -- (2,2);
    \end{tikzpicture}
    \caption{Parallelepiped formed by three vectors}
    \label{fig:parallelepiped}
\end{figure}
\begin{lem}
    Volume of the tetrahedron formed by three vectors $\vec{u}$, $\vec{v}$ and $\vec{w}$ is $\frac{1}{6}\abs{\vec{u}\cdot(\vec{v}\times\vec{w})}$.
\end{lem}
\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \draw[line width=1.5pt, blue, -stealth] (0,0) -- node [above, midway]{$\vec{u}$} (3,0);
        \draw[line width=1.5pt, blue, -stealth] (0,0) -- node [above, midway]{$\vec{v}$} (1,0.5);
        \draw[line width=1.5pt, blue, -stealth] (0,0) -- node[left, midway]{$\vec{w}$} (1,2);
        \draw (3,0) -- (1,0.5) -- (1,2) -- (3,0);
    \end{tikzpicture}
    \caption{Tetrahedron formed by three vectors}
    \label{fig:tetrahedron}
\end{figure}
\newpage
We can also use cross product to determine if 3 points are collinear.
\begin{thm}
    Given three points $A$, $B$ and $C$. They are collinear if and only if $\overrightarrow{AB}\times\overrightarrow{AC}=\vec{0}$.
\end{thm}
\begin{figure}[h]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \begin{tikzpicture}
            \draw (0,0) node[anchor=east]{$A$};
            \draw (1,0) node[anchor=south]{$B$};
            \draw (3,0) node[anchor=west]{$C$};
            \draw[line width=1.5pt, blue, -stealth] (0,0) -- (3,0);
            \draw[line width=1.5pt, blue, -stealth] (0,0) -- (1,0);
        \end{tikzpicture}
        \caption{Collinear}
        \label{fig:collinear}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \begin{tikzpicture}
            \draw (0,0) node[anchor=east]{$A$};
            \draw (1,2) node[anchor=south]{$B$};
            \draw (3,0) node[anchor=west]{$C$};
            \draw[line width=1.5pt, blue, -stealth] (0,0) -- (3,0);
            \draw[line width=1.5pt, blue, -stealth] (0,0) -- (1,2);
        \end{tikzpicture}
        \caption{Not collinear}
        \label{fig:not collinear}
    \end{subfigure}
\end{figure}
What about multiplying matrices?
\begin{defn}
    Given an $m\times n$ matrix $\mathbf{A}$ with elements $a_{ij}$ and an $n\times p$ matrix $\mathbf{B}$ with elements $b_{ij}$. The product of $\mathbf{A}$ and $\mathbf{B}$ is an $m\times p$ matrix $\mathbf{C}=\mathbf{AB}$ with each elements $c_{ij}$ obtained by:
    \begin{equation*}
        c_{ij}=a_{i1}b_{1j}+a_{i2}b_{2j}+\cdots+a_{in}b_{nj}=\sum_{k=1}^{n}a_{in}b_{nj}
    \end{equation*}
\end{defn}
\begin{eg}
    Given that $\mathbf{A}=\begin{pmatrix}
        1 & 2 & 3\\
        4 & 5 & 6
    \end{pmatrix}$ and $\mathbf{B}=\begin{pmatrix}
        7 & 8\\
        9 & 10\\
        11 & 12
    \end{pmatrix}$. Then
    \begin{align*}
        \mathbf{AB}&=\begin{pmatrix}
            1 & 2 & 3\\
            4 & 5 & 6
        \end{pmatrix}\begin{pmatrix}
            7 & 8\\
            9 & 10\\
            11 & 12
        \end{pmatrix}=\begin{pmatrix}
            58 & 64\\
            139 & 154
        \end{pmatrix} & \mathbf{BA}&=\begin{pmatrix}
            7 & 8\\
            9 & 10\\
            11 & 12
        \end{pmatrix}\begin{pmatrix}
            1 & 2 & 3\\
            4 & 5 & 6
        \end{pmatrix}=\begin{pmatrix}
            39 & 54 & 69\\
            49 & 68 & 87\\
            59 & 82 & 105
        \end{pmatrix}
    \end{align*}
\end{eg}
\begin{lem}
    For any scalars $\lambda$ and $\mu$ and matrices $\mathbf{A},\mathbf{B},\mathbf{C}$ that allows for matrix multiplications between them,
    \begin{enumerate}
        \item $\mathbf{A}(\mathbf{BC})=(\mathbf{AB})\mathbf{C}$
        \item $\mathbf{A}(\mathbf{B}+\mathbf{C})=\mathbf{AB}+\mathbf{AC}$
        \item $(\mathbf{A}+\mathbf{B})\mathbf{C}=\mathbf{AC}+\mathbf{BC}$
        \item $(\lambda\mathbf{A})(\mu\mathbf{B})=\lambda\mu(\mathbf{AB})$
        \item $\mathbf{0A}=\mathbf{A0}=\mathbf{0}$
        \item $\mathbf{I_{m}A}=\mathbf{AI_{n}}=\mathbf{A}$ if $\mathbf{A}$ is an $m\times n$ matrix
        \item $\abs{\mathbf{AB}}=\abs{\mathbf{A}}\abs{\mathbf{B}}$
    \end{enumerate}
\end{lem}
\begin{defn}
    Let $\mathbf{A}$ be a square matrix and $n$ be a positive integer. Then
    \begin{equation*}
        \mathbf{A}^{n}=\prod_{i=1}^{n}\mathbf{A}
    \end{equation*}
\end{defn}
Some of the matrices have its multiplicative inverse, just like scalar.
\begin{defn}
    Given an $n\times n$ matrix $\mathbf{A}$. It is \textbf{invertible} (non-singular) if there exists another $n\times n$ matrix $\mathbf{B}$ such that
    \begin{equation*}
        \mathbf{AB}=\mathbf{I}_{n}
    \end{equation*}
    $\mathbf{B}$ is the \textbf{inverse matrix} of $\mathbf{A}$, and is denoted by $\mathbf{A}^{-1}$.
\end{defn}
\begin{thm}
    For any square matrices $\mathbf{A}$ and $\mathbf{B}$, if $\mathbf{AB}=\mathbf{I}$, then $\mathbf{BA}=\mathbf{I}$ and $\mathbf{B}$ is the inverse of $\mathbf{A}$.
\end{thm}
\begin{lem}
    For any invertible matrix $\mathbf{A}$, $\abs{\mathbf{A}^{-1}}=\abs{\mathbf{A}}^{-1}$.
\end{lem}
\begin{thm}(Uniqueness)
    Let $\mathbf{A}$ be an invertible matrix. If both $\mathbf{B}$ and $\mathbf{C}$ are inverses of $\mathbf{A}$, then $\mathbf{B}=\mathbf{C}$.
\end{thm}
\begin{thm}
    Let $\mathbf{A}$ and $\mathbf{B}$ be two invertible $n\times n$ matrices, and $\lambda$ be a non-zero scalar.
    \begin{enumerate}
        \item $\mathbf{A}^{-1}$ is invertible and $(\mathbf{A}^{-1})^{-1}=\mathbf{A}$
        \item $\lambda\mathbf{A}$ is invertible and $(\lambda\mathbf{A})^{-1}=\frac{1}{\lambda}\mathbf{A}^{-1}$
        \item $\mathbf{AB}$ is invertible and $(\mathbf{AB})^{-1}=\mathbf{B}^{-1}\mathbf{A}^{-1}$
    \end{enumerate}
\end{thm}
\begin{lem}
    If $\mathbf{A}$ is invertible and $k$ is a positive integer, then $\mathbf{A}^{k}$ is invertible and $(\mathbf{A}^{k})^{-1}=(\mathbf{A}^{-1})^{k}$.
\end{lem}
Based on definition, it seems we may need to find the inverse by brute force. Luckily, there is a way to easily find the inverse. Before that, we should introduce the transpose.
\begin{defn}
    Given a $m\times n$ matrix $\mathbf{A}$. \textbf{Transpose} of matrix $\mathbf{A}$, denoted by $\mathbf{A}^{T}$, is a $n\times m$ matrix obtained by interchanging the rows and columns of $\mathbf{A}$.
    \begin{align*}
        \mathbf{A}&=\begin{pmatrix}
            a_{11} & a_{12} & \hdots & a_{1n}\\
            a_{21} & a_{22} & \hdots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{m1} & a_{m2} & \hdots & a_{mn}
        \end{pmatrix} & \mathbf{A}^{T}&=\begin{pmatrix}
            a_{11} & a_{21} & \hdots & a_{m1}\\
            a_{12} & a_{22} & \hdots & a_{m2}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{1n} & a_{2n} & \hdots & a_{mn}
        \end{pmatrix}
    \end{align*}
\end{defn}
\begin{lem}
    Let $\lambda$ be a scalar, and matrices $\mathbf{A},\mathbf{B}$ that allows for matrix multiplications.
    \begin{enumerate}
        \item $\mathbf{I}^{T}=\mathbf{I}$
        \item $(\mathbf{A}^{T})^{T}$
        \item $(\mathbf{A}+\mathbf{B})^{T}=\mathbf{A}^{T}+\mathbf{B}^{T}$
        \item $(\lambda\mathbf{A})^{T}=\lambda\mathbf{A}^{T}$
        \item $(\mathbf{AB})^{T}=\mathbf{B}^{T}\mathbf{A}^{T}$
        \item If $\mathbf{A}$ is invertible, then $\mathbf{A}^{T}$ is invertible and $(\mathbf{A}^{T})^{-1}=(\mathbf{A}^{-1})^{T}$.
    \end{enumerate}
\end{lem}
We can now introduce the adjoint matrix.
\begin{defn}
    For any $n\times n$ square matrix $\mathbf{A}$, \textbf{adjoint matrix} is defined by:
    \begin{equation*}
        \adj\mathbf{A}=\begin{pmatrix}
            A_{11} & A_{12} & \hdots & A_{1n}\\
            A_{21} & A_{22} & \hdots & A_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            A_{n1} & A_{n2} & \hdots & A_{nn}
        \end{pmatrix}^{T}
    \end{equation*}
    where $A_{ij}$ is the cofactor of $a_{ij}$.
\end{defn}
Using this matrix, we can get a matrix that is a multiple of identity matrix using cofactor expansion.
\begin{thm}
    Let $\mathbf{A}$ be a square matrix. Then
    \begin{equation*}
        \mathbf{A}(\adj\mathbf{A})=(\adj\mathbf{A})=\begin{pmatrix}
            \abs{\mathbf{A}} & 0 & \hdots & 0\\
            0 & \abs{\mathbf{A}} & \ddots & \vdots\\
            \vdots & \ddots & \ddots & 0\\
            0 & \hdots & 0 & \abs{\mathbf{A}}
        \end{pmatrix}=\abs{\mathbf{A}}\mathbf{I}
    \end{equation*}
\end{thm}
As we can see, we have obtained a formula for inverse matrix.
\begin{thm}
    Let $\mathbf{A}$ be a square matrix.
    \begin{enumerate}
        \item If $\abs{\mathbf{A}}\neq 0$, then $\mathbf{A}^{-1}=\frac{1}{\abs{\mathbf{A}}}\adj\mathbf{A}$
        \item $\mathbf{A}$ is invertible if and only if $\abs{\mathbf{A}}\neq 0$.
    \end{enumerate}
\end{thm}
\begin{eg}
    Given $\mathbf{A}=\begin{pmatrix}
        1 & 1 & 4\\
        2 & 0 & 3\\
        -1 & 2 & 0
    \end{pmatrix}$. We can find that
    \begin{equation*}
        \abs{\mathbf{A}}=\begin{vmatrix}
        1 & 1 & 4\\
        2 & 0 & 3\\
        -1 & 2 & 0
        \end{vmatrix}=7
    \end{equation*}
    Therefore, we can see that $\mathbf{A}$ is invertible. Then
    \begin{align*}
        \adj\mathbf{A}&=\begin{pmatrix}
            \begin{vmatrix}
                0 & 3\\
                2 & 0
            \end{vmatrix} & -\begin{vmatrix}
                2 & 3\\
                -1 & 0
            \end{vmatrix} & \begin{vmatrix}
                2 & 0\\
                -1 & 2
            \end{vmatrix}\\
            -\begin{vmatrix}
                1 & 4\\
                2 & 0
            \end{vmatrix} & \begin{vmatrix}
                1 & 4\\
                -1 & 0
            \end{vmatrix} & -\begin{vmatrix}
                1 & 1\\
                -1 & 2
            \end{vmatrix}\\
            \begin{vmatrix}
                1 & 4\\
                0 & 3
            \end{vmatrix} & -\begin{vmatrix}
                1 & 4\\
                2 & 3
            \end{vmatrix} & \begin{vmatrix}
                1 & 1\\
                2 & 0
            \end{vmatrix}
        \end{pmatrix}^{T}=\begin{pmatrix}
            -6 & 8 & 3\\
            -3 & 4 & 5\\
            4 & -3 & -2
        \end{pmatrix} & \mathbf{A}^{-1}&=\frac{1}{7}\begin{pmatrix}
            -6 & 8 & 3\\
            -3 & 4 & 5\\
            4 & -3 & -2
        \end{pmatrix}=\begin{pmatrix}
            -\frac{6}{7} & \frac{8}{7} & \frac{3}{7}\\
            -\frac{3}{7} & \frac{4}{7} & \frac{5}{7}\\
            \frac{4}{7} & -\frac{3}{7} & -\frac{2}{7}
        \end{pmatrix}
    \end{align*}
\end{eg}

\chapter{System of linear equations}
One of the very important applications of matrices is solving a system of multiple linear equations.
\begin{defn}
    \textbf{A system of $m$ linear equations in $n$ unknowns} is in form:
    \begin{equation*}
        \begin{cases}
            a_{11}x_{1}+a_{12}x_{2}+\cdots+a_{1n}x_{n}=b_{1}\\
            a_{21}x_{1}+a_{22}x_{2}+\cdots+a_{2n}x_{n}=b_{2}\\
            \vdots\\
            a_{m1}x_{1}+a_{m2}x_{2}+\cdots+a_{mn}x_{n}=b_{m}
        \end{cases}
    \end{equation*}
    It is \textbf{consistent} if it has solutions. It is \textbf{inconsistent} if it has no solutions.
\end{defn}
For simplicity, we focus on system of 
$n$ linear equations in $n$ unknown. One of the obvious way is to use invertible matrix.
\begin{thm}
    We can turn the system of linear equations into form:
    \begin{align*}
        \mathbf{AX}&=\mathbf{B} & \mathbf{A}&=\begin{pmatrix}
            a_{11} & a_{12} & \hdots & a_{1n}\\
            a_{21} & a_{22} & \hdots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{m1} & a_{m2} & \hdots & a_{mn}
        \end{pmatrix} & \mathbf{X}&=\begin{pmatrix}
            x_{1}\\
            x_{2}\\
            \vdots\\
            x_{n}
        \end{pmatrix} & \mathbf{B}&=\begin{pmatrix}
            b_{1}\\
            b_{2}\\
            \vdots\\
            b_{m}
        \end{pmatrix}
    \end{align*}
    If $\mathbf{A}$ is a square matrix, $\mathbf{A}$ is invertible if and only if $\mathbf{AX}=\mathbf{B}$ has an unique solution $\mathbf{X}=\mathbf{A}^{-1}\mathbf{B}$.
\end{thm}
\begin{lem}
    Given a system of $n$ linear equation in $n$ unknowns $\mathbf{AX}=\mathbf{B}$.
    \begin{enumerate}
        \item $\mathbf{AX}=\mathbf{B}$ has an unique solution if and only if $\abs{\mathbf{A}}\neq 0$.
        \item $\mathbf{AX}=\mathbf{B}$ has either no solutions or infinitely many solutions if and only if $\abs{\mathbf{A}}=0$.
    \end{enumerate}
\end{lem}
It can be tedious to find invertible matrix in order to solve the system of linear equations. We introduce Cramer's Rule, which make uses of determinant.
\begin{thm}(Cramer's Rule)
    For a system of 2 linear equations in 2 unknowns, we let
    \begin{align*}
        &\begin{cases}
            a_{11}x+a_{12}y=b_{1}\\
            a_{21}x+a_{22}y=b_{2}
        \end{cases} &
        \Delta&=\begin{vmatrix}
            a_{11} & a_{12}\\
            a_{21} & a_{22}
        \end{vmatrix} & \Delta_{x}&=\begin{vmatrix}
            b_{1} & a_{12}\\
            b_{2} & a_{22}
        \end{vmatrix} & \Delta_{y}\begin{vmatrix}
            a_{11} & b_{1}\\
            a_{21} & b_{2}
        \end{vmatrix}
    \end{align*}
    If $\Delta\neq 0$, then $x=\frac{\Delta_{x}}{\Delta}$ and $y=\frac{\Delta_{y}}{\Delta}$ is the unique solution of the system.\\
    For a system of 2 linear equations in 2 unknowns, we let
    \begin{align*}
        &\begin{cases}
            a_{11}x+a_{12}y+a_{13}z=b_{1}\\
            a_{21}x+a_{22}y+a_{23}z=b_{2}\\
            a_{31}x+a_{32}y+a_{33}z=b_{3}
        \end{cases} &
        \Delta&=\begin{vmatrix}
            a_{11} & a_{12} & a_{13}\\
            a_{21} & a_{22} & a_{23}\\
            a_{31} & a_{32} & a_{33}
        \end{vmatrix} & \Delta_{x}&=\begin{vmatrix}
            b_{1} & a_{12} & a_{13}\\
            b_{2} & a_{22} & a_{23}\\
            b_{3} & a_{32} & a_{33}
        \end{vmatrix} & \Delta_{y}&=\begin{vmatrix}
            a_{11} & b_{1} & a_{13}\\
            a_{21} & b_{2} & a_{23}\\
            a_{31} & b_{3} & a_{33}
        \end{vmatrix} & \Delta_{z}&=\begin{vmatrix}
            a_{11} & a_{12} & b_{1}\\
            a_{21} & a_{22} & b_{2}\\
            a_{31} & a_{32} & b_{3}
        \end{vmatrix}
    \end{align*}
    If $\Delta\neq 0$, then $x=\frac{\Delta_{x}}{\Delta}$, $y=\frac{\Delta_{y}}{\Delta}$ and $z=\frac{\Delta_{z}}{\Delta}$ is the unique solution of the system.
\end{thm}
\newpage
Interestingly, Cramer's Rule can help us determine whether a system of linear equations has no solutions or infinitely many solutions.
\begin{thm}
    For a system of 2 linear equations in 2 unknowns:
    \begin{enumerate}
        \item If $\Delta\neq 0$, then the system has an unique solution.
        \item If $\Delta=0$ and either $\Delta_{x}\neq 0$ or $\Delta_{y}\neq 0$, then the system has no solutions.
        \item If $\Delta=0$ and $\Delta_{x}=\Delta_{y}=0$, then the system has infinite many solutions.
    \end{enumerate}
    For a system of 3 linear equations in 3 unknowns:
    \begin{enumerate}
        \item If $\Delta\neq 0$, then the system has an unique solution.
        \item If $\Delta=0$ and either $\Delta_{x}\neq 0$, $\Delta_{y}\neq 0$ or $\Delta_{z}\neq 0$, then the system has no solutions.
        \item If $\Delta=0$ and $\Delta_{x}=\Delta_{y}=\Delta_{z}=0$, then the system has infinite many solutions.
    \end{enumerate}
\end{thm}
We now introduce a third method of solving the system of linear equations, which is Gaussian elimination. Before that, we introduce the row echelon form.
\begin{defn}
    An \textbf{augmented matrix} is a matrix that is obtained by appending an original matrix.
\end{defn}
\begin{eg}
    Assume that we want to represent
    \begin{equation*}
        \begin{cases}
            a_{11}x+a_{12}y=b_{1}\\
            a_{21}x+a_{22}y=b_{2}
        \end{cases}
    \end{equation*}
    We can use the following augmented matrix to represent this system of linear equations.
    \begin{equation*}
        \begin{pmatrix}
            \begin{array}{cc|c}
                a_{11} & a_{12} & b_{1}\\
                a_{21} & a_{22} & b_{2}
            \end{array}  
        \end{pmatrix}
    \end{equation*}
\end{eg}
\begin{rem}
    What's special about system of linear equations is that we ca perform row operations to augmented matrix.
\end{rem}
\begin{defn}
    An augmented matrix is in \textbf{row echelon form} if
    \begin{enumerate}
        \item The first non-zero element in each row is 1.
        \item The first non-zero element in each row occurs in a column to the right of the first non-zero element in the preceding row.
        \item Any zero rows are placed at the bottom of the matrix.
    \end{enumerate}
\end{defn}
\begin{eg}
    Following are some augmented matrices that are in row echelon form.
    \begin{align*}
        &\begin{pmatrix}
            \begin{array}{cc|c}
                1 & 3 & 2\\
                0 & 1 & 3
            \end{array}
        \end{pmatrix} & &\begin{pmatrix}
            \begin{array}{ccc|c}
                1 & 3 & 6 & 1\\
                0 & 1 & 3 & 5\\
                0 & 0 & 1 & 2
            \end{array}
        \end{pmatrix} & &\begin{pmatrix}
            \begin{array}{ccc|c}
                1 & 7 & 6 & 5\\
                0 & 0 & 1 & 9\\
                0 & 0 & 0 & 0
            \end{array}
        \end{pmatrix}
    \end{align*}
\end{eg}
Once we turn an augmented matrix into row echelon form, we can obtain the solution by back substitutions.
\newpage
\begin{eg}
    We want to find the solution of this system of linear equations:
    \begin{equation*}
        \begin{cases}
            x+3y+4z=19\\
            2x+5y+3z=21\\
            3x+7y+3z=26
        \end{cases}
    \end{equation*}
    We turn this system into an augmented matrix, then perform row operations.
    \begin{align*}
        \begin{pmatrix}
            \begin{array}{ccc|c}
                1 & 3 & 4 & 19\\
                2 & 5 & 3 & 21\\
                3 & 7 & 3 & 26
            \end{array}
        \end{pmatrix}&\sim\begin{pmatrix}
            \begin{array}{ccc|c}
                1 & 3 & 4 & 19\\
                0 & -1 & -5 & -17\\
                0 & -2 & -9 & -31
            \end{array}
        \end{pmatrix}\\
        &\sim\begin{pmatrix}
            \begin{array}{ccc|c}
                1 & 3 & 4 & 19\\
                0 & 1 & 5 & 17\\
                0 & -2 & -9 & -31
            \end{array}
        \end{pmatrix}\\
        &\sim\begin{pmatrix}
            \begin{array}{ccc|c}
                1 & 3 & 4 & 19\\
                0 & 1 & 5 & 17\\
                0 & 0 & 1 & 3
            \end{array}
        \end{pmatrix}
    \end{align*}
    From this, we can turn the augmented matrix into linear equations again and get the solution.
    \begin{align*}
        &\begin{cases}
            x+3y+4z=19\\
            y+5z=17\\
            z=3
        \end{cases} & &\begin{cases}
            x=1\\
            y=2\\
            z=3
        \end{cases}
    \end{align*}
\end{eg}
\begin{eg}
    We want to find the solution of this system of linear equations:
    \begin{equation*}
        \begin{cases}
            x+3y+4z=19\\
            2x+5y+3z=21\\
            3x+7y+2z=23
        \end{cases}
    \end{equation*}
    We turn this system into an augmented matrix, then perform row operations.
    \begin{align*}
        \begin{pmatrix}
            \begin{array}{ccc|c}
                1 & 3 & 4 & 19\\
                2 & 5 & 3 & 21\\
                3 & 7 & 2 & 23
            \end{array}
        \end{pmatrix}&\sim\begin{pmatrix}
            \begin{array}{ccc|c}
                1 & 3 & 4 & 19\\
                0 & -1 & -5 & -17\\
                0 & -2 & -10 & -34
            \end{array}
        \end{pmatrix}\\
        &\sim\begin{pmatrix}
            \begin{array}{ccc|c}
                1 & 3 & 4 & 19\\
                0 & 1 & 5 & 17\\
                0 & -2 & -10 & -34
            \end{array}
        \end{pmatrix}\\
        &\sim\begin{pmatrix}
            \begin{array}{ccc|c}
                1 & 3 & 4 & 19\\
                0 & 1 & 5 & 17\\
                0 & 0 & 0 & 0
            \end{array}
        \end{pmatrix}
    \end{align*}
    From this, we can turn the augmented matrix into linear equations again and get the solution. Let $t$ be any real number.
    \begin{align*}
        &\begin{cases}
            x+3y+4z=19\\
            y+5z=17
        \end{cases} & &\begin{cases}
            x=11t-32\\
            y=-5t+17\\
            z=t
        \end{cases}
    \end{align*}
\end{eg}
There are a special kind of system of linear equations called the system of homogeneous linear equations. In this case, it is guarantee to have solutions.
\begin{defn}
    \textbf{A system of $m$ homogeneous linear equations in $n$ unknowns} is in form:
    \begin{equation*}
        \begin{cases}
            a_{11}x_{1}+a_{12}x_{2}+\cdots+a_{1n}x_{n}=0\\
            a_{21}x_{1}+a_{22}x_{2}+\cdots+a_{2n}x_{n}=0\\
            \vdots\\
            a_{m1}x_{1}+a_{m2}x_{2}+\cdots+a_{mn}x_{n}=0
        \end{cases}
    \end{equation*}
\end{defn}
\newpage
\begin{thm}
    For a system of $n$ homogeneous linear equations with $n$ unknowns $\mathbf{AX}=\mathbf{0}$:
    \begin{enumerate}
        \item $\mathbf{AX}=\mathbf{0}$ has trivial solution (all 0) as its only solution if and only if $\abs{\mathbf{A}}\neq 0$.
        \item $\mathbf{AX}=\mathbf{0}$ has a trivial solution and infinitely many non-trivial solutions if and only if $\abs{\mathbf{A}}=0$.
    \end{enumerate}
\end{thm}
There is a special use for augmented matrix. Note that when we represent system of linear equations in $\mathbf{AX}=\mathbf{B}$,
\begin{equation*}
    \begin{pmatrix}
        a_{11} & a_{12} & \hdots & a_{1n}\\
        a_{21} & a_{22} & \hdots & a_{2n}\\
        \vdots & \vdots & \ddots & \vdots\\
        a_{n1} & a_{n2} & \hdots & a_{nn}
    \end{pmatrix}\begin{pmatrix}
        x_{1}\\
        x_{2}\\
        \vdots\\
        x_{n}
    \end{pmatrix}=\begin{pmatrix}
        b_{1}\\
        b_{2}\\
        \vdots\\
        b_{n}
    \end{pmatrix}=\begin{pmatrix}
        1 & 0 & \hdots & 0\\
        0 & 1 & \ddots & \vdots\\
        \vdots & \ddots & \ddots & 0\\
        0 & \hdots & 0 & 1
    \end{pmatrix}\begin{pmatrix}
        b_{1}\\
        b_{2}\\
        \vdots\\
        b_{n}
    \end{pmatrix}
\end{equation*}
If $\abs{\mathbf{A}}\neq 0$, by performing row operations, we get that
\begin{equation*}
    \begin{pmatrix}
        1 & 0 & \hdots & 0\\
        0 & 1 & \ddots & \vdots\\
        \vdots & \ddots & \ddots & 0\\
        0 & \hdots & 0 & 1
    \end{pmatrix}\begin{pmatrix}
        x_{1}\\
        x_{2}\\
        \vdots\\
        x_{n}
    \end{pmatrix}=\begin{pmatrix}
        a_{11}' & a_{12}' & \hdots & a_{1n}'\\
        a_{21}' & a_{22}' & \hdots & a_{2n}'\\
        \vdots & \vdots & \ddots & \vdots\\
        a_{n1}' & a_{n2}' & \hdots & a_{nn}'
    \end{pmatrix}\begin{pmatrix}
        b_{1}\\
        b_{2}\\
        \vdots\\
        b_{n}
    \end{pmatrix}
\end{equation*}
We know that $\mathbf{AX}=\mathbf{IB}$ can become $\mathbf{IX}=\mathbf{A}^{-1}\mathbf{B}$. It just so happen that we just obtained the inverse of $\mathbf{A}$! Therefore,
\begin{thm}
    Given an invertible matrix $\mathbf{A}$. By performing row operations, we can get that
    \begin{equation*}
        \begin{pmatrix}
            \begin{array}{c|c}
                \mathbf{A} & \mathbf{I}
            \end{array}
        \end{pmatrix}\sim\begin{pmatrix}
            \begin{array}{c|c}
                \mathbf{I} & \mathbf{A}^{-1}
            \end{array}
        \end{pmatrix}
    \end{equation*}
\end{thm}
\end{document}
