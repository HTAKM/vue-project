\documentclass{huhtakm-template-book-v2}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\prob}{\mathbb{P}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\Poisson}{Poisson}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\Cauchy}{Cauchy}
\DeclareMathOperator{\NBin}{NBin}
\DeclareMathOperator{\U}{U}
\DeclareMathOperator{\Gam}{Gamma}
\DeclareMathOperator{\Bet}{Beta}
\setlength{\parindent}{0pt}
\title{
	\Huge MATH 3423: Statistical Inference
}
\author{
	HU-HTAKM\\
	\small Website: \url{https://htakm.github.io/htakm_test/}
}
\date{
	Last major change: December 7, 2024\\
	Last small update: December 29, 2024
}
\begin{document}
\maketitle
This is MATH 3423 lecture note made by me. Note that some of the notations are slightly different with what is used in the course for better clarity (adjusting to my notation preference). For example:
\begin{table}[h]
	\centering
	\begin{tabular}{||c|c|c||}
		\hline
		Name & My notation & Dr. YU Chi Wai's notation\\
		\hline
		Transpose & $\mathbf{A}^{T}$ &  $\mathbf{A}'$\\
		Set of real numbers & $\mathbb{R}$ & $R$\\
		Fisher Information & $\mathcal{I}_{X}(\theta)$ & $I_{X}(\theta)$\\
		Convergence in distribution & $X_{n}\xrightarrow{D}X$ & $X_{n}\xrightarrow{d}X$\\
		Probability & $\prob$ & $P$\\
		Expected value & $\E$ & $E$ or $E_{X}$\\
		Indicator function & $\mathbf{1}_{A}$ & $I_{A}$\\
		chisq-value & $\chi_{\alpha,n}^{2}$ & $\chi_{\alpha}^{2}(n)$\\
		t-value & $t_{\alpha,n}$ & $t_{\alpha}(n)$\\
		f-value & $f_{\alpha,(n,m)}$ & $F_{\alpha}(n,m)$ or $f_{\alpha}(n,m)$\\
		\hline
	\end{tabular}
\end{table}

Some things to note about:
\begin{enumerate}
	\item Following all the examples in this lecture note is not adequate to help you survive Dr.YU's exam, but it does help you understand (according to an anonymous reader who got almost high in Dr.YU's exams (wtf))
	\item Some topics may or may not be tested. This lecture note include topics from some MATH 2421/2431 stuff and untested supplementary notes.
	\item If you are about to attend Dr.YU's exams, please try to follow his notations instead.
	\item There may be typos in the notes. Proceed with caution. (?)
\end{enumerate}
\tableofcontents
\chapter{Preliminary}
Statistical inference is a statistical process which investigates how to use the information from the data to make an inference about the distribution of the random variable of our interest. In MATH 3423, we will focus on two core concepts of statistical inference: point estimation and hypothesis testing.
\section{Random variables}
In a particular event, it usually results in an outcome $\omega$. We group all of these possible outcomes into a sample space $\Omega$. However, in order to obtain a reasonable numerical analysis from the sample space, we should map the outcomes into a numerical value. We define it as random variable.
\begin{defn}
	Given a sample space $\Omega$.
	\begin{enumerate}
		\item \textbf{Random variable} $X$ is a function $X:\Omega\to\mathbb{R}$ that maps from a sample space $\Omega$ as a set of possible outcomes to a real number space.
		\item We define the \textbf{probability} of $X$ that takes a value in a set $A$ by:
		\begin{equation*}
			\prob(X\in A)=\prob(\{\omega\in\Omega: X(\omega)\in A\})
		\end{equation*}
		\item The \textbf{cumulative distribution function} (CDF) of $X$ is given by:
		\begin{equation*}
			F_{X}(x)=\prob(X\leq x)
		\end{equation*}
		\item The random variable $X$ is \textbf{discrete} if it has a \textbf{probability mass function} (PMF) $p_{X}$ given by:
		\begin{equation*}
			p_{X}(x)=\prob(X=x)
		\end{equation*}
		\item The random variable $X$ is \textbf{continuous} if it's  CDF can be expressed using a \textbf{probability density function} (PDF) $f_{X}$ by:
		\begin{equation*}
			F_{X}(x)=\int_{-\infty}^{x}f_{X}(u)\,du
		\end{equation*}
	\end{enumerate}
\end{defn}
\begin{defn}
	Two random variables $X$ and $Y$ are \textbf{independent} if either:
	\begin{equation*}
		f_{X,Y}(x,y)=f_{X}(x)f_{Y}(y)\text{ or }F_{X,Y}(x,y)=F_{X}(x)F_{Y}(y)
	\end{equation*}
\end{defn}
This theorem is very important and we will use a lot later.
\begin{thm}
	If $X$ and $Y$ are independent, then $f(X)$ and $g(Y)$ are independent for any functions $f$ and $g$.
\end{thm}

\newpage
\section{Random sample and parametric distribution}
To make a statistical inference about the distribution of the random variable $X$ of our interest, we need to draw a sample of data.
\begin{defn}
	Denote the first observation by $X_{1}$, the second by $X_{2}$, and so on. A set of random variables $\{X_{1},\cdots,X_{n}\}$ are called a \textbf{random sample} of size $n$ from the common distribution of $X$ with a PMF $p_{X}(x)$ or PDF $f_{X}(x)$ if they are independent and identically distributed (i.i.d.).
\end{defn}
\begin{rem}
	Random variables $X_{1},\cdots,X_{n}$ are assumed to be observable with known actual values $x_{1},\cdots,x_{n}$ respectively.
\end{rem}
Under the random sampling setting, it is easy to get the following lemma.
\begin{lem}
	Given a random sample $\{X_{1},\cdots,X_{n}\}$ of a common distribution $X$.
	\begin{enumerate}
		\item If the random sample is discrete with a common PMF $p_{X}(x)$, then the joint PMF of random sample can be obtained by:
		\begin{equation*}
			p_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n})=\prod_{i=1}^{n}p_{X_{i}}(x_{i})
		\end{equation*}
		\item If the random sample is continuous with a common PDF $f_{X}(x)$, then the joint PDF of random sample can be obtained by:
		\begin{equation*}
			f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n})=\prod_{i=1}^{n}f_{X_{i}}(x_{i})
		\end{equation*}
	\end{enumerate}
\end{lem}
The underlying distribution of $X$ is assumed to be unknown or partially known in practice. In most situation, it is reasonable to assume that the form of the PMF $p_{X}$ or PDF $f_{X}$ of the distribution is known but it contains some unknown parameters $\theta$.
\begin{defn}
	\textbf{Parametric distribution} is a distribution which the PMF $p_{X}$ and PDF $f_{X}$ contains some unknown parameters $\theta$. The PMF or PDF is said to be \textbf{parametric}.
\end{defn}
\begin{rem}
	Instead of using parametric distributions of the data, we may assume that the form of the distribution is unknown but that the distribution has some properties. E.g. A distribution is continuous. This distribution is called a \textbf{non-parametric distribution} and the corresponding statistical method is called \textbf{non-parametric statistical approach}. If parameters are involved but the form of the distribution is unknown, then such a distribution is called \textbf{semi-parametric distribution} and the corresponding method is called \textbf{semi-parametric statistical approach}.
\end{rem}
\begin{eg}
	Data are usually assumed from the normal distribution with mean $\mu$ and variance $\sigma^{2}$, where the parameter:
	\begin{equation*}
		\theta=\begin{pmatrix}
			\mu\\
			\sigma^{2}
		\end{pmatrix}
	\end{equation*}
	is unknown but fixed.
\end{eg}
\begin{lem}
	Given a random sample $\{X_{1},\cdots,X_{n}\}$ of a common distribution $X$.
	\begin{enumerate}
		\item If the random sample is discrete with a common parametric PMF $p_{X}(x|\theta)$, then the joint PMF of random sample can be obtained by:
		\begin{equation*}
			p_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)=\prod_{i=1}^{n}p_{X}(x_{i}|\theta)
		\end{equation*}
		\item If the random sample is continuous with a common parametric PDF $p_{X}(x|\theta)$, then the joint PDF of random sample can be obtained by:
		\begin{equation*}
			f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)=\prod_{i=1}^{n}f_{X}(x_{i}|\theta)
		\end{equation*}
	\end{enumerate}
\end{lem}

\newpage
Under parametric setting, we can see that uncertainty of the distribution is the uncertainty of its parameters. One of the central problem in statistics is how to determine which function of data is the best one to estimate $\theta$.
\begin{defn}
	Let $\mathbf{X}=(X_{1}\ \cdots\ X_{n})^{T}$ be a random vector.
	\begin{enumerate}
		\item If $T(\cdot)$ is a real-valued or vector-valued function such that for all $\mathbf{X}\in\Omega$, $T(\mathbf{X})$ does not contain any unknown parameters, then $T(\mathbf{X})$ is called a \textbf{statistic}.
		\item If we use the statistic $T(\mathbf{X})$ to estimate an unknown parameter $\theta$, then we call $T(\mathbf{X})$ and $T(\mathbf{x})$ an \textbf{estimator} and an \textbf{estimate} of $\theta$ respectively, where $\mathbf{x}$ is an observed value of $\mathbf{X}$.
	\end{enumerate}
\end{defn}
\begin{rem}
	We usually denote an estimator of $\theta$ by $\hat{\theta}(\mathbf{X})$ or simply $\hat{\theta}$.
\end{rem}
\begin{rem}
	Since $T(\mathbf{X})$ is also random, we have a distribution of $T(\mathbf{X})$ called \textbf{sampling distribution}.
\end{rem}

\section{Moments}
The population moments of a distribution play an important role in theoretical and applied statistics. Let us first define what a moment is.
\begin{defn}
	Given a random variable $X$.
	\begin{enumerate}
		\item If the random variable is discrete with a PMF $p_{X}(x)$, then the \textbf{expectation} or \textbf{population mean} of $X$ is defined by:
		\begin{equation*}
			\mu=\E(X)=\sum_{x}xp_{X}(x)
		\end{equation*}
		\item If the random variable is continuous with a PDF $f_{X}(x)$, then the \textbf{expectation} or \textbf{population mean} of $X$ is defined by:
		\begin{equation*}
			\mu=\E(X)=\int_{-\infty}^{\infty}xf_{X}(x)
		\end{equation*}
	\end{enumerate}
\end{defn}
\begin{rem}
	One may simplify $\E[(X-a)^{k}]$ to $\E(X-a)^{k}$. Just don't confuse it with $[\E(X-a)]^{k}$.
\end{rem}
\begin{lem}\named{Linearity of expectation}
	Given a set of random variables $\{X_{1},\cdots,X_{n}\}$. For any numbers $a_{i}$, it satisfies:
	\begin{equation*}
		\E\left(\sum_{i=1}^{n}a_{i}X_{i}\right)=\sum_{i=1}^{n}a_{i}\E\left(X_{i}\right)
	\end{equation*}
\end{lem}
\begin{lem}
	Given a set of \text{independent} random variables. It satisfies:
	\begin{equation*}
		\E\left(\prod_{i=1}^{n}X_{i}\right)=\prod_{i=1}^{n}\E(X_{i})
	\end{equation*}
\end{lem}
\begin{defn}
	Given a random variable $X$. The \textbf{population variance} of $X$ is defined by:
	\begin{equation*}
		\sigma^{2}=\Var(X)=\E(X-\E(X))^{2}=\E(X^{2})-[\E(X)]^{2}
	\end{equation*}
\end{defn}
\begin{lem}
	Given a set of \textit{independent} random variables $\{X_{1},\cdots,X_{n}\}$. For any numbers $a_{i}$, it satisfies:
	\begin{equation*}
		\Var\left(\sum_{i=1}^{n}a_{i}X_{i}\right)=\sum_{i=1}^{n}a_{i}^{2}\Var(X_{i})
	\end{equation*}
\end{lem}
\begin{defn}
	Given two random variables $X$ and $Y$. The \text{covariance} of $X$ and $Y$ is defined by:
	\begin{equation*}
		\cov(X,Y)=\E(X-\E(X))\E(Y-\E(Y))=\E(XY)-\E(X)\E(Y)
	\end{equation*}
\end{defn}

\newpage
From these definitions, we may generalize it into many population moments.
\begin{defn}
	For each positive integer $k$, 
	\begin{enumerate}
		\item the \textbf{$k$-th population moment} of $X$ about $0$, denoted by $\mu_{k}'$, is defined by:
		\begin{equation*}
			\mu_{k}'=\E(X^{k})
		\end{equation*}
		if the expectation exists.
		\item the \textbf{$k$-th population central moment} of $X$, denoted by $\mu_{k}$, is defined by:
		\begin{equation*}
			\mu_{k}=\E(X-\mu)^{k}
		\end{equation*}
		if the expectation exists.
	\end{enumerate}
\end{defn}
\begin{rem}
	Don't confuse the population mean $\mu$ and the $k$-th population central moment $\mu_{k}$!
\end{rem}
\begin{eg}
	We have terminologies for some useful population moments.
	\begin{enumerate}
		\item \textbf{Skewness}: $\mu_{3}$, a measure of symmetry or skewness.
		\begin{enumerate}
			\item If $\mu_{3}<0$, then the curve is left-skewed (tail is on the left).
			\item If $\mu_{3}>0$, then the curve is right-skewed (tail is on the right).
			\item If $\mu_{3}=0$, the the curve is symmetrical.
		\end{enumerate}
		The ratio $\frac{\mu_{3}}{\sigma^{3}}$ is called the \textbf{coefficient of skewness}.
		\item \textbf{Kurtosis}: $\mu_{4}$, a measure of excess or kurtosis, which is the degree of flatness of a density near its center. We called $\frac{\mu_{4}}{\sigma^{4}}-3$ the \textbf{coefficient of kurtosis}.
		\begin{enumerate}
			\item If $\frac{\mu_{4}}{\sigma^{4}}-3>0$, then the density has a sharper peak than the density of the normal curve.
			\item If $\frac{\mu_{4}}{\sigma^{4}}-3<0$, then the density has a flatter peak than the density of the normal curve.
		\end{enumerate}
	\end{enumerate}
\end{eg}
We usually use sample moments to estimate the population moments.
\begin{defn}
	Let $X_{1},\cdots,X_{n}$ be a random sample of size $n$. For each positive integer $k$,
	\begin{enumerate}
		\item the \textbf{$k$-th sample moment} about $0$, denoted by $\overline{X^{k}}$, is defined as:
		\begin{equation*}
			\overline{X^{k}}=\frac{1}{n}\sum_{i=1}^{n}X_{i}^{k}
		\end{equation*}
		When $k=1$, $\overline{X}$ is called the \textbf{sample mean} of $X$.
		\item the \textbf{$k$-th sample moment} about $\overline{X}$, denoted by $S_{n}^{k}$, is defined as:
		\begin{equation*}
			S_{n}^{k}=\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\overline{X})^{k}
		\end{equation*}
	\end{enumerate}
\end{defn}
\begin{eg}
	When $k=2$, $S_{n}^{2}$ is called the \textbf{sample variance}. However, we usually don't use this version of sample variance. Instead, we use another sample variance, denoted by $S_{n-1}^{2}$, which is defined by:
	\begin{equation*}
		S_{n-1}^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}
	\end{equation*}
	We do this because $S_{n-1}^{2}$ is unbiased while $S_{n}^{2}$ is not unbiased.
\end{eg}

\newpage
\begin{lem}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$. We have:
	\begin{equation*}
		\E(\overline{X^{k}})=\mu_{k}'
	\end{equation*}
	if $\mu_{k}'$ exists. We also have:
	\begin{equation*}
		\Var(\overline{X^{k}})=\frac{1}{n}[\mu_{2k}'-(\mu_{k}')^{2}]
	\end{equation*}
\end{lem}
\begin{proofing}
	Since $X_{1},\cdots,X_{n}$ have the same distribution, we have:
	\begin{equation*}
		\E(X_{1}^{k})=\cdots=\E(X_{n}^{k})=\E(X^{k})=\mu_{k}'
	\end{equation*}
	Therefore, we have:
	\begin{equation*}
		\E(\overline{X^{k}})=\E\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}^{k}\right)=\frac{1}{n}\sum_{i=1}^{n}\E(X_{i}^{k})=\mu_{k}'
	\end{equation*}
	Since $X_{1}^{k},\cdots,X_{n}^{k}$ are independent,
	\begin{equation*}
		\Var(\overline{X^{k}})=\Var\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}^{k}\right)=\frac{1}{n^{2}}\sum_{i=1}^{n}\Var(X_{i}^{k})=\frac{1}{n^{2}}\sum_{i=1}^{n}\left[\E(X_{i}^{2k})-[\E(X_{i}^{k})]^{2}\right]=\frac{1}{n}[\mu_{2k}'-(\mu_{k}')^{2}]
	\end{equation*}
\end{proofing}

\section{Conditional distribution}
Sometimes, we would deal with cases when a certain information is given. 
\begin{defn}
	Suppose that $X$ and $Y$ are two random variables. The \textbf{conditional distribution function} of $Y$ given $X=x$ for any $x$ such that PMF $p_{X}(x)>0$ or PDF $f_{X}(x)>0$ is defined by:
	\begin{equation*}
		F_{Y|X}(y|x)=\prob(Y\leq y|X=x)
	\end{equation*}
	The \textbf{conditional PDF/PMF} of $Y$ given $X=x$ for any $x$ such that PMF $p_{X}(x)>0$ or PDF $f_{X}(x)>0$ is defined by:
	\begin{equation*}
		\begin{cases}
			p_{Y|X}(y|x)=\frac{p_{X,Y}(x,y)}{p_{X}(x)}, &\text{Discrete case}\\
			f_{Y|X}(y|x)=\frac{f_{X,Y}(x,y)}{f_{X}(x)}, &\text{Continuous case}
		\end{cases}
	\end{equation*}
\end{defn}
The conditional distribution has its corresponding expectation.
\begin{defn}
	Suppose that $X$ and $Y$ are two random variables. The \textbf{conditional expectation} of $Y$ given $X=x$ for any $x$ such that PMF $p_{X}(x)>0$ or PDF $f_{X}(x)>0$ is defined by:
	\begin{equation*}
		\E(Y|X=x)=\begin{cases}
			\sum_{y}yp_{Y|X}(y|x), &\text{Discrete case}\\
			\intinfty yf_{Y|X}(y|x)\,dy, &\text{Continuous case}
		\end{cases}
	\end{equation*}
\end{defn}
\begin{rem}
	$\E(Y|X=x)$ is a function of $x$. Similarly, $\E(Y|X)$ is a function of $X$.
\end{rem}
\begin{eg}
	Suppose that the joint PDF of $X$ and $Y$ is given by:
	\begin{equation*}
		f_{X,Y}(x,y)=\begin{cases}
			\frac{1}{y}e^{x}, &x>0, y>0\\
			0, &\text{Otherwise}
		\end{cases}
	\end{equation*}
	We want to compute $\E(X|Y=y)$. We may find that:
	\begin{align*}
		f_{Y}(y)&=\intinfty f_{X,Y}(x,y)\,dx=e^{-y}\intinfty\frac{1}{y}e^{-\frac{x}{y}}\,dx=e^{-y} & f_{X|Y}(x|y)&=\frac{f_{X,Y}(x,y)}{f_{Y}(y)}=\frac{1}{y}e^{-\frac{x}{y}}
	\end{align*}
	We may see that $(X|Y=y)\sim\Exp(\frac{1}{y})$. Therefore, $\E(X|Y=y)=y$.
\end{eg}

\newpage
Conditional expectation has the following properties.
\begin{lem}
	\label{Chapter 1 (Lemma) Properties of conditional expectation}
	Suppose that $X$, $Y$ and $Z$ are three random variables. The conditional expectation has the following properties:
	\begin{enumerate}
		\item $\E(aY+bZ|X)=a\E(Y|X)+b\E(Z|X)$ for $a,b\in\mathbb{R}$.
		\item $\E(Y|X)\geq 0$ if $Y\geq 0$.
		\item If $X$ and $Y$ are independent, then $\E(Y|X)=\E(Y)$.
	\end{enumerate}
\end{lem}
\begin{proofing}
	Proof for discrete case is similar to continuous case.
	\begin{enumerate}
		\item 
		\begin{align*}
			\E(aY+bZ|X)&=\intinfty\intinfty(ay+bz)f_{Y,Z|X}(y,z|X)\,dy\,dz\\
			&=a\intinfty\intinfty yf_{Y,Z|X}(y,z|X)\,dy\,dz+b\intinfty\intinfty zf_{Y,Z|X}(y,z|X)\,dy\,dz\\
			&=a\intinfty yf_{Y|X}(y|X)\,dy+b\intinfty zf_{Z|X}(z|X)\,dz=a\E(Y|X)+b\E(Z|X)
		\end{align*}
		\item If $Y\geq 0$, then since $f_{Y|X}(y|x)\geq 0$ for any $x$ such that $f_{X}(x)>0$:
		\begin{equation*}
			\E(Y|X)=\int_{0}^{\infty}yf_{Y|X}(y|X)\,dy\geq 0
		\end{equation*} 
		\item If $X$ and $Y$ are independent, then:
		\begin{equation*}
			\E(Y|X)=\intinfty yf_{Y|X}(y|X)\,dy=\intinfty yf_{Y}(y)\,dy=\E(Y)
		\end{equation*}
	\end{enumerate}
\end{proofing}
If $\E(Y|X)$ is a function of $X$, then what is its expectation?
\begin{thm}\named{Law of total expectation}
	Given two random variables $X$ and $Y$. We have:
	\begin{equation*}
		\E(Y)=\E(\E(Y|X))
	\end{equation*}
	if both expectations exist. 
\end{thm}
\begin{proofing}
	We may proof in continuous case. Discrete case works similarly.
	\begin{align*}
		\E(\E(Y|X))=\intinfty\E(Y|X=x)f_{X}(x)\,dx&=\intinfty\intinfty y f_{Y|X}(y|x)f_{X}(x)\,dy\,dx\\
		&=\intinfty\intinfty yf_{Y|X}(y|x)f_{X}(x)\,dx\,dy\\
		&=\intinfty\intinfty yf_{X,Y}(x,y)\,dx\,dy\\
		&=\intinfty yf_{Y}(y)\,dy=\E(Y)
	\end{align*}
\end{proofing}
The following theorem is the generalization of Law of total expectation. We will omit the proof.
\begin{lem}
	\label{Chapter 1 (Lemma) Generalization of Law of total expectation}
	Given two random variables $X$ and $Y$. For any function $g$, we have:
	\begin{equation*}
		\E(\E(Y|X)g(X))=\E(Yg(X))
	\end{equation*}
	if both expectations exist.
\end{lem}

\newpage
Similarly, we have a conditional variance.
\begin{defn}
	Given two random variables $X$ and $Y$. The \textbf{conditional variance} of $Y$ given $X$ is defined by:
	\begin{equation*}
		\Var(Y|X)=\E\left[(Y-\E(Y|X))^{2}|X\right]
	\end{equation*}
\end{defn}
\begin{lem}
	\label{Chapter 1 (Lemma) Conditional variance decomposition}
	Given two random variables $X$ and $Y$. We have:
	\begin{equation*}
		\Var(Y|X)=\E(Y^{2}|X)-[\E(Y|X)]^{2}
	\end{equation*}
\end{lem}
\begin{proofing}
	By Lemma \ref{Chapter 1 (Lemma) Properties of conditional expectation} and Lemma \ref{Chapter 1 (Lemma) Generalization of Law of total expectation},
	\begin{align*}
		\Var(Y|X)&=\E\left[(Y-\E(Y|X))^{2}|X\right]\\
		&=\E(Y^{2}|X)-2\E(Y\E(Y|X)|X)+\E((\E(Y|X))^{2}|X)\\
		\tag{$\E(Y|X)$ is a function of $X$}
		&=\E(Y^{2}|X)-[\E(Y|X)]^{2}
	\end{align*}
\end{proofing}
We have the law of total variance.
\begin{thm}\named{Law of total variance}
	Given two random variables $X$ and $Y$. We have:
	\begin{equation*}
		\Var(Y)=\E[\Var(Y|X)]+\Var(\E(Y|X))
	\end{equation*}
	if the expectation and variances exist.
\end{thm}
\begin{proofing}
	By Lemma \ref{Chapter 1 (Lemma) Conditional variance decomposition} and Law of total expectation,
	\begin{align*}
		\E[\Var(Y|X)]+\Var(\E(Y|X))&=\E[\E(Y^{2}|X)]-\E[\E(Y|X)]^{2}+\E[\E(Y|X)]^{2}-\left[\E(\E(Y|X))\right]^{2}\\
		&=\E(Y^{2})-[\E(Y)]^{2}=\Var(Y)
	\end{align*}
\end{proofing}

\section{Commonly used distribution}
The indicator function is highly important later.
\begin{defn}
	Indicator function of a set $A$ is a function $\mathbf{1}_{A}$ defined as:
	\begin{equation*}
		\mathbf{1}_{A}(x)=\begin{cases}
			1, &x\in A\\
			0, &x\not\in A
		\end{cases}
	\end{equation*}
\end{defn}
Let us recall some useful distributions.
\begin{eg}\named{Bernoulli distribution} $X\sim\Bern(p)$\\
	Random variable $X$ is a Bernoulli random variable with parameters $p\in[0,1]$ if it has a PMF:
	\begin{align*}
		p_{X}(x)&=\begin{cases}
			p^{x}(1-p)^{1-x}, &x=\{0,1\}\\
			0, &\text{Otherwise}
		\end{cases} & \E(X)&=p & \Var(X)&=p(1-p)
	\end{align*}
\end{eg}
\begin{eg}\named{Binomial distribution} $X\sim\Bin(n,p)$\\
	Random variable $X$ is a Binomial random variable with parameters $n\in\mathbb{N}$ and $p\in[0,1]$ if it has a PMF for $x=0,\cdots,n$:
	\begin{align*}
		p_{X}(x)&=\binom{n}{x}p^{x}(1-p)^{n-x} & \E(X)&=np & \Var(X)&=np(1-p)
	\end{align*}
\end{eg}

\newpage
\begin{eg}\named{Geometric distribution} $X\sim\Geom(p)$\\
	Random variable $X$ is geometric with parameter $p\in[0,1]$ if it has a PMF for $x=1,2,\cdots$:
	\begin{align*}
		p_{X}(x)&=p(1-p)^{x-1} & \E(X)&=\frac{1}{p} & \Var(X)&=\frac{1-p}{p^{2}}
	\end{align*}
\end{eg}
\begin{eg}\named{Poisson distribution} $X\sim\Poisson(\lambda)$\\
	Random variable $X$ is a Poisson random variable with parameter $\lambda$ if it has a PMF for $x=0,1,\cdots$:
	\begin{align*}
		p_{X}(x)&=\frac{\lambda^{x}}{x!}e^{-\lambda} & \E(X)&=\lambda & \Var(X)&=\lambda
	\end{align*}
\end{eg}
\begin{eg}\named{Negative Binomial distribution} $X\sim\NBin(r,p)$\\
	Assume that $X_{1},\cdots,X_{r}$ are independent and $X_{i}\sim\Geom(p)$ for $i=1,\cdots,r$. Let $Y=\sum_{i=1}^{n}X_{i}$. Random variable $Y$ is negative Binomial with parameters $r>0$ and $p\in[0,1]$ if for $x>r$:
	\begin{align*}
		p_{X}(x)&=\binom{x-1}{r-1}(1-p)^{x-r}p^{r} & \E(X)&=\frac{r}{p} & \Var(X)&=\frac{r(1-p)}{p^{2}}
	\end{align*}
\end{eg}
\begin{eg}\named{Cauchy distribution} $X\sim\Cauchy(\theta)$\\
	Random variable $X$ is a Cauchy random variable with parameter $\theta$ if it has a PDF:
	\begin{align*}
		f_{X}(x)&=\frac{1}{\pi(1+(x-\theta)^{2})} & \E(X)&\text{ DNE} & \Var(X)&\text{ DNE}
	\end{align*}
\end{eg}
\begin{eg}\named{Uniform distribution} $X\sim\U[a,b]$\\
	Random variable $X$ is uniform if given $a<b$, it has a PDF:
	\begin{align*}
		f_{X}(x)&=\begin{cases}
			\frac{1}{b-a}, &x\in[a,b]\\
			0, &\text{Otherwise}
		\end{cases} & \E(X)&=\frac{a+b}{2} & \Var(X)&=\frac{(b-a)^{2}}{12}
	\end{align*}
\end{eg}
\begin{eg}\named{Exponential distribution} $X\sim\Exp(\lambda)$\\
	Random variable $X$ is exponential with parameter $\lambda$ if it has a PDF:
	\begin{align*}
		f_{X}(x)&=\begin{cases}
			\lambda e^{-\lambda x}, &x\geq 0\\
			0, &x<0
		\end{cases} & \E(X)&=\frac{1}{\lambda} & \Var(X)&=\frac{1}{\lambda^{2}}
	\end{align*}
\end{eg}
\begin{eg}\named{Normal distribution / Gaussian distribution} $X\sim\N(\mu,\sigma^{2})$\\
	Random variable $X$ is normal if it has two parameters $\mu$ and $\sigma^{2}$, and its PDF and CDF is:
	\begin{align*}
		f_{X}(x)&=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right) & F_{X}(x)&=\intlu{-\infty}{x}f_{X}(u)\,du & \E(X)&=\mu & \Var(X)&=\sigma^{2}
	\end{align*}
	Random variable $Z$ is standard normal if it is normal and $\mu=0$, $\sigma^{2}=1$. ($Z\sim\N(0,1)$)
	\begin{align*}
		f_{Z}(z)&=\phi(z)=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{z^{2}}{2}\right) & F_{Z}(z)&=\Phi(z)=\intlu{-\infty}{z}\phi(u)\,du & \E(Z)&=0 & \Var(Z)&=1
	\end{align*}
	We define $z_{\alpha}$ by:
	\begin{equation*}
		\prob(Z\geq z_{\alpha})=\alpha
	\end{equation*}
\end{eg}

\newpage
\begin{eg}\named{Gamma distribution} $X\sim\Gam(\alpha,\beta)$\\
	Random variable $X$ is a gamma random variable with parameters $\alpha>0$ and $\beta>0$ if its PDF is:
	\begin{align*}
		f_{X}(x)&=\begin{cases}
			\frac{1}{\Gamma(\alpha)}\beta^{\alpha}x^{\alpha-1}e^{-\beta x}, &x\geq 0\\
			0, &\text{Otherwise}
		\end{cases} & \E(X)&=\frac{\alpha}{\beta} & \Var(X)&=\frac{\alpha}{\beta^{2}}
	\end{align*}
\end{eg}
\begin{rem}
	For any $z$, gamma function $\Gamma(z)$ has the following properties:
	\begin{enumerate}
		\item $\Gamma(z+1)=z\Gamma(z)$. If $z$ is a positive integer, then $\Gamma(z)=(z-1)!$.
		\item If $\Re(z)>0$, then $\Gamma(z)=\int_{0}^{\infty}t^{z-1}e^{-t}\,dt$.
		\item $\Gamma(\frac{1}{2})=\sqrt{\pi}$.
	\end{enumerate}
\end{rem}
\begin{eg}\named{Beta distribution} $X\sim\Bet(\alpha,\beta)$\\
	Random variable $X$ is a beta random variable with parameters $\alpha>0$ and $\beta>0$ if its PDF is: 
	\begin{align*}
		f_{X}(x)&=\begin{cases}
			\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}, &x\in(0,1)\\
			0, &\text{Otherwise}
		\end{cases} & \E(X)&=\frac{\alpha}{\alpha+\beta} & \Var(X)&=\frac{\alpha\beta}{(\alpha+\beta)^{2}(\alpha+\beta+1)}
	\end{align*}
\end{eg}
\begin{rem}
	For any $z_{1},z_{2}$, beta function $B(z_{1},z_{2})$ has the following properties:
	\begin{enumerate}
		\item $B(z_{1},z_{2})=\frac{\Gamma(z_{1})\Gamma(z_{2})}{\Gamma(z_{1}+z_{2})}$
		\item $B(z_{1},z_{2})=\int_{0}^{1}t^{z_{1}-1}(1-t)^{z_{2}-1}\,dt$
	\end{enumerate}
\end{rem}
We have some more distributions that are associated with normal distribution. For example, Chi-squared distribution, which is a special case of gamma distribution.
\begin{eg}\named{Chi-squared distribution} $Y\sim\chi^{2}(n)$\\
	Assume that $X_{1},X_{2},\cdots,X_{n}$ are independent and $X_{i}\sim\N(0,1)$ for $i=1,\cdots,n$. Let $Y=\sum_{i=1}^{n}X_{i}^{2}$. Random variable $Y$ has a $\chi^{2}$-distribution with $n$ degree of freedom if:
	\begin{align*}
		f_{Y}(x)&=\begin{cases}
			\frac{1}{\Gamma(\frac{n}{2})}2^{-\frac{n}{2}}x^{\frac{n}{2}-1}e^{-\frac{x}{2}}, &x>0\\
			0, \text{Otherwise}
		\end{cases} & \E{Y}&=n & \Var(Y)&=2n
	\end{align*}
	We define $\chi^{2}_{\alpha,n}$ by:
	\begin{equation*}
		\prob(Y\geq\chi^{2}_{\alpha,n})=\alpha
	\end{equation*}
\end{eg} 
\begin{thm}
	\label{Chapter 1 (Theorem) chi-square distribution with 1 degree of freedom}
	If random variable $X\sim\N(\mu,\sigma^{2})$, where $\sigma^{2}>0$, then random variable $V=\frac{(X-\mu)^{2}}{\sigma^{2}}\sim\chi^{2}(1)$.
\end{thm}
\begin{proofing}
	By the properties of normal distribution, we get that:
	\begin{equation*}
		\frac{X-\mu}{\sigma}\sim\N(0,1)
	\end{equation*}
	Therefore, by the definition of chi-squared distribution,
	\begin{equation*}
		V=\left(\frac{X-\mu}{\sigma}\right)^{2}\sim\chi^{2}(1)
	\end{equation*}
\end{proofing}
\begin{thm}
	Given a set of random variables $\{X_{1},\cdots,X_{k}\}$. Let $Y=\sum_{i=1}^{k}X_{i}$ and $X_{i}\sim\chi^{2}(r_{i})$ for all $i=1,\cdots,k$. If they are independent, then $Y\sim\chi^{2}(r_{1}+\cdots+r_{k})$.
\end{thm}
\begin{proofing}
	It suffices to prove that if $Z_{1}\sim\chi^{2}(n_{1})$ and $Z_{2}\sim\chi^{2}(n_{2})$, then $Z_{1}+Z_{2}\sim\chi^{2}(n_{1}+n_{2})$. By repeating applying the relation for two random variables, one can easily have the desired relation for $k$ random variables.
	
	From definition, $Z_{1}=X_{11}^{2}+\cdots+X_{1n_{1}}^{2}$ and $Z_{2}=X_{21}^{2}+\cdots+X_{2n_{2}}^{2}$, where $X_{1i}\sim\N(0,1)$ and $X_{2j}\sim\N(0,1)$ for $i=1,\cdots,n_{1}$ and $j=1,\cdots,n_{2}$. Therefore,
	\begin{equation*}
		Z_{1}+Z_{2}=X_{11}^{2}+\cdots+X_{1n_{1}}^{2}+X_{21}^{2}+\cdots+X_{2n_{2}}^{2}\sim\chi^{2}(n_{1}+n_{2})
	\end{equation*}
\end{proofing}
\begin{thm}
	\label{Chapter 1 (Theorem) Normal and chi-squared distribution related to sample mean and variance}
	If $\{X_{1},\cdots,X_{n}\}$ is a random sample of size $n>1$ of a random variable $X\sim\N(\mu,\sigma^{2})$, then we have:
	\begin{enumerate}
		\item Sample mean $\overline{X}\sim\N(\mu,\frac{\sigma^{2}}{n})$
		\item Sample mean $\overline{X}$ and sample variance $S_{n-1}^{2}$ are independent.
		\item 
		\begin{equation*}
			\frac{(n-1)S_{n-1}^{2}}{\sigma^{2}}=\frac{nS_{n}^{2}}{\sigma^{2}}=\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}\sim\chi^{2}(n-1)
		\end{equation*}
	\end{enumerate}
\end{thm}
\begin{proofing}
	\begin{enumerate}
		\item From definition,
		\begin{equation*}
			\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}
		\end{equation*}
		Since $X_{i}\sim\N(\mu,\sigma^{2})$ for $i=1,\cdots,n$, we can find that $\overline{X}\sim\N(\mu,\frac{\sigma^{2}}{n})$.
		\item Let $\mathbf{X}=(X_{1}\ \cdots\ X_{n})^{T}$. We may find that:
		\begin{align*}
			\begin{pmatrix}
				\overline{X}\\ X_{1}-\overline{X}\\ X_{2}-\overline{X}\\ \vdots\\ X_{n}-\overline{X}
			\end{pmatrix}=\begin{pmatrix}
				\frac{1}{n}X_{1}+\frac{1}{n}X_{2}+\cdots+\frac{1}{n}X_{n}\\
				\left(1-\frac{1}{n}\right)X_{1}-\frac{1}{n}X_{2}-\cdots-\frac{1}{n}X_{n}\\
				-\frac{1}{n}X_{1}+\left(1-\frac{1}{n}\right)X_{2}-\cdots-\frac{1}{n}X_{n}\\
				\vdots\\
				-\frac{1}{n}X_{1}-\frac{1}{n}X_{2}-\cdots+\left(1-\frac{1}{n}\right)X_{n}
			\end{pmatrix}&=\mathbf{AX} & \mathbf{A}&=\begin{pmatrix}
				\frac{1}{n} & \frac{1}{n} & \hdots & \frac{1}{n}\\
				1-\frac{1}{n} & -\frac{1}{n} & \hdots & -\frac{1}{n}\\
				-\frac{1}{n} & 1-\frac{1}{n} & \ddots & \vdots\\
				\vdots & \ddots & \ddots & -\frac{1}{n}\\
				-\frac{1}{n} & \hdots & -\frac{1}{n} & 1-\frac{1}{n}
			\end{pmatrix}
		\end{align*}
		By Lemma \ref{Chapter 1 (Lemma) Distribution of matrix multiplication with random vector}, we have $\mathbf{AX}\sim\N_{n+1}(\mathbf{A}\boldsymbol{\mu},\mathbf{A\sigma^{2}I_{n\times n}A}^{T})$, where $\boldsymbol{\mu}=(\mu\ \cdots\ \mu)^{T}$.\\
		Let $\mathbf{X^{*}}=(X_{1}-\overline{X}\ \cdots\ X_{n}-\overline{X})^{T}$ and $\mathbf{\Sigma^{*}}$ be the variance-covariance matrix of $\mathbf{X}^{*}$. We can notice that:
		\begin{equation*}
			\mathbf{A\sigma^{2}I_{n\times n}A}^{T}=\begin{pmatrix}\begin{array}{c|c}
					\Var(\overline{X}) & \cov(\mathbf{X^{*}},\overline{X})\\
					\hline
					\cov(\mathbf{X^{*}},\overline{X}) & \mathbf{\Sigma^{*}}
			\end{array}\end{pmatrix}
		\end{equation*}
		Since $X_{i}$ are independent for all $i$,
		\begin{equation*}
			\cov(X_{i}-\overline{X},\overline{X})=\cov(X_{i},\overline{X})-\Var(\overline{X})=\frac{1}{n}\Var(X_{i})-\frac{\sigma^{2}}{n}=0
		\end{equation*}
		Therefore, we can find that $\cov(\mathbf{X^{*}},\overline{X})=0$. By Lemma \ref{Chapter 1 (Lemma) Independence iff uncorrelated and bivariate}, $\overline{X}$ and $\mathbf{X^{*}}$ are independent.\\
		Since $S_{n-1}^{2}$ is a function of $\mathbf{X^{*}}$, we can conclude that $\overline{X}$ and $S_{n-1}^{2}$ are independent.
		\item We have:
		\begin{equation*}
			\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}=\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\mu+\mu-\overline{X})^{2}=\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\mu)^{2}-\frac{n(\overline{X}-\mu)^{2}}{\sigma^{2}}=\sum_{i=1}^{n}\left(\frac{X_{i}-\mu}{\sigma}\right)-\left(\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}\right)^{2}
		\end{equation*}
		Let $U=\sum_{i=1}^{n}\left(\frac{X_{i}-\mu}{\sigma}\right)$ and $V=\left(\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}\right)^{2}$. The distribution we are finding is $U-V$.\\
		From definition, we know that $U\sim\chi^{2}(n)$. From Theorem \ref{Chapter 1 (Theorem) chi-square distribution with 1 degree of freedom}, we find the $V\sim\chi^{2}(1)$. From Part 2, since functions of $\mathbf{X}^{*}$ and $\overline{X}$ are independent, 
		\begin{equation*}
			M_{U-V}(t)=\frac{M_{U}(t)}{M_{V}(t)}=\frac{(1-2t)^{-\frac{n}{2}}}{(1-2t)^{-\frac{1}{2}}}=(1-2t)^{-\frac{n-1}{2}}
		\end{equation*}
		Therefore, we can conclude that:
		\begin{equation*}
			\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}\sim\chi^{2}(n-1)
		\end{equation*}
	\end{enumerate}
\end{proofing}
\begin{rem}
	From the same proof of above theorem part 2, we can also find that $\overline{X}$ and $S_{n}^{2}$ are independent.
\end{rem}
\begin{eg}\named{Student's t-distribution} $T\sim t(r)$\\
	Assume that $X\sim\N(0,1)$ and $Y\sim\chi^{2}(r)$. Let:
	\begin{equation*}
		T=\frac{X}{\sqrt{\frac{Y}{r}}}
	\end{equation*}
	Then $T$ has a t-distribution with $t$ degree of freedom and:
	\begin{align*}
		f_{T}(t)&=\frac{\Gamma\left(\frac{r+1}{2}\right)}{\sqrt{r\pi}\Gamma\left(\frac{r}{2}\right)}\left(1+\frac{t^{2}}{r}\right)^{-\frac{r+1}{2}} & \E(T)&=\begin{cases}
			\text{Undefined}, &r\leq 1\\
			0, &n>1\\
		\end{cases} & \Var(T)&=\begin{cases}
			\text{Undefined}, &r\leq 1\\
			\infty, &1<r\leq 2\\
			\frac{r}{r-2}, &r>2
		\end{cases}
	\end{align*}
	We define $t_{\alpha,r}$ by:
	\begin{equation*}
		\prob(T\geq t_{\alpha,r})=\alpha
	\end{equation*}
\end{eg}
\begin{rem}
	As $r\to\infty$, $T\to\N(0,1)$ by CLT.
\end{rem}
\begin{rem}
	If we fix $Y=y$, then we can find that $T\sim\N(0,\frac{r}{y})$.
\end{rem}
The t-distribution has the following properties.
\begin{thm}
	If $\{X_{1},\cdots,X_{n}\}$ is a random sample of size $n>1$ of random variable $X\sim\N(\mu,\sigma^{2})$, then we have:
	\begin{equation*}
		\frac{\sqrt{n}(\overline{X}-\mu)}{S_{n-1}}\sim t(n-1)
	\end{equation*}
\end{thm}
\begin{proofing}
	From Theorem \ref{Chapter 1 (Theorem) Normal and chi-squared distribution related to sample mean and variance}, $\overline{X}$ and $S_{n-1}^{2}$ are independent, and:
	\begin{align*}
		\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}&\sim\N(0,1) & \frac{(n-1)S_{n-1}^{2}}{\sigma^{2}}\sim\chi^{2}(n-1)
	\end{align*}
	Therefore, from definition,
	\begin{equation*}
		\frac{\sqrt{n}(\overline{X}-\mu)}{S_{n-1}}=\frac{\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}}{\sqrt{\frac{1}{n-1}\left(\frac{(n-1)S_{n-1}^{2}}{\sigma^{2}}\right)}}\sim t(n-1)
	\end{equation*}
\end{proofing}
\begin{eg}
	Assume that we want to find the $95\%$ confidence interval of $\mu$ without knowing the population variance $\sigma^{2}$. Then we can find:
	\begin{align*}
		0.95&=\prob\left(-t_{0.025,n-1}\leq\frac{\sqrt{n}(\overline{X}-\mu)}{S_{n-1}}\leq t_{0.025,n-1}\right)\\
		&=\prob\left(\overline{X}-t_{0.025,n-1}\frac{S_{n-1}}{\sqrt{n}}\leq\mu\leq\overline{X}+t_{0.025,n-1}\frac{S_{n-1}}{\sqrt{n}}\right)
	\end{align*}
	Therefore, we can find the $95\%$ confidence interval:
	\begin{equation*}
		\left(\overline{x}-t_{0.025,n-1}\frac{s_{n-1}}{\sqrt{n}},\overline{x}+t_{0.025,n-1}\frac{s_{n-1}}{\sqrt{n}}\right)
	\end{equation*}
	Usually when $n>30$, $t_{0.025,n-1}\approx z_{0.025}$. Therefore, we may find the $95\%$ confidence interval:
	\begin{equation*}
		\left(\overline{x}-z_{0.025}\frac{s_{n-1}}{\sqrt{n}},\overline{x}+z_{0.025}\frac{s_{n-1}}{\sqrt{n}}\right)
	\end{equation*}
\end{eg}

\newpage
\begin{eg}\named{F distribution} $F\sim F(r_{1},r_{2})$\\
	Assume that $X$ and $Y$ are independent random variables with $X\sim\chi^{2}(r_{1})$ and $Y\sim\chi^{2}(r_{2})$. Let:
	\begin{equation*}
		F=\frac{\frac{X}{r_{1}}}{\frac{Y}{r_{2}}}
	\end{equation*}
	Then $F$ has a F-distribution with $r_{1}$ and $r_{2}$ degrees of freedom with:
	\begin{equation*}
		f_{F}(w)=\frac{\Gamma\left(\frac{r_{1}+r_{2}}{2}\right)}{\Gamma\left(\frac{r_{1}}{2}\right)\Gamma\left(\frac{r_{2}}{2}\right)}\left(\frac{r_{1}}{r_{2}}\right)^{\frac{r_{1}}{2}}w^{\frac{r_{1}}{2}-1}\left(1+\frac{r_{1}w}{r_{2}}\right)^{-\frac{r_{1}+r_{2}}{2}}
	\end{equation*}
	where $0<w<\infty$. We define $f_{\alpha,(r_{1},r_{2})}$ by:
	\begin{equation*}
		\prob(F\geq f_{\alpha,(r_{1},r_{2})})=\alpha
	\end{equation*}
\end{eg}
\begin{lem}
	Let $U\sim F(r_{1},r_{2})$. The F-distribution has the following properties:
	\begin{enumerate}
		\item $\frac{1}{U}\sim F(r_{2},r_{1})$
		\item If $f_{\alpha,(r_{1},r_{2})}$ is defined by $\prob(U\geq f_{\alpha,(r_{1},r_{2})})=\alpha$, then:
		\begin{equation*}
			\frac{1}{f_{\alpha,(r_{1},r_{2})}}=f_{1-\alpha,(r_{2},r_{1})}
		\end{equation*}
	\end{enumerate}
\end{lem}
\begin{proofing}
	\begin{enumerate}
		\item By definition,
		\begin{equation*}
			U=\frac{\frac{X}{r_{1}}}{\frac{Y}{r_{2}}}
		\end{equation*}
		where $X\sim\chi^{2}(r_{1})$ and $Y\sim\chi^{2}(r_{2})$. Therefore,
		\begin{equation*}
			\frac{1}{U}=\frac{\frac{Y}{r_{2}}}{\frac{X}{r_{1}}}\sim F(r_{2},r_{1})
		\end{equation*}
		\item With $\prob(U\geq f_{\alpha,(r_{1},r_{2})})=\alpha$, since $f_{U}(w)$ is only defined for $w>0$, we can get that:
		\begin{align*}
			\prob\left(\frac{1}{U}\leq\frac{1}{f_{\alpha,(r_{1},r_{2})}}\right)&=\alpha\\
			\prob\left(\frac{1}{U}>\frac{1}{f_{\alpha,(r_{1},r_{2})}}\right)&=1-\alpha
		\end{align*}
		From Part 1, we find that $\frac{1}{U}\sim F(r_{2},r_{1})$. Therefore,
		\begin{equation*}
			\prob\left(\frac{1}{U}\geq f_{1-\alpha,(r_{2},r_{1})}\right)=1-\alpha
		\end{equation*}
		We find that $\frac{1}{f_{\alpha,(r_{1},r_{2})}}=f_{1-\alpha,(r_{2},r_{1})}$
	\end{enumerate}
\end{proofing}

\newpage
\begin{eg}
	Assume that we try to compare two populations. Let $X_{1}\sim\N(\mu_{1},\sigma_{1}^{2})$ represents the random variable of the first population and $X_{2}\sim\N(\mu_{2},\sigma_{2}^{2})$ represents the random variable of the second population. We want to find an interval guess of their ratio of variance $\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}$.\\
	Let $\{X_{11},\cdots,X_{1n}\}$ be a random sample of size $n$ from $X_{1}$ and $\{X_{21},\cdots,X_{2m}\}$ be a random sample of size $m$ from $X_{2}$. We can find that:
	\begin{align*}
		\frac{(n-1)S_{n-1,1}^{2}}{\sigma_{1}^{2}}&\sim\chi^{2}(n-1) & \frac{(m-1)S_{m-1,2}^{2}}{\sigma_{2}^{2}}&\sim\chi^{2}(m-1)
	\end{align*}
	We can also find that $S_{n-1},1$ and $S_{m-1},2$ are independent since they are from different population. Therefore, we have:
	\begin{equation*}
		\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\left(\frac{S_{m-1,2}^{2}}{S_{n-1,1}^{2}}\right)=\frac{\frac{1}{m-1}\left(\frac{(m-1)S_{m-1,2}^{2}}{\sigma_{2}^{2}}\right)}{\frac{1}{n-1}\left(\frac{(n-1)S_{n-1,1}^{2}}{\sigma_{1}^{2}}\right)}\sim F(m-1,n-1)
	\end{equation*}
	Then we can find the $95\%$ confidence interval by:
	\begin{align*}
		0.95&=\prob\left(f_{0.975,(m-1,n-1)}\leq\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\left(\frac{S_{m-1,2}^{2}}{S_{n-1,1}^{2}}\right)\leq f_{0.025,(m-1,n-1)}\right)\\
		&=\prob\left(\frac{S_{n-1,1}^{2}}{S_{m-1,2}^{2}}f_{0.975,(m-1,n-1)}\leq\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\leq \frac{S_{n-1,1}^{2}}{S_{m-1,2}^{2}}f_{0.025,(m-1,n-1)}\right)
	\end{align*}
\end{eg}

\section{Moment generating function}
It would be useful if we have a function that could generate all moments.
\begin{defn}
	The \textbf{moment generating function} (MGF) of a random variable $X$, denoted by $M_{X}(t)$, is:
	\begin{equation*}
		M_{X}(t)=\E(e^{tX})
	\end{equation*}
	if the expectation exists for $t$ in some neighbourhood of $0$.
\end{defn}
\begin{rem}
	More precisely, there exists $h>0$ such that for all $t$ in $(-h,h)$, $\E(e^{tX})$ exists.
\end{rem}
\begin{rem}
	MGF of $X$ may not always exist. However, if it exists, then $M_{X}(t)$ is continuously differentiable in some neighbourhood of the origin.
\end{rem}
\begin{rem}
	If we replace $e^{tX}$ by its Taylor series, then we would get:
	\begin{equation*}
		M_{X}(t)=\E\left(\sum_{i=0}^{\infty}\frac{(tX)^{i}}{i!}\right)=\sum_{i=0}^{\infty}\frac{t^{i}}{i!}\E(X^{i})=\sum_{i=0}^{\infty}\frac{t^{i}}{i!}\mu_{i}'
	\end{equation*}
\end{rem}
\begin{lem}
	If $M_{X}(t)$ is the MGF of a random variable $X$, then:
	\begin{equation*}
		\left.\odv*[order={k}]{M_{X}(t)}{t}\right|_{t=0}=\E(X^{k})=\mu_{k}'
	\end{equation*}
\end{lem}
\begin{proofing}
	From the Taylor series of MGF, we would see that:
	\begin{equation*}
		\left.\odv*[order={k}]{M_{X}(t)}{t}\right|_{t=0}=\left.\sum_{i=k}^{\infty}\frac{t^{i-k}}{(i-k)!}\E(X^{i})\right|_{t=0}=\E(X^{k})
	\end{equation*}
\end{proofing}
\begin{eg}
	What is the MGF of $X\sim\Bern(p)$? We have:
	\begin{equation*}
		M_{X}(t)=\E(e^{tX})=e^{t(0)}(1-p)+e^{t(1)}(p)=pe^{t}+1-p
	\end{equation*}
\end{eg}
\begin{lem}
	Random variables $X$ and $Y$ are independent if and only if:
	\begin{equation*}
		M_{X,Y}(s,t)=M_{X}(s)M_{Y}(t)
	\end{equation*}
\end{lem}
\begin{lem}
	If random variables $X$ and $Y$ are independent, then:
	\begin{equation*}
		M_{X+Y}(t)=M_{X}(t)M_{Y}(t)
	\end{equation*}
\end{lem}
\begin{proofing}
	Since $X$ and $Y$ are independent,
	\begin{equation*}
		M_{X+Y}(t)=\E(e^{t(X+Y)})=\E(e^{tX})\E(e^{tY})=M_{X}(t)M_{Y}(t)
	\end{equation*}
\end{proofing}
\begin{eg}
	By definition, if $Y=\Bin(n,p)$, then $Y=X_{1}+\cdots+X_{n}$ where $X_{i}\sim\Bern(p)$ for all $i$ and they are independent. Therefore,
	\begin{equation*}
		M_{Y}(t)=\prod_{i=1}^{n}M_{X_{i}}(t)=(pe^{t}+1-p)^{n}
	\end{equation*}
	We may also solve it without using the definition.
	\begin{equation*}
		M_{Y}(t)=\E(e^{tY})=\sum_{i=0}^{n}\binom{n}{i}(pe^{t})^{i}(1-p)^{n-i}=(pe^{t}+1-p)^{n}
	\end{equation*}
\end{eg}
\begin{eg}
	Consider $X\sim\Poisson(\lambda)$, The MGF of $X$ can be obtained by:
	\begin{equation*}
		M_{X}(t)=\sum_{k=0}^{\infty}e^{tk}\frac{\lambda^{k}e^{-\lambda}}{k!}=e^{-\lambda}\sum_{k=0}^{\infty}\frac{(\lambda e^{t})^{k}}{k!}=e^{\lambda(e^{t}-1)}
	\end{equation*}
\end{eg}
\begin{eg}
	Consider $X\sim\Exp(\lambda)$. If $t<\lambda$, we have:
	\begin{equation*}
		M_{X}(t)=\E(e^{tX})=\int_{0}^{\infty}e^{tx}\lambda e^{-\lambda x}\,dx=\lambda\int_{0}^{\infty}e^{-(\lambda-t)x}\,dx=\frac{\lambda}{\lambda-t}
	\end{equation*}
\end{eg}
\begin{eg}
	What is the MGF of $X\sim\N(\mu,\sigma^{2})$? We may first find the MGF of $Z\sim\N(0,1)$.
	\begin{align*}
		M_{Z}(t)=\E(e^{tZ})=\intinfty\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(z^{2}-2tz)}\,dz=\intinfty\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}((z-t)^{2}-t^{2})}\,dz=e^{\frac{t^{2}}{2}}\intinfty\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(z-t)^{2}}\,dz=e^{\frac{t^{2}}{2}}
	\end{align*}
	Therefore, by having $X=\sigma Z+\mu$, we have:
	\begin{equation*}
		M_{X}(t)=\E(e^{tX})=e^{\mu t}\E(e^{t\sigma Z})=e^{\mu t+\frac{1}{2}\sigma^{2}t^{2}}
	\end{equation*}
\end{eg}
\begin{eg}
	Consider $X\sim\U[a,b]$ where $a<b$, we have:
	\begin{equation*}
		M_{X}(t)=\E(e^{tX})=\int_{a}^{b}\frac{e^{tx}}{b-a}\,dt=\left[\frac{e^{tx}}{t(b-a)}\right]_{a}^{b}=\frac{e^{bt}-e^{at}}{t(b-a)}
	\end{equation*}
\end{eg}
\begin{eg}
	If $X\sim\NBin(r,p)$, then for $t<-\ln(1-p)$:
	\begin{equation*}
		M_{X}(t)=\left(\frac{pe^{t}}{1-(1-p)e^{t}}\right)^{r}
	\end{equation*}
	If $X\sim\Gam(\alpha,\beta)$, then for $t<\beta$:
	\begin{equation*}
		M_{X}(t)=\left(\frac{\beta}{\beta-t}\right)^{\alpha}
	\end{equation*}
\end{eg}

\newpage
\begin{eg}
	Given $Y\sim\chi^{2}(r)$. How do we find the MGF of $Y$?\\
	Note that chi-squared distribution is actually a special case of gamma distribution. We have $\chi^{2}(r)=\Gamma(\frac{r}{2},\frac{1}{2})$. Therefore, by substituting, for $t<\frac{1}{2}$, we may get:
	\begin{equation*}
		M_{Y}(t)=\left(\frac{\frac{1}{2}}{\frac{1}{2}-t}\right)^{\frac{r}{2}}=(1-2t)^{-\frac{r}{2}}
	\end{equation*}
\end{eg}
\begin{eg}
	Given that $Y\sim\chi^{2}(r)$. How do we find $\E(Y)$ without using the MGF of $Y$?\\
	By definition, we may let $Y=\sum_{i=1}^{r}X_{i}^{2}$, where $X_{i}\sim\N(0,1)$. Therefore,
	\begin{equation*}
		\E(Y)=\sum_{i=1}^{r}\E(X_{i}^{2})=\sum_{i=1}^{r}\left.\odv*[order={2}]{}{t}e^{\frac{1}{2}t^{2}}\right|_{t=0}=\left.r(1+t^{2})e^{\frac{1}{2}t^{2}}\right|_{t=0}=r
	\end{equation*}
\end{eg}
Ultimately, the reason why we use moment generating function is the following fact.
\begin{thm}\named{Uniqueness of MGF}
	Let $X$ and $Y$ be two random variables. Suppose that their MGFs exist and are equal for all $t\in(-h,h)$ for some $h>0$, then the distribution functions $F_{X}$ and $F_{Y}$ are equal.
\end{thm}
This means that by knowing the MGF of a particular random variable $X$, we can know its distribution.
\begin{eg}
	Assume that $X_{1},\cdots,X_{n}$ are independent and $X_{i}\sim\Bin(m_{i},p)$ for all $i=1,\cdots,n$. Then we have:
	\begin{equation*}
		M_{X_{1}+\cdots+X_{n}}(t)=\prod_{i=1}^{n}M_{X_{i}}(t)=\prod_{i=1}^{n}(pe^{t}+1-p)^{m_{i}}=(pe^{t}+1-p)^{\sum_{i=1}^{n}m_{i}}
	\end{equation*}
	Therefore, we have $X_{1}+\cdots+X_{n}\sim\Bin(\sum_{i=1}^{n}m_{i},p)$.
\end{eg}
\begin{eg}
	Assume that $X_{1},\cdots,X_{n}$ are independent and $X_{i}\sim\Poisson(\lambda_{i})$ for all $i=1,\cdots,n$. Then we have:
	\begin{equation*}
		M_{X_{1}+\cdots+X_{n}}(t)=\prod_{i=1}^{n}M_{X_{i}}(t)=\prod_{i=1}^{n}e^{\lambda_{i}(e^{t}-1)}=e^{\sum_{i=1}^{n}\lambda_{i}(e^{t}-1)}
	\end{equation*}
	Therefore, we have $X_{1}+\cdots+X_{n}\sim\Poisson(\sum_{i=1}^{n}\lambda_{i})$.
\end{eg}
\begin{eg}
	Similarly, given a set of independent random variables $\{X_{1},\cdots,X_{n}\}$.
	\begin{enumerate}
		\item If $X_{i}\sim\NBin(r_{i},p)$, then $X_{1}+\cdots+X_{n}\sim\NBin(\sum_{i=1}^{n}r_{i},p)$.
		\item If $X_{i}\sim\N(\mu_{i},\sigma_{i}^{2})$, then $X_{1}+\cdots+X_{n}\sim\N(\sum_{i=1}^{n}\mu_{i},\sum_{i=1}^{n}\sigma_{i}^{2})$.
		\item If $X_{i}\sim\Gam(\alpha_{i},\beta)$, then $X_{1}+\cdots+X_{n}\sim\Gam(\sum_{i=1}^{n}\alpha_{i},\beta)$ and $cX_{i}\sim\Gam(\alpha_{i},\frac{\beta}{c})$ for $c\neq 0$.
	\end{enumerate}
\end{eg}
\begin{rem}
	Not all sum of distributions will result in the same type of distribution.
\end{rem}
More generally, we would deal with problems of limiting distribution.
\begin{thm}
	Suppose the $\{X_{n}\}$ is a sequence of random variables, each with MGF $M_{X_{n}}(t)$. If we have:
	\begin{equation*}
		\lim_{n\to\infty}M_{X_{n}}(t)=M_{Y}(t)
	\end{equation*}
	for all $t$ in a neighbourhood of $0$, where $M_{Y}(t)$ is a MGF for some random variables $Y$, then there is an unique distribution function $F_{Y}$ with corresponding $M_{Y}(t)$ such that:
	\begin{equation*}
		\lim_{n\to\infty}F_{X_{n}}(y)=F_{Y}(y)
	\end{equation*}
	for all $y$ where $F_{Y}(y)$ is continuous. We denote by $X_{n}\to Y$ or $X_{n}\xrightarrow{D}X$.
\end{thm}
\begin{rem}
	Simply, the limiting distribution of $X_{n}$ is equal to the distribution of $Y$.
\end{rem}

\newpage
We may define the limiting convergence in truly theoretical way.
\begin{defn}
	A sequence of random variables $\{X_{n}\}$ \textbf{converges in distribution} to a random variable $X$, denoted by $X_{n}\xrightarrow{D}X$, if for all continuity point $x$ of $F_{X}$, as $n\to\infty$,
	\begin{equation*}
		F_{X_{n}}(x)\to F_{X}(x)
	\end{equation*}
\end{defn}
We also have a more strict convergence.
\begin{defn}
	A sequence of random variables $\{X_{n}\}$ \textbf{converges in probability} to a random variable $X$, denoted by $X_{n}\xrightarrow{\prob}X$, if for any $\varepsilon>0$, as $n\to\infty$,
	\begin{align*}
		\prob(\abs{X_{n}-X}<\varepsilon)&\to 1 & \prob(\abs{X_{n}-X}\geq\varepsilon)&\to 0
	\end{align*}
\end{defn}
\begin{rem}
	If $X_{n}\xrightarrow{\prob}X$, then $X_{n}\xrightarrow{D}X$. The converse is not necessarily true.
\end{rem}
\begin{rem}
	After this point, we only need $X_{n}\xrightarrow{D}X$ on most cases. We may simplify it into $X_{n}\to X$.
\end{rem}

\section{Limit Theorems}
Using the last two theorems, the next two theorems are very useful in both statistics and probability theory by giving us approximate distribution of an average without a lot of distributional assumption.
\begin{thm}\named{Weak Law of Large Numbers (WLLN)}
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables. Let $\E(X_{i})=\mu$ for all $i=1,2,\cdots$. Define by $\overline{X}$ the sample mean of the random variables. Then as $n\to\infty$
	\begin{equation*}
		\overline{X}\xrightarrow{D}\mu
	\end{equation*}
\end{thm}
\begin{thm}\named{Classical Central Limit Theorem (CLT)}
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables whose MGFs exist in a neighbourhood of $0$. Let $\E(X_{i})=\mu$ and $\Var(X_{i})=\sigma^{2}>0$ for all $i=1,2,\cdots$. Define by $\overline{X}$ the sample mean of the random variables. Then as $n\to\infty$,
	\begin{equation*}
		\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}=\frac{\sum_{i=1}^{n}X_{i}-n\mu}{\sqrt{n}\sigma}\xrightarrow{D}\N(0,1)
	\end{equation*}
\end{thm}
\begin{rem}
	This is just an abuse of notations.
\end{rem}
This works generally to most of the distribution. However, it is probably very tedious to find the MGF. We cam apply the following version of CLT instead.
\begin{thm}\named{L\'evy-Linderberg Central Limit Theorem}
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables with common population means $\mu$ and common population variance $\sigma^{2}$. Assume that $0<\sigma^{2}<\infty$. Define by $\overline{X}$ the sample mean of the random variables. Then as $n\to\infty$,
	\begin{equation*}
		\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}=\frac{\sum_{i=1}^{n}X_{i}-n\mu}{\sqrt{n}\sigma}\xrightarrow{D}\N(0,1)
	\end{equation*}
\end{thm}
Sometimes, we will deal with a function of multiple random variables. We must establish how they converges.
\begin{thm}\named{Slutsky's Theorem}
	If $X_{n}\xrightarrow{D}X$ and $Y_{n}\xrightarrow{\prob}c$, then:
	\begin{enumerate}
		\item $X_{n}+Y_{n}\xrightarrow{D}X+c$
		\item $X_{n}Y_{n}\xrightarrow{D}cX$
		\item $\frac{X_{n}}{Y_{n}}\xrightarrow{D}\frac{X}{c}$ if $c\neq 0$.
	\end{enumerate}
\end{thm}

\newpage
\begin{eg}
	Assume that $X_{i}\sim\Bern(p)$ for all $i$. We want to estimate the unknown $p$. We have common mean $\mu=p$ and common variance $\sigma^{2}=p(1-p)$. By applying CLT, as $n\to\infty$,
	\begin{equation*}
		\overline{X}\to\N\left(p,\frac{p(1-p)}{n}\right)
	\end{equation*}
	Therefore, we can use normal distribution to approximate the unknown parameter. We want an estimation that we can confident about, and commonly we use probability $0.95$.
	\begin{equation*}
		0.95=\prob\left(-z_{0.025}\leq\frac{\overline{X}-p}{\sqrt{\frac{p(1-p)}{n}}}\leq z_{0.025}\right)=\prob\left((\overline{X}-p)^{2}\leq z_{0.025}^{2}\frac{p(1-p)}{n}\right)
	\end{equation*} 
	Solving the inequality, we would find an interval that estimates the parameter $p$. However, this is highly inconvenient. We may use another method. Let us replace $\sqrt{\frac{p(1-p)}{n}}$ with $\sqrt{\frac{\overline{X}(1-\overline{X})}{n}}$. As $n\to\infty$,
	\begin{equation*}
		\sqrt{\frac{\overline{X}(1-\overline{X})}{n}}=\sqrt{\frac{p(1-p)}{n}}\sqrt{\frac{p(1-p)}{\overline{X}(1-\overline{X})}}\to\sqrt{\frac{p(1-p)}{n}}
	\end{equation*}
	since by Slutsky's Theorem, $\sqrt{\frac{p(1-p)}{\overline{X}(1-\overline{X})}}\to 1$ as $\overline{X}\to p$ by WLLN. We have:
	\begin{equation*}
		0.95=\prob\left(-z_{0.025}\leq\frac{\overline{X}-p}{\sqrt{\frac{\overline{X}(1-\overline{X})}{n}}}\leq z_{0.025}\right)=\prob\left(\overline{X}-z_{0.025}\sqrt{\frac{\overline{X}(1-\overline{X})}{n}}\leq p\leq\overline{X}+z_{0.025}\sqrt{\frac{\overline{X}(1-\overline{X})}{n}}\right)
	\end{equation*} 
\end{eg}
\begin{eg}
	In a survey before an election, a poll was taken of $300$ potential voters. Among them, $120$ said that they would vote for candidate A. Determine a $95\%$ confidence interval for the population proportion $p_{A}$ of voters who would vote for candidate A in the election.\\
	From the poll, we have a point estimate $\overline{x}=\hat{p}_{A}=\frac{120}{300}=0.4$. From the last example, we have found that the $95\%$ confidence interval is:
	\begin{equation*}
		\left(\overline{x}-z_{0.025}\sqrt{\frac{\overline{x}(1-\overline{x})}{n}},\overline{x}+z_{0.025}\sqrt{\frac{\overline{x}(1-\overline{x})}{n}}\right)\approx(0.3446,0.4554)
	\end{equation*}
	Equivalently, the percentage of voters of candidate A would be from $34.46\%$ to $45.54\%$, with a margin of error $5.54\%$.
\end{eg}
\begin{eg}
	Following the previous example. Assume that we have been given a margin of error $D$. How many data should we collect in order to to have the margin of error?\\
	From how we find the margin of error:
	\begin{equation*}
		z_{0.025}\sqrt{\frac{\overline{x}(1-\overline{x})}{n}}=D\implies n=p(1-p)\frac{z_{0.025}^{2}}{D^{2}}
	\end{equation*}
	Since $p(1-p)\leq\frac{1}{4}$, if we specify that $D=0.05$, we have:
	\begin{equation*}
		n\leq\frac{z_{0.025}^{2}}{4D^{2}}=\frac{1.96^{2}}{4(0.05)^{2}}\leq\frac{2^{2}}{4(0.05)^{2}}=400
	\end{equation*}
	We may use this to determine whether we have obtained enough data.\\
	Assume that we have $n^{*}$ respondents. Is it enough? The number of required respondents is obtained by:
	\begin{equation*}
		n_{\text{required}}=\frac{\overline{x^{*}}(1-\overline{x^{*}})z_{0.025}^{2}}{D^{2}}
	\end{equation*}
	If $n^{*}<n_{\text{required}}$, then the current number of data is not enough. We would need to find more respondents.\\
	If $n^{*}\geq n_{\text{required}}$, then it is enough.
\end{eg}

\newpage
\begin{eg}
	We try to use Poisson random variables to prove that as $n\to\infty$,
	\begin{equation*}
		e^{-n}\sum_{k=0}^{n}\frac{n^{k}}{k!}\to\frac{1}{2}
	\end{equation*}
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables and $X_{i}\sim\Poisson(1)$ for $i=1,2,\cdots$. Let $Y_{n}=\sum_{i=1}^{n}X_{i}$. By CLT, we have:
	\begin{equation*}
		\frac{Y_{n}-n}{\sqrt{n}}\to\N(0,1)
	\end{equation*}
	Therefore, since $Y_{n}\sim\Poisson(n)$, we have:
	\begin{equation*}
		e^{-n}\sum_{k=0}^{n}\frac{n^{k}}{k!}=\prob(Y_{n}\leq n)=\prob\left(\frac{Y_{n}-n}{\sqrt{n}}\leq 0\right)\to\frac{1}{2}
	\end{equation*}
\end{eg}
\begin{eg}
	Given a sequence of i.i.d. random variables $\{X_{n}\}$. We want to find the asymptotic distribution for the $k$-th sample moment $\overline{X^{k}}$ as $n\to\infty$. Notice that $X_{i}^{k}$ are independent for $i=1,2,\cdots$. By CLT,
	\begin{equation*}
		\frac{\sqrt{n}(\overline{X^{k}}-\mu_{k}')}{\sqrt{\mu_{2k}'-(\mu_{k}')^{2}}}\to\N(0,1)
	\end{equation*}
	Therefore, the asymptotic distribution for $\overline{X^{k}}$ when $n\to\infty$ is $\N(\mu_{k}',\frac{1}{n}[\mu_{2k}'-(\mu_{k}')^{2}])$.
\end{eg}
Central Limit Theorem can provide us a limiting standard normal distribution of sample mean. However, we usually deal with some functions of the sample mean.
\begin{thm}\named{Continuous mapping theorem}
	Let $\{X_{n}\}$ be a sequence of random variables and $X$ be a random variable. Suppose there is a function $g$ has a set of discontinuity points $D_{g}$ such that $\prob(X\in D_{g})=0$, then:
	\begin{enumerate}
		\item If $X_{n}\xrightarrow{D}X$, then $g(X_{n})\xrightarrow{D}g(X)$.
		\item If $X_{n}\xrightarrow{\prob}X$, then $g(X_{n})\xrightarrow{\prob}g(X)$.
	\end{enumerate}
\end{thm}
\begin{thm}\named{Delta method}
	Let $\{X_{n}\}$ be a sequence of random variables such that for constants $a$ and $b>0$, as $n\to\infty$,
	\begin{equation*}
		\sqrt{n}(X_{n}-a)\xrightarrow{D}\N(0,b^{2})
	\end{equation*}
	Then for a given function $g$, suppose that $g'(a)$ exists and not $0$, as $n\to\infty$:
	\begin{equation*}
		\sqrt{n}(g(X_{n})-g(a))\xrightarrow{D}\N(0,[g'(a)b]^{2})
	\end{equation*}
\end{thm}
\begin{cor}
	\label{Chapter 1 (Corollary) CLT for functions of random variables}
	If $\overline{X}$ is the sample mean of a random sample $X_{1},\cdots,X_{n}$ of size $n$ from a distribution with a finite mean $\mu$ and finite variance $\sigma^{2}>0$. For a given function $g$, suppose that $g'(\mu)$ exists and is not $0$, as $n\to\infty$:
	\begin{equation*}
		\sqrt{n}(g(\overline{X})-g(\mu))\xrightarrow{D}\N(0,[g'(\mu)\sigma]^{2})
	\end{equation*}  
\end{cor}
\begin{proofing}
	By Central Limit Theorem, we get:
	\begin{equation*}
		\sqrt{n}(\overline{X}-\mu)\xrightarrow{D}\N(0,\sigma^{2})
	\end{equation*}
	Therefore, by the Delta method, for any function $g$ such that $g'(\mu)$ exists and not $0$,
	\begin{equation*}
		\sqrt{n}(g(\overline{X})-g(\mu))\xrightarrow{D}\N(0,[g'(\mu)\sigma]^{2})
	\end{equation*}
\end{proofing}

\newpage
\begin{eg}
	Assume that there are $70$ respondents, $68$ of which would vote for one candidate.\\
	If we use the same process from previous examples, we find that the $95\%$ confidence interval is $(0.9324,1.0105)$, which is out of the range. In fact, if the point estimate $\hat{p}$ is quite close to $0$ or $1$, the resulting interval guess may include values that is out of the range of $p$. This is a poor interval guess.\\
	We take a transformation, say $g(p)$, such that $g(p)\in(-\infty,\infty)$. We may find that since $0<p<1$, $\ln{p}<0$. Therefore, we can find that:
	\begin{equation*}
		g(p)=\ln(-\ln{p})\in(-\infty,\infty)
	\end{equation*}
	By Delta method,
	\begin{equation*}
		\frac{g(\overline{X})-g(p)}{g'(p)\sqrt{\frac{\overline{X}(1-\overline{X})}{n}}}\to\N(0,1)
	\end{equation*}
	By WLLN and continuous mapping theorem, we can replace $g'(p)$ with $g'(\overline{X})$. Therefore, we have:
	\begin{align*}
		0.95=\prob\left(-z_{0.025}\leq	\frac{g(\overline{X})-g(p)}{g'(p)\sqrt{\frac{\overline{X}(1-\overline{X})}{n}}}\leq z_{0.025}\right)
	\end{align*}
	Solving the formula and we would get the a good $95\%$ confidence interval of $p$.
\end{eg}
\begin{eg}
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables and $X_{i}\sim\Bern(\theta)$ for $i=1,2,\cdots$. Show that:
	\begin{equation*}
		Z_{n}=2\sqrt{n}\left(\sin^{-1}{\sqrt{\overline{X}}}-\sin^{-1}{\sqrt{\theta}}\right)\to\N(0,1)
	\end{equation*}
	Let $g(t)=\sin^{-1}{\sqrt{t}}$. We may obtain that:
	\begin{equation*}
		g'(t)=\frac{1}{2\sqrt{t}\sqrt{1-t}}
	\end{equation*}
	We can find that the derivative is well-defined and non-zero for $0<\theta<1$ by substituting $x=\theta$. Note that $\E(X_{i})=\theta$ and $\Var(X_{i})=\theta(1-\theta)$ for $i=1,\cdots,n$. By Corollary \ref{Chapter 1 (Corollary) CLT for functions of random variables},
	\begin{equation*}
		\sqrt{n}(g(\overline{X})-g(\theta))\to\N\left(0,\frac{1}{4}\right)
	\end{equation*}
	Since $Z_{n}=2\sqrt{n}(g(\overline{X})-g(\theta))$, we may find that as $n\to\infty$,
	\begin{equation*}
		Z_{n}\to\N(0,1)
	\end{equation*}
\end{eg}
\begin{eg}
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables and $X_{i}\sim\Exp(\theta)$ for $i=1,2,\cdots$. We want to find a variance-stabilizing transformation, which is to find a function $g(x)$ such that the limiting distribution of:
	\begin{equation*}
		Y_{n}=\sqrt{n}[g(\overline{X_{n}})-g(\theta)]
	\end{equation*}
	does not depend on $\theta$. We may find that $\E(X_{i})=\frac{1}{\theta}$ and $\Var(X_{i})=\frac{1}{\theta^{2}}$ for $i=1,2,\cdots$. 
	
	We claim that $g(x)=\ln{x}$ is what we want. We have:
	\begin{equation*}
		g'(x)=\frac{1}{x}
	\end{equation*}
	By substituting $x=\frac{1}{\theta}$, we can see that the derivative is non-zero. Applying Corollary \ref{Chapter 1 (Corollary) CLT for functions of random variables},
	\begin{equation*}
		\sqrt{n}\left(g(\overline{X})-g\left(\frac{1}{\theta}\right)\right)\to\N(0,1)
	\end{equation*}
	Therefore, $g(x)=\ln{x}$ is the variance-stabilizing transformation.
\end{eg}

\newpage
However, we usually deal with more than 1 variables. Before we extend the theorems into multivariate case, we must first introduce the multivariate normal distribution.
\begin{eg}\named{Multivariate Normal Distribution} $\mathbf{X}\sim\N_{k}(\boldsymbol{\mu},\mathbf{\Sigma})$\\
	Given a random vector $\mathbf{X}$. Let the $k\times 1$ vector $\boldsymbol{\mu}$ be the expected value of $\mathbf{X}$ and the $k\times k$ matrix $\mathbf{\Sigma}$ be its variance-covariance matrix. Assume that $\mathbf{\Sigma}$ is positive-definite. (For all non-zero vectors $\mathbf{z}$ with real entries, we have $\mathbf{z}^{T}\mathbf{\Sigma z}>0$) The random vector $\mathbf{X}$ is $k$-dimensional normal if its PDF is:
	\begin{equation*}
		f_{\mathbf{X}}(\mathbf{x})=(2\pi)^{-\frac{k}{2}}\abs{\mathbf{\Sigma}}^{-\frac{1}{2}}e^{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T}\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})}
	\end{equation*}
\end{eg}
\begin{rem}
	The i-th row and j-th column of the $k\times k$ variance-covariance matrix $\mathbf{\Sigma}$ is the element $a_{ij}$ found by:
	\begin{equation*}
		a_{ij}=\cov(X_{i},X_{j})
	\end{equation*}
	Note that if $i=j$, then $\cov(X_{i},X_{i})=\Var(X_{i})$.
\end{rem}
\begin{eg}
	If $k=2$, then $X\sim\N_{2}(\boldsymbol{\mu},\mathbf{\Sigma})$ is bivariate normal.
\end{eg}
\begin{lem}
	\label{Chapter 1 (Lemma) Distribution of matrix multiplication with random vector}
	If $\mathbf{X}\sim\N_{p}(\boldsymbol{\mu},\mathbf{\Sigma})$, then for any $q\times p$ matrix $\mathbf{A}$, we have:
	\begin{equation*}
		\mathbf{AX}\sim\N_{q}(\mathbf{A}\boldsymbol{\mu},\mathbf{A\Sigma A}^{T})
	\end{equation*}
\end{lem}
\begin{eg}
	Using this lemma, one can isolate some of the random variables that made up the random vector $\mathbf{X}=(X_{1}\ \cdots\ X_{p})^{T}\sim\N_{p}(\boldsymbol{\mu},\mathbf{\Sigma})$. For example, setting $(p-1)\times p$ matrix $\mathbf{A}$ as:
	\begin{equation*}
		\mathbf{A}=\begin{pmatrix}
			0 & 1 & 0 & \hdots & 0\\
			\vdots & \ddots & \ddots & \ddots & \vdots\\
			\vdots & & \ddots & \ddots & 0\\
			0 & \hdots & \hdots & 0 & 1
		\end{pmatrix}=\begin{pmatrix}\begin{array}{c|c}
			0 & \\
			\vdots & \mathbf{I}_{(p-1)\times(p-1)}\\
			0 &
		\end{array}\end{pmatrix}
	\end{equation*}
	We may find that:
	\begin{equation*}
		\mathbf{AX}=\begin{pmatrix}
		X_{2}\\
		\vdots\\
		X_{p}
		\end{pmatrix}\sim\N_{p-1}(\mathbf{A}\boldsymbol{\mu},\mathbf{A\Sigma A}^{T})
	\end{equation*}
	where $\mathbf{A}\boldsymbol{\mu}$ is the mean vector of $(X_{2}\ \cdots\ X_{p})^{T}$ and $\mathbf{A\Sigma A}^{T}$ is the variance-covariance matrix of $(X_{2}\ \cdots\ X_{p})^{T}$.
\end{eg}
\begin{lem}
	\label{Chapter 1 (Lemma) Independence iff uncorrelated and bivariate}
	If we have:
	\begin{equation*}
		\begin{pmatrix}X_{1}\\X_{2}\end{pmatrix}\sim\N_{2}\left(\begin{pmatrix}\mu_{1}\\\mu_{2}\end{pmatrix},\begin{pmatrix}\sigma_{11}^{2}&\sigma_{12}^{2}\\\sigma_{21}^{2}&\sigma_{22}^{2}\end{pmatrix}\right),
	\end{equation*}
	then $X_{1}$ and $X_{2}$ are independent if and only if $\sigma_{12}^{2}=\sigma_{21}^{2}=0$.
\end{lem}
\begin{proofing}
	From the properties of covariance,
	\begin{equation*}
		\sigma_{12}^{2}=\cov(X_{1},X_{2})=\cov(X_{2},X_{1})=\sigma_{21}^{2}
	\end{equation*}
	Suppose that $X_{1}$ and $X_{2}$ are independent. We have:
	\begin{equation*}
		\cov(X_{1},X_{2})=\E[(X_{1}-\E(X_{1}))(X_{2}-\E(X_{2}))]=\E(X_{1}X_{2})-\E(X_{1})\E(X_{2})=0
	\end{equation*}
	Therefore, $\sigma_{12}^{2}=\sigma_{21}^{2}=0$.\\
	Suppose that $\sigma_{12}^{2}=\sigma_{21}^{2}=0$. We have $\cov(X_{1},X_{2})=0$. Therefore,
	\begin{align*}
		f_{X_{1},X_{2}}(x_{1},x_{2})&=\frac{1}{2\pi\sigma_{11}\sigma_{22}}\exp\left(-\frac{1}{2}\left(\frac{(x_{1}-\mu_{1})^{2}}{\sigma_{11}^{2}}+\frac{(x_{2}-\mu_{2})^{2}}{\sigma_{22}^{2}}\right)\right)\\
		&=\frac{1}{\sqrt{2\pi\sigma_{11}^{2}}}\exp\left(-\frac{(x_{1}-\mu_{1})^{2}}{2\sigma_{11}^{2}}\right)\frac{1}{\sqrt{2\pi\sigma_{22}^{2}}}\exp\left(-\frac{(x_{2}-\mu_{2})^{2}}{2\sigma_{22}^{2}}\right)=f_{X_{1}}(x_{1})f_{X_{2}}(x_{2})
	\end{align*}
	Therefore, $X_{1}$ and $X_{2}$ are independent.
\end{proofing}
\begin{rem}
	Two random variables are uncorrelated does not mean they are independent. It is only true if they are bivariate normal.
\end{rem}
We may extend the CLT to multivariate case.
\begin{thm}\named{Multivariate Central Limit Theorem}
	Let $\{\mathbf{X}_{n}=(X_{n1}\ \cdots\ X_{nk})^{T}\in\mathbb{R}^{k}\}$ be a sequence of i.i.d. random vectors with variance-covariance matrix $\mathbf{\Sigma}$. We assume that $\E(X_{ij}^{2})<\infty$ for $i=1,2,\cdots$ and $j=1,\cdots,k$. Define by $\mathbf{\overline{X}}$ the sample mean of the random vectors. Then as $n\to\infty$,
	\begin{equation*}
		\sqrt{n}(\mathbf{\overline{X}}-\boldsymbol{\mu})\xrightarrow{D}\N_{k}(\mathbf{0},\mathbf{\Sigma})
	\end{equation*}
\end{thm}
We may extend the Delta method to multivariate cases.
\begin{thm}\named{Multivariate 1st-order Delta Method}
	Let $\{\mathbf{X}_{n}\in\mathbb{R}^{k}\}$ be a sequence of random vectors such that for constant vector $\mathbf{a}\in\mathbb{R}^{k}$, as $n\to\infty$,
	\begin{equation*}
		\sqrt{n}(\mathbf{X}_{n}-\mathbf{a})\xrightarrow{D}\mathbf{U}
	\end{equation*}
	where $\mathbf{U}$ is a random vector in $\mathbb{R}^{k}$. If a function $h:\mathbb{R}^{k}\to\mathbb{R}$ has a derivative $\nabla h(\mathbf{a})\neq\mathbf{0}$, then as $n\to\infty$:
	\begin{equation*}
		\sqrt{n}(h(\mathbf{X}_{n})-h(\mathbf{a}))\xrightarrow{D}\nabla h(\mathbf{a})\mathbf{U}
	\end{equation*}
	where
	\begin{equation*}
		\nabla h=\left(\pdv*[order=1]{h(t_{1},\cdots,t_{k})}{t_{1}},\cdots,\pdv*[order=1]{h(t_{1},\cdots,t_{k})}{t_{k}}\right)
	\end{equation*}
\end{thm}

\chapter{Point Estimation}
In this chapter, we will study two different general approaches to estimate an unknown parameters of any given parametric distribution.\\
The basic idea of point estimation is that we use a statistic $T$, an estimate $T(\mathbf{x})$ or an estimator $T(\mathbf{X})$ to estimate the unknown parameter $g(\theta)$, where $\mathbf{x}=(x_{1}\ \cdots\ x_{n})^{T}$ is a realization of random vector $\mathbf{X}=(X_{1}\ \cdots\ X_{n})^{T}$ with a PDF $f(x|\theta)$ or PMF $p(x|\theta)$ and $\theta$ in parameter space $\Theta$.
\begin{rem}
	Most often, the parameters of our interest to be estimated (\textbf{estimand}) is a function of the unknown distribution parameters $\theta$. E.g. $\mu^{2},\frac{\sigma}{\mu}$.
\end{rem}
\begin{rem}
	We only estimate unknown parameters. There is no point to estimate an already known parameters.
\end{rem}
\begin{defn}
	An estimator or estimate $\hat{\theta}$ is \textbf{unbiased} or \textbf{mean-unbiased} for $\theta$ if $\E(\hat{\theta})=\theta$.
\end{defn}
\section{Methods of Moments Estimation}
Methods of moments estimation is one of the most popular methods in statistics to estimate an unknown parameters. As the name suggests, it is related to moments. The motivation is that in some situations, the parameter of interest can be written as a function of population moments about $0$.
\begin{defn}
	Suppose that there are $k$ unknown parameters $\theta_{1},\cdots,\theta_{k}$. If we can write them in terms of $k$ or more moments, i.e.:
	\begin{equation*}
		\begin{cases}
			\theta_{1}=g_{1}(\mu_{1}',\mu_{2}',\cdots,\mu_{k}',\cdots)\\
			\theta_{2}=g_{2}(\mu_{1}',\mu_{2}',\cdots,\mu_{k}',\cdots)\\
			\vdots\\
			\theta_{k}=g_{k}(\mu_{1}',\mu_{2}',\cdots,\mu_{k}',\cdots)
		\end{cases}
	\end{equation*}
	then the \textbf{method of moments estimator} (MME) of $(\theta_{1},\theta_{2},\cdots,\theta_{k})$, denoted by $(\widetilde{\theta}_{1},\widetilde{\theta}_{2},\cdots,\widetilde{\theta}_{k})$, is:
	\begin{equation*}
		\begin{cases}
			\widetilde{\theta}_{1}=g_{1}(\overline{X},\overline{X^{2}},\cdots,\overline{X^{k}},\cdots)\\
			\widetilde{\theta}_{2}=g_{2}(\overline{X},\overline{X^{2}},\cdots,\overline{X^{k}},\cdots)\\
			\vdots\\
			\widetilde{\theta}_{k}=g_{k}(\overline{X},\overline{X^{2}},\cdots,\overline{X^{k}},\cdots)
		\end{cases}
	\end{equation*}
\end{defn}
\begin{rem}
	The method of moments estimate is obtained by $\widetilde{\theta}_{i}=g_{i}(\overline{x},\overline{x^{2}},\cdots,\overline{x^{k}},\cdots)$ for $i=1,\cdots,k$.
\end{rem}
\begin{rem}
	This estimation is quick and easy, but the MME obtained are often biased and heavily relies on the existence of the required population moments.
\end{rem}
\begin{rem}
	Do not mix up method of moment estimator or method of moment estimate.
\end{rem}
\begin{rem}
	Do not write the MME as $(\theta_{1},\theta_{2},\cdots,\theta_{k})$. This is wrong.
\end{rem}

\newpage
\begin{eg}
	Consider a random sample of size $n$ from $X\sim\N(10,\sigma^{2})$. We want to estimate $\sigma^{2}$.\\
	We have $k=1$, $\theta_{1}=\sigma^{2}$. We can write it in terms of moments:
	\begin{equation*}
		\sigma^{2}=\E(X^{2})-100
	\end{equation*}
	Therefore, the MME of $\sigma^{2}$ is:
	\begin{equation*}
		\widetilde{\sigma}^{2}=\overline{X^{2}}-100
	\end{equation*}
\end{eg}
\begin{eg}
	Consider a random sample of size $n$ from $X\sim\N(\mu,\sigma^{2})$. We want to estimate $\mu$ and $\sigma^{2}$.\\
	We have $k=2$, $(\theta_{1},\theta_{2})=(\mu,\sigma^{2})$. We can write them in terms of moments: 
	\begin{equation*}
		\begin{cases}
			\mu=E(X)\\
			\sigma^{2}=\E(X^{2})-[\E(X)]^{2}
		\end{cases}
	\end{equation*}
	Therefore, the MME of $\mu$ and $\sigma^{2}$ are:
	\begin{equation*}
		\begin{cases}
			\widetilde{\mu}=\overline{X}\\
			\widetilde{\sigma}^{2}=\overline{X^{2}}-(\overline{X})^{2}=\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}
		\end{cases}
	\end{equation*}
\end{eg}
\begin{rem}
	MME may not be unique because the parameter can be written as different functions of moments. To fix this problem, we usually prefer using fewer or lower moments to get MME.
\end{rem}
\begin{eg}
	Consider a random sample of size $n$ from $X\sim\Poisson(\lambda)$. We want to estimate $\lambda$. We have $k=1$, $\theta_{1}=\lambda$. We have multiple ways to write it in terms of moments. It can be $\lambda=\E(X)$, $\lambda=\E(X^{2})-[\E(X)]^{2}$ or any other combinations. From the remark, we would choose the one with fewer or lower moments, which is:
	\begin{equation*}
		\lambda=\E(X)
	\end{equation*}
	Therefore, the MME of $\lambda$ is:
	\begin{equation*}
		\lambda=\overline{X}
	\end{equation*}
\end{eg}
\begin{eg}
	Consider a random sample of size $n$ from $X\sim\Gam(\alpha,\beta)$. Assume that we know that $\E(X)=3423$. We have $k=2$, $(\theta_{1},\theta_{2})=(\alpha,\beta)$. We can write them in terms of moments:
	\begin{equation*}
		\begin{cases}
			3423=\frac{\alpha}{\beta}\\
			\E(X^{2})=\frac{\alpha}{\beta^{2}}+3423^{2}
		\end{cases}\implies\begin{cases}
			\alpha=\frac{3423^{2}}{\E(X^{2})-3423^{2}}\\
			\beta=\frac{3423}{\E(X^{2})-3423^{2}}
		\end{cases}
	\end{equation*}
	Therefore, the MME of $\alpha$ and $\beta$ is:
	\begin{equation*}
		\begin{cases}
			\widetilde{\alpha}=\frac{3423^{2}}{\overline{X^{2}}-3423^{2}}\\
			\widetilde{\beta}=\frac{3423}{\overline{X^{2}}-3423^{2}}
		\end{cases}
	\end{equation*}
\end{eg}
\begin{lem}\named{Invariance property of MME}
	If $\widetilde{\theta}_{i}$ is the MME for $\theta_{i}$ for $i=1,\cdots,k$, then $h(\widetilde{\theta}_{1},\cdots,\widetilde{\theta}_{k})$ is the MME for $h(\theta_{1},\cdots,\theta_{k})$, where $h$ is a known function.
\end{lem}
\begin{thm}
	\label{Chapter 2 (Thoerem) Sequence of MME is asympt. normal}
	A sequence of MME $\{\widetilde{\theta}_{n}\in\mathbb{R}^{k}\}$ is consistent, asymptotically unbiased for $\theta$, and asymptotically normally distributed. More precisely, under certain assumption like $\E\abs{X}^{2k}<\infty$, as $n\to\infty$, we have:
	\begin{equation*}
		\sqrt{n}(\widetilde{\theta}_{n}-\theta)\to\N_{k}(\mathbf{0},\mathbf{GHG}^{T})
	\end{equation*}
	where $\mathbf{G}$ is a $k\times k$ matrix with $\pdv{g_{i}}{\mu_{j}'}$ as its $(i,j)$-th entry and $\mathbf{H}$ is a $k\times k$ matrix with $\mu_{i+j}'-\mu_{i}'\mu_{j}'$ as its $(i,j)$-th entry, for $i=1,\cdots,k$ and $j=1,\cdots,k$.
\end{thm}
\begin{rem}
	In the theorem, "consistent" means convergence in probability. For any $\varepsilon>0$, as $n\to\infty$,
	\begin{equation*}
		\prob(|\widetilde{\theta}_{n}-\theta|>\varepsilon)\to 0
	\end{equation*}
\end{rem}

\newpage
\begin{rem}
	Also in the theorem, "asymptotically unbiased" means that we have:
	\begin{equation*}
		\lim_{n\to\infty}\E(\widetilde{\theta}_{n})=\theta
	\end{equation*}
	Note that it may be true that $\E(\widetilde{\theta}_{n})\neq\theta$ for some $n$.
\end{rem}
\begin{eg}
	Consider a random sample of size $n$ from a random variable $X$ with $\E\abs{X}^{4}<\infty$. We take:
	\begin{equation*}
		\theta=\begin{pmatrix}
			\mu\\ \sigma^{2}
		\end{pmatrix}=\begin{pmatrix}
			\mu_{1}'\\ \mu_{2}'-(\mu_{1}')^{2}
		\end{pmatrix}
	\end{equation*}
	We have:
	\begin{align*}
		\mathbf{G}&=\begin{pmatrix}
			1 & 0\\
			-2\mu_{1}' & 1
		\end{pmatrix} & \mathbf{H}&=\begin{pmatrix}
			\mu_{2}'-(\mu_{1}')^{2} & \mu_{3}'-\mu_{1}'\mu_{2}'\\
			\mu_{3}'-\mu_{2}'\mu_{1}' & \mu_{4}'-(\mu_{2}')^{2}
		\end{pmatrix}
	\end{align*}
	Therefore,
	\begin{align*}
		\mathbf{GHG}^{T}&=\begin{pmatrix}
			1 & 0\\
			-2\mu_{1}' & 1
		\end{pmatrix}\begin{pmatrix}
		\mu_{2}'-(\mu_{1}')^{2} & \mu_{3}'-\mu_{1}'\mu_{2}'\\
		\mu_{3}'-\mu_{2}'\mu_{1}' & \mu_{4}'-(\mu_{2}')^{2}
		\end{pmatrix}\mathbf{G}^{T}\\
		&=\begin{pmatrix}
			\mu_{2}'-(\mu_{1}')^{2} & \mu_{3}'-\mu_{1}'\mu_{2}'\\
			\mu_{3}'-3\mu_{1}'\mu_{2}'+2(\mu_{1}')^{3} & \mu_{4}'-2\mu_{1}'\mu_{3}'-(\mu_{2}')^{2}+2(\mu_{1}')^{2}\mu_{2}'
		\end{pmatrix}\begin{pmatrix}
			1 & -2\mu_{1}'\\
			0 & 1
		\end{pmatrix}\\
		&=\begin{pmatrix}
			\mu_{2}'-(\mu_{1}')^{2} & \mu_{3}'-3\mu_{1}'\mu_{2}'+2(\mu_{1}')^{3}\\
			\mu_{3}'-3\mu_{1}'\mu_{2}'+2(\mu_{1}')^{3} & \mu_{4}'-4\mu_{1}'\mu_{3}'-(\mu_{2}')^{2}+8\mu_{2}'(\mu_{1}')^{2}-4(\mu_{1}')^{4}
		\end{pmatrix}
	\end{align*}
	Using the fact that:
	\begin{align*}
		\mu_{3}&=\mu_{3}'-3\mu_{2}'\mu_{1}'+2(\mu_{1}')^{3}\\ \mu_{4}&=\mu_{4}'-4\mu_{3}'\mu_{1}'+6\mu_{2}'(\mu_{1}')^{2}-3(\mu_{1}')^{4}\\
		\sigma^{4}&=(\mu_{2}')^{2}-2\mu_{2}'(\mu_{1}')^{2}+(\mu_{1}')^{4}
	\end{align*}
	We can find the resultant matrix:
	\begin{equation*}
		\mathbf{GHG}^{T}=\begin{pmatrix}
			\sigma^{2} & \mu_{3}\\
			\mu_{3} & \mu_{4}-\sigma^{4}
		\end{pmatrix}
	\end{equation*}
	Using Theorem \ref{Chapter 2 (Thoerem) Sequence of MME is asympt. normal}, denote:
	\begin{equation*}
		\widetilde{\theta}_{n}=\begin{pmatrix}
			\overline{X}_{n}\\ S_{n}^{2}
		\end{pmatrix}
	\end{equation*}
	As $n\to\infty$,
	\begin{equation*}
		\sqrt{n}\left[\begin{pmatrix}
			\overline{X}_{n}\\ S_{n}^{2}
		\end{pmatrix}-\begin{pmatrix}
			\mu\\ \sigma^{2}
		\end{pmatrix}\right]\to\N_{2}\left(\begin{pmatrix}
			0\\ 0
		\end{pmatrix},\begin{pmatrix}
			\sigma^{2} & \mu_{3}\\
			\mu_{3} & \mu_{4}-\sigma^{4}
		\end{pmatrix}\right)
	\end{equation*}
	Based on the properties of variance-covariance matrix, we may find that as $n\to\infty$:
	\begin{equation*}
		\sqrt{n}(S_{n}^{2}-\sigma^{2})\to\N(0,\mu_{4}-\sigma^{4})
	\end{equation*}
	By Delta Method, in condition that $\sigma^{2}>0$,
	\begin{equation*}
		\sqrt{n}(S_{n}-\sigma)\to\N\left(0,\frac{\mu_{4}-\sigma^{4}}{4\sigma^{2}}\right)
	\end{equation*}
\end{eg}

\newpage
\section{Maximum Likelihood Estimation}
The method of maximum likelihood is by far the most popular technique for deriving estimators, popularized by Ronald Aylmer Fisher in 1922. Currently, there are still a lot of research studying the properties of this estimation method.
\begin{defn}
	Consider a random sample of size $n$ from a population with a PDF $f(\mathbf{x}|\theta)$ or a PMF $p(\mathbf{x}|\theta)$. Given a realization $\mathbf{x}=(x_{1}\ \cdots\ x_{n})^{T}$. The \textbf{likelihood function} is defined by:
	\begin{equation*}
		L(\theta)=L(\theta_{1},\cdots,\theta_{k}|\mathbf{x})=\begin{cases}
			\prod_{i=1}^{n}f(x_{i}|\theta), &\text{Continuous case}\\
			\prod_{i=1}^{n}p(x_{i}|\theta), &\text{Discrete case}
		\end{cases}
	\end{equation*}
	The likelihood function can be used to quantify how the observed data is likely to occur.
\end{defn}
\begin{rem}
	The likelihood function $L(\theta)$ is a function of $\theta$ with fixed $\mathbf{x}$.
\end{rem}
\begin{rem}
	Do not replace $x_{i}$ with $x$.
	\begin{equation*}
		L(\theta)=\begin{cases}
			\prod_{i=1}^{n}f(x_{i}|\theta)\neq\prod_{i=1}^{n}f(x|\theta), &\text{Continuous case}\\
			\prod_{i=1}^{n}p(x_{i}|\theta)\neq\prod_{i=1}^{n}p(x|\theta), &\text{Discrete case}
		\end{cases}
	\end{equation*}
\end{rem}
The idea is that for each realization of $\mathbf{x}$, we want to estimate a value of $\theta\in\Theta$ at which $L(\theta)$ attains its maximum.
\begin{defn}
	The \textbf{maximum likelihood estimate} (MLE), denoted by $\hat{\theta}$, is obtained by:
	\begin{equation*}
		\hat{\theta}=\argmax_{\theta\in\Theta}L(\theta)
	\end{equation*}
\end{defn}
\begin{rem}
	In some cases, especially when differentiation is used, it is easier to work with \textbf{log likelihood} defined by:
	\begin{equation*}
		l(\theta)=\ln{L(\theta)}
	\end{equation*}
	We can do this because $l(\theta)$ and $L(\theta)$ are strictly increasing and they have the same maxima. 
\end{rem}
\begin{eg}
	Consider a random sample of size $n=10$ from $\Bern(\theta)$, where $\theta$ is unknown. Therefore,
	\begin{equation*}
		L(\theta)=\prod_{i=1}^{n}p(x_{i}|\theta)=\theta^{n\overline{x}}(1-\theta)^{n-n\overline{x}}
	\end{equation*}
	Suppose that there are only two possible values of $\theta$. We have either $\theta=0.1$ or $\theta=0.5$.\\
	From the data observed, assumed that we have found that $\overline{x}=0.4$. Substituting gets:
	\begin{align*}
		L(0.1)&=(0.1)^{4}(0.9)^{6}=0.0000531441 & L(0.5)&=(0.5)^{4}(0.5)^{6}=0.0009765625
	\end{align*}
	Therefore, the MLE of $\theta$ is $\hat{\theta}=0.5$.
\end{eg}
\begin{eg}
	In the case when $L(\theta)$ is differentiable on the interior of $\theta$. One possible way of finding an MLE of $\theta=(\theta_{1}\ \cdots\ \theta_{k})^{T}$ is to solve the first order equation for $i=1,\cdots,k$:
	\begin{equation*}
		\pdv*{L(\theta)}{\theta_{i}}=0\text{ or }\pdv*{l(\theta)}{\theta_{i}}=0
	\end{equation*}
	and check all the extrema.
\end{eg}
\begin{rem}
	Solving the first-order likelihood only gives you the maximum at critical points. You need to also check the extreme values too.
\end{rem}

\newpage
\begin{eg}
	Consider a random sample of size $n$ from $\N(\theta,1)$, where $\theta$ is unknown. We may obtain the log likelihood:
	\begin{equation*}
		l(\theta)=\ln\left(\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x_{i}-\theta)^{2}}\right)=-\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\theta)^{2}-\frac{n}{2}\ln(2\pi)
	\end{equation*}
	We find the critical points by solving:
	\begin{equation*}
		0=\pdv*{l(\theta)}{\theta}=\sum_{i=1}^{n}(x_{i}-\theta)
	\end{equation*}
	This has the solution $\hat{\theta}=\overline{x}$. To check that the solution is in fact a global maximum, we can check that:
	\begin{equation*}
		\pdv*[order={2}]{l(\theta)}{\theta}=-n<0
	\end{equation*}
	Therefore, the MLE of $\theta$ is $\hat{\theta}=\overline{x}$.
\end{eg}
\begin{eg}
	Continues the previous example. Alternatively, we may find that for any $\theta\in\Theta$,
	\begin{equation*}
		\sum_{i=1}^{n}(x_{i}-\theta)^{2}\geq\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}
	\end{equation*}
	Thus, for any $\theta\in\Theta$,
	\begin{equation*}
		L(\theta)\leq L(\overline{x})
	\end{equation*}
	Therefore, the MLE of $\theta$ is $\hat{\theta}=\overline{x}$.
\end{eg}
\begin{eg}
	Consider a random sample of size $n$ from $\N(\theta,1)$, where $\theta$ is unknown. Previously, we have found that $\hat{\theta}=\overline{x}$, which maximize the log likelihood. Let us restrict that $\theta\geq 0$.\\
	If $\overline{x}\geq 0$, then it satisfies the constraint $\theta\geq 0$. Therefore, the MLE would be:
	\begin{equation*}
		\hat{\theta}=\overline{x}
	\end{equation*}
	If $\overline{x}<0$, then it does not satisfy the constraint $\mu\geq 0$. We analyse the log likelihood again:
	\begin{equation*}
		l(\theta)=-\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\theta)^{2}-\frac{n}{2}\ln(2\pi)=-\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}-\frac{n}{2}(\overline{x}-\theta)-\frac{n}{2}\ln(2\pi)
	\end{equation*}
	The term $(\overline{x}-\theta)^{2}$ is minimized while satisfying the constraint is when $\theta=0$.\\
	Therefore, if we restrict $\theta\geq 0$, the MLE of $\theta$ would be:
	\begin{equation*}
		\hat{\theta}=\max\{\overline{x},0\}
	\end{equation*}
\end{eg}
\begin{rem}
	Remember, when we estimate a parameter, we must use the data we have obtained.
\end{rem}
\begin{eg}
	Consider a random sample of size $n$ from $\U[0,\theta]$, where $\theta\in(0,\infty)$ is unknown. The likelihood function is:
	\begin{equation*}
		L(\theta)=\frac{1}{\theta^{n}}\mathbf{1}_{0\leq x_{(1)}\leq\cdots\leq x_{(n)}\leq\theta}
	\end{equation*}
	where $x_{(i)}$ represents the $i$-th smallest data for $i=1,\cdots,n$. Therefore, the MLE is:
	\begin{equation*}
		\hat{\theta}=x_{(n)}
	\end{equation*}
\end{eg}
\begin{rem}
	MLE may be biased and it may not exist in $\Theta$, especially when $\Theta$ is an open s\end{rem}

\newpage
\begin{rem}
	MLE defined may not be unique.
\end{rem}
\begin{eg}
	Consider a random sample of size $n$ from $\U[\theta-1,\theta+1]$, where $\theta$ is unknown. The likelihood function is:
	\begin{equation*}
		L(\theta)=\frac{1}{2^{n}}\mathbf{1}_{\theta-1\leq x_{(1)}\leq\cdots\leq x_{(n)}\leq\theta+1}=\frac{1}{2^{n}}\mathbf{1}_{x_{(n)}-1\leq\theta\leq x_{(1)}+1}
	\end{equation*}
	where $x_{(i)}$ represents the $i$-th smallest data for $i=1,\cdots,n$. We may find that any estimates in $[x_{(n)}-1,x_{(1)}+1]$ maximize $L(\theta)$. Therefore, there are infinitely many MLEs of $\theta$.
\end{eg}
\begin{lem}\named{Invariance property of MLE}
	If $\hat{\theta}_{i}$ is the MLE of $\theta_{i}$ for $i=1,\cdots,k$, then $h(\hat{\theta}_{1},\cdots,\hat{\theta}_{k})$ is the MLE for $h(\theta_{1},\cdots,\theta_{k})$, where $h$ is a known function.
\end{lem}
\begin{thm}
	\label{Chapter 2 (Thoerem) Sequence of MLE is asympt. normal}
	A sequence of MLE $\{\hat{\theta}_{n}\in\mathbb{R}^{k}\}$ is consistent, asymptotically unbiased for $\theta$, asymptotically efficient and asymptotically normally distributed. More precisely, under regularity assumption, as $n\to\infty$, we have:
	\begin{equation*}
		\sqrt{n}(\hat{\theta}_{n}-\theta)\to\N_{k}(\mathbf{0},\mathcal{I}_{X}^{-1}(\theta))
	\end{equation*}
	where $\mathcal{I}_{X}(\theta)$ is known as the \textbf{Fisher Information matrix} and is a $k\times k$ matrix with the $(i,j)$-th entry defined as:
	\begin{equation*}
		\begin{cases}
			\E\left[\left(\pdv*{\ln{f_{X}(X|\theta)}}{\theta_{i}}\right)\left(\pdv*{\ln{f_{X}(X|\theta)}}{\theta_{j}}\right)\right], &\text{Continuous case}\\
			\E\left[\left(\pdv*{\ln{p_{X}(X|\theta)}}{\theta_{i}}\right)\left(\pdv*{\ln{p_{X}(X|\theta)}}{\theta_{j}}\right)\right], &\text{Discrete case}
		\end{cases}
	\end{equation*}
	for $i=1,\cdots,k$ and $j=1,\cdots,k$.
\end{thm}
\begin{rem}
	In the theorem, "asymptotically efficient" means that the limiting variance is the smallest possible. This will be further discussed in Chapter 3.
\end{rem}
Notice that we have used a special matrix called "Fisher Information Matrix". What is Fisher Information?
\begin{defn}
	Given a set of random variables $\{X_{1},\cdots,X_{n}\}$. The \textbf{Fisher Information}, or \textbf{Fisher Information matrix} if more than one unknown parameter is considered, of the set is defined by:
	\begin{equation*}
		\mathcal{I}_{X_{1},\cdots,X_{n}}(\theta)=\begin{cases}
			\E\left[\odv*{}{\theta}\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}\right]^{2}, &\text{Continuous case}\\
			\E\left[\odv*{}{\theta}\ln{p_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}\right]^{2}, &\text{Discrete case}
		\end{cases}
	\end{equation*}
\end{defn}
\begin{rem}
	Fisher Information is a measure of amount of information about an unknown parameter $\theta$ that a random variable or data carries. It is very important because we do want to know how to quantify this amount appropriately.
\end{rem}
\begin{eg}
	If $X\sim\N(\mu,\sigma^{2})$, where $\sigma^{2}$ is known but $\mu\in(-\infty,\infty)$ is unknown, then the Fisher Information about $\mu$ contained in $X$ is:
	\begin{equation*}
		\mathcal{I}_{X}(\mu)=\E\left[\odv*{\ln{f_{X}(X|\mu)}}{\mu}\right]^{2}=\E\left[\odv*{}{\mu}\left(-\frac{1}{2\sigma^{2}}(X-\mu)^{2}-\frac{1}{2}\ln(2\pi\sigma^{2})\right)\right]^{2}=\E\left[\frac{1}{\sigma^{2}}(X-\mu)\right]^{2}=\frac{1}{\sigma^{2}}
	\end{equation*}
\end{eg}
\begin{eg}
	If $X\sim\Bern(p)$, where $p\in(0,1)$ is unknown, then the Fisher Information about $p$ contained in $X$ is:
	\begin{align*}
		\mathcal{I}_{X}(p)=\E\left[\odv*{\ln{f_{X}(X|p)}}{p}\right]^{2}&=\E\left[\odv*{}{p}\left(X\ln{p}+(1-X)\ln(1-p)\right)\right]^{2}\\
		&=\E\left[\frac{X}{p}-\frac{1-X}{1-p}\right]^{2}\\
		&=\E\left[\frac{X-p}{p(1-p)}\right]^{2}\\
		&=\frac{p(1-p)}{p^{2}(1-p)^{2}}=\frac{1}{p(1-p)}
	\end{align*}
\end{eg}

\newpage
We will see some properties of the Fisher Information. \textit{For simplicity, we will only discuss continuous random variables.} Notice that we used something called "regularity assumption"? The following are the regularity conditions that we need.
\begin{enumerate}
	\item $\odv*{}{\theta}\ln{f_{X_{1},\cdots,X_{n}}}(x_{1},\cdots,x_{n}|\theta)$ exists for all $x_{1},\cdots,x_{n}$ and all $\theta\in\Theta$
	\item For any statistic $T(x_{1},\cdots,x_{n})$,
	\begin{align*}
		&\odv*{}{\theta}\int\cdots\int T(x_{1},\cdots,x_{n})f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)\,dx_{1}\,\cdots\,dx_{n}\\
		=&\int\cdots\int T(x_{1},\cdots,x_{n})\odv*{}{\theta}f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)\,dx_{1}\,\cdots\,dx_{n}
	\end{align*}
	\item $0<\mathcal{I}_{X_{1},\cdots,X_{n}}(\theta)<\infty$ for all $\theta\in\Theta$
\end{enumerate}
Condition 2 can be satisfied when the support of $X$ does not depend on $\theta$, where the support of $X$ is defined in below:
\begin{defn}
	Suppose $X$ is a random variable with a PMF $p(x)$ or a PDF $f(x)$. The \textbf{support} of $X$ is defined as:
	\begin{equation*}
		\supp(X)=\begin{cases}
			\{x:p_{X}(x)>0\}, &\text{Discrete case}\\
			\{x:f_{X}(x)>0\}, &\text{Continuous case}
		\end{cases}
	\end{equation*}
\end{defn}
\begin{lem}
	\label{Chapter 2 (Lemma) Expectation of Fisher Information but with first moment is zero}
	Suppose the $X$ is a random variable with PDF $f_{X}$. Under the regularity conditions, we have:
	\begin{equation*}
		\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]=0
	\end{equation*}
\end{lem}
\begin{proofing}
	\begin{equation*}
		0=\odv*{}{\theta}\intinfty f_{X}(x|\theta)\,dx=\intinfty\odv*{}{\theta}f_{X}(x|\theta)\,dx=\intinfty\left(\odv*{}{\theta}\ln{f_{X}(x|\theta)}\right)f_{X}(x|\theta)\,dx=\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]
	\end{equation*}
\end{proofing}
\begin{rem}
	Using this lemma, we can find that:
	\begin{equation*}
		\mathcal{I}_{X}(\theta)=\Var\left(\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right)
	\end{equation*}
\end{rem}
\begin{lem}
	\label{Chapter 2 (Lemma) Relationship between expectation of l'' and I}
	Suppose that $\{X_{1},\cdots,X_{n}\}$ is a set of random variables. Under the regularity conditions and the assumption that $\odv*[order={2}]{}{\theta}\ln{f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)}$ exists for all $x_{1},\cdots,x_{n}$ and all $\theta\in\Theta$, we have:
	\begin{equation*}
		\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]^{2}=-\E\left[\odv*[order={2}]{}{\theta}\ln{f_{X}(X|\theta)}\right]
	\end{equation*}
\end{lem}
\begin{proofing}
	From the proof of last Lemma,
	\begin{align*}
		0&=\odv*{}{\theta}\intinfty\left(\odv*{}{\theta}\ln{f_{X}(x|\theta)}\right)f_{X}(x|\theta)\,dx\\
		&=\intinfty\odv*{}{\theta}\left[\left(\odv*{}{\theta}\ln{f_{X}(x|\theta)}\right)f_{X}(x|\theta)\right]\,dx\\
		&=\intinfty\left(\odv*[order=2]{}{\theta}\ln{f_{X}(x|\theta)}\right)f_{X}(x|\theta)\,dx+\intinfty\left(\odv*{}{\theta}\ln{f_{X}(x|\theta)}\right)\odv*{}{\theta}f_{X}(x|\theta)\,dx\\
		&=\intinfty\left(\odv*[order=2]{}{\theta}\ln{f_{X}(x|\theta)}\right)f_{X}(x|\theta)\,dx+\intinfty\left(\odv*{}{\theta}\ln{f_{X}(x|\theta)}\right)^{2}f_{X}(x|\theta)\,dx\\
		&=\E\left[\odv*[order={2}]{}{\theta}\ln{f_{X}(X|\theta)}\right]+\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]^{2}\\
		-\E\left[\odv*[order={2}]{}{\theta}\ln{f_{X}(X|\theta)}\right]&=\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]^{2}
	\end{align*}
\end{proofing}

\newpage
Assume that we consider two independent random variables $X$ and $Y$. We can find the Fisher Information about $\theta$ contained in $(X,Y)$ by finding the Fisher Information about $\theta$ contained in each of them.
\begin{lem}
	If $X$ and $Y$ are independent and their PDFs satisfy the regularity conditions, then:
	\begin{equation*}
		\mathcal{I}_{X,Y}(\theta)=\mathcal{I}_{X}(\theta)+\mathcal{I}_{Y}(\theta)
	\end{equation*}
\end{lem}
\begin{proofing}
	Since $X$ and $Y$ are independent,
	\begin{align*}
		\mathcal{I}_{X,Y}(\theta)&=\E\left[\odv*{}{\theta}\ln{f_{X,Y}(X,Y|\theta)}\right]^{2}\\
		&=\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}+\odv*{}{\theta}\ln{f_{Y}(Y|\theta)}\right]^{2}\\
		&=\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]^{2}+2\E\left[\left(\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right)\left(\odv*{}{\theta}\ln{f_{Y}(Y|\theta)}\right)\right]+\E\left[\odv*{}{\theta}\ln{f_{Y}(Y|\theta)}\right]^{2}\\
		\tag{Lemma \ref{Chapter 2 (Lemma) Expectation of Fisher Information but with first moment is zero}}
		&=\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]^{2}+\E\left[\odv*{}{\theta}\ln{f_{Y}(Y|\theta)}\right]^{2}\\
		&=\mathcal{I}_{X}(\theta)+\mathcal{I}_{Y}(\theta)
	\end{align*}
\end{proofing}
By applying the same result to a random sample of size $n$, we can obtain the following property.
\begin{lem}
	Suppose the $\{X_{1},\cdots,X_{n}\}$ is a random sample of size $n$ from a distribution. Then,
	\begin{equation*}
		\mathcal{I}_{X_{1},\cdots,X_{n}}(\theta)=\sum_{i=1}^{n}\mathcal{I}_{X_{i}}(\theta)=n\mathcal{I}_{X_{1}}(\theta)
	\end{equation*}
\end{lem}
\begin{rem}
	For any $i\neq j$, $\mathcal{I}_{X_{i}}(\theta)=\mathcal{I}_{X_{j}}(\theta)$ only means that $X_{i}$ and $X_{j}$ carries the same amount of the information about $\theta$. It does not mean they carry identical information.
\end{rem}
\begin{eg}
	Consider a set of i.i.d. random variables $\{X_{1},\cdots,X_{n}\}$ where for all $i=1,\cdots,n$, $X_{i}\sim\Cauchy(\theta)$ and has a PDF:
	\begin{equation*}
		f_{X_{i}}(x|\theta)=\frac{1}{\pi(1+(x-\theta)^{2})}
	\end{equation*}
	We may find that:
	\begin{align*}
		\mathcal{I}_{X_{i}}(\theta)=\E\left[\odv*{}{\theta}\ln{f_{X_{i}}(X_{i}|\theta)}\right]^{2}&=\E\left(\frac{\frac{2(X_{i}-\theta)}{\pi(1+(X_{i}-\theta)^{2})^{2}}}{\frac{1}{\pi(1+(X_{i}-\theta)^{2})}}\right)^{2}\\
		&=\E\left(\frac{2(X_{i}-\theta)}{1+(X_{i}-\theta)^{2}}\right)^{2}\\
		&=\intinfty\left(\frac{2(x-\theta)}{1+(x-\theta)^{2}}\right)^{2}\frac{1}{\pi(1+(x-\theta)^{2})}\,dx\\
		\tag{$u=x-\theta$, $du=dx$}
		&=\frac{4}{\pi}\intinfty\frac{u^{2}}{(1+u^{2})^{3}}\,du\\
		&=\frac{8}{\pi}\int_{0}^{\infty}\frac{u^{2}}{(1+u^{2})^{3}}\,du\\
		\tag{$y=\frac{1}{1+u^{2}}$, $dy=-\frac{2u}{(1+u^{2})^{2}}\,du$}
		&=\frac{4}{\pi}\int_{0}^{1}\sqrt{y}\sqrt{1-y}\,dy\\
		\tag{Beta integral}
		&=\frac{4}{\pi}\int_{0}^{1}(y)^{\frac{3}{2}-1}(1-y)^{\frac{3}{2}-1}\,dy\\
		\tag{$\frac{\Gamma(z_{1})\Gamma(z_{2})}{\Gamma(z_{1}+z_{2})}=\int_{0}^{1}t^{z_{1}-1}(1-t)^{z_{2}-1}\,dt$}
		&=\frac{4\Gamma(\frac{3}{2})\Gamma(\frac{3}{2})}{\pi\Gamma(3)}=\frac{4(0.5\sqrt{\pi})^{2}}{\pi(2!)}=\frac{1}{2}
	\end{align*}
	Therefore, $\mathcal{I}_{X_{1},\cdots,X_{n}}(\theta)=n\mathcal{I}_{X_{1}}(\theta)=\frac{n}{2}$.
\end{eg}

\newpage
Note that a statistic or an estimator can be considered as a function for data condensation because we condense a random sample into a lower-dimensional quantity.
\begin{lem}
	Suppose that $\mathbf{X}$ is a random vector. Under the regularity conditions, for any statistic $T(\mathbf{X})$ for $\theta$, we have:
	\begin{equation*}
		\mathcal{I}_{T(\mathbf{X})}(\theta)\leq\mathcal{I}_{\mathbf{X}}(\theta)
	\end{equation*}
\end{lem}
\begin{rem}
	The Fisher Information of $T(\mathbf{X})$ is defined by:
	\begin{equation*}
		\mathcal{I}_{T(\mathbf{X})}(\theta)=\E\left[\odv*{}{\theta}\ln{f_{T(\mathbf{X})}(T(\mathbf{X})|\theta)}\right]^{2}
	\end{equation*}
\end{rem}
We may prove Theorem \ref{Chapter 2 (Thoerem) Sequence of MLE is asympt. normal} in one-parameter case:
\begin{thm}
	Consider a random sample $\{X_{1},\cdots,X_{n}\}$ of size $n$ from a parametric distribution with a PDF $f_{X}$. Then under the regularity and some other conditions, for $\theta\in\mathbb{R}$, a sequence of MLE $\{\hat{\theta}_{n}\in\mathbb{R}\}$ allows,
	\begin{equation*}
		\sqrt{n}(\hat{\theta}_{n}-\theta)\to\N\left(0,\frac{1}{\mathcal{I}_{X}(\theta)}\right)
	\end{equation*}
\end{thm}
\begin{proofing}
	Since the MLE $\hat{\theta}_{n}$ is the solution to $l'(\theta)=0$, we can apply a Taylor expansion of $l'(\hat{\theta}_{n})$ at $\theta$ to find that:
	\begin{align*}
		0&=l'(\theta)+l''(\theta)(\hat{\theta}_{n}-\theta)+o((\hat{\theta}_{n}-\theta))\\
		\sqrt{n}(\hat{\theta}_{n}-\theta)&=\frac{\frac{1}{\sqrt{n}}l'(\theta)}{-\frac{1}{n}l''(\theta)}-o((\hat{\theta}_{n}-\theta))
	\end{align*}
	We first consider the numerator. Note that $\odv*{}{\theta}\ln{f_{X}(X_{1}|\theta)},\cdots,\odv*{}{\theta}\ln{f_{X}(X_{1}|\theta)}$ are i.i.d.. By CLT, we have:
	\begin{equation*}
		\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^{n}\odv*{}{\theta}\ln{f_{X}(X_{i}|\theta)}-\E\left[\odv*{}{\theta}\ln{f_{X}(X_{1}|\theta)}\right]\right)\to\N\left(0,\Var\left[\odv*{}{\theta}\ln{f_{X}(X_{1}|\theta)}\right]\right)
	\end{equation*}
	By Lemma \ref{Chapter 2 (Lemma) Expectation of Fisher Information but with first moment is zero}, we have:
	\begin{equation*}
		\frac{1}{\sqrt{n}}l'(\theta)=\frac{1}{\sqrt{n}}\sum_{i=1}^{n}\odv*{}{\theta}\ln{f_{X}(X_{i}|\theta)}\to\N(0,\mathcal{I}_{X}(\theta))
	\end{equation*}
	Now we consider the denominator. By WLLN and Lemma \ref{Chapter 2 (Lemma) Relationship between expectation of l'' and I}, since $\odv*[order={2}]{}{\theta}\ln{f_{X}(X_{1}|\theta)},\cdots,\odv*[order={2}]{}{\theta}\ln{f_{X}(X_{1}|\theta)}$ are i.i.d.,
	\begin{equation*}
		-\frac{1}{n}l''(\theta)=-\frac{1}{n}\sum_{i=1}^{n}\odv*[order={2}]{}{\theta}\ln{f_{X}(X_{i}|\theta)}\to-\E\left[\odv*[order={2}]{}{\theta}\ln{f_{X}(X|\theta)}\right]=\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]^{2}=\mathcal{I}_{X}(\theta)
	\end{equation*}
	Consequently, we have:
	\begin{equation*}
		\sqrt{n}(\hat{\theta}_{n}-\theta)=\frac{\frac{1}{\sqrt{n}}l'(\theta)}{-\frac{1}{n}l''(\theta)}-o((\hat{\theta}_{n}-\theta))\to\N\left(0,\frac{1}{\mathcal{I}_{X}(\theta)}\right)
	\end{equation*}
\end{proofing}
\begin{rem}
	\label{Chapter 2 (Remark) Cases when Fisher Information cannot be determined easily}
	Sometime, $\mathcal{I}_{X}(\theta)$ cannot be determined easily. We will replace it by an observed Fisher Information defined by $-\frac{1}{n}l''(\hat{\theta}_{n})$. Since $\hat{\theta}_{n}$ is consistent for $\theta$, $-\frac{1}{n}l''(\hat{\theta}_{n})$ is also consistent for $\mathcal{I}_{X}(\theta)$ by continuous mapping theorem. Therefore,
	\begin{equation*}
		\sqrt{-l''(\hat{\theta}_{n})}(\hat{\theta}_{n}-\theta)=\sqrt{n}\sqrt{-\frac{1}{n}l''(\hat{\theta}_{n})}(\hat{\theta}_{n}-\theta)\to\N(0,1)
	\end{equation*}
\end{rem}

\newpage
\begin{eg}\named{Principal of Numerical solution to likelihood equations}
	Consider a random sample of size $n$ from $X\sim\Cauchy(\theta)$, similar to the previous example. We want to find the MLE of $\theta$. We have:
	\begin{equation*}
		l(\theta)=-n\ln{\pi}-\sum_{i=1}^{n}\ln(1+(x_{i}-\theta)^{2})
	\end{equation*}
	We want to find the solution of $l'(\theta)=0$, which is the MLE. Setting:
	\begin{equation*}
		\sum_{i=1}^{n}\frac{2(x_{i}-\theta)}{1+(x_{i}-\theta)^{2}}=0
	\end{equation*}
	However, this is extremely hard to be solved explicitly. We need a numerical method to solve this.
\end{eg}
\begin{eg}\named{Newton-Raphson Algorithm}
	By Taylor expansion, we can get:
	\begin{equation*}
		0=\frac{1}{n}l'(\hat{\theta})\approx\frac{1}{n}l'(\theta)+\frac{1}{n}(\hat{\theta}-\theta)l''(\theta)
	\end{equation*}
	We may modify it to find that:
	\begin{equation*}
		\hat{\theta}\approx\theta-\frac{l'(\theta)}{l''(\theta)}
	\end{equation*}
	We may initially guess a number, say $\theta_{0}$. By iteratively applying the procedure for $j=0,1,\cdots$:
	\begin{equation*}
		\theta_{j+1}=\theta_{j}-\frac{l'(\theta_{j})}{l''(\theta_{j})}
	\end{equation*}
	and stop it at a certain stopping criterion, say $\abs{\theta_{j+1}-\theta_{j}}<K$ for some chosen constant $K$. E.g. $K=10^{-5}$.\\
	Using this algorithm, we can obtain or approximate the MLE of $\theta$.
\end{eg}
\begin{eg}
	Consider a random sample of size $n$ from $X\sim\Gam(\alpha,\beta)$, where $\beta=3423$ and $\alpha$ is unknown. The PDF is defined by:
	\begin{equation*}
		f_{X}(x|\theta)=\begin{cases}
			\frac{3423^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-3423x}, &x>0\\
			0, &\text{Otherwise}
		\end{cases}
	\end{equation*}
	We can find the log likelihood:
	\begin{equation*}
		l(\alpha)=\sum_{i=1}^{n}\ln{f_{X}(x_{i}|\alpha)}=n\alpha\ln{3423}-n\ln(\Gamma(\alpha))+(\alpha-1)\sum_{i=1}^{n}\ln{x_{i}}-3423\sum_{i=1}^{n}x_{i}
	\end{equation*}
	To find MLE, we try to solve the equation:
	\begin{equation*}
		0=\odv*{}{\alpha}l(\alpha)=n\ln{3423}-n\odv*{}{\alpha}\ln(\Gamma(\alpha))+\sum_{i=1}^{n}\ln{x_{i}}
	\end{equation*}
	However, we have no idea how to find $\odv*{}{\alpha}\ln(\Gamma(\alpha))$. Therefore, instead, we would use the numerical methods to approximate the MLE.
\end{eg}

\chapter{Uniformly Minimum Variance Unbiased Estimator}
We usually want to find the best estimator that can approximate some parameters. However, there are a lot of estimators we can provide based on the information given. In this chapter, we would try to find the best out of them.
\section{Introduction to UMVUE}
Consider a class $M$ defined as all the estimators for $\theta$. If there exists an estimator $\hat{\theta}^{*}\in M$ that is uniformly better than any other estimator in $M$, then we say $\hat{\theta}^{*}$ is the best estimator of $\theta$ in $M$. However, in general, this estimator does not exist partly because there are too many estimators to consider and some of them are poor or not reasonable. To avoid this problem, we only consider a particular class of estimators, which is the mean-unbiased estimators.
\begin{rem}
	In this context, $\hat{\theta}^{*}$ is "uniformly better" means that $\Var(\hat{\theta}^{*})<\Var(\hat{\theta})$ for any other $\hat{\theta}\in M$.
\end{rem}
Recall the definition of mean-unbiased estimator. If an estimator $\hat{\theta}$ satisfies:
\begin{equation*}
	\E(\hat{\theta})=\theta
\end{equation*}
for all $\theta\in\Theta$, then it is mean-unbiased or simply unbiased for $\theta$. Otherwise, it is biased.\\
From past experiences, we may get the following remarks.
\begin{rem}
	Unbiasedness means that by repeated sampling, $\hat{\theta}=\theta$ on average. The underestimation and overestimation will balance in the long run.
\end{rem}
\begin{rem}
	Sample variance $S_{n-1}^{2}$ is unbiased for $\sigma^{2}$ but $S_{n}^{2}$ is not. This is why we use $S_{n-1}^{2}$ to estimate $\sigma^{2}$ instead of $S_{n}^{2}$.
\end{rem}
\begin{rem}
	MME and MLE are usually biased, but they are asymptotically unbiased.
\end{rem}
\begin{rem}
	It is possible to have infinitely many different unbiased estimators for $\theta$.
\end{rem}
\begin{eg}
	Consider $\{X_{1},\cdots,X_{n}\}$ a random sample of size $n$ from a distribution with a finite mean $\theta$.\\
	Any estimators $\hat{\theta}$ in the form of:
	\begin{equation*}
		\hat{\theta}=\frac{\sum_{i=1}^{n}a_{i}X_{i}}{\sum_{i=1}^{n}a_{i}}
	\end{equation*}
	where $a_{i}\in\mathbb{R}$ for $i=1,\cdots,n$ and $\sum_{i=1}^{n}a_{i}\neq 0$, are unbiased for $\theta$.
\end{eg}
\begin{rem}
	It is possible to have no unbiased estimators for $\theta$.
\end{rem}
\begin{eg}
	Consider a random sample of size $n$ from a random variable $X\sim\Bin(1,\theta)$ with $g(\theta)=\frac{\theta}{1-\theta}$ as the parameter being estimated. There does not exist an unbiased estimator for $g(\theta)$.
\end{eg}
\begin{rem}
	Unbiasedness does not have an invariance property. $\hat{\theta}$ is unbiased for $\theta$ does not mean $h(\hat{\theta})$ is unbiased for $h(\theta)$.
\end{rem}
\begin{eg}
	We have $\overline{X}$ is unbiased for $\mu$, but $(\overline{X})^{2}$ is not unbiased for $\mu^{2}$ when $\sigma>0$.
\end{eg}
The best unbiased estimator is the unbiased estimator with the smallest variance.
\begin{defn}
	The \textbf{Uniformly Minimum Variance Unbiased Estimator (UMVUE)} $\hat{\theta}^{*}$ for $\theta$ is an unbiased estimator such that for all other unbiased estimator $\hat{\theta}$ for $\theta$,
	\begin{equation*}
		\Var(\hat{\theta}^{*})\leq\Var(\hat{\theta})
	\end{equation*}
	for all $\theta\in\Theta$.
\end{defn}
\begin{lem}\named{Uniqueness of UMVUE}
	Assume that UMVUE for $\theta$ exists. Then, it is unique.
\end{lem}
\begin{proofing}
	Assume that there are two distinct UMVUEs, $\hat{\theta}^{*}$ and $\hat{\theta}^{**}$, for $\theta$. We may find that for all $\theta\in\Theta$ and any unbiased estimator $\hat{\theta}$ of $\theta$:
	\begin{equation*}
		\Var(\hat{\theta}^{*})=\Var(\hat{\theta}^{**})\leq\Var(\hat{\theta})
	\end{equation*}
	Let $\hat{\theta}'=\frac{1}{2}(\hat{\theta}^{*}+\hat{\theta}^{**})$. We can easily find that $\hat{\theta}'$ is unbiased for $\theta$. We have:
	\begin{align*}
		\Var(\hat{\theta}')&=\frac{1}{4}\Var(\hat{\theta}^{*})+\frac{1}{4}\Var(\hat{\theta}^{**})+\frac{1}{2}\cov(\hat{\theta}^{*},\hat{\theta}^{**})\\
		&\leq\frac{1}{2}\Var(\hat{\theta}^{*})+\frac{1}{2}\sqrt{\Var(\hat{\theta}^{*})\Var(\hat{\theta}^{**})}\\
		&\leq\Var(\hat{\theta}^{*})\leq\Var(\hat{\theta}')
	\end{align*}
	Thus $\Var(\hat{\theta}')=\Var(\hat{\theta}^{*})$ and $\cov(\hat{\theta}^{*},\hat{\theta}^{**})=\sqrt{\Var(\hat{\theta}^{*})\Var(\hat{\theta}^{**})}$.\\
	We recall the Pearson correlation coefficient. We can find that:
	\begin{equation*}
		\rho=\frac{\cov(\hat{\theta}^{*},\hat{\theta}^{**})}{\sqrt{\Var(\hat{\theta}^{*})\Var(\hat{\theta}^{**})}}=1
	\end{equation*}
	This means $\hat{\theta}^{*}$ and $\hat{\theta}^{**}$ have a perfectly linear positive relationship. We say $\hat{\theta}^{*}=a\hat{\theta}^{**}+b$ where $a>0$ and $b\in\mathbb{R}$.\\
	We solve the equation:
	\begin{equation*}
		\begin{cases}
			\Var(\hat{\theta}^{**})=\Var(\hat{\theta}^{*})=a^{2}\Var(\hat{\theta}^{**})\\
			\E(\hat{\theta}^{**})=\theta=\E(\hat{\theta}^{*})=a\E(\hat{\theta}^{**})+b
		\end{cases}
	\end{equation*}
	We may find that $a=1$ and $b=0$. Therefore, $\hat{\theta}^{*}=\hat{\theta}^{**}$ and UMVUE is unique.
\end{proofing}

\section{Sufficient Statistic}
It is not easy to find UMVUE for a parameter being estimated. However, we have Rao-Blackwell Theorem (we will talk about it later) which tells us that UMVUE must be a function of sufficient statistic. Let us discuss about sufficient statistic.\\
Note that a statistic or an estimator can be considered as a function for data condensation because we condense a random sample into a lower-dimensional quantity. However, in the process, we may lose some information about the parameter $\theta$.\\
We recall this lemma. Under regularity conditions, for any statistic $T=T(\mathbf{X})$ for $\theta$, we have:
\begin{equation*}
	\mathcal{I}_{T}(\theta)\leq\mathcal{I}_{\mathbf{X}}(\theta)
\end{equation*}
Most statistics would lose some information about $\theta$, but there exists some statistics that we can substantially reduce the dimension without losing any information. We call this sufficient statistic.
\begin{defn}
	Under regularity conditions, the \textbf{sufficient statistic} for $\theta$, denoted by $S=S(\mathbf{X})$, is a statistic that satisfies:
	\begin{equation*}
		\mathcal{I}_{S}(\theta)=\mathcal{I}_{\mathbf{X}}(\theta)
	\end{equation*}
\end{defn}
\begin{rem}
	If the conditional distribution of the sample given a statistic $T$ depends on $\theta$, then there is still some information about $\theta$ contained in the sample that $T$ does not carry. Therefore, $T$ is not sufficient.
\end{rem}
\begin{eg}
	Let $\{X_{1},X_{2}\}$ be a random sample of size $2$ from $X\sim\Bin(m,\theta)$. We show that $T=T(X_{1},X_{2})=X_{1}+X_{2}$ is sufficient.
	\begin{align*}
		p_{X_{1},X_{2}|T}(x_{1},x_{2}|t)&=\frac{p_{X_{1},X_{2},T}(x_{1},x_{2},t)}{p_{T}(t)}\\
		&=\frac{p_{X_{1},X_{2},T}(x_{1},t-x_{1},t)}{p_{T}(t)}\\
		&=\frac{p_{X_{1}}(x_{1})p_{X_{2}}(t-x_{1})}{p_{T}(t)}\\
		&=\frac{\binom{m}{x_{1}}\binom{m}{t-x_{1}}\theta^{t}(1-\theta)^{n-t}}{\binom{2m}{t}\theta^{t}(1-\theta)^{n-t}}=\frac{\binom{m}{x_{1}}\binom{m}{t-x_{1}}}{\binom{2m}{t}}
	\end{align*}
	Therefore, since the conditional distribution of the sample given a statistic $T$ does not depend on $\theta$, we can find that $T$ is a sufficient statistic.
\end{eg}
We may rewrite the definition of sufficient statistic as follows:
\begin{defn}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random vector of random sample of size $n$ from a PDF $f(x|\theta)$ or PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$ for integer $k>1$. A set of statistics $\{S_{1},S_{2},\cdots,S_{r}\}$, where $r\geq k$ and $S_{i}=S_{i}(\mathbf{X})$ for $i=1,\cdots,r$, is said to be \textbf{jointly sufficient}, if and only if the conditional distribution:
	\begin{equation*}
		\begin{cases}
			p_{\mathbf{X}|S_{1},\cdots,S_{r}}(\mathbf{x}|S_{1}=s_{1},\cdots,S_{r}=s_{r},\theta), &\text{Discrete case}\\
			f_{\mathbf{X}|S_{1},\cdots,S_{r}}(\mathbf{x}|S_{1}=s_{1},\cdots,S_{r}=s_{r},\theta), &\text{Continuous case}
		\end{cases}
	\end{equation*}
	does not depend on $\theta$, for all values $s_{1}$ of $S_{1}$, $\cdots$, $s_{r}$ of $S_{r}$.
\end{defn}
or in one-parameter case,
\begin{defn}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a PDF $f(x|\theta)$ or PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}$. A statistic $S=S(\mathbf{X})$ is said to be \textbf{sufficient} if and only if the conditional distribution:
	\begin{equation*}
		\begin{cases}
			p_{\mathbf{X}|S}(\mathbf{x}|S=s,\theta), &\text{Discrete case}\\
			f_{\mathbf{X}|S}(\mathbf{x}|S=s,\theta), &\text{Continuous case}
		\end{cases}
	\end{equation*}
	does not depend on $\theta$, for all values $s$ of $S$.
\end{defn}
\begin{thm}\named{Fisher-Neyman Factorization Theorem}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$. A set of statistics $\{S_{1},\cdots,S_{r}\}$, where $r\geq k$ and $S_{i}=S_{i}(\mathbf{X})$ for $i=1,\cdots,r$, is jointly sufficient if and only if:
	\begin{equation*}
		\begin{cases}
			p_{\mathbf{X}}(\mathbf{x}|\theta)=g(S_{1}(\mathbf{x}),\cdots,S_{r}(\mathbf{x})|\theta)h(\mathbf{x}), &\text{Discrete case}\\
			f_{\mathbf{X}}(\mathbf{x}|\theta)=g(S_{1}(\mathbf{x}),\cdots,S_{r}(\mathbf{x})|\theta)h(\mathbf{x}), &\text{Continuous case}
		\end{cases}
	\end{equation*}
	where $g$ is a non-negative function of $x_{1},\cdots,x_{n}$ only through the statistics $S_{1},\cdots,S_{r}$ and depends on $\theta$, and $h$ is a non-negative function of $x_{1},\cdots,x_{n}$ not depending on $\theta$.
\end{thm}
\begin{proofing}
	We shall prove it in discrete case with only one statistic. The proof in continuous case or more than one statistic is out of our scope.\\
	Note that if $\mathbf{X}\in B$ for a set $B$, then $S(\mathbf{X})\in S(B)$. Therefore, for $i=1,\cdots,n$,
	\begin{equation*}
		\{\mathbf{X}\in B\}\cap\{S(\mathbf{X})\in S(B)\}=\{\mathbf{X}\in B\}
	\end{equation*}
	\begin{align*}
		\prob(\mathbf{X}\in B|\theta)&=\prob(\mathbf{X}\in B,S(\mathbf{X})\in S(B)|\theta)\\
		\tag{$\prob(A\cap B|D)=\prob(A|B\cap D)\prob(B|D)$}
		&=\prob(\mathbf{X}\in B|S(\mathbf{X})\in S(B),\theta)\prob(S(\mathbf{X})\in S(B)|\theta)
	\end{align*}
	
	\newpage
	Suppose that $S$ is sufficient. Then by definition,
	\begin{equation*}
		\prob(\mathbf{X}\in B|S(\mathbf{X})\in S(B),\theta)=\prob(X_{i}\in B|S(X_{i})\in S(B))
	\end{equation*}
	Therefore, by substituting $B=\{\mathbf{x}\}$, we get:
	\begin{equation*}
		p_{\mathbf{X}}(\mathbf{x}|\theta)=p_{\mathbf{X}|S}(\mathbf{x}|S(\mathbf{x}))p_{S}(S(\mathbf{x})|\theta)
	\end{equation*}
	We may find that $g(S(\mathbf{x})|\theta)=p_{S}(S(\mathbf{x})|\theta)$ and $h(\mathbf{x})=p_{\mathbf{X}|S}(\mathbf{x}|S(\mathbf{x}))$.\\
	Suppose that $p_{X}(\mathbf{x}|\theta)=g(T(\mathbf{x})|\theta)h(\mathbf{x})$. Then:
	\begin{equation*}
		p_{\mathbf{X}|T}(\mathbf{x}|t)=\frac{p_{\mathbf{X},T}(\mathbf{x},t|\theta)}{p_{T}(t|\theta)}=\begin{cases}
			0, &t\neq T(\mathbf{x})\\
			\frac{p_{\mathbf{X}}(\mathbf{x}|\theta)}{p_{T}(t|\theta)}, &t=T(\mathbf{x})
		\end{cases}
	\end{equation*}
	 We only need to consider when $t=T(\mathbf{x})$, we have:
	\begin{equation*}
		p_{\mathbf{X}|T}(\mathbf{x}|t)=\frac{p_{\mathbf{X}}(\mathbf{x}|\theta)}{\sum_{\mathbf{x}:T(\mathbf{x})=t}p_{\mathbf{x}}(\mathbf{x}|\theta)}=\frac{g(t|\theta)h(\mathbf{x})}{\sum_{\mathbf{x}:T(\mathbf{x})=t}g(t|\theta)h(\mathbf{x})}=\frac{h(\mathbf{x})}{\sum_{\mathbf{x}:T(\mathbf{x})=t}h(\mathbf{x})}
	\end{equation*}
	We have found that $p_{\mathbf{X}|T}(\mathbf{x}|t)$ does not depend on $\theta$. Therefore, by definition, $T$ is sufficient.
\end{proofing}
\begin{eg}
	\label{Chapter 1 (Example) Sufficient statistics of Bern(theta)}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample from $\Bern(\theta)$, where $\theta\in[0,1]$ is unknown. The joint PMF of the random sample is:
	\begin{align*}
		p_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)=\prod_{i=1}^{n}\theta^{x_{i}}(1-\theta)^{1-x_{i}}&=\theta^{\sum_{i=1}^{n}x_{i}}(1-\theta)^{n-\sum_{i=1}^{n}x_{i}}\times 1\\
		&=g\left(\left.\sum_{i=1}^{n}x_{i}\right|\theta\right)\times h(x_{1},\cdots,x_{n})		
	\end{align*}
	Therefore, $S=\sum_{i=1}^{n}X_{i}$ is a sufficient statistic.
\end{eg}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample from $\N(\mu,\sigma^{2})$, where $\mu$ and $\sigma^{2}>0$ are unknown. The joint PDF of the random sample is:
	\begin{align*}
		f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\mu,\sigma^{2})&=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(x_{i}-\mu)^{2}}{2\sigma^{2}}\right)\\
		&=\frac{1}{(2\pi\sigma^{2})^{\frac{n}{2}}}\exp\left(-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(x_{i}-\mu)^{2}\right)\\
		&=\sigma^{-n}\exp\left[-\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}x_{i}^{2}-2\mu\sum_{i=1}^{n}x_{i}+n\mu^{2}\right)\right]\times\left(\frac{1}{(2\pi)^{
		\frac{n}{2}}}\right)\\
		&=g\left(\left.\sum_{i=1}^{n}x_{i},\sum_{i=1}^{n}x_{i}^{2}\right|\mu,\sigma^{2}\right)\times h(x_{1},\cdots,x_{n})
	\end{align*}
	Therefore, $S_{1}=\sum_{i=1}^{n}X_{i}$ and $S_{2}=\sum_{i=1}^{n}X_{i}^{2}$ are jointly sufficient.
\end{eg}
\begin{eg}
	\label{Chapter 3 (Example) Sufficient statistic for U[0,theta]}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\U[0,\theta]$, where $\theta>0$ is unknown. The joint PDF of the random sample is:
	\begin{align*}
		f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)&=\frac{1}{\theta^{n}}\prod_{i=1}^{n}\mathbf{1}_{0\leq x_{i}\leq\theta}\\
		\tag{$x_{(i)}$ is the $i$-th smallest sample}
		&=\frac{1}{\theta^{n}}\mathbf{1}_{0\leq x_{(1)}<x_{(n)}\leq\theta}\\
		&=\frac{1}{\theta^{n}}\mathbf{1}_{x_{(n)}\leq\theta}\times\mathbf{1}_{x_{(1)}\geq 0}\\
		&=g(x_{(n)}|\theta)\times h(x_{1},\cdots,x_{n})
	\end{align*}
	Therefore, $S=X_{(n)}=\max\{X_{1},\cdots,X_{n}\}$ is sufficient.
\end{eg}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\U[\theta-\frac{1}{2},\theta+\frac{1}{2}]$, where $\theta$ is unknown. The joint PDF of the random sample is:
	\begin{align*}
		f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)&=\prod_{i=1}^{n}\mathbf{1}_{\theta-\frac{1}{2}\leq x_{i}\leq\theta+\frac{1}{2}}\\
		\tag{$x_{(i)}$ is the $i$-th smallest sample}
		&=\mathbf{1}_{\theta-\frac{1}{2}\leq x_{(1)}<x_{(n)}\leq\theta+\frac{1}{2}}\\
		&=\mathbf{1}_{x_{(n)}-\frac{1}{2}\leq\theta\leq x_{(1)}+\frac{1}{2}}\times 1\\
		&=g(x_{(1)},x_{(n)}|\theta)\times h(x_{1},\cdots,x_{n})
	\end{align*}
	Therefore, $S_{1}=X_{(1)}=\min\{X_{1},\cdots,X_{n}\}$ and $S_{2}=X_{(n)}=\max\{X_{1},\cdots,X_{n}\}$ are jointly sufficient.
\end{eg}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\U[\theta_{1},\theta_{2}]$, where $\theta_{1},\theta_{2}$ are unknown with $\theta_{1}<\theta_{2}$ and $\theta_{1}$ is not a function of $\theta_{2}$. The joint PDF of the random sample is:
	\begin{align*}
		f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta_{1},\theta_{2})&=\frac{1}{(\theta_{2}-\theta_{1})^{n}}\prod_{i=1}^{n}\mathbf{1}_{\theta_{1}\leq x_{i}\leq\theta_{2}}\\
		\tag{$x_{(i)}$ is the $i$-th smallest sample}
		&=\frac{1}{(\theta_{2}-\theta_{1})^{n}}\mathbf{1}_{\theta_{1}\leq x_{(1)}<x_{(n)}\leq\theta_{2}}\times 1\\
		&=g(x_{(1)},x_{(n)}|\theta_{1},\theta_{2})\times h(x_{1},\cdots,x_{n})
	\end{align*}
	Therefore, $S_{1}=X_{(1)}=\min\{X_{1},\cdots,X_{n}\}$ and $S_{2}=X_{(n)}=\max\{X_{1},\cdots,X_{n}\}$ are jointly sufficient.
\end{eg}
\begin{rem}
	Sufficient statistic may not be unique because we may have more than one factorization.
\end{rem}
\begin{eg}
	\label{Chapter 3 (Example) Sufficient statistic for normal distribution with unit variance}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\N(\mu,1)$, where $\mu$ is unknown. The joint PDF of the random sample is:
	\begin{equation*}
		f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\mu)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}(x_{i}-\mu)^{2}\right)=\frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left(-\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\mu)^{2}\right)		
	\end{equation*}
	One of the ways to factorize is:
	\begin{align*}
		\frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left(-\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\mu)^{2}\right)&=\frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left[-\frac{1}{2}\left(\sum_{i=1}^{n}x_{i}^{2}-2\mu\sum_{i=1}^{n}x_{i}+n\mu^{2}\right)\right]\\
		&=\frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left(\mu\sum_{i=1}^{n}x_{i}-\frac{n}{2}\mu^{2}\right)\times\exp\left(-\frac{1}{2}\sum_{i=1}^{n}x_{i}^{2}\right)\\
		&=g\left(\left.\sum_{i=1}^{n}x_{i}\right|\mu\right)\times h(x_{1},\cdots,x_{n})
	\end{align*}
	Therefore, we find that $S_{1}=\sum_{i=1}^{n}X_{i}$ is sufficient.\\
	Another way to factorize is:
	\begin{align*}
		\frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left(-\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\mu)^{2}\right)&=	\frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left[-\frac{1}{2}\left(\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}+n(\overline{x}-\mu)^{2}\right)\right]\\
		&=\frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left(-\frac{n}{2}(\overline{x}-\mu)^{2}\right)\times\exp\left(-\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}\right)\\
		&=g(\overline{x}|\mu)\times h(x_{1},\cdots,x_{n})
	\end{align*}
	Therefore, we find that $S_{2}=\overline{X}$ is sufficient.
\end{eg}

\newpage
Based on the above example, we may notice that $\overline{X}$ and $\sum_{i=1}^{n}X_{i}$ are functions of each other. Is any transformation of $S$ also sufficient? Yes if it is one-to-one!
\begin{lem}\named{One-to-one sufficiency}
	\label{Chapter 3 (Lemma) One-to-one sufficiency}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$. If a set of statistics $\{S_{1},\cdots,S_{r}\}$, where $r\geq k$ and $S_{i}=S_{i}(X_{1},\cdots,X_{r})$ for $i=1,\cdots,r$, is jointly sufficient, then any set of one-to-one functions $\{h_{1},\cdots,h_{m}\}$ for some $m$, where $m\geq r$ for $i=1,\cdots,m$:
	\begin{equation*}
		h_{i}=h_{i}(S_{1},\cdots,S_{r})
	\end{equation*}
	is also jointly sufficient.
\end{lem}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$. Assume that $\sum_{i=1}^{n}X_{i}$ and $\sum_{i=1}^{n}X_{i}^{2}$ are jointly sufficient. We may find that:
	\begin{align*}
		\overline{X}&=\frac{1}{n}\sum_{i=1}^{n}X_{i} & \sum_{i=1}^{n}(X_{i}-\overline{X})^{2}&=\sum_{i=1}^{n}X_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}X_{i}\right)^{2}
	\end{align*}
	Both are one-to-one functions of $\sum_{i=1}^{n}X_{i}$ and $\sum_{i=1}^{n}X_{i}^{2}$.\\
	Therefore, by Lemma \ref{Chapter 3 (Lemma) One-to-one sufficiency}, $\overline{X}$ and $\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}$ are jointly sufficient.\\
	However,
	\begin{equation*}
		(\overline{X})^{2}=\frac{1}{n^{2}}\left(\sum_{i=1}^{n}X_{i}\right)^{2}
	\end{equation*}
	It is not an one-to-one function. Therefore, $(\overline{X})^{2}$ and $\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}$ may not be jointly sufficient.
\end{eg}
From previous examples, the number of sufficient statistics can sometimes be more than the number of unknown parameters. How much should data be condense most without losing any information about unknown parameter $\theta$?
\begin{defn}
	A set of jointly sufficient statistics $\{S_{1},\cdots,S_{n}\}$ is \textbf{minimal jointly sufficient} if and only if for any other set of jointly sufficient statistics $\{T_{1},\cdots,T_{m}\}$, there exists a set of functions $\{f_{1},\cdots,f_{n}\}$ such that for $i=1,\cdots,n$,
	\begin{equation*}
		S_{i}=f_{i}(T_{1},\cdots,T_{m})
	\end{equation*}
\end{defn}
or in one-statistic case,
\begin{defn}
	A sufficient statistic $S$ is \textbf{minimal sufficient} if and only if for any other sufficient statistic $T$, there exists a function $f$ such that:
	\begin{equation*}
		S=f(T)
	\end{equation*}
\end{defn}
\begin{rem}
	Minimal jointly sufficient statistics may not be unique. We can say minimal joint sufficiency is closed under any one-to-one transformation.
\end{rem}
In general, it is not easy to find the minimal jointly sufficient statistics except form some special distribution. One of those special distributions are called exponential family, which we will discuss later.

\newpage
\section{Relationship of Sufficiency with UMVUE}
Recall that we actually want to find UMVUE. Does sufficiency help us find UMVUE? By Rao-Blackwell Theorem, it helps us find an improved unbiased estimator!
\begin{thm}\named{Rao-Blackwell Theorem}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample from a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$, and a set of jointly sufficient statistics $\{S_{1},\cdots,S_{r}\}$, where $r\geq k$ and $S_{i}=S_{i}(\mathbf{X})$ for $i=1,\cdots,r$. Suppose that $T=T(\mathbf{X})$ is an unbiased estimator for $g(\theta)$ for a function $g$. Define $T'$ by $\E(T|S_{1},\cdots,S_{r})$. Then:
	\begin{enumerate}
		\item $T'$ is a statistic, and it is a function of jointly sufficient statistics.
		\item $T'$ is unbiased for $g(\theta)$.
		\item $\Var(T')\leq\Var(T)$.
	\end{enumerate}
\end{thm}
\begin{proofing}
	\begin{enumerate}
		\item 
		\begin{equation*}
			T'=\E(T|S_{1},\cdots,S_{r})=\intinfty tf_{T|S_{1},\cdots,S_{r}}(t|S_{1},\cdots,S_{r})\,dt
		\end{equation*}
		By definition, $T'$ is a statistic and it is a function of jointly sufficient statistic $\{S_{1},\cdots,S_{r}\}$.
		\item 
		\begin{equation*}
			\E(T')=\E(\E(T|S_{1},\cdots,S_{r}))=\E(T)=g(\theta)
		\end{equation*}
		Therefore, $T'$ is unbiased for $g(\theta)$.
		\item 
		\begin{align*}
			\Var(T')&=\Var(\E(T|S_{1},\cdots,S_{r}))\\
			\tag{$\Var(Y)=\Var(\E(Y|X))+\E[\Var(Y|X)]$}
			&=\Var(T)-\E[\Var(T|S_{1},\cdots,S_{r})]\leq\Var(T)
		\end{align*}
	\end{enumerate}
\end{proofing}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Bern(\theta)$ where $\theta$ is unknown.
	\begin{enumerate}
		\item Since $\E(X_{1})=\theta$, $X_{1}$ is an unbiased estimator of $\theta$.
		\item From Example \ref{Chapter 1 (Example) Sufficient statistics of Bern(theta)}, we have found that $\sum_{i=1}^{n}X_{i}$ is a sufficient statistic. It is also obvious that it is minimal.
		\item By Rao-Blackwell Theorem, $T'=\E(X_{1}|\sum_{i=1}^{n}X_{i})$ is an unbiased estimator for $\theta$ with $\Var(T')\leq\Var(X_{1})$.
	\end{enumerate}
	We want to find $T'$. Assume that we have given that $\sum_{i=1}^{n}X_{i}=s$. We have:
	\begin{align*}
		\prob\left(X_{1}=0\left|\sum_{i=1}^{n}X_{i}=s\right.\right)=\frac{\prob\left(X_{1}=0,\sum_{i=1}^{n}X_{i}=s\right)}{\prob\left(\sum_{i=1}^{n}X_{i}=s\right)}&=\frac{\prob(X_{1}=0)\prob\left(\sum_{i=2}^{n}X_{i}=s\right)}{\prob\left(\sum_{i=1}^{n}X_{i}=s\right)}\\
		&=\frac{(1-\theta)\binom{n-1}{s}\theta^{s}(1-\theta)^{n-1-s}}{\binom{n}{s}\theta^{s}(1-\theta)^{n-s}}\\
		&=\frac{n-s}{n}
	\end{align*}
	Therefore, we have:
	\begin{equation*}
		\E\left(X_{1}\left|\sum_{i=1}^{n}X_{i}=s\right.\right)=\prob\left(X_{1}=1\left|\sum_{i=1}^{n}X_{i}=s\right.\right)=1-\prob\left(X_{1}=0\left|\sum_{i=1}^{n}X_{i}=s\right.\right)=\frac{s}{n}
	\end{equation*}
	We have found that $T'=\E\left(X_{1}|\sum_{i=1}^{n}X_{i}\right)=\frac{1}{n}\sum_{i=1}^{n}X_{i}$. We may find that:
	\begin{equation*}
		\Var(T')=\frac{1}{n}\theta(1-\theta)\leq\theta(1-\theta)=\Var(X_{1})
	\end{equation*}
\end{eg}

\newpage
\begin{rem}
	If $T$ is already a function of a jointly sufficient statistics, then $T'$ would be identical to $T$.
\end{rem}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Bern(\theta)$ and let $\overline{X}$ be the sample mean. We know that:
	\begin{equation*}
		\E(\overline{X})=\theta
	\end{equation*}
	Therefore, $\overline{X}$ is an unbiased estimator of $\theta$. We want to find $T'$. We have:
	\begin{equation*}
		T'=\E\left(\overline{X}\left|\sum_{i=1}^{n}X_{i}\right.\right)=\overline{X}=T
	\end{equation*}
\end{eg}
\begin{rem}
	Although Rao-Blackwell Theorem can provide us with a constructive way to improve a given unbiased estimator. it does not guarantee that the one constructed must be a UMVUE.
\end{rem}
\begin{eg}
	Consider a random sample $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ of size $n$ from $\N(\theta,1)$, where $\theta$ is unknown. Let $g(\theta)=\theta$. Consider $T=T(\mathbf{X})=X_{1}$ and the random sample is a set of jointly sufficient statistics $\{S_{1},\cdots,S_{n}\}$. We may find that:
	\begin{equation*}
		\tag{Expectation of $X_{1}$ given $X_{1}$ is of course $X_{1}$}
		\E(X_{1}|X_{1},\cdots,X_{n})=X_{1}
	\end{equation*}
	However, from Example \ref{Chapter 3 (Example) Sufficient statistic for normal distribution with unit variance}, we have found a better statistic $S^{*}=\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}$ since:
	\begin{equation*}
		\Var(\overline{X})=\frac{1}{n}\leq\Var(X_{1})
	\end{equation*}
	Therefore, $T'=X_{1}$ is not a UMVUE.
\end{eg}

\section{Complete Statistics}
In addition to sufficiency, we would need completeness in order to find the UMVUE.
\begin{defn}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a PDF $f(x|\theta)$ and a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$ for an integer $k>1$. A set of statistics $\{T_{1},\cdots,T_{r}\}$, where $r\geq k$ and $T_{1}=T_{1}(\mathbf{X})$ for $i=1,\cdots,r$, is said to be \textbf{jointly complete} if and only if for any function $g$:
	\begin{equation*}
		\E[g(T_{1},\cdots,T_{r})]=0\text{ for all }\theta\in\Theta\implies\prob(g(T_{1},\cdots,T_{r})=0)=1\text{ for all }\theta\in\Theta
	\end{equation*}
\end{defn}
or in one-parameter case,
\begin{defn}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}$. A statistic $T=T(\mathbf{X})$ is said to be \textbf{complete} if and only if for any function $g$:
	\begin{equation*}
		\E[g(T)]=0\text{ for all }\theta\in\Theta\implies\prob(g(T)=0)=1\text{ for all }\theta\in\Theta
	\end{equation*} 
\end{defn}
\begin{rem}
	Function $g(T)$ or $g(T_{1},\cdots,T_{r})$ is not an unbiased estimator for $\theta$.
\end{rem}
\begin{rem}
	If there exists a function $g^{*}$ such that $\E[g^{*}(T)]=0$ but $\prob(g^{*}(T)\neq 0)>0$, then $T$ is not complete. This is the same for joint completeness. 
\end{rem}

\newpage
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Bern(\theta)$, where $\theta\in(0,1)$ is unknown. Let $T_{1}=X_{1}-X_{2}$. We may easily find that for all $\theta\in(0,1)$:
	\begin{equation*}
		\E(X_{1}-X_{2})=0\text{ but }\prob(X_{1}-X_{2}\neq 0)>0
	\end{equation*}
	Therefore, $T_{1}=X_{1}-X_{2}$ is not a complete statistic.\\
	Let $T_{2}=\sum_{i=1}^{n}X_{i}$. For any function $g$,
	\begin{equation*}
		\E[g(T_{2})]=\sum_{i=0}^{n}g(t)\binom{n}{t}\theta^{t}(1-\theta)^{n-t}=(1-\theta)^{n}\sum_{i=1}^{n}g(t)\binom{n}{t}\left(\frac{\theta}{1-\theta}\right)^{t}
	\end{equation*}
	Thus, $\E(g(T_{2}))=0$ for all $\theta\in(0,1)$ implies that the equation $\sum_{i=1}^{n}g(t)\binom{n}{t}\left(\frac{\theta}{1-\theta}\right)^{t}=0$ for all $\theta\in(0,1)$.\\
	If not all coefficients $g(t)\binom{n}{t}$ are equal to zero, then there are at most $n$ solutions of the equation for all $\theta\in(0,1)$. This means only $n$ values of $\theta\in\Theta$ satisfy the equation, but not all $\theta\in(0,1)$.\\
	Therefore, $g(t)\binom{n}{t}=0$ and thus $g(t)=0$ for $t=0,\cdots,n$ and for all $\theta\in(0,1)$.\\
	Since the only possible values of $T_{2}=\sum_{i=1}^{n}X_{i}$ are in $\{0,\cdots,n\}$, we find that:
	\begin{equation*}
		\prob(g(T_{2})=0)=1
	\end{equation*}
	We conclude that $T_{2}=\sum_{i=1}^{n}X_{i}$ is complete.
\end{eg}
\begin{eg}
	\label{Chapter 3 (Example) complete and sufficient statistic of U[0,theta]}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\U[0,\theta]$, where $\theta>0$ is unknown. We check if the sufficient statistic $X_{(n)}$ is complete. Note that for any function $g$,
	\begin{equation*}
		\E[g(X_{(n)})]=\intinfty g(y)f_{X_{(n)}}(y)\,dy=\frac{n}{\theta^{n}}\int_{0}^{\theta}g(y)y^{n-1}\,dy
	\end{equation*}
	Therefore, if $\E[g(X_{(n)})]=0$ for all $\theta>0$, then:
	\begin{equation*}
		\int_{0}^{\theta}g(y)y^{n-1}\,dy=0
	\end{equation*}
	Differentiating both sides with respect to $\theta$ will get $g(\theta)\theta^{n-1}=0$ and hence $g(\theta)=0$ for $\theta>0$. We replace a number $y$ with the parameter $\theta$ and get $g(y)=0$ for $y\in(0,\theta]$ for all $\theta>0$.\\
	Since $0\leq x_{(n)}\leq\theta$, we may find that for all $\theta\in\Theta$:
	\begin{equation*}
		\prob(g(X_{(n)})=0)=1
	\end{equation*}
	Therefore, $X_{(n)}$ is complete.
\end{eg}

\section{Exponential Family}
In most of the time, it is quite difficult to check the completeness and minimal sufficiency of a statistic by definition, especially for joint completeness. However, there is one special distribution which we can check easily. It is called exponential family.
\begin{defn}
	Suppose that a random variable $X$ has a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$ for an integer $k>1$. If we find that:
	\begin{enumerate}
		\item $\supp(X)$ does not depend on $\theta$.
		\item The PDF or PMF of $X$ can be written in the form of:
		\begin{equation*}
			\exp\left(a(\theta)+b(x)+\sum_{j=1}^{k}c_{j}(\theta)d_{j}(x)\right)
		\end{equation*}
		where $a(\theta)$, $b(x)$, $c_{i}(\theta)$ and $d_{i}(x)$ for $i=1,\cdots,k$, are real-valued functions.
	\end{enumerate}
	then the distribution of $X$ is said to be an member of \textbf{$k$-parameter exponential family}.
\end{defn}

\newpage
or in one-parameter case,
\begin{defn}
	Suppose that a random variable $X$ has a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}$. If we find that:
	\begin{enumerate}
		\item $\supp(X)$ does not depend on $\theta$.
		\item The PDF or PMF of $X$ can be written in the form of:
		\begin{equation*}
			\exp[a(\theta)+b(x)+c(\theta)d(x)]
		\end{equation*}
		where $a(\theta)$, $b(x)$, $c(\theta)$ and $d(x)$ are real-valued functions.
	\end{enumerate}
	then the distribution of $X$ is said the be an member of \textbf{one-parameter exponential family}.
\end{defn}
\begin{rem}
	The distribution whose support depends on $\theta$ does not belong to the exponential family. E.g. $\U[0,\theta]$.
\end{rem}
\begin{rem}
	Most of the parametric distributions we discussed are members of an exponential family. E.g. normal distribution, gamma distribution, Poisson distribution, binomial distribution, and so on.
\end{rem}
The following results show that we can find complete and minimal sufficient and statistic from exponential family.
\begin{thm}
	\label{Chapter 3 (Theorem) Exponential family to complete and minimal sufficient statistic}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a distribution in an one-parameter exponential family with a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}$, that can be written in the form of:
	\begin{equation*}
		\exp[a(\theta)+b(x)+c(\theta)d(x)]
	\end{equation*}
	Then, $\sum_{i=1}^{n}d(X_{i})$ is a complete and minimal sufficient statistic.
\end{thm}
\begin{thm}
	\label{Chapter 3 (Theorem) Exponential family to jointly complete and minimal sufficient statistics}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a distribution in a $k$-parameter exponential family with a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$ for an integer $k>1$, that can be written in the form of:
	\begin{equation*}
		\exp\left(a(\theta)+b(x)+\sum_{j=1}^{k}c_{j}(\theta)d_{j}(x)\right)
	\end{equation*}
	Then the set $\{\sum_{i=1}^{n}d_{1}(X_{i}),\cdots,\sum_{i=1}^{n}d_{k}(X_{i})\}$ is a set of jointly complete and minimal sufficient statistics.
\end{thm}
\begin{eg}
	\label{Chapter 3 (Example) Complete and minimal sufficient statistic of Poisson distribution}
	Consider a random sample from $\Poisson(\lambda)$, where $\lambda\in(0,\infty)$ is unknown. We have:
	\begin{equation*}
		p(x|\lambda)=\frac{\lambda^{x}e^{-\lambda}}{x!}=\exp[-\lambda-\ln(x!)+x\ln{\lambda}]
	\end{equation*}
	Since the support $\{0,1,\cdots\}$ does not depend on $\lambda$, by Theorem \ref{Chapter 3 (Theorem) Exponential family to complete and minimal sufficient statistic}, $\sum_{i=1}^{n}X_{i}$ is a complete and minimal sufficient statistic.
\end{eg}
\begin{eg}
	Consider a random sample from $\Bern(\theta)$, where $\theta\in(0,1)$ is unknown. We have:
	\begin{equation*}
		p(x|\theta)=\theta^{x}(1-\theta)^{1-x}=\exp\left[\ln(1-\theta)+x\ln\left(\frac{\theta}{1-\theta}\right)\right]
	\end{equation*}
	Since the support $\{0,1\}$ does not depend on $\theta$, by Theorem \ref{Chapter 3 (Theorem) Exponential family to complete and minimal sufficient statistic}, $\sum_{i=1}^{n}X_{i}$ is a complete and minimal sufficient statistic.
\end{eg}
\begin{eg}
	Consider a random sample from $\N(\mu,\sigma^{2})$, where $\mu$ and $\sigma>0$ are unknown. We have:
	\begin{equation*}
		f(x|\mu,\sigma^{2})=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right)=\exp\left(-\frac{\mu^{2}}{2\sigma^{2}}-\frac{1}{2}\ln(2\pi\sigma^{2})+\frac{\mu}{\sigma^{2}}x-\frac{1}{2\sigma^{2}}x^{2}\right)
	\end{equation*}
	Since the support does not depend on $\mu$ and $\sigma^{2}$, by Theorem \ref{Chapter 3 (Theorem) Exponential family to jointly complete and minimal sufficient statistics}, $\sum_{i=1}^{n}X_{i}$ and $\sum_{i=1}^{n}X_{i}^{2}$ are jointly complete and minimal sufficient statistics.
\end{eg}

\newpage
\section{Relationship of completeness and sufficiency with UMVUE}
We still haven't explained why complete and minimal sufficient statistic can lead to UMVUE. This is due to the following theorem.
\begin{thm}\named{Lehmann-Scheff\'e Theorem} Let $CS$ be a complete and (minimal) sufficient statistic. If there exists a function $h(CS)$ which is unbiased for $g(\theta)$, then $h(CS)$ is the unique UMVUE of $g(\theta)$.
\end{thm}
\begin{thm}
	Let $CS$ be a complete and (minimal) sufficient statistic. If $\E[f(\mathbf{X})]=g(\theta)$ for all $\theta$, then $h(CS)=\E[f(\mathbf{X})|CS]$ is the UMVUE for $g(\theta)$.
\end{thm}
\begin{rem}
	From this theorem, we can formulate some strategies to find the UMVUE which is a function of $CS$:
	\begin{enumerate}
		\item Guess the correct form of the function of $CS$.
		\item Solve for $h(CS)$ by $\E[h(CS)]=g(\theta)$.
		\item Use Rao-Blackwell Theorem to construct $h(CS)$ by guessing or finding any unbiased estimator $T$ for $g(\theta)$ and then evaluating $h(CS)=\E(T|CS)$.
	\end{enumerate}
\end{rem}
\begin{eg}
	\label{Chapter 3 (Example) UMVUE of Exp(theta)}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $X\sim\Exp(\theta)$, where $\theta\in(0,\infty)$ is unknown. Find the UMVUE of $g(\theta)=\theta$. We try Strategy 1.\\
	We have $\theta=\frac{1}{\E(X)}$. Since exponential distribution of $X$ belongs to an exponential family, we can find that $CS=\sum_{i=1}^{n}X_{i}$. We suspect that UMVUE is related to $\frac{n}{\sum_{i=1}^{n}X_{i}}$. For $n>1$, since $\Exp(\theta)=\Gam(1,\theta)$,
	\begin{equation*}
		\E\left(\frac{1}{\sum_{i=1}^{n}X_{i}}\right)=\int_{0}^{\infty}\frac{\theta^{n}}{x\Gamma(n)}x^{n-1}e^{-\theta x}\,dx=\frac{\theta}{\Gamma(n)}\int_{0}^{\infty}\theta(\theta x)^{n-2}e^{-\theta x}\,dx=\frac{\theta\Gamma(n-1)}{\Gamma(n)}=\frac{\theta}{n-1}
	\end{equation*}
	Therefore, we have found that $\frac{n-1}{\sum_{i=1}^{n}X_{i}}$ is the UMVUE.
\end{eg}
\begin{eg}
	We continue the example above. This time we try Strategy 2, which is solving for $h(CS)$ by $\E[h(CS)]=g(\theta)=\theta$.
	\begin{align*}
		\int_{0}^{\infty}h(x)\frac{\theta^{n}}{\Gamma(n)}x^{n-1}e^{-\theta x}\,dx&=\theta\\
		\int_{0}^{\infty}h(x)\frac{\theta^{n-1}}{\Gamma(n)}x^{n-1}e^{-\theta x}\,dx&=1\\
		\int_{0}^{\infty}\left(h(x)\frac{x}{n-1}\right)\frac{\theta^{n-1}}{\Gamma(n-1)}x^{(n-1)-1}e^{-\theta x}\,dx&=1
	\end{align*} 
	It is only true if $h(x)\frac{x}{n-1}=1$ for all $s>0$. Thus, $h(x)=\frac{n-1}{x}$ and therefore:
	\begin{equation*}
		h\left(\sum_{i=1}^{n}X_{i}\right)=\frac{n-1}{\sum_{i=1}^{n}X_{i}}
	\end{equation*}
	Since it is unbiased for $\theta$ and is a function of $CS$, by Lehmann-Scheff\'e Theorem, it is the UMVUE of $\theta$.
\end{eg}

\newpage
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Poisson(\lambda)$, where $\lambda\in(0,\infty)$ is unknown. Find the UMVUE of $g(\lambda)=e^{-\lambda}$. We try Strategy 3.\\
	From Example \ref{Chapter 3 (Example) Complete and minimal sufficient statistic of Poisson distribution}, $\sum_{i=1}^{n}X_{i}$ is a complete and minimal sufficient statistic. Note that $g(\lambda)=e^{-\lambda}=\prob(X_{1}=0)=\mathbf{1}_{X_{1}=0}$ and thus it is a trivial unbiased estimator of $g(\lambda)$. By Rao-Blackwell Theorem, $\E(\mathbf{1}_{X_{1}=0}|\sum_{i=1}^{n}X_{i})$ is unbiased for $g(\lambda)$. By Lehmann-Scheff\'e Theorem, it is the unique UMVUE of $g(\lambda)$. We compute the UMVUE.\\
	For $n=1$,
	\begin{equation*}
		\E\left(\mathbf{1}_{X_{1}=0}\left|\sum_{i=1}^{n}X_{i}\right.\right)=\E(\mathbf{1}_{X_{1}=0}|X_{1})=\prob(X_{1}=0|X_{1})=\mathbf{1}_{X_{1}=0}
	\end{equation*}
	For $n>1$,
	\begin{align*}
		\E\left(\mathbf{1}_{X_{1}=0}\left|\sum_{i=1}^{n}X_{i}=s\right.\right)=\prob\left(X_{1}=0\left|\sum_{i=1}^{n}X_{i}=s\right.\right)&=\frac{\prob(X_{1}=0,\sum_{i=1}^{n}X_{i}=s)}{\prob(\sum_{i=1}^{n}X_{i}=s)}\\
		&=\frac{\prob(X_{1}=0)\prob(\sum_{i=2}^{n}X_{i}=s)}{\prob(\sum_{i=1}^{n}X_{i}=s)}\\
		&=\frac{e^{-\lambda}e^{-(n-1)\lambda}[(n-1)\lambda]^{s}s!}{e^{-n\lambda}(n\lambda)^{s}s!}\\
		&=\left(\frac{n-1}{n}\right)^{s}
	\end{align*}
	Therefore, the UMVUE for $g(\lambda)=e^{-\lambda}$ is:
	\begin{equation*}
		\E\left(\mathbf{1}_{X_{1}=0}\left|\sum_{i=1}^{n}X_{i}\right.\right)=\begin{cases}
			\mathbf{1}_{X_{1}=0}, &n=1\\
			\left(\frac{n-1}{n}\right)^{\sum_{i=1}^{n}X_{i}}, &n>1
		\end{cases}
	\end{equation*}
\end{eg}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\U[0,\theta]$, where $\theta>0$ is unknown. Since uniform distribution is not in an exponential family, we cannot use Theorem \ref{Chapter 3 (Theorem) Exponential family to complete and minimal sufficient statistic} to find a complete and minimal sufficient statistic.\\
	From Example \ref{Chapter 3 (Example) complete and sufficient statistic of U[0,theta]}, we have found that $X_{(n)}$ is a complete and sufficient statistic. By checking for unbiasedness,
	\begin{equation*}
		\E(X_{(n)})=\frac{n}{n+1}\theta
	\end{equation*}
	Therefore, by Lehmann-Scheff\'e Theorem, the UMVUE of $\theta$ is $\frac{n+1}{n}X_{(n)}$.
\end{eg}
\section{Cram\'er-Rao Inequality}
Recall Theorem \ref{Chapter 2 (Thoerem) Sequence of MME is asympt. normal}, we claim that a sequence of MLE is asymptotically efficient, which means that:
\begin{equation*}
	\mathcal{I}_{X}^{-1}(\theta)
\end{equation*}
is the lowest possible bound for any unbiased estimator. The reason is because of the Cram\'er-Rao Inequality (C-R Inequality).
\begin{thm}\named{Cram\'er-Rao Inequality}
	Under the regularity conditions, the variance of an unbiased estimator $T(\mathbf{X})=T(X_{1},\cdots,X_{n})$ for $\theta$, based on a set of random variables $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ from their joint PDF $f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)$ satisfies the following inequality:
	\begin{equation*}
		\Var(T(\mathbf{X}))\geq\frac{1}{\mathcal{I}_{X_{1},\cdots,X_{n}}(\theta)}=\frac{1}{\E\left[\odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}\right]^{2}}
	\end{equation*}
	The lower bound is called \textbf{Cram\'er-Rao lower bound} (CRLB).
\end{thm}
\begin{rem}
	The Cram\'er-Rao Inequality can be written as:
	\begin{equation*}
		\Var(T(\mathbf{X}))\geq\frac{1}{-\E\left[\odv*[order={2}]{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}\right]}
	\end{equation*}
\end{rem}

\newpage
\begin{rem}
	If $\mathbf{X}$ is a random sample of size $n$, then we have:
	\begin{equation*}
		\Var(T(\mathbf{X}))\geq\frac{1}{n\mathcal{I}_{X_{1}}(\theta)}=\frac{1}{n\E\left[\odv*{\ln{f_{X_{1}}(X_{1}|\theta)}}{\theta}\right]^{2}}=\frac{1}{-n\E\left[\odv*[order={2}]{\ln{f_{X_{1}}(X_{1}|\theta)}}{\theta}\right]}
	\end{equation*}
\end{rem}
\begin{eg}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\N(\theta,\sigma^{2})$, where $\sigma^{2}$ is known and $\theta$ is unknown. The CRLB for $\theta$ is:
	\begin{equation*}
		\frac{1}{n\mathcal{I}_{X_{1}}(\theta)}=\frac{\sigma^{2}}{n}
	\end{equation*}
\end{eg}
\begin{eg}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Bern(p)$, where $p$ is unknown. The CRLB for $p$ is:
	\begin{equation*}
		\frac{1}{n\mathcal{I}_{X_{1}}(p)}=\frac{p(1-p)}{n}
	\end{equation*}
\end{eg}
Often time, we want to estimate a function of $\theta$, $g(\theta)$, instead of $\theta$.
\begin{thm}
	Under the regularity conditions, if $T(\mathbf{X})=T(X_{1},\cdots,X_{n})$ is an unbiased estimator for $g(\theta)$, then the Cram\'er-Rao Inequality for $g(\theta)$ is:
	\begin{equation*}
		\Var(T(\mathbf{X}))\geq\frac{\left[\odv*{g(\theta)}{\theta}\right]^{2}}{\mathcal{I}_{X_{1},\cdots,X_{n}}(\theta)}=\frac{\left[\odv*{g(\theta)}{\theta}\right]^{2}}{\E\left[\odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}\right]^{2}}
	\end{equation*}
\end{thm}
\begin{proofing}
	Let $U=\odv*{\ln{f_{\mathbf{X}}(\mathbf{X}|\theta)}}{\theta}$ and $V=T(\mathbf{X})$. We have:
	\begin{equation*}
		-1\leq\frac{\cov(U,V)}{\sqrt{\Var(U)\Var(V)}}\leq 1\implies\frac{(\cov(U,V))^{2}}{\Var(U)}\leq\Var(V)
	\end{equation*}
	We may find that $\Var(U)=\Var\left(\odv*{\ln{f_{\mathbf{X}}(\mathbf{X}|\theta)}}{\theta}\right)=\mathcal{I}_{\mathbf{X}}(\theta)$. In addition,
	\begin{align*}
		\cov\left(\odv*{\ln{f_{\mathbf{X}}(\mathbf{X}|\theta)}}{\theta},T(\mathbf{X})\right)&=\E\left[T(\mathbf{X})\odv*{\ln{f_{\mathbf{X}}(\mathbf{X}|\theta)}}{\theta}\right]-\E\left[\odv*{\ln{f_{\mathbf{X}}(\mathbf{X}|\theta)}}{\theta}\right]\E[T(\mathbf{X})]\\
		&=\E\left[T(\mathbf{X})\odv*{\ln{f_{\mathbf{X}}(\mathbf{X}|\theta)}}{\theta}\right]\\
		&=\intinfty\cdots\intinfty T(\mathbf{x})\left(\odv*{\ln{f_{\mathbf{X}}(\mathbf{x}|\theta)}}{\theta}\right)f_{\mathbf{X}}(\mathbf{x}|\theta)\,d\mathbf{x}\\
		&=\intinfty\cdots\intinfty T(\mathbf{x})\odv*{f_{\mathbf{X}}(\mathbf{x}|\theta)}{\theta}\,d\mathbf{x}\\
		&=\odv*{\E[T(\mathbf{X})]}{\theta}\\
		&=\odv*{g(\theta)}{\theta}
	\end{align*}
	Therefore, we have that:
	\begin{equation*}
		\Var(T(\mathbf{X}))\geq\frac{\left[\odv*{g(\theta)}{\theta}\right]^{2}}{\mathcal{I}_{\mathbf{X}}(\theta)}
	\end{equation*}
\end{proofing}
\begin{rem}
	Since CRLB is the lowest bound of variance for any unbiased estimator, any unbiased estimator whose variance can achieve the CRLB for $g(\theta)$ is the UMVUE for $g(\theta)$.
\end{rem}
\begin{rem}
	It is not necessary that a UMVUE has a variance equal to the CRLB.
\end{rem}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Exp(\theta)$, where $\theta\in(0,\infty)$ is unknown.\\
	The CRLB of $\theta$ is $\frac{\theta^{2}}{n}$. From Example \ref{Chapter 3 (Example) UMVUE of Exp(theta)}, we have found that $\frac{n-1}{\sum_{i=1}^{n}X_{i}}$ is the UMVUE of $\theta$ when $n>1$.\\
	After some tedious calculation, for $n>2$, we would find that $\Var\left(\frac{n-1}{\sum_{i=1}^{n}X_{i}}\right)=\frac{\theta^{2}}{n-2}\geq\frac{\theta^{2}}{n}$.
\end{eg}

\newpage
When would the equality for C-R inequality holds?
\begin{thm}
	\label{Chapter 3 (Theorem) C-R equality}
	Under the regularity conditions, the C-R equality holds if and only if:
	\begin{equation*}
		\odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}=A(\theta,n)[T(X_{1},\cdots,X_{n})-g(\theta)]
	\end{equation*}
	where $A(\theta,n)$ is a non-zero function. The statistic $T(X_{1},\cdots,X_{n})$ is an UMVUE of $g(\theta)$. 
\end{thm}
This theorem has an interesting result that if we can write $\odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}$ into $A(\theta,n)[T(X_{1},\cdots,X_{n})-g(\theta)]$, then the statistic must be an UMVUE.
\begin{lem}
	If $T(X_{1},\cdots,X_{n})$ is an UMVUE of $g(\theta)$ such that C-R equality holds, then $aT(X_{1},\cdots,X_{n})+b$ is an UMVUE of $ag(\theta)+b$, where $a\neq 0$.
\end{lem}
\begin{proofing}
	\begin{equation*}
		\odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}=A(\theta,n)[T(X_{1},\cdots,X_{n})-g(\theta)]=\frac{A(\theta,n)}{a}[(aT(X_{1},\cdots,X_{n})+b)-(ag(\theta)+b)]
	\end{equation*}
	By having $A^{*}(\theta,n)=\frac{A(\theta,n)}{a}$, we can find that $aT(X_{1},\cdots,X_{n})+b$ is an UMVUE of $ag(\theta)+b$.
\end{proofing}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Poisson(\lambda)$, where $\lambda$ is unknown.
	\begin{align*}
		\odv*{\ln{p_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\lambda)}}{\lambda}&=\odv*{\ln\left(\prod_{i=1}^{n}\frac{\lambda^{X_{i}}e^{-\lambda}}{X_{i}!}\right)}{\lambda}\\
		&=\odv*{\sum_{i=1}^{n}[X_{i}\ln{\lambda}-\lambda-\ln(X_{i}!)]}{\lambda}\\
		&=\sum_{i=1}^{n}\left(\frac{X_{i}}{\lambda}-1\right)\\
		&=\frac{n}{\lambda}(\overline{X}-\lambda)
	\end{align*}
	Therefore, by Theorem \ref{Chapter 3 (Theorem) C-R equality}, $\overline{X}$ is an UMVUE of $\lambda$ and:
	\begin{equation*}
		\Var(\overline{X})=\frac{1}{n\mathcal{I}_{X_{1}}(\lambda)}=\frac{\lambda}{n}
	\end{equation*}
\end{eg}
\begin{rem}
	For any particular function of $\theta$ other than Euclidean transformation, Theorem \ref{Chapter 3 (Theorem) C-R equality} is not useful.
\end{rem}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Exp(\theta)$, where $\theta$ is unknown.
	\begin{align*}
		\odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}&=\odv*{\ln\left(\prod_{i=1}^{n}\theta e^{-\theta X_{i}}\right)}{\theta}\\
		&=\odv*{\sum_{i=1}^{n}(\ln{\theta}-\theta X_{i})}{\theta}\\
		&=\sum_{i=1}^{n}\left(\frac{1}{\theta}-X_{i}\right)\\
		&=-n\left(\overline{X}-\frac{1}{\theta}\right)
	\end{align*}
	Therefore, by Theorem \ref{Chapter 3 (Theorem) C-R equality}, $\overline{X}$ is an UMVUE of $\frac{1}{\theta}$ and:
	\begin{equation*}
		\Var(\overline{X})=\frac{1}{n\mathcal{I}_{X_{1}}(\theta)}=\frac{\theta^{2}}{n}
	\end{equation*}
	However, since $\theta$ cannot be written as Euclidean transformation of $\frac{1}{\theta}$, we cannot use the Theorem to find the UMVUE of $\theta$.
\end{eg}
\begin{rem}
	Theorem \ref{Chapter 3 (Theorem) C-R equality} also can only be used under regularity conditions. For instance, for $\U[0,\theta]$, we cannot use this Theorem.
\end{rem}

\chapter{Hypothesis Testing}
This chapter will be primarily focus on comparing different unbiased point estimators. 

In engineering and science field, people usually hypothesize something about a system. Before they prove the conjecture using experimental data, they need to define a hypothesis.
\begin{defn}
	\textbf{Statistical hypothesis} is an assertion or conjecture of the random variable of our interest. If a parametric distribution is considered, then a statistical hypothesis can be a conjecture about the true value of the unknown parameters of the parametric distribution.
\end{defn}
\begin{eg}
	\label{Chapter 4 (Example) Engineer example}
	An engineer decides on the basis of sample data whether the true average lifetime of a certain kind of tire is at least $22,000$ miles.
	
	The engineer has to test the hypothesis that $\theta$ in $\Exp(\theta)$ is at least $22,000$.
\end{eg}
\begin{eg}
	An agronomist want to decide on the basis of experiments whether one kind of fertilizer produces a higher yield of soybeans than another.
	
	The agronomist has to test the hypothesis that $\mu_{1}>\mu_{2}$ from two distributions $\N(\mu_{1},\sigma_{1}^{2})$ and $\N(\mu_{2},\sigma_{2}^{2})$.
\end{eg}
\begin{eg}
	\label{Chapter 4 (Example) Manufacturer example}
	A manufacturer of pharmaceutical products decides on the basis of samples whether $90\%$ of all patients given a new medication will recover from a certain disease.
	
	The manufacturer has to test the hypothesis that $\theta$ in $\Bin(n,\theta)$ equals $0.90$.
\end{eg}

\section{Null and Alternative Hypotheses}
The hypothesis of our interest is related to a particular class of $\theta$, say $\Theta_{0}$, and its complement $\Theta_{1}$. This two classes are subsets of the parameter space $\Theta$ of $\theta$. We have $\Theta_{0}\cup\Theta_{1}\subseteq\Theta$ and $\Theta_{0}\cap\Theta_{1}=\emptyset$.
\begin{defn}
	The hypothesis with $\theta\in\Theta_{0}$ is the \textbf{null hypothesis} $H_{0}$, and the hypothesis with $\theta\in\Theta_{1}$ is the \textbf{alternative hypothesis}.
\end{defn}
\begin{rem}
	We usually use the signs with implied equals in $H_{0}$.
\end{rem}
\begin{defn}
	The hypothesis is \textbf{simple} if the parametric distribution would be fully specified under the hypothesis. Otherwise, the hypothesis is \textbf{composite}.
\end{defn}
\begin{eg}
	Using the Example \ref{Chapter 4 (Example) Engineer example}, we have that:
	\begin{equation*}
		\begin{cases}
			H_{0}: & \theta\geq 22,000\\
			H_{1}: & \theta<22,000
		\end{cases}
	\end{equation*}
	Both $H_{0}$ and $H_{1}$ are composite because it does not specify the parameter.
\end{eg}

\newpage
\begin{eg}
	Using the Example \ref{Chapter 4 (Example) Manufacturer example}, we have that:
	\begin{equation*}
		\begin{cases}
			H_{0}: &\theta=0.9\\
			H_{1}: &\theta\neq 0.9
		\end{cases}
	\end{equation*}
	$H_{0}$ is simple while $H_{1}$ is composite.
\end{eg}
In hypothesis testing, we want to see whether or not we can find an evidence to say that $H_{0}$ is false.
\begin{rem}
	Hypothesis testing usually follows these three steps:
	\begin{enumerate}
		\item Determine $H_{0}$ and $H_{1}$.
		\item Under $H_{0}$, define a rare event, event that happens with a very small probability in one experiment with $n$ data.
		\item Collect data.
		\begin{enumerate}
			\item If data causes the rare event to happen, it contradicts $H_{0}$. This mean we can say that $H_{0}$ is false and reject $H_{0}$.
			\item If data does not cause the rare event to happen, it does not contradicts $H_{0}$. This mean we cannot say that $H_{0}$ is false and we do not reject $H_{0}$.
		\end{enumerate}
	\end{enumerate}
\end{rem}
\begin{rem}
	Not rejecting $H_{0}$ does not mean we accept $H_{0}$. It just mean there is no sufficient evidence to reject $H_{0}$. The whole idea is to try gathering enough evidence to have great confidence that $H_{0}$ is false and $H_{1}$ is true.
\end{rem}
\begin{eg}
	We want to know whether or not a coin is fair. Consider a random experiment of flipping the coin $10$ times. We may determine that:
	\begin{equation*}
		\begin{cases}
			H_{0}: &\prob(\{H\})=\prob(\{T\})=0.5\\
			H_{1}: &\prob(\{H\})\neq\prob(\{T\})
		\end{cases}
	\end{equation*}
	Under $H_{0}$, we define that the event of getting $10$ tails to be the rare event under $H_{0}$ since the probability of getting $10$ tails in one experiment is $0.5^{10}\approx 0.00098$.
	
	We can them preform the experiment to collect data by flipping the coin $10$ times. If we get $10$ tails, then the collected data tells us that getting $10$ tails is not a rare event, which contradicts $H_{0}$. Therefore, we have evidence to suspect the reliability of $H_{0}$ and thus reject $H_{0}$ and accept $H_{1}$.
\end{eg}

\section{Test Errors and Error Probabilities}
After we decide the null and alternative hypotheses, we need to determine a test statistic, i.e. the point estimator, to construct a test of rejecting or not rejecting the null hypothesis.
\begin{defn}
	\textbf{Non-rejection region} $C_{0}$ is a subset of $\Theta$ such that we do not reject $H_{0}$, i.e.
	\begin{equation*}
		C_{0}=\{\mathbf{x}:\text{Not reject }H_{0}\}
	\end{equation*}
	\textbf{Rejection region} $C_{1}$ is a subset of $\Theta$ such that we reject $H_{0}$, i.e.
	\begin{equation*}
		C_{1}=\{\mathbf{x}:\text{Reject }H_{0}\}
	\end{equation*}
\end{defn}
However, there is no prefect test statement due to randomness of sample of data. Each test would lead to the following two kinds of errors.
\begin{defn}
	\textbf{Type I Error} is the error of rejecting $H_{0}$ when it is true. \textbf{Type II Error} is the error of not rejecting $H_{0}$ when it is false.
\end{defn}
\begin{figure}[h]
	\centering
	\begin{tabular}{||c|c|c|}
		\hline
		& Not reject $H_{0}$ & Reject $H_{0}$\\
		\hline
		If $H_{0}$ is true & No error & Type I Error\\
		If $H_{0}$ is false & Type II Error & No error\\
		\hline
	\end{tabular}
\end{figure}

\newpage
We may define their corresponding probabilities.
\begin{defn}
	\textbf{Type I error probability}, denoted by $\gamma(\theta)$, is the probability of rejecting $H_{0}$ for $\theta\in\Theta_{0}$.
	\begin{equation*}
		\gamma(\theta)=\prob(\text{Reject }H_{0}|\theta)=\prob(\mathbf{X}\in C_{1}|\theta)
	\end{equation*}
	\textbf{Type II error probability}, denoted by $\beta(\theta)$, is the probability of not rejecting $H_{0}$ for $\theta\in\Theta_{1}$.
	\begin{equation*}
		\beta(\theta)=\prob(\text{Not reject }H_{0}|\theta)=\prob(\mathbf{X}\in C_{0}|\theta)
	\end{equation*}
\end{defn}
\begin{rem}
	Since we cannot control $\gamma(\theta)$ and $\beta(\theta)$ at the same time, conventionally, we assign an upper bound to $\gamma(\theta)$ over $\Theta_{0}$ and find a test with $\beta(\theta)$ as small as possible. If we are dealing with continuous case,
	\begin{equation*}
		\sup_{\theta\in\Theta_{0}}\gamma(\theta)=\alpha
	\end{equation*}
	If we are dealing with discrete case,
	\begin{equation*}
		\sup_{\theta\in\Theta_{0}}\gamma(\theta)\leq\alpha
	\end{equation*}
	We usually call $\alpha$ the \textbf{significance threshold} or \textbf{significance level}. $\sup$ can be replaced with $\max$ if it exists.
\end{rem}
\begin{rem}
	We use a point estimator $T(\mathbf{X})$ to formulate our test with:
	\begin{enumerate}
		\item \textbf{One-sided right tests}: $C_{1}=\{\mathbf{x}:T(\mathbf{x})>k\}$ or $H_{1}:\theta>\theta_{0}$
		\item \textbf{One-sided left tests}: $C_{1}=\{\mathbf{x}:T(\mathbf{x})<k\}$ or $H_{1}:\theta<\theta_{0}$
		\item \textbf{Two-sided tests}: $C_{1}=\{\mathbf{x}:T(\mathbf{x})<k_{1}\text{ or }T(\mathbf{x})>k_{2}\}$ or $H_{1}:\theta\neq\theta_{0}$
	\end{enumerate}
	For one-sided tests, $k$ can be obtained by solving:
	\begin{equation*}
		\sup_{\theta\in\Theta_{0}}\prob(\mathbf{X}\in C_{1}|\theta)\begin{cases}
			=\alpha, &\text{Continuous case}\\
			\leq\alpha, &\text{Discrete case}
		\end{cases}
	\end{equation*}
	For two-sided tests, $k_{1}$ and $k_{2}$ can be obtained by solving:
	\begin{align*}
		\prob(T(\mathbf{X})<k_{1}|\theta_{0})&=\frac{\alpha}{2} & \prob(T(\mathbf{X})>k_{2}|\theta_{0})&=\frac{\alpha}{2}
	\end{align*}
	If they cannot be found exactly (such as when we cannot determine the exact distribution of $T(\mathbf{X})$), then we can approximate them by using the limiting distribution of $T(\mathbf{X})$ or simplified terms.
\end{rem}
\begin{eg}
	Assume that we have a random sample $X\sim\N(\mu,\sigma^{2})$ where $\sigma$ is known. We want to see if $\mu\geq 3423$ with $2\%$ significance level. Let $\mathbf{X}$ be a random sample of size $n$ from $X$. We define the test as follows:
	\begin{align*}
		T(\mathbf{X})&=\overline{X} & &\begin{cases}
			H_{0}: &\mu\geq3423\\
			H_{1}: &\mu<3423
		\end{cases} & C_{1}&=\{\mathbf{x}:\overline{x}<k\}
	\end{align*}
	We find $k$ by solving:
	\begin{equation*}
		0.02=\max_{\mu\geq3423}\prob(\overline{X}<k|\mu)
	\end{equation*}
	It is obvious to see that:
	\begin{align*}
		0.02&=\max_{\mu\geq3423}\prob\left(Z<\frac{\sqrt{n}(k-\mu)}{\sigma}\right)=\prob\left(Z<\frac{\sqrt{n}(k-3423)}{\sigma}\right) & 0.98&=\prob\left(Z\geq\frac{\sqrt{n}(k-3423)}{\sigma}\right)
	\end{align*}
	Therefore, we can find that:
	\begin{equation*}
		k=3423-z_{0.02}\frac{\sigma}{\sqrt{n}}=3423+z_{0.98}\frac{\sigma}{\sqrt{n}}
	\end{equation*}
	We would reject $H_{0}$ at $\alpha=0.02$ if $\overline{x}<3423-z_{0.02}\frac{\sigma}{\sqrt{n}}$.
\end{eg}
However, in a lot of cases, $\theta$ may not be $\mu$ or $\sigma^{2}$. What do we do if the distribution of $T(\mathbf{X})$ cannot be easily determined?

\newpage
\section{Likelihood Test}
Similar to finding the MLE in Chapter 2, we also have a general method called likelihood ratio test.
\begin{defn}
	\textbf{Likelihood ratio test} (LRT) for testing $H_{0}:\theta\in\Theta_{0}$ against $H_{1}:\theta\in\Theta_{1}$ at a significance level of $\alpha$ is a test with a rejection region:
	\begin{equation*}
		C_{1}=\{\mathbf{x}:\lambda(\mathbf{x})\leq k\}
	\end{equation*}
	where $k\in(0,1)$ satisfies $\max_{\theta\in\Theta_{0}}\prob(\lambda(\mathbf{X})\leq k|\theta)=\alpha$ and the LRT statistic $\lambda$ is given by:
	\begin{equation*}
		\lambda(\mathbf{x})=\frac{L(\hat{\theta}_{0})}{L(\hat{\theta})}
	\end{equation*}
	with MLE $\hat{\theta}_{0}$ of $\theta$ over $\Theta_{0}$ and MLE $\hat{\theta}$ over $\Theta^{*}=\Theta_{0}\cup\Theta_{1}\subseteq\Theta$.
\end{defn}
\begin{rem}
	Since $\Theta_{0}\subset\Theta_{0}\cup\Theta_{1}$, we can see that $L(\hat{\theta}_{0})\leq L(\hat{\theta})$. Therefore, $0<\lambda(\mathbf{x})\leq 1$.
\end{rem}
\begin{rem}
	If $\lambda(\mathbf{x})$ is close to $0$, then it suggests that the data is not compatible with $H_{0}$. Therefore, $H_{0}$ should be rejected.
\end{rem}
\begin{rem}
	If the hypothesis is simple, then there is no point at finding the MLE. We use the hypothesized value of $\theta$ instead of MLE.
\end{rem}
\begin{eg}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Exp(\theta)$, where $\theta$ is unknown. We can construct a LRT at a significant level of $\alpha$:
	\begin{equation*}
		\begin{cases}
			H_{0}: &\theta=\theta_{0}\\
			H_{1}: &\theta>\theta_{0}
		\end{cases}
	\end{equation*}
	where $\theta_{0}$ is known and positive. Note that $\Theta_{0}=\{\theta_{0}\}$ and $\Theta_{1}=(\theta_{0},\infty)$. Parameter space $\Theta^{*}$ is restricted to be at least $\theta_{0}$. We may find that:
	\begin{equation*}
		\odv*{l(\theta)}{\theta}=\frac{n}{\theta}-n\overline{x}
	\end{equation*}
	Therefore, the MLE of $\theta$ over $\Theta^{*}$ is:
	\begin{equation*}
		\hat{\theta}=\max\biggl\{\theta_{0},\frac{1}{\overline{x}}\biggr\}
	\end{equation*}
	The likelihood ratio test statistic can be found by:
	\begin{align*}
		L(\hat{\theta}_{0})&=\theta_{0}^{n}e^{-n\theta_{0}\overline{x}} & L(\hat{\theta})&=\begin{cases}
			\left(\frac{1}{\overline{x}}\right)^{n}e^{-n}, &\text{if }\frac{1}{\overline{x}}>\theta_{0}\\
			\theta_{0}^{n}e^{-n\theta_{0}\overline{x}}, &\text{if }\frac{1}{\overline{x}}\leq\theta_{0}
		\end{cases}
	\end{align*}
	\begin{equation*}
		\lambda(\mathbf{x})=\begin{cases}
			\frac{\theta_{0}^{n}e^{-n\theta_{0}\overline{x}}}{(\overline{x})^{-n}e^{-n}}, &\text{if }\frac{1}{\overline{x}}>\theta_{0}\\
			1, &\text{if }\frac{1}{\overline{x}}\leq\theta_{0}
		\end{cases}=\begin{cases}
			(\theta_{0}\overline{x})^{n}e^{-n(\theta_{0}\overline{x}-1)}, &\text{if }\frac{1}{\overline{x}}>\theta_{0}\\
			1, &\text{if }\frac{1}{\overline{x}}\leq\theta_{0}
		\end{cases}
	\end{equation*}
	Therefore, we reject $H_{0}$ if $\frac{1}{\overline{x}}>\theta_{0}$ and $(\theta_{0}\overline{x})^{n}e^{-n(\theta_{0}\overline{x}-1)}\leq k$. But how do we determine $k$?
	
	If the term $(\theta_{0}\overline{x})^{n}e^{-n(\theta_{0}\overline{x}-1)}$ is a function of some quantity $y$, where the distribution of $Y$ can be easily determined, then the test based on $y$ will be equivalent to the original test.
	
	Let $y=\theta_{0}\overline{x}$. The function $y^{n}e^{-n(y-1)}$ attain its maximum at $y=1$. Therefore,
	\begin{equation*}
		C_{1}=\bigl\{\mathbf{x}:y<1\text{ and }y^{n}e^{-n(y-1)}\leq k\bigr\}=\{\mathbf{x}:y\leq K\in(0,1)\}=\biggl\{\mathbf{x}:\sum_{i=1}^{n}x_{i}\leq\frac{nK}{\theta_{0}}=K'\biggr\}
	\end{equation*}
	Therefore, we can reject $H_{0}$ when $\sum_{i=1}^{n}x_{i}\leq K'$, where $\sum_{i=1}^{n}X_{i}\sim\Gam(n,\theta)$. $K'$ can be determined by:
	\begin{equation*}
		\prob\left(\left.\sum_{i=1}^{n}X_{i}\leq K'\right|\theta_{0}\right)=\alpha
	\end{equation*}
\end{eg}

\newpage
\begin{eg}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\N(\theta,\sigma^{2})$, where both $\theta$ and $\sigma$ are unknown. We want to test at a significance level of $\alpha$ that:
	\begin{equation*}
		\begin{cases}
			H_{0}: &\theta=\theta_{0}\\
			H_{1}: &\theta\neq\theta_{0}
		\end{cases}
	\end{equation*}
	Since $H_{0}$ is simple, we have:
	\begin{align*}
		\hat{\theta}_{0}&=\theta_{0} & \hat{\sigma}_{0}^{2}=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\theta_{0})^{2}
	\end{align*}
	To find the denominator, we find that the MLE of $\theta$ and $\sigma^{2}$:
	\begin{align*}
		\hat{\theta}&=\overline{x} & \hat{\sigma}^{2}&=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}
	\end{align*}
	Therefore,
	\begin{align*}
		L(\hat{\theta}_{0},\hat{\sigma}_{0}^{2})&=\left(\frac{2\pi}{n}\sum_{i=1}^{n}(X_{i}-\theta_{0})^{2}\right)^{-\frac{n}{2}}\exp\left(-\frac{\sum_{i=1}^{n}(X_{i}-\theta_{0})^{2}}{\frac{2}{n}\sum_{i=1}^{n}(X_{i}-\theta_{0})^{2}}\right)=\left(\frac{2\pi}{n}\sum_{i=1}^{n}(X_{i}-\theta_{0})^{2}\right)^{-\frac{n}{2}}\exp\left(-\frac{n}{2}\right)\\
		L(\hat{\theta},\hat{\sigma}^{2})&=\left(\frac{2\pi}{n}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}\right)^{-\frac{n}{2}}\exp\left(-\frac{\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}}{\frac{2}{n}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}}\right)=\left(\frac{2\pi}{n}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}\right)^{-\frac{n}{2}}\exp\left(-\frac{n}{2}\right)\\
		\lambda(\mathbf{X})&=\frac{L(\hat{\theta}_{0},\hat{\sigma}_{0}^{2})}{L(\hat{\theta},\hat{\sigma}^{2})}=\left(\frac{\sum_{i=1}^{n}(X_{i}-\theta_{0})^{2}}{\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}}\right)^{-\frac{n}{2}}
	\end{align*}
	By CLT, we have that if $\theta=\theta_{0}$,
	\begin{align*}
		\frac{\sqrt{n}(\overline{X}-\theta_{0})}{\sigma}&\sim\N(0,1) & \frac{n(\overline{X}-\theta_{0})^{2}}{\sigma^{2}}&\sim\chi^{2}(1)
	\end{align*}
	By Theorem \ref{Chapter 1 (Theorem) Normal and chi-squared distribution related to sample mean and variance}, we find that:
	\begin{equation*}
		\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}\sim\chi^{2}(n-1)
	\end{equation*}
	Therefore, using the definition of F-distribution, define $F$ by:
	\begin{equation*}
		F=(n-1)\frac{n(\overline{X}-\theta_{0})^{2}}{\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}}=\frac{\frac{n(\overline{X}-\theta_{0})^{2}}{\sigma^{2}}}{\frac{\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}}{\sigma^{2}(n-1)}}\sim F(1,n-1)
	\end{equation*}
	We may find that:
	\begin{equation*}
		\lambda(\mathbf{X})=\left(\frac{\sum_{i=1}^{n}(X_{i}-\theta_{0})^{2}}{\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}}\right)^{-\frac{n}{2}}=\left(1+\frac{\sum_{i=1}^{n}(\overline{X}-\theta_{0})^{2}}{\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}}\right)^{-\frac{n}{2}}=\left(1+\frac{F}{n-1}\right)^{-\frac{n}{2}}
	\end{equation*}
	We can now modify the problem to:
	\begin{equation*}
		\lambda(\mathbf{X})\leq k\implies F\geq(n-1)\left(k^{-\frac{2}{n}}-1\right)=K'
	\end{equation*}
	Finally, we find that:
	\begin{equation*}
		\alpha=\prob(\lambda(\mathbf{X})\leq k)=\prob(F\geq K')
	\end{equation*}
	We can find that $K'=f_{\alpha,(1,n-1)}$. Therefore, we reject $H_{0}$ if $F\geq f_{\alpha,(1,n-1)}$.
\end{eg}

\newpage
For the tests with large $n$, if the large-$n$ results such as CLT can be used for the point estimator. then we can easily draw the conclusion.
\begin{eg}
	Consider the MLE $\hat{\theta}_{n}(\mathbf{X})$. Similar to Remark \ref{Chapter 2 (Remark) Cases when Fisher Information cannot be determined easily}, we can use:
	\begin{equation*}
		T_{1}=\sqrt{-l''(\hat{\theta}_{n})}(\hat{\theta}_{n}(\mathbf{X})-\theta_{0})\to\N(0,1)
	\end{equation*}
	For $H_{1}:\theta>\theta_{0}$, we reject $H_{0}$ at a significant level of $\alpha$ if the actual value of $T_{1}>z_{\alpha}$.
	
	For $H_{1}:\theta<\theta_{0}$, we reject $H_{0}$ at a significant level of $\alpha$ if the actual value of $T_{1}<z_{\alpha}$.
\end{eg}
However, on likelihood ratio test, it is a bit more complicated. We only consider the cases with only one unknown parameter for two-sided tests. Since the parameter space is restricted ($\theta\geq\theta_{0}$ or $\theta\leq\theta_{0}$), the result for the large-sample likelihood ratio test has to be further adjusted. We omit it due to its complexity.

\begin{defn}
	\textbf{Large-sample likelihood ratio test statistic} is defined by:
	\begin{equation*}
		X_{L}=-2\ln{\lambda(\mathbf{X})}=2\left[l\left(\hat{\theta}_{n}(\mathbf{X})\right)-l(\theta_{0})\right]
	\end{equation*}
\end{defn}
\begin{thm}
	Under $H_{0}$, the large-sample likelihood ratio test statistic follows an asymptotic $\chi^{2}(1)$. Thus, we reject $H_{0}$ at a significance level of $\alpha$ when:
	\begin{equation*}
		\mathbf{x}_{L}>\chi_{\alpha,1}^{2}
	\end{equation*}
	which is the $(1-\alpha)$-th quantile of chi-square distribution with $1$ degree of freedom.
\end{thm}
\begin{rem}
	$X_{L}$ and $T_{1}^{2}$ are asymptotically equivalent for two-sided tests.
\end{rem}
\begin{eg}
	Suppose that following data from $\Exp(\lambda)$:
	\begin{equation*}
		1,3,5,8,10,15,18,19,22,25
	\end{equation*}
	Perform a likelihood ratio test with $H_{0}:\lambda=0.06$ against $H_{1}:\lambda\neq 0.06$ and draw a conclusion at a significance level of $0.05$. Use the fact that $\chi_{0.95,1}^{2}=3.841459$.
	
	We have:
	\begin{equation*}
		l(\lambda)=n\ln{\lambda}-\lambda\sum_{i=1}^{n}x_{i}
	\end{equation*}
	Over $\Theta=(0,\infty)$, the MLE for $\lambda$ is $\hat{\lambda}_{n}=\frac{1}{\overline{x}}$. Therefore,
	\begin{equation*}
		l(\hat{\lambda}_{n})=-n\ln{\overline{x}}-n=-35.33697
	\end{equation*}
	Since $H_{0}$ is simple, we have:
	\begin{equation*}
		l(\hat{\lambda}_{0})=n\ln(0.06)-0.06(126)=-35.69411
	\end{equation*}
	Therefore, the actual value of $X_{L}$ is $2(-35.33697+35.69411)=0.7143\leq\chi_{0.95,1}^{2}$. We do not reject $H_{0}$ at $\alpha=0.05$.
\end{eg}

\newpage
\section{Power Function and Power of a Test Statement}
In parameter estimation, we have many point estimators to estimate an unknown parameters. In hypothesis testing, we can also use different test statistics to construct tests for testing $H_{0}$ against $H_{1}$. Which one of them is the best? We need a quantity for comparison.
\begin{defn}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$. For a test, the \textbf{power function} $Q:\Theta\to[0,1]$ is defined for $\theta\in\Theta$ by:
	\begin{equation*}
		Q(\theta)=\begin{cases}
			\sum_{\mathbf{x}\in C_{1}}p_{\mathbf{X}}(\mathbf{x}|\theta), &\text{Discrete case}\\
			\int_{C_{1}}f_{\mathbf{X}}(\mathbf{x}|\theta)\,d\mathbf{x}, &\text{Continuous case}
		\end{cases}
	\end{equation*}
\end{defn}
\begin{rem}
	The power function of a test is the probability of rejecting $H_{0}$. In particular, for $\theta\in\Theta_{1}$, $Q(\theta)=1-\beta(\theta)$ is called the \textbf{power of the test at $\theta$}, which is the probability of rejecting $H_{0}$ at $\theta\in\Theta_{1}$.
\end{rem}
\begin{rem}
	In terms of the power function, our goal is to find a test for which the value of the power at $\theta\in\Theta_{1}$ is as large as possible, subject to the condition that:
	\begin{equation*}
		\max_{\theta\in\Theta_{0}}Q(\theta)=\alpha
	\end{equation*}
\end{rem}
Given two tests, we would first require them at the same significance level $\alpha$. How do we compare them?
\begin{defn}
	A test is said to be \textbf{more powerful} at a value $\theta^{*}\in\Theta_{1}$ if it has a higher power at $\theta^{*}$.
	
	A test is said to be the \textbf{most powerful} at $\theta^{*}$ if it is more powerful than any other tests at $\theta^{*}$.
	
	A test is said to be \textbf{uniformly most powerful} (UMP) if it is the most powerful for all $\theta\in\Theta_{1}$. More precisely, the UMP test at a significance level $\alpha$ is the test with a power function $Q(\theta)$ satisfying:
	\begin{enumerate}
		\item $\max_{\theta\in\Theta_{0}}Q(\theta)=\alpha$
		\item $Q(\theta)\geq Q^{*}(\theta)$ for all $\theta\in\Theta_{1}$ for any test with $Q^{*}(\theta)$
	\end{enumerate}
\end{defn}
\begin{rem}
	The UMP test in hypothesis testing has a similar role to the best estimator under the criterion of MSE in parameter estimation.
\end{rem}
\begin{rem}
	In general, a UMP test does not exist for two-sided tests. To fix this, we usually consider a smaller class, which is the class of unbiased estimators.
\end{rem}
We first consider how to find the UMP test for simple tests.

For testing $H_{0}:\theta=\theta_{0}$ against $H_{1}:\theta=\theta_{1}$ at a significance level $\alpha=\gamma(\theta_{0})=Q(\theta_{0})$ and its power $Q(\theta_{1})=1-\beta(\theta_{1})$, we have the Neyman-Pearson Lemma.
\begin{lem}\named{Neyman-Pearson Lemma}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a PDF $f(\mathbf{x}|\theta)$ or a PMF $p(\mathbf{x}|\theta)$, where $\theta\in\Theta=\{\theta_{0},\theta_{1}\}$ and $\mathbf{x}$ is its realization. Then, at a significance level $\alpha$, a test with a rejection region:
	\begin{equation*}
		C_{1}=\biggl\{\mathbf{x}:\frac{L(\theta_{0})}{L(\theta_{1})}\leq k\biggr\}
	\end{equation*}
	is the UMP test for testing $H_{0}:\theta=\theta_{0}$ against $H_{1}:\theta=\theta_{1}$, at a significance level $\alpha$, where $k>0$.
\end{lem}
\begin{thm}
	The likelihood ratio test for a simple test is a UMP test.
\end{thm}
\begin{proofing}
	For a simple test, the LRT at a significance level of $\alpha$ has a rejection region:
	\begin{equation*}
		C_{1}=\{\mathbf{x}:\lambda(\mathbf{x})\leq k<1\}
	\end{equation*}
	where $\prob(\lambda(\mathbf{X})\leq k|\theta_{0})=\alpha$ and:
	\begin{equation*}
		\lambda(\mathbf{x})=\frac{L(\theta_{0})}{\max\{L(\theta_{0}),L(\theta_{1})\}}=\begin{cases}
			1, &\text{if }L(\theta_{0})\geq L(\theta_{1})\\
			\frac{L(\theta_{0})}{L(\theta_{1})}, &\text{if }L(\theta_{0})<L(\theta_{1})
		\end{cases}
	\end{equation*}
	Therefore, we have:
	\begin{equation*}
		C_{1}=\{\mathbf{x}:\lambda(\mathbf{x})\leq k<1\}=\biggl\{\mathbf{x}:\frac{L(\theta_{0})}{L(\theta_{1})}\leq k<1\biggr\}
	\end{equation*}
	By Neyman-Pearson Lemma, LRT is a UMP test for a simple test.
\end{proofing}
\begin{eg}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\N(\theta,\sigma_{0}^{2})$, where $\theta$ is unknown but $\sigma_{0}^{2}$ is known. We want to construct a UMP test for testing $H_{0}:\theta=\theta_{0}$ against $H_{1}:\theta=\theta_{1}$ at a significance level of $\alpha$, where $\theta_{0}<\theta_{1}$. Note that:
	\begin{equation*}
		\frac{L(\theta_{0})}{L(\theta_{1})}=\exp\left(\frac{n[\theta_{1}^{2}-\theta_{0}^{2}-2\overline{x}(\theta_{1}-\theta_{0})]}{2\sigma_{0}^{2}}\right)
	\end{equation*}
	Thus, we can modify the rejection region into:
	\begin{equation*}
		C_{1}=\biggl\{\mathbf{x}:\frac{L(\theta_{0})}{L(\theta_{1})}\leq k\biggr\}=\biggl\{\mathbf{x}:\overline{x}\geq \frac{1}{2}(\theta_{0}+\theta_{1})-\frac{\sigma_{0}^{2}\ln{k}}{n(\theta_{1}-\theta_{0})}=K\biggr\}
	\end{equation*}
	Since $\overline{X}\sim\N(\theta,\frac{\sigma_{0}^{2}}{n})$, we can determine $K$ by:
	\begin{equation*}
		\alpha=\prob(\overline{X}\geq K|\theta_{0})=\prob\left(Z\geq\frac{\sqrt{n}(K-\theta_{0})}{\sigma_{0}}\right)
	\end{equation*}
	We have that $K=\theta+z_{\alpha}\frac{\sigma_{0}}{\sqrt{n}}$. Therefore, by Neyman-Pearson Lemma, the UMP test for testing $H_{0}:\theta=\theta_{0}$ against $H_{1}:\theta=\theta_{1}$ at a significant level of $\alpha$ is the test with a rejection region:
	\begin{equation*}
		C_{1}=\biggl\{\mathbf{x}:\overline{x}\geq\theta_{0}+z_{\alpha}\frac{\sigma_{0}}{\sqrt{n}}\biggr\}
	\end{equation*}
	where $\theta_{0}<\theta_{1}$.
\end{eg}
Neyman-Pearson Lemma only provides us with a way of constructing UMP test for a simple test. For UMP one-sided tests, we need another result. Without loss of generality, we only discuss how to find the UMP test for one-sided right test.
\begin{defn}
	A distribution has the property of \textbf{monotone likelihood ratio} (MLR) in $T$ if the likelihood ratio $\frac{L(\theta')}{L(\theta'')}$ is non-decreasing in $T$ for $\theta'>\theta''$, where at least one of $L(\theta')$ and $L(\theta'')$ is positive.
\end{defn}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Bern(\theta)$, where $\theta\in(0,1)$ is unknown. For $\theta'>\theta''$, the likelihood ratio:
	\begin{equation*}
		\frac{L(\theta')}{L(\theta'')}=\left(\frac{1-\theta'}{1-\theta''}\right)^{n}\left(\frac{\theta'(1-\theta'')}{\theta''(1-\theta')}\right)^{\sum_{i=1}^{n}x_{i}}
	\end{equation*}
	is non-decreasing in $T=\sum_{i=1}^{n}x_{i}$ because $\frac{\theta'(1-\theta'')}{\theta''(1-\theta')}>1$. Therefore, MLR holds for $\Bern(\theta)$ in $T=\sum_{i=1}^{n}x_{i}$.
\end{eg}
\begin{eg}
	\label{Chapter 4 (Example) MLR for N(mu,sigma^2)}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\N(\theta,\sigma_{0}^{2})$, where $\theta$ is unknown but $\sigma_{0}^{2}$ is known. For $\theta'>\theta''$, the likelihood ratio:
	\begin{equation*}
		\frac{L(\theta')}{L(\theta'')}=\exp\left(\frac{n[(\theta'')^{2}-(\theta')^{2}-2\overline{x}(\theta''-\theta')]}{2\sigma_{0}^{2}}\right)
	\end{equation*}
	is non-decreasing in $T=\overline{x}$. Therefore, MLR holds for $\N(\theta,\sigma_{0}^{2})$ in $T=\overline{x}$.
\end{eg}
We have now some theorems that we can use to obtain the UMP one-sided right test.
\begin{thm}\named{Karlin-Rubin Theorem}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a distribution with a parameter $\theta$ having MLR in $T(\mathbf{x})$, where $\mathbf{x}$ is the realization of $\mathbf{X}$. At a significance level of $\alpha$, a test with a rejection region:
	\begin{equation*}
		C_{1}=\{\mathbf{x}:T(\mathbf{x})\geq K\}
	\end{equation*}
	is a UMP one-sided right test for $H_{0}:\theta\leq\theta_{0}$ against $H_{1}:\theta>\theta_{0}$ at a significance level $\alpha$ for some $K$. The test is also a UMP one-sided right test for $H_{0}:\theta=\theta_{0}$ against $H_{1}:\theta>\theta_{0}$.
\end{thm}

\newpage
\begin{thm}\named{Karlin-Rubin Theorem with sufficient statistic}
	\label{Chapter 4 (Theorem) Karlin-Rubin Theorem for sufficient statistic}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample from a distribution with a parameter $\theta$ and $S(\mathbf{X})$ is a sufficient statistic for $\theta$. If the distribution of $S(\mathbf{X})$ has MLR in itself, then at a significance level $\alpha$, a test with a rejection region:
	\begin{equation*}
		C_{1}=\{\mathbf{x}:S(\mathbf{x})\geq K\}
	\end{equation*}
	is a UMP one-sided right test for $H_{0}:\theta\leq\theta_{0}$ against $H_{1}:\theta>\theta_{0}$ at a significance level $\alpha$ for some $K$. The test is also a UMP one-sided right test for $H_{0}:\theta=\theta_{0}$ against $H_{1}:\theta>\theta_{0}$.
\end{thm}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\N(\theta,\sigma_{0}^{2})$, where $\theta$ is unknown but $\sigma_{0}^{2}$ is known. We try to construct a UMP test for testing at a significance level $\alpha$:
	\begin{equation*}
		\begin{cases}
			H_{0}: &\theta\leq\theta_{0}\\
			H_{1}: &\theta>\theta_{0}
		\end{cases}
	\end{equation*}
	The MLE of $\theta$ over $\Theta_{0}$ is $\min\{\overline{x},\theta_{0}\}$. Therefore,
	\begin{align*}
		\lambda(\mathbf{x})&=\begin{cases}
			1, &\text{if }\overline{x}\leq\theta_{0}\\
			\exp\left(-\frac{1}{2\sigma_{0}^{2}}\left[\sum_{i=1}^{n}(x_{i}-\theta_{0})^{2}-\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}\right]\right), &\text{if }\overline{x}>\theta_{0}
		\end{cases}\\
		&=\begin{cases}
			1, &\text{if }\overline{x}\leq\theta_{0}\\
			\exp\left(-\frac{n(\overline{x}-\theta_{0})^{2}}{2\sigma_{0}^{2}}\right), &\text{if }\overline{x}>\theta_{0}
		\end{cases}
	\end{align*}
	We find that the rejection region is:
	\begin{equation*}
		C_{1}=\biggl\{\mathbf{x}:\overline{x}>\theta_{0}\text{ and }\exp\left(-\frac{n(\overline{x}-\theta_{0})^{2}}{2\sigma_{0}^{2}}\right)\leq k<1\biggr\}=\biggl\{\mathbf{x}:\overline{x}\geq\theta_{0}+\sqrt{-\frac{2\sigma_{0}^{2}}{n}\ln{k}}=K'\biggr\}
	\end{equation*}
	Since $\overline{X}\sim\N(\theta,\frac{\sigma_{0}^{2}}{n})$, we can easily find that $K'=\theta_{0}+z_{\alpha}\frac{\sigma_{0}}{\sqrt{n}}$.
	
	From Example \ref{Chapter 4 (Example) MLR for N(mu,sigma^2)}, we have found that MLR holds for $\N(\theta,\sigma_{0}^{2})$ in $T=\overline{x}$. By Karlin-Rubin Theorem, a test with a rejection region:
	\begin{equation*}
		C_{1}=\{\mathbf{x}:\overline{x}\geq K'\}
	\end{equation*}
	is a UMP one-sided right test for testing $H_{0}:\theta\leq\theta_{0}$ against $H_{1}:\theta>\theta_{0}$ at a significance level of $\alpha$. Therefore, we have found the UMP one-sided test.
\end{eg}
Remember exponential family, it is also very useful on hypothesis testing.
\begin{cor}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a distribution belonging to a one-parameter exponential family in form of:
	\begin{equation*}
		\exp[a(\theta)+b(x)+c(\theta)d(x)]
	\end{equation*}
	For the test at a significance level $\alpha$ of:
	\begin{equation*}
		\begin{cases}
			H_{0}: &\theta\leq\theta_{0}\\
			H_{1}: &\theta>\theta_{0}
		\end{cases}\text{ or }\begin{cases}
			H_{0}: &\theta=\theta_{0}\\
			H_{1}: &\theta>\theta_{0}
		\end{cases}
	\end{equation*}
	the test with a rejection region:
	\begin{enumerate}
		\item for an increasing function $c(\theta)$,
		\begin{equation*}
			C_{1}=\biggl\{\mathbf{x}:\sum_{i=1}^{n}d(x_{i})\geq K\biggr\}
		\end{equation*}
		\item for an decreasing function $c(\theta)$,
		\begin{equation*}
			C_{1}=\biggl\{\mathbf{x}:\sum_{i=1}^{n}d(x_{i})\leq K\biggr\}
		\end{equation*}
	\end{enumerate}
	is the UMP test at a significance level $\alpha$ for some $K$.
\end{cor}

\newpage
\begin{proofing}
	For $\theta'>\theta''$, the likelihood ratio is:
	\begin{equation*}
		\frac{L(\theta')}{L(\theta'')}=\exp\left[n(a(\theta')-a(\theta''))+(c(\theta')-c(\theta''))\sum_{i=1}^{n}d(x_{i})\right]
	\end{equation*}
	If $c(\theta)$ is increasing, then $c(\theta')-c(\theta'')>0$. We find that $\frac{L(\theta')}{L(\theta'')}$ is non-decreasing in $\sum_{i=1}^{n}d(x_{i})$. Therefore, by Karlin-Rubin Theorem, a test with a rejection region:
	\begin{equation*}
		C_{1}=\biggl\{\mathbf{x}:\sum_{i=1}^{n}d(x_{i})\geq K\biggr\}
	\end{equation*}
	is a UMP test at a significance level $\alpha$ for some $K$.
	
	If $c(\theta)$ is decreasing, then $c(\theta'')-c(\theta')<0$. We find that $\frac{L(\theta')}{L(\theta'')}$ is non-decreasing in $-\sum_{i=1}^{n}d(x_{i})$. Therefore, by Karlin-Rubin Theorem, at test with a rejection region:
	\begin{equation*}
		C_{1}=\biggl\{\mathbf{x}:-\sum_{i=1}^{n}d(x_{i})\geq K'\biggr\}=\biggl\{\mathbf{x}:\sum_{i=1}^{n}d(x_{i})\leq -K'=K\biggr\}
	\end{equation*}
	is a UMP test at a significance level $\alpha$ for some $K$.
\end{proofing}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\U[0,\theta]$, where $\theta>0$ is unknown. We want to construct a UMP test for testing:
	\begin{equation*}
		\begin{cases}
			H_{0}: &\theta\leq\theta_{0}\\
			H_{1}: &\theta>\theta_{0}
		\end{cases}
	\end{equation*}
	at a significance level $\alpha$, where $\theta_{0}>0$. From Example \ref{Chapter 3 (Example) Sufficient statistic for U[0,theta]}, we have found that $X_{(n)}$ is sufficient for $\theta$ with PDF:
	\begin{equation*}
		f_{X_{(n)}}(y|\theta)=\frac{ny^{n-1}}{\theta^{n}}\mathbf{1}_{y\leq\theta}
	\end{equation*}
	For $\theta'>\theta''$, since it only has one term, we find the likelihood ratio of itself:
	\begin{equation*}
		\frac{L(\theta')}{L(\theta'')}=\frac{f_{X_{(n)}}(y|\theta')}{f_{X_{(n)}}(y|\theta'')}=\begin{cases}
			\left(\frac{\theta''}{\theta'}\right)^{n}<1, &\text{if } y\leq\theta''\\
			\infty, &\text{if } \theta''<y\leq\theta'
		\end{cases}
	\end{equation*}
	is non-decreasing in itself. Note that we only consider $y\leq\theta'$ since both $L(\theta')$ and $L(\theta'')$ would be zero if $y>\theta'$. Therefore, MLR holds in $X_{(n)}$ itself. By Theorem \ref{Chapter 4 (Theorem) Karlin-Rubin Theorem for sufficient statistic}, a test with a rejection region:
	\begin{equation*}
		C_{1}=\{\mathbf{x}:x_{(n)}\geq K\}
	\end{equation*}
	is a UMP test for testing $H_{0}:\theta\leq\theta_{0}$ against $H_{1}:\theta>\theta_{0}$ at a significance level $\alpha$.
	
	To find $K$, we consider:
	\begin{align*}
		\alpha&=\max_{\theta\in\Theta_{0}}\prob(X_{(n)}\geq K)\\
		&=\max_{\theta\in\Theta_{0}}\int_{K}^{\theta}\frac{ny^{n-1}}{\theta^{n}}\,dy\\
		&=\max_{\theta\in\Theta_{0}}\left[1-\left(\frac{K}{\theta}\right)^{n}\right]\\
		&=1-\left(\frac{K}{\theta_{0}}\right)^{n}
	\end{align*}
	Therefore, we find that $K=\theta_{0}\left(1-\alpha\right)^{\frac{1}{n}}$.
\end{eg}

\appendix
\renewcommand{\thechapter}{\Alph{chapter}}
\chapter{Over-simplified Summary}
\begin{thm}\named{Weak Law of Large Numbers (WLLN)}
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables. Let $\E(X_{i})=\mu$ for all $i=1,2,\cdots$. As $n\to\infty$, we have:
	\begin{equation*}
		\overline{X}\xrightarrow{D}\mu
	\end{equation*}
\end{thm}
\begin{thm}\named{Central Limit Theorem (CLT)}
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables whose MGFs exist on a neighbourhood of $0$. Let $\E(X_{i})=\mu$ and $\Var(X_{i})=\sigma^{2}>0$ for all $i=1,2,\cdots$. As $n\to\infty$, we have:
	\begin{equation*}
		\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}=\frac{\sum_{i=1}^{n}X_{i}-n\mu}{\sqrt{n}\sigma}\to\N(0,1)
	\end{equation*}
\end{thm}
\begin{thm}\named{L\'evy-Linderberg Central Limit Theorem} 
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables with common population mean $\mu$ and population variance $\sigma^{2}$. Assume that $0<\sigma^{2}<\infty$. As $n\to\infty$,
	\begin{equation*}
		\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}=\frac{\sum_{i=1}^{n}X_{i}-n\mu}{\sqrt{n}\sigma}\to\N(0,1)
	\end{equation*}
\end{thm}
\begin{thm}\named{Slutsky's Theorem}
	If $X_{n}\xrightarrow{D}X$ and $Y_{n}\xrightarrow{\prob}c$ for some constant $c$, then:
	\begin{enumerate}
		\item $X_{n}+Y_{n}\xrightarrow{D}X+c$
		\item $X_{n}Y_{n}\xrightarrow{D}cX$
		\item $\frac{X_{n}}{Y_{n}}\xrightarrow{D}\frac{X}{c}$ if $c\neq 0$
	\end{enumerate}
\end{thm}
\begin{thm}\named{Continuous Mapping Theorem}
	Let $\{X_{n}\}$ be a sequence of random variables and $X$ be a random variable. Suppose there is a function $g$ with a set of discontinuity points $D_{g}$ such that $\prob(X\in D_{g})=0$. We have:
	\begin{enumerate}
		\item If $X_{n}\xrightarrow{D}X$, then $g(X_{n})\xrightarrow{D}g(X)$.
		\item If $X_{n}\xrightarrow{\prob}X$, then $g(X_{n})\xrightarrow{\prob}g(X)$.
	\end{enumerate}
\end{thm}
\begin{thm}\named{Delta method}
	Let $\{X_{n}\}$ be a sequence of random variables such that for constants $a$ and $b>0$, as $n\to\infty$,
	\begin{equation*}
		\sqrt{n}(X_{n}-a)\to\N(0,b^{2})
	\end{equation*}
	Then for a given function $g$, suppose that $g'(a)$ exists and not $0$, as $n\to\infty$:
	\begin{equation*}
		\sqrt{n}(g(X_{n})-g(a))\to\N(0,[g'(a)b]^{2})
	\end{equation*}
	In particular, if $\{X_{n}\}$ is a random sample of size $n$ from a distribution with a finite mean $\mu$ and variance $\sigma^{2}>0$, such that $g'(\mu)$ exists and not $0$, then as $n\to\infty$,
	\begin{equation*}
		\sqrt{n}(g(\overline{X})-g(\mu))\to\N(0,[g'(\mu)\sigma]^{2})
	\end{equation*}
\end{thm}
\begin{thm}
	A sequence of MME $\{\tilde{\theta}_{n}\in\mathbb{R}^{k}\}$ is consistent, asymptotically unbiased for $\theta$ and asymptotically normally distributed. More precisely, under certain assumption like $\E\abs{X}^{2k}<\infty$, as $n\to\infty$,
	\begin{equation*}
		\sqrt{n}(\tilde{\theta}_{n}-\theta)\to\N_{k}(\mathbf{0},\mathbf{GHG}^{T})
	\end{equation*}
	where $\mathbf{G}$ is a $k\times k$ matrix with $\pdv{g_{i}}{\mu_{j}'}$ as its $(i,j)$-th entry and $\mathbf{H}$ is a $k\times k$ matrix with $\mu_{i+j}'-\mu_{i}'\mu_{j}'$ as its $(i,j)$-th entry, for $i=1,\cdots,k$ and $j=1,\cdots,k$.
\end{thm}
\begin{thm}
	A sequence of MLE $\{\hat{\theta}_{n}\in\mathbb{R}^{k}\}$ is consistent, asymptotically unbiased for $\theta$, asymptotically efficient and asymptotically normally distributed. More precisely, under regularity assumption, as $n\to\infty$,
	\begin{equation*}
		\sqrt{n}(\hat{\theta}_{n}-\theta)\to\N_{k}(\mathbf{0},\mathcal{I}_{X}^{-1}(\theta))
	\end{equation*}
	where $\mathcal{I}_{X}(\theta)$ is the $k\times k$ Fisher Information matrix with $(i,j)$-th entry defined as:
		\begin{equation*}
		\begin{cases}
			\E\left[\left(\pdv*{\ln{f_{X}(X|\theta)}}{\theta_{i}}\right)\left(\pdv*{\ln{f_{X}(X|\theta)}}{\theta_{j}}\right)\right], &\text{Continuous case}\\
			\E\left[\left(\pdv*{\ln{p_{X}(X|\theta)}}{\theta_{i}}\right)\left(\pdv*{\ln{p_{X}(X|\theta)}}{\theta_{j}}\right)\right], &\text{Discrete case}
		\end{cases}
	\end{equation*}
	for $i=1,\cdots,k$ and $j=1,\cdots,k$.
\end{thm}
\begin{thm}\named{Fisher-Neyman Factorization Theorem}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$. A set of statistics $\{S_{1},\cdots,S_{r}\}$, where $r\geq k$ and $S_{i}=S_{i}(\mathbf{X})$ for $i=1,\cdots,r$, is jointly sufficient if and only if:
	\begin{equation*}
		\begin{cases}
			p_{\mathbf{X}}(\mathbf{x}|\theta)=g(S_{1}(\mathbf{x}),\cdots,S_{r}(\mathbf{x})|\theta)h(\mathbf{x}), &\text{Discrete case}\\
			f_{\mathbf{X}}(\mathbf{x}|\theta)=g(S_{1}(\mathbf{x}),\cdots,S_{r}(\mathbf{x})|\theta)h(\mathbf{x}), &\text{Continuous case}
		\end{cases}
	\end{equation*}
	where $g$ is a non-negative function of $x_{1},\cdots,x_{n}$ only through the statistics $S_{1},\cdots,S_{r}$ and depends on $\theta$, and $h$ is a non-negative function of $x_{1},\cdots,x_{n}$ not depending on $\theta$.
\end{thm}
\begin{thm}\named{Rao-Blackwell Theorem}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$, and a set of jointly sufficient statistic $\{S_{1},\cdots,S_{r}\}$, where $r\geq k$ and $S_{i}=S_{i}(\mathbf{X})$ for $i=1,\cdots,r$. Suppose that $T=T(\mathbf{X})$ is an unbiased estimator for $g(\theta)$ for a function $g$. Then:
	\begin{enumerate}
		\item $\E(T|S_{1},\cdots,S_{r})$ is a statistic, and it is a function of jointly sufficient statistics.
		\item $\E(T|S_{1},\cdots,S_{r})$ is unbiased for $g(\theta)$
		\item $\Var(\E(T|S_{1},\cdots,S_{r}))\leq\Var(T)$
	\end{enumerate}
\end{thm}
\begin{thm}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a distribution in an one-parameter exponential family with a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}$, in form of:
	\begin{equation*}
		\exp[a(\theta)+b(x)+c(\theta)d(x)]
	\end{equation*}
	Then, $\sum_{i=1}^{n}d(X_{i})$ is a complete and minimal sufficient statistic.
\end{thm}
\begin{thm}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a distribution in an one-parameter exponential family with a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$, in form of:
	\begin{equation*}
		\exp\left(a(\theta)+b(x)+\sum_{j=1}^{k}c_{j}(\theta)d_{j}(x)\right)
	\end{equation*}
	Then, the set $\{\sum_{i=1}^{n}d_{1}(X_{i}),\cdots,\sum_{i=1}^{n}d_{k}(X_{i})\}$ is a set of jointly complete and minimal sufficient statistics.
\end{thm}
\begin{thm}\named{Lehmann-Scheff\'e Theorem}
	Let $CS$ be a complete and (minimal) sufficient statistic. If there exists a function $h(CS)$ which is unbiased for $g(\theta)$, then $h(CS)$ is the unique UMVUE of $g(\theta)$. In particular, if $\E[f(\mathbf{X})]=g(\theta)$, then $h(CS)=\E[f(\mathbf{X})|CS]$ is the UMVUE for $g(\theta)$.
\end{thm}
\begin{thm}\named{Cram\'er-Rao Inequality}
	Under the regularity conditions, the variance of an unbiased estimator $T(\mathbf{X})$ for $\theta$ based on a set of random variables $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ from their joint PDF $f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)$ satisfies the following inequality:
	\begin{equation*}
		\Var(T(\mathbf{X}))\geq\frac{1}{\mathcal{I}_{X_{1},\cdots,X_{n}(\theta)}}=\frac{1}{\E\left[\odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}\right]^{2}}
	\end{equation*}
	with the lower bound be the Cram\'er-Rao lower bound.
	
	If $T(\mathbf{X})$ is an unbiased estimator for $g(\theta)$, then it becomes:
	\begin{equation*}
		\Var(T(\mathbf{X}))\geq\frac{\left[\odv*{g(\theta)}{\theta}\right]^{2}}{\mathcal{I}_{X_{1},\cdots,X_{n}(\theta)}}=\frac{\left[\odv*{g(\theta)}{\theta}\right]^{2}}{\E\left[\odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}\right]^{2}}
	\end{equation*}
	
	The equality only holds if and only if:
	\begin{equation*}
		\odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}=A(\theta,n)[T(X_{1},\cdots,X_{n})-g(\theta)]
	\end{equation*}
	where $A(\theta,n)$ is a non-zero function. Thus, $T(X_{1},\cdots,X_{n})$ would be an UMVUE of $g(\theta)$.
\end{thm}
\begin{thm}
	Under $H_{0}$, the large-sample likelihood ratio test statistic follows an asymptotic $\chi^{2}(1)$. We reject $H_{0}$ at a significance level $\alpha$ when:
	\begin{equation*}
		\mathbf{x}_{L}>\chi^{2}_{\alpha,1}
	\end{equation*}
\end{thm}
\begin{lem}\named{Neyman-Pearson Lemma}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a PDF $f(\mathbf{x}|\theta)$ or a PMF $p(\mathbf{x}|\theta)$, where $\theta\in\Theta=\{\theta_{0},\theta_{1}\}$ and $\mathbf{x}$ is its realization. Then, at a significance level $\alpha$, a test with a rejection region:
	\begin{equation*}
		C_{1}=\biggl\{\mathbf{x}:\frac{L(\theta_{0})}{L(\theta_{1})}\leq k\biggr\}
	\end{equation*}
	is the UMP test for testing $H_{0}:\theta=\theta_{0}$ against $H_{1}:\theta=\theta_{1}$, at a significance level $\alpha$, where $k>0$.
\end{lem}
\begin{thm}\named{Karlin-Rubin Theorem}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a distribution with a parameter $\theta$ having MLR in $T(\mathbf{x})$. At a significance level of $\alpha$, a test with a rejection region:
	\begin{equation*}
		C_{1}=\{\mathbf{x}:T(\mathbf{x})\geq K\}
	\end{equation*}
	is a UMP one-sided right test for:
	\begin{equation*}
		\begin{cases}
			H_{0}: &\theta\leq\theta_{0}\\
			H_{1}: &\theta>\theta_{0}
		\end{cases}\text{ or }\begin{cases}
			H_{0}: &\theta=\theta_{0}\\
			H_{1}: &\theta>\theta_{0}
		\end{cases}
	\end{equation*}
	at a significance level $\alpha$ for some $K$.
\end{thm}
\begin{thm}\named{Karlin-Rubin Theorem with sufficient statistic}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample from a distribution with a parameter $\theta$ and $S(\mathbf{X})$ is a sufficient statistic for $\theta$. If the distribution of $S(\mathbf{X})$ has MLR in itself, then at a significance level $\alpha$, a test with a rejection region:
	\begin{equation*}
		C_{1}=\{\mathbf{x}:S(\mathbf{x})\geq K\}
	\end{equation*}
	is a UMP one-sided right test for:
	\begin{equation*}
		\begin{cases}
			H_{0}: &\theta\leq\theta_{0}\\
			H_{1}: &\theta>\theta_{0}
		\end{cases}\text{ or }\begin{cases}
			H_{0}: &\theta=\theta_{0}\\
			H_{1}: &\theta>\theta_{0}
		\end{cases}
	\end{equation*}
	at a significance level $\alpha$ for some $K$.
\end{thm}
\end{document}