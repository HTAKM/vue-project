\documentclass{huhtakm-template-book}
\usepackage{tikz-cd}
\newcommand{\independent}{\perp\!\!\!\perp}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\expect}{\mathbb{E}}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\NBin}{NBin}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\Poisson}{Poisson}
\DeclareMathOperator{\U}{U}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\Cauchy}{Cauchy}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\setlength{\parindent}{0pt}
\title{MATH 2431: Honors Probability}
\author{
	HU-HTAKM\\
	Website: \url{https://htakm.github.io/htakm_test/}
}
\date{
	Last major change: May 31, 2024\\
	Last small update (fixed typo): September 14, 2025
}
\begin{document}
\maketitle
This lecture note is based on the HKUST MATH 2431 lecture notes by Prof. Bao, Zhigang (Spring 2023-24). To clarify certain topics, I have also included material from the textbook "Probability and Random Processes" (Third Edition) by G. Grimmett and D. Stirzaker. The chapters follow the textbook's structure.\\
Some proofs are written by myself, as they are not found in either the lecture notes or the textbook. These may contain errors. If you notice any, you likely possess a strong understanding of the topic or a keen eye for detail. ;)\\
This course requires a co-requisite in multivariable calculus (MATH 2011 and MATH 2023 for HKUST students). However, it is strongly recommended to be familiar with multivariable calculus beforehand, as it is used early in the course. Knowledge of mathematical analysis is also very helpful.\\
If any topics are unclear or not well explained, it is likely due to my non-mathematics background. ;)
\begin{figure}[h]
    \begin{subfigure}{0.45\textwidth}
        \centering
        \begin{tabular}{cc}
            Notations & Meaning\\
            \hline
            $\mathbb{N}_{+}$ & Set of positive integers\\
            $\mathbb{N}$ & Set of natural numbers\\
            $\mathbb{Z}$ & Set of integers\\
            $\mathbb{Q}$ & Set of rational numbers\\
            $\mathbb{R}$ & Set of real numbers\\
            $\emptyset$ & Empty set\\
            $\Omega$ & Sample space / Entire set\\
            $\omega$ & Outcome\\
            $\mathcal{F},\mathcal{G},\mathcal{H}$ & $\sigma$-field / $\sigma$-algebra\\
            $A,B,C,\cdots$ & Events\\
            $A^{\complement}$ & Complement of events\\
            $\prob$ & Probability measure\\
            $X$ & Random variable\\
            $\mathcal{B}(\mathbb{R})$ & Borel $\sigma$-field of $\mathbb{R}$ \\
            $f_{X}$ & PMF/PDF of $X$\\
            $F_{X}$ & CDF of $X$\\
            $\mathbf{1}_{A}$ & Indicator function\\
            $\mathbb{E}$ & Expectation\\
            $\psi$ & Conditional expectation\\
			$\mathbf{u},\mathbf{v},\mathbf{w},\cdots$ & Vector\\
            $\mathbf{A},\mathbf{B},\mathbf{C},\cdots$ & Matrix\\
            $\mathbf{X}$ & Random vector\\
            $G_{X}$ & Probability generating function of $X$\\
            $M_{X}$ & Moment generating function of $X$\\
            $\phi$ & CF / PDF of $X\sim\N(0,1)$\\
            $\Phi$ & CDF of $X\sim\N(0,1)$
        \end{tabular}
        \caption{Notations}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \begin{tabular}{cc}
            Abbreviations & Meaning\\
            \hline
            CDF & Cumulative distribution function\\
            JCDF & Joint cumulative distribution function\\
            PMF & Probability mass function\\
            JPMF & Joint probability mass function\\
            PDF & Probability density function\\
            JPDF & Joint probability density function\\
            PGF & Probability generating function\\
            MGF & Moment generating function\\
            CF & Characteristic function\\
            JCF & Joint characteristic function\\
            i.i.d. & independent and identically distributed\\
            WLLN & Weak Law of Large Numbers\\
            SLLN & Strong Law of Large Numbers\\
            CLT & Central Limit Theorem\\
            BCI & Borel-Cantelli Lemma I\\
            BCII & Borel-Cantelli Lemma II\\
            i.o. & infinitely often\\
            f.o. & finitely often\\
            a.s. & almost surely
        \end{tabular}
        \caption{Abbreviations}
    \end{subfigure}
\end{figure}
\begin{defn}
    This is definition.
\end{defn}
\begin{rem}
    This is remark.
\end{rem}
\begin{lem}
    This is lemma.
\end{lem}
\begin{prop}
    This is proposition.
\end{prop}
\begin{thm}
    This is theorem.
\end{thm}
\begin{cla}
    This is claim.
\end{cla}
\begin{cor}
    This is corollary.
\end{cor}
\begin{eg}
    This is example.
\end{eg}
\tableofcontents

\chapter{Events and their probabilities}
\section{Fundamental terminologies}
    In everyday life, we often perceive the future as largely unpredictable. This belief is reflected in our understanding of random phenomena, to which we assign both quantitative and qualitative meanings.\\
    We start with some basic terminology.
    \begin{defn} % Sample space and outcomes
        A \textbf{sample space} is the set of all outcomes of an experiment and is denoted by $\Omega$. \textbf{Outcomes} are denoted by $\omega$.
    \end{defn}
    \begin{eg}
        For a coin flip, the sample space is $\Omega=\{H,T\}$.
    \end{eg}
    \begin{eg}
        For a die roll, the sample space is $\Omega=\{1,2,3,4,5,6\}$.
    \end{eg}
    \begin{eg}
        For the lifetime of a bulb, the sample space is $\Omega=[0,\infty)$.
    \end{eg}
    \begin{eg}
        For two coins flipping, the sample space is $\Omega=\{(H,H),(H,T),(T,H),(T,T)\}$.
    \end{eg}
    Many statements in probability take the form "the probability of event $A$ is $p$," where the events typically include certain elements of the sample space.
    \begin{defn} % Event and elementary event
        An \textbf{event} is a subset of the sample space. Outcomes are \textbf{elementary events}.
    \end{defn}
    \begin{rem}
        Not every subset of $\Omega$ must be considered an event. However, we will not address this issue at present.
    \end{rem}
    \begin{eg}
        For a dice roll, the sample space is $\Omega=\{1,2,\cdots,6\}$. An example of an event is rolling an even number: $A=\{2,4,6\}$.
    \end{eg}
    \begin{rem}
        If only the outcome $\omega=2$ is given, then there are many events that could result in this outcome. For example, $\{2\}$, $\{2,4\}$, etc.
    \end{rem}
    \begin{defn} % Complement
        The \textbf{complement} of a subset $A$ is the set $A^{\complement}$, which contains all elements in the sample space $\Omega$ that are not in $A$.
    \end{defn}
    We can define a collection of subsets of the sample space.
    \begin{defn} % Field
        A \textbf{field} $\mathcal{F}$ is any collection of subsets of $\Omega$ which satisfies the following conditions:
        \begin{enumerate}
            \item If $A\in\mathcal{F}$, then $A^{\complement}\in\mathcal{F}$.
            \item If $A,B\in\mathcal{F}$, then $A\cup B\in\mathcal{F}$ and $A\cap B=(A^{\complement}\cup B^{\complement})^{\complement}\in\mathcal{F}$. (Closed under \textit{finite} unions or intersections)
            \item $\emptyset\in\mathcal{F}$ and $\Omega=A\cup A^{\complement}\in\mathcal{F}$.
        \end{enumerate}
    \end{defn}

    \newpage
    We are particularly interested in $\sigma$-fields, which are closed under countably infinite unions.
    \begin{defn} % Sigma-field
        A \textbf{$\sigma$-field} (or \textbf{$\sigma$-algebra}) $\mathcal{F}$ is any collection of subsets of $\Omega$ which satisfies the following conditions:
        \begin{enumerate}
            \item If $A\in\mathcal{F}$, then $A^{\complement}\in\mathcal{F}$.
            \item If $A_{1},A_{2},\cdots\in\mathcal{F}$, then $\bigcup_{i=1}^{\infty}A_{i}\in\mathcal{F}$. (Closed under \textit{countably infinite} unions)
            \item $\emptyset\in\mathcal{F}$ and $\Omega=A\cup A^{\complement}\cup\cdots\in\mathcal{F}$.
        \end{enumerate}
    \end{defn}
    \begin{rem}
        From this point onwards, $\mathcal{F}$ will denote the $\sigma$-field.
    \end{rem}
    \begin{eg}
        Smallest $\sigma$-field: $\mathcal{F}=\{\emptyset,\Omega\}$.
    \end{eg}
    \begin{eg}
        If $A$ is any subset of $\Omega$, then $\mathcal{F}=\{\emptyset,A,A^{\complement},\Omega\}$ is a $\sigma$-field.
    \end{eg}
    \begin{eg}
        Largest $\sigma$-field: Power set of $\Omega$: $2^{\Omega}=\{0,1\}^{\Omega}:=\{\text{All subsets of }\Omega\}$.\\
        When $\Omega$ is infinite, the power set is too large a collection for probabilities to be assigned reasonably.
    \end{eg}
    \begin{rem}
        The following two formulae are particularly useful:
        \begin{align*}
            (a,b)&=\bigcup_{n=1}^{\infty}\left[a+\frac{1}{n},b-\frac{1}{n}\right] &            [a,b]&=\bigcap_{n=1}^{\infty}\left[a-\frac{1}{n},b+\frac{1}{n}\right]
        \end{align*}
    \end{rem}
    
\section{Probability measure}
    We wish to discuss the likelihood of the occurrence of events.\\
    Now that we have defined some fundamental terminologies, we can define probability.
    \begin{defn} % Measurable space and measure
        A \textbf{measurable space} $(\Omega,\mathcal{F})$ is a pair comprising a sample space $\Omega$ and a $\sigma$-field $\mathcal{F}$.\\
        A \textbf{measure} $\mu$ on a measurable space $(\Omega,\mathcal{F})$ is a function $\mu:\mathcal{F}\to [0,\infty]$ satisfying:
        \begin{enumerate}
            \item $\mu(\emptyset)=0$.
            \item If $A_{i}\in\mathcal{F}$ for all $i$ and they are disjoint ($A_{i}\cap A_{j}=\emptyset$ for all $i\neq j$), then $\mu(\bigcup_{i=1}^{\infty}A_{i})=\sum_{i=1}^{\infty}\mu(A_{i})$. (Countable additivity)
        \end{enumerate}
        A \textbf{probability measure} $\prob$ is a measure with $\prob(\Omega)=1$.
    \end{defn}
    You might wonder, "Isn't this just probability?" The probability we are familiar with is indeed a probability measure, which we will define shortly. However, there exist other measures that satisfy the definition of a probability measure, such as the risk-neutral measure.\\
    The following examples are not probability measures:
    \begin{eg}
        Lebesgue measure: $\mu((a,b))=b-a$, $\Omega=\mathbb{R}$.
    \end{eg}
    \begin{eg}
        Counting measure: $\mu(A)=\#\{A\}$, $\Omega=\mathbb{R}$.
    \end{eg}
    We can combine a measurable space and a measure to form a measure space.
    \begin{defn} % Measure space and probability space
        A \textbf{measure space} is the triple $(\Omega,\mathcal{F},\mu)$, comprising:
        \begin{enumerate}
            \item A sample space $\Omega$.
            \item A $\sigma$-field $\mathcal{F}$ of certain subsets of $\Omega$.
            \item A measure $\mu$ on $(\Omega,\mathcal{F})$.
        \end{enumerate}
        A \textbf{probability space} $(\Omega,\mathcal{F},\prob)$ is a measure space with a probability measure $\prob$ as the measure.
    \end{defn}

    \newpage
    \begin{eg}
        Consider a coin flip. The sample space is $\Omega=\{H,T\}$, and the $\sigma$-field is $\mathcal{F}=\{\emptyset,H,T,\Omega\}$. Let $\prob(H)=p$, where $p\in[0,1]$. Define $A=\{\omega\in\Omega:\omega = H\}$. Then:
        \begin{equation*}
            \prob(A)=\begin{cases}
                0, &A=\emptyset\\
                p, &A=\{H\}\\
                1-p, &A=\{T\}\\
                1, &A=\Omega
            \end{cases}
        \end{equation*}
        If $p=\frac{1}{2}$, then the coin is fair.   
    \end{eg}
    \begin{eg}
        Consider a die roll. The sample space is $\Omega=\{1,2,3,4,5,6\}$, and the $\sigma$-field is $\mathcal{F}=\{0,1\}^{\Omega}$. Let $p_{i}=\prob(\{i\})$, where $i\in\Omega$. For all $A\in\mathcal{F}$:
        \begin{equation*}
            \prob(A)=\sum_{i\in A}p_{i}
        \end{equation*}
        If $p_{i}=\frac{1}{6}$ for all $i$, then the die is fair, and $\prob(A)=\frac{|A|}{6}$.
    \end{eg}
    The following properties are fundamental and form the basis of probability theory:
    \begin{lem}
        Basic properties of $\prob$:
        \begin{enumerate}
            \item $\prob(A^{\complement})=1-\prob(A)$.
            \item If $A\subseteq B$, then $\prob(B)=\prob(A)+\prob(B\setminus A)\geq\prob(A)$.
            \item $\prob(A\cup B)=\prob(A)+\prob(B)-\prob(A\cap B)$. If $A$ and $B$ are disjoint, then $\prob(A\cup B)=\prob(A)+\prob(B)$.
            \item (\textbf{Inclusion-exclusion formula}) For any set of events $\{A_{1},\cdots,A_{n}\}$:
            \begin{equation*}
                \prob\left(\bigcup_{i=1}^{n}A_{i}\right)=\sum_{i}\prob(A_{i})-\sum_{i<j}\prob(A_{i}\cap A_{j})+\cdots+(-1)^{n+1}\prob(A_{1}\cap A_{2}\cap\cdots\cap A_{n})
            \end{equation*}
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item $A\cup A^{\complement}=\Omega$ and 
            $A\cap A^{\complement}=\emptyset \Longrightarrow \prob(A\cup A^{\complement})=\prob(A)+\prob(A^{\complement})=1$
            \item $A\subseteq B\Longrightarrow B=A\cup (B\setminus A)\Longrightarrow\prob(B)=\prob(A)+\prob(B\setminus A)$
            \item $A\cup B=A\cup (B\setminus A)\Longrightarrow\prob(A\cup B)=\prob(A)+\prob(B\setminus A)=\prob(A)+\prob(B\setminus(A\cap B))=\prob(A)+\prob(B)-\prob(A\cap B)$
		    \item By induction. When $n=1$, it is obviously true. Assume it is true for some positive integers $m$. When $n=m+1$,
            \begin{align*}
			    \tag{Item 3}
                \prob\left(\bigcup_{i=1}^{m+1}A_{i}\right)&=\prob\left(\bigcup_{i=1}^{m}A_{i}\right)+\prob(A_{m+1})-\prob\left(\bigcup_{i=1}^{m}A_{i}\cap A_{m+1}\right)\\
			    &=\sum_{i=1}^{m+1}\prob(A_{i})-\sum_{1\leq i<j\leq m}\prob(A_{i}\cap A_{j})+\cdots(-1)^{m+1}\prob\left(\bigcap_{i=1}^{m}A_{i}\right)-\prob\left(\bigcup_{i=1}^{m}A_{i}\cap A_{m+1}\right)\\
			    &=\sum_{i=1}^{m+1}\prob(A_{i})-\sum_{1\leq i<j\leq m+1}\prob(A_{i}\cap A_{j})+\cdots+(-1)^{m+2}\prob\left(\bigcap_{i=1}^{m+1}A_{i}\right)
            \end{align*}
		    Therefore, by induction, the Inclusion-exclusion formula is true for any set of events $\{A_{1},\cdots,A_{n}\}$ for any $n\in\mathbb{N}_{+}$.
        \end{enumerate}
    \end{proofing}
    We recall the continuity of function $f:\mathbb{R}\to\mathbb{R}$. $f$ is continuous at some point $x$ if for all $x_{n}$, $x_{n}\to x$ when $n\to\infty$. We have:
    \begin{equation*}
	    \lim_{n\to\infty}f(x_{n})=f\left(\lim_{n\to\infty}x_{n}\right)=f(x)
    \end{equation*}
    Similarly, we say a set function $\mu$  is continuous if for all $A_{n}$ with $A=\lim_{n\to\infty}A_{n}$, we have:
    \begin{equation*}
        \lim_{n\to\infty}\mu(A_{n})=\mu\left(\lim_{n\to\infty}A_{n}\right)=\mu(A)
    \end{equation*}
    \begin{rem} 
    Given a sequence of sets $A_{n}$. We have two types of set limit:
        \begin{align*}
            \limsup_{n\to\infty}A_{n}&=\lim_{n\uparrow\infty}\sup_{m\geq n}A_{m}=\bigcap_{n=1}^{\infty}\bigcup_{m=n}^{\infty}A_{m}=\{\omega\in\Omega:\omega\in A_{n}\text{ for infinitely many }n\}\\
            \liminf_{n\to\infty}A_{n}&=\lim_{n\uparrow\infty}\inf_{m\geq n}A_{m}=\bigcup_{n=1}^{\infty}\bigcap_{m=n}^{\infty}A_{m}=\{\omega\in\Omega: \omega\in A_{n}\text{ for all but finitely many }n\}
        \end{align*}
        Apparently, $\liminf_{n\to\infty}A_{n}\subseteq\limsup_{n\to\infty}A_{n}$
    \end{rem}
    \begin{defn}
        We say a sequence of events $A_{n}$ \textbf{converges} and $\lim_{n\to\infty}A_{n}$ exists if:
        \begin{equation*}
            \limsup_{n\to\infty}A_{n}=\liminf_{n\to\infty}A_{n}
        \end{equation*}
        Given a probability space $(\Omega,\mathcal{F},\prob)$. If $A_{1},A_{2},\cdots\in\mathcal{F}$ such that $A=\lim_{n\to\infty}A_{n}$ exists, then:
        \begin{equation*}
            \lim_{n\to\infty}\prob(A_{n})=\prob\left(\lim_{n\to\infty}A_{n}\right)
        \end{equation*}
    \end{defn}
    From the definition, we can get the following important lemma.
    \begin{lem}
        \label{Continuous probability}
        If $A_{1},A_{2},\cdots$ are an increasing sequence of events ($A_{1}\subseteq A_{2}\subseteq\cdots$), then:
        \begin{equation*}
            \prob(A)=\prob\left(\bigcup_{n=1}^{\infty}A_{n}\right)=\lim_{n\to\infty}\prob(A_{n})
        \end{equation*}
        Similarly, if $A_{1},A_{2},\cdots$ are a decreasing sequence of events ($A_{1}\supseteq A_{2}\supseteq\cdots)$, then:
        \begin{equation*}
            \prob(A)=\prob\left(\bigcap_{n=1}^{\infty}A_{n}\right)=\lim_{n\to\infty}\prob(A_{n})
        \end{equation*}
    \end{lem}
    \begin{proofing}
        For $A_{1}\subseteq A_{2}\subseteq\cdots$, let $B_{n}=A_{n}\setminus A_{n-1}$
        \begin{equation*}
		    \prob\left(\bigcup_{n=1}^{\infty}A_{n}\right)=\prob\left(\bigcup_{n=1}^{\infty}B_{n}\right)=\sum_{m=1}^{\infty}\prob(B_{m})=\lim_{n\to\infty}\sum_{m=1}^{n}\prob(B_{m})=\lim_{n\to\infty}\prob\left(\bigcup_{m=1}^{n}B_{m}\right)=\lim_{n\to\infty}\prob(A_{n})
        \end{equation*}
	    For $A_{1}\supseteq A_{2}\supseteq\cdots$, we get $A^{\complement}=\bigcup_{i=1}^{\infty}A_{i}^{\complement}$ and $A_{1}^{\complement}\subseteq A_{2}^{\complement}\subseteq\cdots$. Therefore,
        \begin{equation*}
            \prob\left(\bigcap_{n=1}^{\infty}A_{n}\right)=1-\prob\left(\bigcup_{n=1}^{\infty}A_{n}^{\complement}\right)=1-\lim_{n\to\infty}\prob(A_{n}^{\complement})=\lim_{n\to\infty}\prob(A_{n})
        \end{equation*}
    \end{proofing}
    We can give some terminology to some special probabilities.
    \begin{defn} % Null event
        An event $A$ is \textbf{null} if $\prob(A)=0$.
    \end{defn}
    \begin{rem}
        Null events need not to be impossible. For example, the probability of choosing a point in a plane is $0$.
    \end{rem}
    \begin{defn} % Almost sure event
        An event $A$ occurs \textbf{almost surely} if $\prob(A)=1$.
    \end{defn}
    \newpage

\section{Conditional probability}
    Sometimes, we are interested in the probability of a certain event given that another event has occurred.
    \begin{defn} % Conditional probability
        If $\prob(B)>0$, then the \textbf{conditional probability} that $A$ occurs given that $B$ occurs is:
        \begin{equation*}
            \prob(A|B)=\frac{\prob(A\cap B)}{\prob(B)}
        \end{equation*}
    \end{defn}
    \begin{rem}
        For any event $A$, $\prob(A)$ can be regarded as $\prob(A|\Omega)$.
    \end{rem}
    \begin{rem}
        When $\prob(E)=\prob(E|F)$, $E$ and $F$ are said to be \textbf{independent}.
    \end{rem}
    \begin{rem}
        Given an event $B$. $\prob(\cdot |B)$ is also a probability measure on $\mathcal{F}$.
    \end{rem}
    \begin{eg}
        Two fair dice are thrown. Given that the first shows $3$, what is the probability that the sum of number shown exceeds $6$?
        \begin{equation*}
            \prob(\text{Sum}>3|\text{First die shows }3)=\frac{\frac{3}{36}}{\frac{1}{6}}=\frac{1}{6}
        \end{equation*}
    \end{eg}
    It is obvious that a certain event occurs when another event either occurs or not occurs.
    \begin{lem}
        For any events $A$ and $B$ such that $0<\prob(B)<1$:
        \begin{equation*}
            \prob(A)=\prob(A|B)\prob(B)+\prob(A|B^{\complement})\prob(B^{\complement})
        \end{equation*}
    \end{lem}
    \begin{proofing}
        $A=(A\cap B)\cup(A\cap B^{\complement})\Longrightarrow\prob(A)=\prob(A\cap B)+\prob(A\cap B^{\complement})=\prob(A|B)\prob(B)+\prob(A|B^{\complement})\prob(B^{\complement})$
    \end{proofing}
    There are some cases when multiple events allow a certain event to occur.
    \begin{lem}(Law of total probability)
        Let $\{B_{1},B_{2},\cdots,B_{n}\}$ be a partition of $\Omega$ ($B_{i}\cap B_{j}=\emptyset$ for all $i\neq j$ and $\bigcup_{i=1}^{n}=\Omega$).\\
        Suppose that $\prob(B_{i})>0$ for all $i$. Then:
        \begin{equation*}
            \prob(A)=\sum_{i=1}^{n}\prob(A|B_{i})\prob(B_{i})
        \end{equation*}
    \end{lem}
    \begin{proofing}
        \begin{equation*}
            \prob(A)=\prob(A\cap\Omega)=\prob\left(A\cap\left(\bigcup_{i=1}^{n}B_{i}\right)\right)=\prob\left(\bigcup_{i=1}^{n}(A\cap B_{i})\right)=\sum_{i=1}^{n}\prob(A\cap B_{i})=\sum_{i=1}^{n}\prob(A|B_{i})\prob(B_{i})
        \end{equation*}
    \end{proofing}
    \newpage

\section{Independence}
    In general, the probability of a certain event is affected by the occurrence of other events. There are some exceptions.
    \begin{defn} % Independence
        Two events $A$ and $B$ are \textbf{independent} if $\prob(A\cap B)=\prob(A)\prob(B)$. It is denoted by $A\independent B$.\\
        More generally, a family of events $\{A_{i}:i\in I\}$ is \textbf{(mutually) independent} if for all subsets $J$ of $I$:
        \begin{equation*}
            \prob\left(\bigcap_{i\in J}A_{i}\right)=\prod_{i\in J}\prob(A_{i})
        \end{equation*}
    \end{defn}
    \begin{rem}
        If the family of events $\{A_{i}:i\in I\}$ has the property that $\prob(A_{i}\cap A_{j})=\prob(A_{i})\prob(A_{j})$ for all $i\neq j$, then it is \textbf{pairwise independent}.
    \end{rem}
    \begin{eg}
        Roll for dice twice: $\Omega=\{1,2,\cdots,6\}\times\{1,2,\cdots,6\}$ and $\mathcal{F}=2^{\Omega}$\\
        Let $A$ be event that the sum is $7$. Event $A=\{(1,6),(2,5),(3,4),(4,3),(5,2),(6,1)\}$.\\
        Let $B$ be event that the first roll is $4$. Event $B=\{(4,1),(4,2),(4,3),(4,4),(4,5),(4,6)\}$\\
        Let $C$ be event that the second roll is $3$. Event $C=\{(1,3),(2,3),(3,3),(4,3),(5,3),(6,3)\}$
        \begin{align*}
            \prob(A\cap B)&=\prob((4,3))=\frac{1}{36}=\frac{1}{6}\left(\frac{1}{6}\right)=\prob(A)\prob(B)\\
            \prob(B\cap C)&=\prob((4,3))=\frac{1}{36}=\frac{1}{6}\left(\frac{1}{6}\right)=\prob(B)\prob(C)\\
            \prob(A\cap C)&=\prob((4,3))=\frac{1}{36}=\frac{1}{6}\left(\frac{1}{6}\right)=\prob(A)\prob(C)\\
            \prob(A\cap B\cap C)&=\prob((4,3))=\frac{1}{36}\neq\prob(A)\prob(B)\prob(C)
        \end{align*}
        Therefore, events $A$, $B$ and $C$ are pairwise independent, but not mutually independent.
    \end{eg}
    \begin{prop}
        If events $A$ and $B$ are independent, then so are $A\independent B^{\complement}$ and $A^{\complement}\independent B^{\complement}$.
    \end{prop}
    \begin{proofing}
        \begin{equation*}
            \prob(A\cap B^{\complement})=\prob(A)-\prob(A\cap B)=\prob(A)-\prob(A)\prob(B)=\prob(A)(1-\prob(B))=\prob(A)\prob(B^{\complement})
        \end{equation*}
        Therefore, $A\independent B^{\complement}$ and also $A^{\complement}\independent B^{\complement}$.
    \end{proofing}
    \begin{prop}
        If events $A,B,C$ are independent, then:
        \begin{enumerate}
            \item $A\independent(B\cup C)$
            \item $A\independent(B\cap C)$
        \end{enumerate}
    \end{prop}
    \begin{proofing}
        \begin{enumerate}
            \item Using the properties of probability,
            \begin{align*}
                \prob(A\cap(B\cup C))&=\prob((A\cap B)\cup(A\cap C))\\
                &=\prob(A\cap B)+\prob(A\cap C)-\prob(A\cap B\cap C)\\
                &=\prob(A)\prob(B)+\prob(A)\prob(C)-\prob(A)\prob(B)\prob(C)\\
                &=\prob(A)\prob(B\cup C)
            \end{align*}
            \item
            \begin{equation*}
                \prob(A\cap(B\cap C))=\prob(A)\prob(B)\prob(C)=\prob(A)\prob(B\cap C)
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    \begin{rem}
        If events $A$ and $B$ are independent and $A\cap B=\emptyset$, then either $\prob(A)=0$ or $\prob(B)=0$.
    \end{rem}
    \newpage

\section{Product space}
    There are many $\sigma$-fields you can generate using a collection of subset of $\Omega$. However, many of those may be too big to be useful. Therefore, we have the following definition. 
    \begin{defn} % Sigma-field generated by a collection
        Let $A$ be a collection of subsets of $\Omega$. The \textbf{$\sigma$-field generated by $A$} is:
        \begin{equation*}
            \sigma(A)=\bigcap_{A\subseteq\mathcal{G}}\mathcal{G}
        \end{equation*}
        where $\mathcal{G}$ is also a $\sigma$-field. 
    \end{defn}
    \begin{rem}
    	$\sigma(A)$ is the smallest $\sigma$-field containing $A$.
    \end{rem}
    \begin{eg}
        Let $\Omega=\{1,2,\cdots,6\}$ and $A=\{\{1\}\}\subseteq 2^{\Omega}$. $\sigma(A)=\{\emptyset,\{1\},\{2,3,\cdots,6\},\Omega\}$
    \end{eg}
    \begin{cor}
        Suppose $(\mathcal{F}_{i})_{i\in I}$ is a system of $\sigma$-fields in $\Omega$. Then:
        \begin{equation*}
            \bigcap_{i\in I}\mathcal{F}_{i}=\{A\in\Omega: A\in\mathcal{F}_{i}\text{ for all }i\in I\}
        \end{equation*}
    \end{cor}
    Now that we know which $\sigma$-field we should generate, we can finally combine two probability spaces together to form a new probability space.
    \begin{defn} % Product space
        \textbf{Product space} of two probability spaces $(\Omega_{1},\mathcal{F}_{1},\prob_{1})$ and $(\Omega_{2},\mathcal{F}_{2},\prob_{2})$ is the probability space $(\Omega_{1}\times\Omega_{2},\mathcal{G},\prob_{12})$ comprising:
        \begin{enumerate}
        	\item a collection of ordered pairs $\Omega_{1}\times\Omega_{2}=\{(\omega_{1},\omega_{2}):\omega_{1}\in\Omega_{1},\omega_{2}\in\Omega_{2}\}$
        	\item a $\sigma$-algebra $\mathcal{G}=\sigma(\mathcal{F}_{1}\times\mathcal{F}_{2})$ where $\mathcal{F}_{1}\times\mathcal{F}_{2}=\{A_{1}\times A_{2}:A_{1}\in\mathcal{F}_{1},A_{2}\in\mathcal{F}_{2}\}$ 
        	\item a probability measure $\prob_{12}:\mathcal{F}_{1}\times\mathcal{F}_{2}\to [0,1]$ given by:
        	\begin{equation*}
        		\prob_{12}(A_{1}\times A_{2})=\prob_{1}(A_{1})\prob_{2}(A_{2})
        	\end{equation*}
        	for $A_{1}\in\mathcal{F}_{1},A_{2}\in\mathcal{F}_{2}$.
        \end{enumerate}
    \end{defn}
    
\chapter{Random variables and their distribution}
\section{Introduction of random variables}
    Sometimes, we are not interested in the experiment itself but rather in the consequences of its random outcomes. These consequences can be represented as functions mapping a sample space to the real number field. Such functions are called "random variables."
    \begin{defn}
        A \textbf{random variable} is a function $X:\Omega\to\mathbb{R}$ with the property that for any $x\in\mathbb{R}$,
        \begin{equation*}
            X^{-1}((-\infty,x])=\{\omega\in\Omega:X(\omega)\leq x\}\in\mathcal{F}.
        \end{equation*}
    \end{defn}
    \begin{rem}
        More generally, a random variable is a function $X$ such that for all intervals $A\subseteq\mathbb{R}$,
        \begin{equation*}
            X^{-1}(A)=\{\omega\in\Omega: X(\omega)\in A\}\in\mathcal{F}.
        \end{equation*}
        Such a function is said to be \textbf{$\mathcal{F}$-measurable}.
    \end{rem}
    \begin{rem}
        The intervals can be replaced by any of the following classes:
        \begin{enumerate}
            \item $(a,b)$ for all $a<b$,
            \item $(a,b]$ for all $a<b$,
            \item $[a,b)$ for all $a<b$,
            \item $[a,b]$ for all $a<b$,
            \item $(-\infty,x]$ for all $x\in\mathbb{R}$.
        \end{enumerate}
        This is due to the following reasons:
        \begin{enumerate}
            \item $X^{-1}$ can be interchanged with any set functions.
            \item $\mathcal{F}$ is a $\sigma$-field.
        \end{enumerate}
    \end{rem}
    \begin{cla}
        Suppose $X^{-1}(B)\in\mathcal{F}$ for all open sets $B$. Then $X^{-1}(B')\in\mathcal{F}$ for all closed sets $B'$.
    \end{cla}
    \begin{proofing}
        For any $a,b\in\mathbb{R}$,
        \begin{equation*}
            X^{-1}([a,b])=X^{-1}\left(\bigcap_{n=1}^{\infty}\left(a-\frac{1}{n},b+\frac{1}{n}\right)\right)=\bigcap_{n=1}^{\infty}X^{-1}\left(\left(a-\frac{1}{n},b+\frac{1}{n}\right)\right)\in\mathcal{F}.
        \end{equation*}
    \end{proofing}
    \begin{rem}
        The $\mathcal{F}$-measurability of $X$ is necessary because $\prob(X\in A)=\prob(\{\omega:X(\omega)\in A\})=\prob(X^{-1}(A))$. Thus, $X^{-1}(A)$ must belong to $\mathcal{F}$.
    \end{rem}
    
    \newpage
    \begin{eg}
        \label{Toss coin twice}
        A fair coin is tossed twice. $\Omega=\{HH,HT,TH,TT\}$. For all $\omega\in\Omega$, let $X(\omega)$ be the number of heads.
        \begin{align*}
            X(\omega)&=\begin{cases}
                0, &\omega\in\{TT\}\\
                1, &\omega\in\{HT,TH\}\\
                2, &\omega\in\{HH\}
            \end{cases} & X^{-1}((-\infty,x])&=\begin{cases}
                \emptyset, & x<0\\
                \{TT\}, & x\in[0,1)\\
                \{HT,TH,TT\}, & x\in[1,2)\\
                \Omega, &x\in[2,\infty)
            \end{cases}
        \end{align*}
        If we choose $\mathcal{F}=\{\emptyset,\Omega\}$, then $X$ is not a random variable. If we choose $\mathcal{F}=2^{\Omega}$, then $X$ is a random variable.
    \end{eg}
    Before we continue, it is best if we know about Borel set first.
    \begin{defn}
        \textbf{Borel set} is a set which can be obtained by taking countable union, intersection or complement repeatedly. (Countably many steps)
    \end{defn}
    \begin{defn}
        \textbf{Borel $\sigma$-field} of $\mathbb{R}$ is a $\sigma$-field $\mathcal{B}(\mathbb{R})$ that is generated by all open sets. It is a collection of Borel sets.
    \end{defn}
    \begin{eg}
        $\{(a,b),[a,b],\{a\},\mathbb{Q},\mathbb{R}\setminus\mathbb{Q}\}\subset\mathcal{B}(\mathbb{R})$. Note that closed sets can be generated by open sets.
    \end{eg}
    \begin{rem}
        In modern way of understanding, $(\Omega,\mathcal{F},\prob)\xrightarrow{X}(\mathbb{R},\mathcal{B},\prob\circ X^{-1})$
    \end{rem}
    \begin{cla}
        $\prob\circ X^{-1}$ is a probability measure on $(\mathbb{R},\mathcal{B})$.
    \end{cla}
    \begin{proofing}
        \begin{enumerate}
            \item For all $B\in\mathcal{B}$, $\prob\circ X^{-1}(B)=\prob(\{\omega:X(\omega)\in B\})\in [0,1]$
            \begin{align*}
                \prob\circ X^{-1}(\emptyset)&=\prob(\{\omega:X(\omega)\in\emptyset\})=\prob(\emptyset)=0\\
                \prob\circ X^{-1}(\mathbb{R})&=\prob(\{\omega:X(\omega)\in\mathbb{R}\})=\prob(\Omega)=1
            \end{align*}
            \item For any disjoint $B_{1},B_{2},\cdots\in\mathcal{B}$,
            \begin{equation*}
                \prob\circ X^{-1}\left(\bigcup_{i=1}^{\infty}B_{i}\right)=\prob\left(\bigcup_{i=1}^{\infty}X^{-1}(B_{i})\right)=\sum_{i=1}^{\infty}\prob(X^{-1}(B_{i}))=\sum_{i=1}^{\infty}\prob\circ X^{-1}(B_{i})
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    \begin{rem}
        We can derive the probability of all $A\in\mathcal{B}$.
        \begin{align*}
            \prob([a,b])&=\prob((-\infty,b])-\prob((-\infty,a))\\
            &=\prob((-\infty,b])-\prob\left(\bigcup_{n=1}^{\infty}\left(-\infty,a-\frac{1}{n}\right]\right)\\
            &=\prob((-\infty,b])-\lim_{n\to\infty}\prob\left(\left(-\infty,a-\frac{1}{n}\right]\right)
        \end{align*}
    \end{rem}
    \newpage

\section{CDF of random variables}
    Every random variable has an associated distribution function.
    \begin{defn}
        The \textbf{(cumulative) distribution function} (CDF) of a random variable $X$ is a function $F_{X}:\mathbb{R}\to [0,1]$ defined as:
        \begin{equation*}
            F_{X}(x)=\prob(X\leq x):=\prob\circ X^{-1}((-\infty,x]).
        \end{equation*}
    \end{defn}
    \begin{eg}
        From Example \ref{Toss coin twice},
        \begin{align*}
            \prob(\omega)&=\frac{1}{4}, & F_{X}(x)&=\prob(X\leq x)=\begin{cases}
                0, &x<0,\\
                \frac{1}{4}, &0\leq x<1,\\
                \frac{3}{4}, &1\leq x<2,\\
                1, &x\geq 2.
            \end{cases}
        \end{align*}
    \end{eg}
    \begin{lem}
        The CDF $F_{X}$ of a random variable $X$ satisfies the following properties:
        \begin{enumerate}
            \item $\lim_{x\to -\infty}F_{X}(x)=0$ and $\lim_{x\to\infty}F_{X}(x)=1$.
            \item If $x<y$, then $F_{X}(x)\leq F_{X}(y)$.
            \item $F_{X}$ is right-continuous ($F_{X}(x+h)\to F_{X}(x)$ as $h\downarrow 0$).
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item Let $B_{n}=\{\omega\in\Omega:X(\omega)\leq -n\}=\{X\leq -n\}$. Since $B_{1}\supseteq B_{2}\supseteq\cdots$, by Lemma \ref{Continuous probability},
            \begin{equation*}
                \lim_{x\to -\infty}F_{X}(x)=\prob\left(\lim_{i\to\infty}B_{i}\right)=\prob(\emptyset)=0.
            \end{equation*}
            Alternative proof:
            \begin{equation*}
                \lim_{x\to-\infty}F_{X}(x)=\lim_{x\to-\infty}\prob\circ X^{-1}((-\infty,x])=\lim_{n\to\infty}\prob\circ X^{-1}((-\infty,-n])=\prob\circ X^{-1}(\emptyset)=0
            \end{equation*}
            Let $C_{n}=\{\omega\in\Omega:X(\omega)\leq n\}=\{X\leq n\}$. Since $C_{1}\subseteq C_{2}\subseteq\cdots$, by Lemma \ref{Continuous probability},
            \begin{equation*}
                \lim_{x\to\infty}F_{X}(x)=\prob\left(\lim_{i\to\infty}C_{i}\right)=\prob(\Omega)=1.
            \end{equation*}
            Alternative Proof:
            \begin{equation*}
                \lim_{x\to\infty}F_{X}(x)=\lim_{x\to\infty}\prob\circ X^{-1}((-\infty,x])=\prob\circ X^{-1}(\mathbb{R})=1.
            \end{equation*}
            \item Let $A(x)=\{X\leq x\}, A(x,y)=\{x<X\leq y\}$. Then $A(y)=A(x)\cup A(x,y)$ is a disjoint union.
            \begin{equation*}
                F_{X}(y)=\prob(A(y))=\prob(A(x))+\prob(A(x,y))=F_{X}(x)+\prob(x<X\leq y)\geq F_{X}(x)
            \end{equation*}
            \item Let $B_{n}=\{\omega\in\Omega:X(\omega)\leq x+\frac{1}{n}\}$. Since $B_{1}\supseteq B_{2}\supseteq\cdots$, by Lemma \ref{Continuous probability},
            \begin{equation*}
                \lim_{h\downarrow 0}F_{X}(x+h)=\prob\left(\bigcap_{i=1}^{\infty}B_{i}\right)=\prob\left(\lim_{n\to\infty}B_{n}\right)=\prob(\{\omega\in\Omega:X(\omega)\leq x\})=F_{X}(x)
            \end{equation*}
            Alternative Proof:
            \begin{equation*}
                \lim_{h\downarrow 0}F_{X}(x+h)=\lim_{h\downarrow 0}\prob\circ X^{-1}((-\infty,x+h])=\lim_{n\to\infty}\prob\circ X^{-1}\left(\left(-\infty,x+\frac{1}{n}\right]\right)=\prob\circ X^{-1}((-\infty,x])=F_{X}(x)
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    \begin{rem}
        $F$ is not left-continuous because:
        \begin{equation*}
            \lim_{h\downarrow 0}F_{X}(x-h)=\lim_{n\to\infty}\prob\circ X^{-1}\left(\left(-\infty,x-\frac{1}{n}\right)\right)=\prob\circ X^{-1}((-\infty,x))=F_{X}(x)-\prob\circ X^{-1}(\{x\})
        \end{equation*}
    \end{rem}
    \begin{lem}
        Let $F_{X}$ be the CDF of a random variable $X$. Then
        \begin{enumerate}
            \item $\prob(X>x)=1-F_{X}(x)$.
            \item $\prob(x<X\leq y)=F_{X}(y)-F_{X}(x)$.
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item $\prob(X>x)=\prob(\Omega\setminus\{X\leq x\})=\prob(\Omega)-\prob(X\leq x)=1-F_{X}(x)$.
            \item $\prob(x<X\leq y)=\prob(\{X\leq y\}\setminus\{X\leq x\})=\prob(X\leq y)-\prob(X\leq x)=F_{X}(y)-F_{X}(x)$.
        \end{enumerate}
    \end{proofing}
    \begin{eg}(\textbf{Constant variables})
        Let $X:\Omega\to\mathbb{R}$ be defined by $X(\omega)=c$ for all $\omega\in\Omega$. For all $B\in\mathcal{B}$,
        \begin{equation*}
            F_{X}(x)=\prob\circ X^{-1}(B)=\begin{cases}
                0, &B\cap\{c\}=\emptyset\\
                1, &B\cap\{c\}=\{c\}
            \end{cases}
        \end{equation*}
        $X$ is constant almost surely if there exists $c\in\mathbb{R}$ such that $\prob(X=c)=1$.
    \end{eg}
    \begin{eg}(\textbf{Bernoulli variables})
        Consider flipping coin once. Let $X:\Omega\to\mathbb{R}$ be defined by $X(H)=1$ and $X(T)=0$.
        \begin{equation*}
            F_{X}(x)=\begin{cases}
                0, &x<0\\
                1-p, &0\leq x<1\\
                1, &x\geq 1
            \end{cases}
        \end{equation*}
        $X$ have \textbf{Bernoulli distribution}, denoted by $\Bern(p)$.
    \end{eg}
    \begin{eg}
        Let $A$ be an event in $\mathcal{F}$ and \textbf{indicator functions} $\mathbf{1}_{A}:\Omega\to\mathbb{R}$ such that for all $B\in\mathcal{B}(\mathbb{R})$:
        \begin{align*}
            \mathbf{1}_{A}(\omega)&=\begin{cases}
                1, &\omega\in A\\
                0, &\omega\in A^{\complement}
            \end{cases} & \mathbf{1}_{A}^{-1}(B)&=\begin{cases}
                \emptyset, &B\cap\{0,1\}=\emptyset\\
                A^{\complement}, & B\cap\{0,1\}=\{0\}\\
                A, &B\cap\{0,1\}=\{1\}\\
                \Omega, &B\cap\{0,1\}=\{0,1\}
            \end{cases} & \prob\circ \mathbf{1}_{A}^{-1}(B)&=\begin{cases}
                0, &B\cap\{0,1\}=\emptyset\\
                \prob(A^{\complement}), & B\cap\{0,1\}=\{0\}\\
                \prob(A), &B\cap\{0,1\}=\{1\}\\
                1, &B\cap\{0,1\}=\{0,1\}
            \end{cases}
        \end{align*}
        Then $\mathbf{1}_{A}$ is a Bernoulli random variable taking values $1$ and $0$ with probabilities $\prob(A)$ and $\prob(A^{\complement})$ respectively.
    \end{eg}

\section{PMF / PDF of random variables}
    We can classify some random variables into either discrete or continuous. This two will be further discussed in the next two chapters.
    \begin{defn}
        Random variable $X$ is \textbf{discrete} if it takes value in some countable subsets $\{x_{1},x_{2},\cdots\}$ only of $\mathbb{R}$.\\
        Discrete random variable $X$ has \textbf{probability mass function} (PMF) $f_{X}:\mathbb{R}\to [0,1]$ given by: 
        \begin{equation*}
            f_{X}(x)=\prob(X=x)=\prob\circ X^{-1}(\{x\})
        \end{equation*}
    \end{defn}
    \begin{lem}
        \label{Relationship between pmf and cdf}
        Relationship between PMF $f_{X}$ and CDF $F_{X}$ of a random variable $X$:
        \begin{enumerate}
            \item $F_{X}(x)=\sum_{i\leq x}f_{X}(i)$
            \item $f_{X}(x)=F_{X}(x)-\lim_{y\uparrow x}F_{X}(y)$
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item 
            \begin{equation*}
			    F_{X}(x)=\prob(X\leq x)=\sum_{i:x_{i}\leq x}\prob(X=x_{i})=\sum_{y\leq x}f_{X}(y)
            \end{equation*}
            \item Let $B_{n}=\{x-\frac{1}{n}<X\leq x\}$. Since $B_{1}\supseteq B_{2}\supseteq\cdots$, by Lemma \ref{Continuous probability},
            \begin{equation*}
                F_{X}(x)-\lim_{y\uparrow x}F_{X}(y)=\prob\left(\bigcap_{i=1}^{\infty}B_{i}\right)=\prob\left(\lim_{n\to\infty}B_{n}\right)=\prob\left(\left\{\lim_{n\to\infty}\left(x-\frac{1}{n}\right)<X\leq x\right\}\right)=\prob(X=x)
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    This is problematic when random variable $X$ is continuous because using PMF will get the result of $f_{X}(x)=0$ for all $x$. Therefore, we would need another definition for continuous random variable.
    \begin{defn}
        Random variable $X$ is called \textbf{continuous} if its distribution function can be expressed as:
        \begin{align*}
            F_{X}(x)&=\intlu{-\infty}{x}f(u)\,du & x&\in\mathbb{R}
        \end{align*}
        for some integrable \textbf{probability density function} (PDF) $f_{X}:\mathbb{R}\to [0,\infty)$ of $X$. 
    \end{defn}
    \begin{rem}
        For small $\delta>0$:
        \begin{equation*}
            \prob(x<X\leq x+\delta)=F_{X}(x+\delta)-F_{X}(x)=\intlu{x}{x+\delta}f_{X}(u)\,du\approx f_{X}(x)\delta
        \end{equation*}
    \end{rem}
    \begin{rem}
        On discrete random variable, the distribution is \textbf{atomic} because the distribution function has jump discontinuities at values $x_{1},x_{2},\cdots$ and is constant in between.
    \end{rem}
    \begin{rem}
        On continuous random variable, the CDF of a continuous variable is \textbf{absolutely continuous}.\\
        Not every continuous function can be written as $\intlu{-\infty}{x}f_{X}(u)\,du$. E.g. Canton function
    \end{rem}
    \begin{rem}
        It is possible that a random variable is neither continuous nor discrete.
    \end{rem}

\section{JCDF of random variables}
    How do we deal with cases when there are more than one random variables?
    \begin{defn}
	Let $X_{1},X_{2}:\Omega\to\mathbb{R}$ be random variables. We define \textbf{random vector} $\mathbf{X}=(X_{1},X_{2}):\Omega^{2}\to\mathbb{R}^{2}$ with properties
        \begin{equation*}
		    \mathbf{X}^{-1}(D)=\{\omega\in\Omega:\mathbf{X}(\omega)=(X_{1}(\omega),X_{2}(\omega))\in D\}\in\mathcal{F}
        \end{equation*}
        for all $D\in\mathcal{B}(\mathbb{R}^{2})$.\\
	    We can also say $\mathbf{X}=(X_{1},X_{2})$ is a random vector if both $X_{1},X_{2}:\Omega\to\mathbb{R}$ are random variables. That means:
        \begin{equation*}
            X_{a}^{-1}(B)\in\mathcal{F}
        \end{equation*}
        for all $B\in\mathcal{B}(\mathbb{R}),a=1,2$.
    \end{defn}
    \begin{cla}
        Both definitions of random vectors are equivalent.
    \end{cla}
    \begin{proofing}
	    By first definition, $\mathbf{X}^{-1}(A_{1}\times A_{2})\in\mathcal{F}$. If we choose $A_{2}=\mathbb{R}$,
        \begin{align*}
		    \mathbf{X}^{-1}(A_{1}\times \mathbb{R})&=\{\omega\in\Omega:(X_{1}(\omega),X_{2}(\omega))\in A_{1}\times\mathbb{R}\}\\
            &=\{\omega\in\Omega:X_{1}(\omega)\in A_{1}\}\cap\{\omega\in\Omega:X_{2}(\omega)\in\mathbb{R}\}\\
            &=X_{1}^{-1}(A_{1})
        \end{align*}
        This means $X_{1}$ is a random variable. Using similar method, we can also find that $X_{2}$ is a random variable.\\
        Therefore, we can obtain the second definition from the first definition.\\
        By second definition, $X_{1}$ and $X_{2}$ are random variables. Therefore,
        \begin{align*}
		    \mathbf{X}^{-1}(A_{1}\times A_{2})&=\{\omega\in\Omega:(X_{1}(\omega),X_{2}(\omega))\in A_{1}\times A_{2}\}\\
            &=\{\omega\in\Omega:X_{1}(\omega)\in A_{1}\}\cap\{\omega\in\Omega:X_{2}(\omega)\in A_{2}\}\\
            &=X_{1}^{-1}(A_{1})\cap X_{2}^{-1}(A_{2})\in\mathcal{F}
        \end{align*}
        Therefore, we can obtain the first definition from the second definition.\\
        Therefore, two definitions are equivalent.
    \end{proofing}
    \begin{rem}
	    We can write $\prob\circ\mathbf{X}^{-1}(D)=\prob(\mathbf{X}\in D)=\prob(\{\omega\in\Omega:\mathbf{X}(\omega)=(X_{1}(\omega),X_{2}(\omega))\in D\})$.
    \end{rem}
    \newpage
    Of course, there is a distribution function corresponding to the random vector.
    \begin{defn}
	    \textbf{Joint distribution function} (JCDF) $F_{\mathbf{X}}:\mathbb{R}^{2}\to [0,1]$ is defined as
        \begin{equation*}
		    F_{\mathbf{X}}(x_{1},x_{2})=F_{X_{1},X_{2}}(x_{1},x_{2})=\prob\circ\mathbf{X}^{-1}((-\infty,x_{1}]\times(-\infty,x_{2}])=\prob(X_{1}\leq x_{1},X_{2}\leq x_{2})
        \end{equation*}
    \end{defn}
    \begin{rem}
        We can replace all Borel sets by the form $[a_{1},b_{1}]\times[a_{2},b_{2}]\times\cdots\times[a_{n},b_{n}]$.
    \end{rem}
    Joint distribution function has quite similar properties with normal distribution function.
    \begin{lem}
        JCDF $F_{X,Y}$ of random vector $(X,Y)$ has the following properties:
        \begin{enumerate}
            \item $\lim_{(x,y)\to (-\infty,-\infty)}F_{X,Y}(x,y)=0$ and $\lim_{(x,y)\to (\infty,\infty)}F_{X,Y}(x,y)=1$.
		    \item If $x_{1}\leq y_{1}$ and $x_{2}\leq y_{2}$, then $F_{X,Y}(x_{1},y_{1})\leq F_{X,Y}(x_{2},y_{2})$.
            \item $F_{X,Y}$ is continuous from above, in that $F_{X,Y}(x+u,y+v)\to F_{X,Y}(x,y)$ as $u,v\downarrow 0$.
        \end{enumerate}
    \end{lem}
    We can find the probability distribution of one random variable by disregarding another variable. We get the following distribution.
    \begin{defn}
        Let $X,Y$ be random variables. We can get a \textbf{marginal distribution} (marginal CDF) by having:
        \begin{equation*}
            F_{X}(x)=\prob\circ X^{-1}((-\infty,x])=\prob\left(X^{-1}((-\infty,x])\cap Y^{-1}((-\infty,\infty))\right)=\lim_{y\uparrow\infty}\prob\left(X^{-1}((-\infty,x])\cap Y^{-1}((-\infty,y])\right)=\lim_{y\uparrow\infty}F_{X,Y}(x,y)
        \end{equation*}
    \end{defn}
    Joint distribution function also has its probability mass function and probability density function too.
    \begin{defn}
        Two random variables $X$ and $Y$ on $(\Omega,\mathcal{F},\prob)$ are \textbf{jointly discrete} if the vector $(X,Y)$ takes values in some countable subset of $\mathbb{R}^{2}$ only. The corresponding \textbf{joint (probability) mass function} (JPMF) $f:\mathbb{R}^{2}\to [0,1]$ is given by
        \begin{align*}
            f_{X,Y}(x,y)&=\prob((X,Y)=(x,y))=\prob\circ(X,Y)^{-1}(\{x,y\}) & F_{X,Y}(x,y)&=\sum_{u\leq x}\sum_{v\leq y}f(u,v) & x,y&\in\mathbb{R}
        \end{align*}
    \end{defn}
    \begin{rem}
        \begin{equation*}
            f_{X,Y}(x,y)=F_{X,Y}(x,y)-F_{X,Y}(x^{-},y)-F_{X,Y}(x,y^{-})+F_{X,Y}(x^{-},y^{-})
        \end{equation*}
    \end{rem}
    \begin{rem}
        More generally, for all $B\in\mathcal{B}(\mathbb{R}^{2})$,
        \begin{equation*}
            \prob\circ(X,Y)^{-1}(B)=\sum_{(u,v)\in B}f_{X,Y}(u,v)
        \end{equation*}
    \end{rem}
    \begin{defn}
        Two random variables $X$ and $Y$ on $(\Omega,\mathcal{F},\prob)$ are \textbf{jointly continuous} if the \textbf{joint probability density function} (JPDF) $f:\mathbb{R}^{2}\to [0,\infty)$ of $(X,Y)$ can be expressed as:
        \begin{align*}
            f_{X,Y}(x,y)&=\pdv*{F_{X,Y}(x,y)}{x,y} & F_{X,Y}(x,y)&=\intlu{-\infty}{x}\intlu{-\infty}{y}f_{X,Y}(u,v)\,du\,dv & x,y&\in\mathbb{R}
        \end{align*}
    \end{defn}
    \begin{rem}
        More generally, for all $B\in\mathcal{B}(\mathbb{R}^{2})$,
        \begin{equation*}
            \prob\circ(X,Y)^{-1}(B)=\prob((X,Y)\in B)=\iint_{B}f_{X,Y}(u,v)\,du\,dv
        \end{equation*}
    \end{rem}
    \begin{eg}
    	Assume that a special three-sided coin is provided. Each toss results in head (H), tail (T) or edge (E) with equal probabilities. What is the probability of having $h$ heads, $t$ tails and $e$ edges after $n$ tosses?\\
    	Let $H_{n},T_{n},E_{n}$ be the numbers of such outcomes in $n$ tosses of the coin. The vector $(H_{n},T_{n},E_{n})$ satisfy $H_{n}+T_{n}+E_{n}=n$.
    	\begin{equation*}
    		\prob((H_{n},T_{n},E_{n})=(h,t,e))=\frac{n!}{h!t!e!}\left(\frac{1}{3}\right)^{n}
    	\end{equation*}
    \end{eg}
    \begin{rem}
        It is not generally true for two continuous random variables $X$ and $Y$ to be jointly continuous.
    \end{rem}
    \begin{eg}
        Let $X$ be uniformly distributed on $[0,1]$ ($f_{X}(x)=\mathbf{1}_{[0,1]}$). This means $f_{X}(x)=1$ when $x\in[0,1]$ and $0$ otherwise.\\
	    Let $Y=X$ ($Y(\omega)=X(\omega)$ for all $\omega\in\Omega$). That means $(X,Y)=(X,X)$. Let $B=\{(x,y):x=y\text{ and }x\in[0,1]\}\in\mathcal{B}(\mathbb{R}^{2})$.\\
	    Since $y=x$ is just a line,
        \begin{align*}
            \prob\circ(X,Y)^{-1}(B)&=1\\
            \iint_{B}f_{X,Y}(u,v)\,du\,dv&=0\neq\prob\circ(X,Y)^{-1}(B)
        \end{align*}
        Therefore, $X$ and $Y$ are not jointly continuous.
    \end{eg}
    
\chapter{Discrete random variables}
\section{Introduction of discrete random variables}
    Let us revisit some key definitions related to discrete random variables from the previous chapter.
    \begin{defn}
        A random variable $X$ is said to be \textbf{discrete} if it takes values in a countable subset $\{x_{1},x_{2},\cdots\}$ of $\mathbb{R}$.\\
        The \textbf{(cumulative) distribution function} (CDF) of a discrete random variable $X$ is the function $F_{X}:\mathbb{R}\to [0,1]$ defined as:
        \begin{equation*}
            F_{X}(x)=\prob(X\leq x).
        \end{equation*}
        The \textbf{probability mass function} (PMF) of a discrete random variable $X$ is the function $f_{X}:\mathbb{R}\to [0,1]$ defined as:
        \begin{equation*}
            f_{X}(x)=\prob(X=x).
        \end{equation*}
        The CDF and PMF are related by the following equations:
        \begin{align*}
            F_{X}(x)&=\sum_{i:x_{i}\leq x}f_{X}(x_{i}), & f_{X}(x)&=F_{X}(x)-\lim_{y\uparrow x}F_{X}(y).
        \end{align*}
    \end{defn}
    \begin{lem}
        The PMF $f_{X}:\mathbb{R}\to [0,1]$ of a discrete random variable $X$ satisfies the following properties:
        \begin{enumerate}
            \item The set of $x$ values for which $f_{X}(x)\neq 0$ is countable.
            \item $\sum_{i}f_{X}(x_{i})=1$, where $x_{1},x_{2},\cdots$ are the values of $x$ such that $f_{X}(x)\neq 0$.
        \end{enumerate}
    \end{lem}
    Next, we recall the definitions of joint distribution and joint mass functions.
    \begin{defn}
        For jointly discrete random variables $X$ and $Y$, the \textbf{joint probability mass function} (JPMF) $f_{X,Y}:\mathbb{R}^{2}\to [0,1]$ is defined as:
        \begin{align*}
            f_{X,Y}(x,y)&=\prob((X,Y)=(x,y))=\prob\circ(X,Y)^{-1}(\{x,y\}), & F_{X,Y}(x,y)&=\sum_{u\leq x}\sum_{v\leq y}f(u,v), & x,y&\in\mathbb{R}.
        \end{align*}
    \end{defn}
    Recall that two events $A$ and $B$ are independent if the occurrence of $A$ does not affect the probability of $B$ occurring.
    \begin{defn}
        Discrete random variables $X$ and $Y$ are \textbf{independent} if the events $\{X=x\}$ and $\{Y=y\}$ are independent for all $x,y$. Equivalently, $X$ and $Y$ are independent if:
        \begin{enumerate}
            \item $\prob((X,Y)\in A\times B)=\prob(X\in A)\prob(Y\in B)$ for all $A,B\in\mathcal{B}(\mathbb{R})$.
            \item $F_{X,Y}(x,y)=F_{X}(x)F_{Y}(y)$ for all $x,y\in\mathbb{R}$.
            \item $f_{X,Y}(x,y)=f_{X}(x)f_{Y}(y)$ for all $x,y\in\mathbb{R}$.
        \end{enumerate}
    \end{defn}

    \newpage
    \begin{cla}
        Three definitions are equivalent.
    \end{cla}
    \begin{proofing}
        We can get definition 2 from definition 1.
        \begin{align*}
            F_{X,Y}(x,y)&=\prob(X\leq x,Y\leq y)=\prob(X\leq x)\prob(Y\leq y)=F_{X}(x)F_{Y}(y)
        \end{align*}
        We can get definition 3 from definition 2.
        \begin{align*}
            f_{X,Y}(x,y)&=F_{X,Y}(x,y)-F_{X,Y}(x^{-},y)-F_{X,Y}(x,y^{-})+F_{X,Y}(x^{-},y^{-})\\
            &=F_{X}(x)F_{Y}(y)-F_{X}(x^{-})F_{Y}(y)-F_{X}(x)F_{Y}(y^{-})+F_{X}(x^{-})F_{Y}(y^{-})\\
            &=(F_{X}(x)-F_{X}(x^{-}))(F_{Y}(y)-F_{Y}(y^{-}))=f_{X}(x)f_{Y}(y)
        \end{align*}
        We can get definition 1 from definition 3.
        \begin{equation*}
            \prob\circ(X,Y)^{-1}(E\times F)=\sum_{(x,y)\in E\times F}f_{X,Y}(x,y)=\sum_{x\in E}\sum_{y\in F}f_{X}(x)f_{Y}(y)=(\prob\circ X^{-1}(E))(\prob\circ Y^{-1}(F))
        \end{equation*}
        Therefore, three definitions are equivalent.
    \end{proofing}
    \begin{rem}
        More generally, let $X_{1},X_{2},\cdots,X_{n}:\Omega\to\mathbb{R}$ be discrete random variables. They are \textbf{independent} if
        \begin{enumerate}
            \item For all $A_{i}\in\mathcal{B}(\mathbb{R})$,
            \begin{equation*}
                \prob\circ(X_{1},X_{2},\cdots,X_{n})^{-1}(A_{1}\times A_{2}\times\cdots\times A_{n})=\prod_{i=1}^{n}\prob\circ X_{i}^{-1}(A_{i})
            \end{equation*}
            \item For all $x_{i}\in\mathbb{R}$,
            \begin{equation*}
                F_{X_{1},X_{2},\cdots,X_{n}}(x_{1},x_{2},\cdots,x_{n})=\prod_{i=1}^{n}F_{X_{i}}(x_{i})
            \end{equation*}
            \item For all $x_{i}\in\mathbb{R}$,
            \begin{equation*}
                f_{X_{1},X_{2},\cdots,X_{n}}(x_{1},x_{2},\cdots,x_{n})=\prod_{i=1}^{n}f_{X_{i}}(x_{i})
            \end{equation*}
        \end{enumerate}
    \end{rem}
    Recall that we say $A_{1},A_{2},\cdots,A_{n}$ are independent if for any $I\subseteq\{1,2,\cdots,n\}$:
    \begin{equation*}
        \prob\left(\bigcap_{i\in I}A_{i}\right)=\prod_{i\in I}\prob(A_{i})
    \end{equation*}
    \begin{rem}
        From the definition, we can see that $X\independent Y$ means that $X^{-1}(E)\independent Y^{-1}(F)$ for all $E,F\in\mathcal{B}(\mathbb{R})$.
    \end{rem}
    \begin{rem}
        We can generate $\sigma$-field using random variables by defining $\sigma$-field generated by random variable $X$
        \begin{equation*}
        	\sigma(X)=\{X^{-1}(E):E\in\mathcal{B}(\mathbb{R})\}\subseteq\mathcal{F}
        \end{equation*}
    \end{rem}
    From the remarks, we can extend the definition of independence from random variables to $\sigma$-fields.
    \begin{defn}
        Let $\mathcal{G},\mathcal{H}\subseteq\mathcal{F}$ be two $\sigma$-fields. We say $\mathcal{G}$ and $\mathcal{H}$ are \textbf{independent} if $A\independent B$ for all $A\in\mathcal{G},B\in\mathcal{H}$.
    \end{defn}
    \begin{rem}
        $\sigma(X)\independent\sigma(Y)\iff X\independent Y$
    \end{rem}
    \begin{thm}
        Given two random variables $X$ and $Y$. If $X\independent Y$ and we have two functions $g,h:\mathbb{R}\to\mathbb{R}$ such that $g(X)$ and $h(Y)$ are still random variables, then $g(X)\independent h(Y)$.
    \end{thm}
    \begin{proofing}
        For all $A,B\in\mathcal{B}$,
        \begin{align*}
            \prob((g(X),h(Y))\in A\times B)&=\prob(g(X)\in A,h(Y)\in B)\\
            &=\prob(X\in\{x:g(x)\in A\},Y\in\{y:h(y)\in B\})\\
            &=\prob(X\in\{x:g(x)\in A\})\prob(Y\in\{y:h(y)\in B\})\\
            &=\prob(g(X)\in A)\prob(h(Y)\in B)
        \end{align*}
        Therefore, $g(X)\independent h(Y)$.
    \end{proofing}
    \begin{rem}
        We assume a product space $(\Omega,\mathcal{F},\prob)$ of two probability space $(\Omega_{1},\mathcal{F}_{1},\prob_{1})$ and $(\Omega_{2},\mathcal{F}_{2},\prob_{2})$.\\
        ($\Omega=\Omega_{1}\times\Omega_{2}$, $\mathcal{F}=\sigma(\mathcal{F}_{1}\times\mathcal{F}_{2})$, $\prob(A_{1}\times A_{2})=\prob_{1}(A_{1})\prob_{2}(A_{2})$).\\
        Any pair of events of the form $E_{1}\times\Omega_{2}$ and $\Omega_{1}\times E_{2}$ are independent.
        \begin{equation*}
            \prob((E_{1}\times\Omega_{2})\cap(\Omega_{1}\times E_{2}))=\prob(E_{1}\times E_{2})=\prob_{1}(E_{1})\prob_{2}(E_{2})=\prob(E_{1}\times\Omega_{2})\prob(\Omega_{1}\times E_{2})
        \end{equation*}
    \end{rem}
    We have some important examples of random variables that have wide number of applications.
    \begin{eg}(\textbf{Bernoulli random variable}) $X\sim \Bern(p)$\\
        Let $A\in\mathcal{F}$ be a specific event. A Bernoulli trial is considered a success if $A$ occurs. Let $X:\Omega\to\mathbb{R}$ be such that
        \begin{align*}
            X(\omega)&=\mathbf{1}_{A}(\omega)=\begin{cases}
                1, &\omega\in A\\
                0, &\omega\in A^{\complement}
            \end{cases} & \prob(A)&=\prob(X=1)=p & \prob(A^{\complement})&=\prob(X=0)=1-p
        \end{align*}
    \end{eg}
    \begin{eg}(\textbf{Binomial distribution}) $Y\sim\Bin(n,p)$\\
        Suppose we perform $n$ independent Bernoulli trials $X_{1},X_{2},\cdots,X_{n}$. Let $Y=X_{1}+X_{2}+\cdots+X_{n}$ be total number of successes.
        \begin{equation*}
            f_{Y}(k)=\prob(Y=k)=\prob\left(\sum_{i=1}^{k}X_{i}=k\right)=\prob(\{\#\{i:X_{i}=1\}=k\})
        \end{equation*}
        We denote $A=\{\#\{i:X_{i}=1\}=k\}=\bigcup_{\sigma}A_{\sigma}$ where $\sigma=(\sigma_{1},\sigma_{2},\cdots,\sigma_{n})$ can be any sequence satisfying $\#\{i:\sigma_{i}=1\}=k$ and $A_{\sigma}:=$ events that $(X_{1},X_{2},\cdots,X_{n})=(\sigma_{1},\sigma_{2},\cdots,\sigma_{n})$. Events $A_{\sigma}$ are mutually exclusive. Hence $\prob(A)=\sum_{\sigma}\prob(A_{\sigma})$.\\
        There are totally $\binom{n}{k}$ different $\sigma$'s in the sum. By independence, we have
        \begin{equation*}
            \prob(A_{\sigma})=\prob(X_{1}=\sigma_{1},X_{2}=\sigma_{2},\cdots,X_{n}=\sigma_{n})=\prob(X_{1}=\sigma_{1})\prob(X_{2}=\sigma_{2})\cdots\prob(X_{n}=\sigma_{n})=p^{k}(1-p)^{n-k}
        \end{equation*}
        Hence, $f_{Y}(k)=\prob(A)=\binom{n}{k}p^{k}(1-p)^{n-k}$.
    \end{eg}
    \begin{eg}(\textbf{Trinomial distribution})
        Suppose we perform $n$ trials, each of which result in three outcomes $A$, $B$ and $C$, where $A$ occurs with probability $p$, $B$ with probability $q$, and $C$ with probability $1-p-q$. Probability of $r$ $A$'s, $w$ $B$'s, and $n-r-w$ $C$'s is
        \begin{equation*}
            \prob(\#A=r, \#B=w, \#C=n-r-w)=\frac{n!}{r!w!(n-r-w)!}p^{r}q^{w}(1-p-q)^{n-r-w}
        \end{equation*}
    \end{eg}
    \begin{eg}(\textbf{Geometric distribution}) $W\sim\Geom(p)$\\
        Suppose we keep performing independent Bernoulli trials until the first success shows up. Let $p$ be the probability of success and $W$ be the \textbf{waiting time} which elapses before first success.
        \begin{align*}
            \prob(W>k)&=(1-p)^{k} & \prob(W=k)&=\prob(W>k-1)-\prob(W>k)=p(1-p)^{k-1}
        \end{align*}
    \end{eg}
    \begin{eg}(\textbf{Negative binomial distribution}) $W_{r}\sim\NBin(r,p)$\\
        Similar with examples of geometric distribution, let $W_{r}$ be the waiting time for the $r$-th success. For $k\geq r$,
        \begin{equation*}
            f_{W_{r}}(k)=\prob(W_{r}=k)=\binom{k-1}{r-1}p^{r}(1-p)^{k-r}
        \end{equation*}
    \end{eg}
    \begin{rem}
        $W_{r}$ is the sum of $r$ independent geometric variables. 
    \end{rem}
    \begin{eg}(\textbf{Poisson distribution}) $X\sim\Poisson(\lambda)$\\
        \textbf{Poisson variable} is a discrete random variable with Poisson PMF:
        \begin{align*}
            f_{X}(k)&=\frac{\lambda^{k}}{k!}e^{-\lambda} & k&=0,1,2,\cdots
        \end{align*}
        for some parameter $\lambda>0$.\\
        This is used for approximation of binomial random variable $\Bin(n,p)$ when $n$ is large, $p$ is small and $np$ is moderate.\\
        Let $X\sim\Bin(n,p)$ and $\lambda=np$.
        \begin{align*}
            \prob(X=k)&=\binom{n}{k}p^{k}(1-p)^{n-k}=\frac{n!}{(n-k)!k!}\left(\frac{\lambda}{n}\right)^{k}\left(1-\frac{\lambda}{n}\right)^{n-k}=\frac{\lambda^{k}}{k!}\left(\frac{n!}{n^{k}(n-k)!}\right)\frac{\left(1-\frac{\lambda}{n}\right)^{n}}{\left(1-\frac{\lambda}{n}\right)^{k}}\approx\frac{\lambda^{k}}{k!}(1)\left(\frac{e^{-\lambda}}{1}\right)=\frac{\lambda^{k}}{k!}e^{-\lambda}
        \end{align*}
    \end{eg}
    \newpage
    
    We have an interesting example concerning independence with Poisson distribution involved.
    \begin{eg}(\textbf{Poisson flips)}
        A coin is tossed once and head turns up with probability $p$.\\
        Let random variables $X$ and $Y$ be the numbers of heads and tails respectively. $X$ and $Y$ are not independent since
        \begin{align*}
            \prob(X=1,Y=1)&=0 & \prob(X=1)\prob(Y=1)&=p(1-p)\neq 0
        \end{align*}
        Suppose now that the coin is tosses $N$ times, where $N$ has the Poisson distribution with parameter $\lambda$.\\
        In this case, random variables $X$ and $Y$ are independent since
        \begin{align*}
            \prob(X=x,Y=y)&=\prob(X=x,Y=y|N=x+y)\prob(N=x+y)\\
            &=\binom{x+y}{x}p^{x}(1-p)^{y}\frac{\lambda^{x+y}}{(x+y)!}e^{-\lambda}\\
            &=\frac{(\lambda p)^{x}(\lambda(1-p))^{y}}{x!y!}e^{-\lambda}\\
            \prob(X=x)\prob(Y=y)&=\sum_{i\geq x}\prob(X=x|N=i)\prob(N=i)\sum_{j\geq y}\prob(Y=y|N=j)\prob(N=j)\\
            &=\sum_{i\geq x}\binom{i}{x}p^{x}(1-p)^{i-x}\frac{\lambda^{i}}{i!}e^{-\lambda}\sum_{j\geq y}\binom{j}{y}p^{j-y}(1-p)^{y}\frac{\lambda^{j}}{j!}e^{-\lambda}\\
            &=\frac{(\lambda p)^{x}}{x!}e^{-\lambda}\left(\sum_{i\geq x}\frac{(\lambda(1-p))^{i-x}}{(i-x)!}\right)\frac{(\lambda(1-p))^{y}}{y!}e^{-\lambda}\left(\sum_{j\geq y}\frac{(\lambda p)^{j-y}}{(j-y)!}\right)\\
            &=\frac{(\lambda p)^{x}}{x!}e^{-\lambda+\lambda(1-p)}\frac{(\lambda(1-p))^{y}}{y!}e^{-\lambda+\lambda p}\\
            &=\frac{(\lambda p)^{x}(\lambda(1-p))^{y}}{x!y!}e^{-\lambda}=\prob(X=x,Y=y)
        \end{align*}
    \end{eg}
    
\section{Expectation of discrete random variables}
    In real-world scenarios, we often want to determine the expected outcome based on calculated probabilities.\\
    The expected result is typically a theoretical approximation of the empirical average.\\
    Assume we have random variables $X_{1},X_{2},\cdots,X_{N}$ that take values in $\{x_{1},x_{2},\cdots,x_{n}\}$ with a probability mass function $f_{X}(x)$.\\
    The empirical average is given by:
    \begin{equation*}
        \mu=\frac{1}{N}\sum_{i=1}^{N}X_{i}\approx\frac{1}{N}\sum_{i=1}^{n}x_{i}Nf(x_{i})=\sum_{i=1}^{N}x_{i}f(x_{i}).
    \end{equation*}
    \begin{defn}
        Suppose we have a discrete random variable $X$ taking values from $\{x_{1},x_{2},\cdots\}$ with PMF $f_{X}(x)$. The \textbf{mean value}, \textbf{expectation}, or \textbf{expected value} of $X$ is defined as:
        \begin{equation*}
            \expect X=\expect(X):=\sum_{i}x_{i}f_{X}(x_{i})=\sum_{x:f_{X}(x)>0}xf_{X}(x),
        \end{equation*}
        whenever this sum is absolutely convergent. Otherwise, we say $\expect X$ does not exist.
    \end{defn}
    \begin{eg}
        Suppose a product is sold seasonally. Let $b$ represent the net profit per sold unit, $\ell$ the net loss per unsold unit, and $X$ the number of products ordered by customers. If $y$ units are stocked, the expected profit $Q(y)$ is given by:
        \begin{equation*}
            Q(y)=\begin{cases}
                bX-(y-X)\ell, &X\leq y,\\
                yb, &X>y.
            \end{cases}
        \end{equation*}
    \end{eg}
    \newpage
    
    \begin{lem}
        \label{Mass function expectation}
        If discrete random variable $X$ has a PMF $f_{X}$ and $g:\mathbb{R}\to\mathbb{R}$ such that $g(X)$ is still a discrete random variable, then
        \begin{equation*}
            \expect(g(X))=\sum_{x}g(x)f_{X}(x)
        \end{equation*}
        whenever this sum is absolutely convergent.
    \end{lem}
    \begin{proofing}
        Denote by $Y:=g(X)$.
        \begin{align*}
            \sum_{x}g(x)f_{X}(x)=\sum_{y}\sum_{x:g(x)=y}g(x)f_{X}(x)=\sum_{y}y\left(\sum_{x:g(x)=y}f_{X}(x)\right)&=\sum_{y}y\left(\sum_{x:g(x)=y}\{\omega\in\Omega:X(\omega)=x\}\right)\\
            &=\sum_{y}y\prob(\{\omega\in\Omega:g(X(\omega))=y\})\\
            &=\sum_{y}y\prob(\{\omega\in\Omega:Y(\omega)=y\})\\
            &=\sum_{y}yf_{Y}(y)=\expect Y=\expect g(X)
        \end{align*}
    \end{proofing}
    \begin{lem}
        Let $(X,Y)$ be a discrete random vector with JPMF $f_{X,Y}(x,y)$. Let $g:\mathbb{R}^{2}\to\mathbb{R}$ such that $g(X,Y)$ is a discrete random variable. Then
        \begin{equation*}
            \expect g(X,Y)=\sum_{x,y}g(x,y)f_{X,Y}(x,y)
        \end{equation*}
    \end{lem}
    \begin{proofing}
        Denote by $Z:=g(X,Y)$.
        \begin{align*}
            \sum_{x,y}g(x,y)f_{X,Y}(x,y)=\sum_{z}\sum_{x,y:g(x,y)=z}g(x,y)f_{X,Y}(x,y)&=\sum_{z}z\left(\sum_{x,y:g(x,y)=z}f_{X,Y}(x,y)\right)\\
            &=\sum_{z}z\left(\sum_{x,y:g(x,y)=z}\prob((X,Y)=(x,y))\right)\\
            &=\sum_{z}z\prob(\{\omega\in\Omega:g(X,Y)(\omega)=z\})\\
            &=\sum_{z}z\prob(\{\omega\in\Omega:Z(\omega)=z\})=\sum_{z}zf_{Z}(z)=\expect Z=\expect g(X,Y)
        \end{align*}
    \end{proofing}
    The lemmas have provided a method to calculate the moments of a discrete distribution. Most of the time, we only care about the expectation and variance.
    \begin{defn}
        Let $k\in\mathbb{N}_{+}$. We have a special term for each of the following expectations:
        \begin{enumerate}
        	\item The \textbf{$k$-th moment} $m_{k}$ of $X$ is defined to be $m_{k}=\expect(X^{k})$.
        	\item The \textbf{$k$-th central moment} $\alpha_{k}$ is $\alpha_{k}=\expect((X-\expect X)^{k})=\expect((X-m_{1})^{k})$.
        	\item \textbf{Mean} of $X$ is the $1$st moment $m_{1}=\expect(X)$ and is denoted by $\mu$.
        	\item \textbf{Variance} of $X$ is the $2$nd central moment $\alpha_{2}=\Var(X)=\expect((X-m_{1})^{2})=\expect(X^{2})-(\expect X)^{2}=\expect(X^{2})-\mu^{2}$.
        	\item \textbf{Standard deviation} of $X$ is defined as $\sqrt{\Var(X)}$ and is denoted by $\sigma$.
        \end{enumerate}
    \end{defn}
    \begin{rem}
        Not all random variables have $k$-th moments for all $k\in\mathbb{N}_{+}$.
    \end{rem}
    \begin{rem}
        We cannot use collection of moments to uniquely determine a distribution that has $k$-th moments for all $k\in\mathbb{N}$.
    \end{rem}
    \newpage
    
    We have the expectation and the variance of following distribution.
    \begin{eg}
        \begin{align*}
            \text{Bernoulli}&: & \expect X&=p & \Var(X)&=p(1-p)\\
            \text{Binomial}&: & \expect X&=np & \Var(X)&=np(1-p)\\
            \text{Geometric}&: & \expect X&=p^{-1} & \Var(X)&=(1-p)p^{-2}\\
            \text{Poisson}&: & \expect X&=\lambda & \Var(X)&=\lambda
        \end{align*}
    \end{eg}
    \begin{thm}
        Expectation operator $\expect$ has the following properties:
        \begin{enumerate}
            \item If $X\geq 0$, then $\expect X\geq 0$.
            \item If $a,b\in\mathbb{R}$, then $\expect(aX+bY)=a\expect X+b\expect Y$.
            \item The random variable $\mathbf{1}$, taking the value $1$ always, has expectation $\expect(1)=1$.
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item Since $f_{X}(x)\geq 0$ for all $x$, $\expect X=\sum_{x}xf_{X}(x)\geq 0$ if $X\geq 0$.
            \item Let $g(X,Y)=aX+bY$. Then,
            \begin{align*}
                \expect(aX+bY)=\sum_{x,y}(ax+by)f_{X,Y}(x,y)&=a\sum_{x}x\left(\sum_{y}f_{X,Y}(x,y)\right)+b\sum_{y}y\left(\sum_{x}f_{X,Y}(x,y)\right)\\
                &=a\sum_{x}xf_{X}(x)+b\sum_{y}yf_{Y}(y)=a\expect X+b\expect Y
            \end{align*}
            \item $\expect(1)=1(1)=1$.
        \end{enumerate}
    \end{proofing}
    \begin{rem}
        More generally, we have
        \begin{equation*}
            \expect\left(\sum_{i=1}^{n}a_{i}X_{i}\right)=\sum_{i=1}^{n}a_{i}\expect X_{i}
        \end{equation*}
    \end{rem}
    \begin{eg}
        Assume we have $N$ different types of card and each time one gets a card to be any one of the $N$ types. Each types is equally likely to be gotten. What is the expected number of types of card we can get if we get $n$ cards?\\
        Let $X=X_{1}+X_{2}+\cdots+X_{N}$ where $X_{i}=1$ if at least one type $i$ card is among the $n$ cards and otherwise $0$.
        \begin{align*}
            \expect X_{i}&=\prob(X_{i}=1)=1-\left(\frac{N-1}{N}\right)^{n}\\
            \expect X&=\sum_{i=1}^{N}\expect X_{i}=N\left(1-\left(\frac{N-1}{N}\right)^{n}\right)
        \end{align*}
        What is the expected number of cards one needs to collect in order to get all $N$ types?\\
        Let $Y=Y_{0}+Y_{1}+\cdots+Y_{N-1}$ where $Y_{i}$ is the number of additional cards we need to get in order to get a new type after having $i$ distinct types.
        \begin{align*}
            \tag{$Y_{i}\sim\Geom\left(\frac{N-i}{N}\right)$}
            \prob(Y_{i}=k)&=\left(\frac{i}{N}\right)^{k-1}\frac{N-i}{N}\\
            \expect Y_{i}&=\frac{N}{N-i}\\
            \expect Y&=\sum_{i=0}^{N-1}\expect Y_{i}=N\left(\frac{1}{N}+\frac{1}{N-1}+\cdots+1\right)
        \end{align*}
    \end{eg}
    \begin{lem}
        If two discrete random variables $X$ and $Y$ are independent, then $\expect(XY)=\expect X\expect Y$.
    \end{lem}
    \begin{proofing}
        \begin{equation*}
            \expect(XY)=\sum_{x,y}xyf_{X,Y}(x,y)=\sum_{x,y}xyf_{X}(x)f_{Y}(y)=\sum_{x}xf_{X}(x)\sum_{y}yf_{Y}(y)=\expect X\expect Y
        \end{equation*}
    \end{proofing}
    \begin{lem}
        Given two discrete random variables $X$ and $Y$. Let $g,h:\mathbb{R}\to\mathbb{R}$ such that $g(X),h(Y)$ are still discrete random variables. If $X\independent Y$ and $\expect(g(X)h(Y)),\expect g(X)$ and $\expect h(Y)$ exist, then $\expect(g(X)h(Y))=\expect g(X)\expect h(Y)$.
    \end{lem}
    \begin{proofing}
        \begin{equation*}
            \expect(g(X)h(Y))=\sum_{x,y}g(x)h(y)f_{X,Y}(x,y)=\sum_{x,y}g(x)h(y)f_{X}(x)f_{Y}(y)=\sum_{x}g(x)f_{X}(x)\sum_{y}h(y)f_{Y}(y)=\expect g(X)\expect h(Y)
        \end{equation*}
    \end{proofing}
    We can now say that two independent random variables are uncorrelated when they are independent.
    \begin{defn}
        Random variables $X$ and $Y$ are \textbf{uncorrelated} if $\expect(XY)=\expect X\expect Y$.
    \end{defn}
    \begin{rem}
        If $X$ and $Y$ are independent, then they are uncorrelated. The converse is generally not true.
    \end{rem}
    \begin{eg}
        Let $X$ be such that $f_{X}(0)=f_{X}(1)=f_{X}(-1)=\frac{1}{3}$ and $Y$ be such that $Y=0$ if $X\neq0$ and $Y=1$ if $X=0$.
        \begin{align*}
            \expect(XY)&=0 &  \expect X&=0=\expect(XY)
        \end{align*}
        However,
        \begin{align*}
            \prob(X=0,Y=0)&=0 & \prob(X=0)&\neq 0 & \prob(Y=0)&\neq 0 & \prob(X=0)\prob(Y=0)&\neq 0
        \end{align*}
        Therefore, $X$ and $Y$ are uncorrelated, but they are not independent.
    \end{eg}
    We can now use the properties of expectations to deduce the properties of variance.
    \begin{thm}
        For random variables $X$ and $Y$,
        \begin{enumerate}
            \item $\Var(aX+b)=a^{2}\Var(X)$ for $a\in\mathbb{R}$.
            \item $\Var(X+Y)=\Var(X)+\Var(Y)$ if $X$ and $Y$ are uncorrelated.
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item Using linearity of $\expect$,
            \begin{equation*}
                \Var(aX+b)=\expect((aX+b-\expect(aX+b))^{2})=\expect(a^{2}(X-\expect X)^{2})=a^{2}\expect((X-\expect X)^{2})=a^{2}\Var(X)
            \end{equation*}
            \item When $X$ and $Y$ are uncorrelated,
            \begin{align*}
                \Var(X+Y)&=\expect((X+Y-\expect(X+Y))^{2})\\
                &=\expect((X-\expect X)^{2}+2(XY-\expect X\expect Y)+(Y-\expect Y)^{2})\\
                &=\Var(X)+2(\expect(XY)-\expect(X)\expect(Y))+\Var(Y)\\
                &=\Var(X)+\Var(Y)
            \end{align*}
        \end{enumerate}
    \end{proofing}
    \begin{defn}
        \textbf{Covariance} of two random variables $X$ and $Y$ is:
        \begin{equation*}
            \cov(X,Y)=\expect((X-\expect X)(Y-\expect Y))=\expect(XY)-\expect X\expect Y
        \end{equation*}
    \end{defn}
    \begin{rem}
        \begin{equation*}
            \Var(X)=\cov(X,X)
        \end{equation*}
    \end{rem}
    \begin{rem}
        In general, for any random variables $X_{1},X_{2},\cdots,X_{n}$,
        \begin{equation*}
            \Var(X_{1}+X_{2}+\cdots+X_{n})=\sum_{i=1}^{n}\Var(X_{i})+2\sum_{i<j}(\expect(X_{i}X_{j})-\expect X_{i}\expect X_{j})=\sum_{i=1}^{n}\Var(X_{i})+2\sum_{i<j}\cov(X_{i},X_{j})
        \end{equation*}
    \end{rem}
    \begin{rem}
        If $X_{i}$ are (pairwise) independent or uncorrelated, we can get that $\cov(X_{i},X_{j})=0$ for all $i\neq j$.
    \end{rem}
    \begin{eg}
        If $X_{i}$ are independent and $\Var(X_{i})=1$ for all $i$, then:
        \begin{equation*}
            \Var\left(\sum_{i=1}^{n}X_{i}\right)=\sum_{i=1}^{n}\Var(X_{i})=n
        \end{equation*}
        If $X_{i}=X$ for all $i$ and $\Var(X)=1$, then:
        \begin{equation*}
            \Var\left(\sum_{i=1}^{n}X_{i}\right)=\Var(nX)=n^{2}
        \end{equation*}
    \end{eg}
    
\section{Conditional distribution of discrete random variables}
    In the first chapter, we have discussed the conditional probability $\prob(B|A)$. We can use this to define a distribution function.
    \begin{defn}
        Suppose $X,Y:\Omega\to\mathbb{R}$ are two discrete random variables. \textbf{Conditional distribution} of $Y$ given $X=x$ for any $x$ such that $\prob(X=x)>0$ is defined by
        \begin{equation*}
            \prob(Y\in \cdot|X=x)
        \end{equation*}
        \textbf{Conditional distribution function} (Conditional CDF) of $Y$ given $X=x$ for any $x$ such that $\prob(X=x)>0$ is defined by
        \begin{equation*}
            F_{Y|X}(y|x)=\prob(Y\leq y|X=x)
        \end{equation*}
        \textbf{Conditional mass function} (Conditional PMF) of $Y$ given $X=x$ or any $x$ such that $\prob(X=x)>0$ is defined by
        \begin{equation*}
            f_{Y|X}(y|x)=\prob(Y=y|X=x)
        \end{equation*}
    \end{defn}
    \begin{rem}
        By definition, 
        \begin{equation*}
            f_{Y|X}(y|x)=\frac{\prob(Y=y,X=x)}{\prob(X=x)}=\frac{\prob(Y=y,X=x)}{\sum_{v}\prob((X,Y)=(x,v))}
        \end{equation*}.
    \end{rem}
    \begin{rem}
        For any $x\in\mathbb{R}$, the conditional PMF $f_{Y|X}(y|x)$ is a probability mass function in $y$.
    \end{rem}
    \begin{rem}
        If $X$ and $Y$ are independent, then $f_{Y|X}(y|x)=f_{Y}(y)$.
    \end{rem}
    Conditional distributions still have properties of original distribution.
    \begin{lem}
        Given two discrete random variables $X$ and $Y$. Conditional distributions have following properties:
        \begin{enumerate}
            \item $F_{Y|X}(y|x)=\sum_{v\leq y}f_{Y|X}(v|x)$
            \item $f_{Y|X}(y|x)=F_{Y|X}(y|x)-F_{Y|X}(y^{-}|x)$
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item 
            \begin{equation*}
                \sum_{v\leq y}f_{Y|X}(v|x)=\sum_{v\leq y}\prob(Y=v|X=x)=\prob(Y\leq y|X=x)=F_{Y|X}(y|x)
            \end{equation*}
            \item 
            This is just Lemma \ref{Relationship between pmf and cdf}.
        \end{enumerate}
    \end{proofing}

    \newpage
    \begin{defn}
        Given two discrete random variables $X$ and $Y$. \textbf{Conditional expectation} $\psi$ of $Y$ given $X=x$ for any $x$ is defined by:
        \begin{equation*}
            \psi(x)=\expect(Y|X=x)=\sum_{y}yf_{Y|X}(y|x)
        \end{equation*}
        \textbf{Conditional expectation} $\psi$ of $Y$ given $X$ is defined by:
        \begin{equation*}
            \psi(X)=\expect(Y|X)
        \end{equation*}
    \end{defn}
    \begin{eg}
        Assume we roll a fair dice.
        \begin{align*}
            \Omega&=\{1,2,\cdots,6\} & Y(\omega)&=\omega & X(\omega)&=\begin{cases}
                1, &\omega\in\{2,4,6\}\\
                0, &\omega\in\{1,3,5\}
            \end{cases}
        \end{align*}
        We try to guess $Y$. If we do not have any information about $X$, 
        \begin{equation*}
            \expect Y=\argmin_{e}(\expect((Y-e)^{2}))=3.5
        \end{equation*}
        If we know that $X=x$, in which we have two cases: $X=1$ and $X=0$
        \begin{align*}
            f_{Y|X}(y|1)&=\frac{\prob(X=1,Y=y)}{\prob(X=1)}=\begin{cases}
                \frac{1}{3}, &y=2,4,6\\
                0, &y=1,3,5
            \end{cases} & f_{Y|X}(y|0)&=\frac{\prob(X=0,Y=y)}{\prob(X=0)}=\begin{cases}
                0, &y=2,4,6\\
                \frac{1}{3}, &y=1,3,5
            \end{cases}\\
            \expect(Y|X=1)&=\sum_{y}yf_{Y|X}(y|1)=\frac{2+4+6}{3}=4 & \expect(Y|X=0)&=\frac{1+3+5}{3}=3
        \end{align*}
        Finally, if we want to guess $Y$ based on the future information of $X$,
        \begin{equation*}
            \psi(X)=\expect(Y|X)=4(\mathbf{1}_{X=1})+3(\mathbf{1}_{X=0})
        \end{equation*}
    \end{eg}
    \begin{eg}
        If $Y=X$, then $\psi(X)=\expect(Y|X)=\expect(X|X)=X$.
    \end{eg}
    \begin{eg}
        If $Y\independent X$, then $\psi(X)=\expect Y$.
    \end{eg}
    In fact, we can extend the definition of conditional expectation into $\sigma$-field.
    \begin{defn}
        Given a random variable $Y$ and a $\sigma$-field $\mathcal{H}\subseteq\mathcal{F}$.\\
        $\expect(Y|\mathcal{H})$ is any random variable $Z$ satisfying the following two properties:
        \begin{enumerate}
            \item $Z$ is $\mathcal{H}$-measurable. ($Z^{-1}(B)\in\mathcal{H}$ for all $B\in\mathcal{B}(\mathbb{R})$)
            \item $\expect(Y\mathbf{1}_{A})=\expect(Z\mathbf{1}_{A})$ for all $A\in\mathcal{H}$.
        \end{enumerate}
    \end{defn}
    \begin{rem}
        Under this definition,
        \begin{equation*}
            \expect(Y|X)=\expect(Y|\sigma(X))
        \end{equation*}
    \end{rem}
    \begin{thm}(Law of total expectation)
    Given two discrete random variables $X$ and $Y$. Conditional expectation $\psi(X)=\expect(Y|X)$ satisfies:
        \begin{equation*}
            \expect(\psi(X))=\expect(Y)
        \end{equation*}
    \end{thm}
    \begin{proofing}
        By Lemma \ref{Mass function expectation},
        \begin{equation*}
            \expect(\psi(X))=\sum_{x}\psi(x)f_{X}(x)=\sum_{x,y}yf_{Y|X}(y|x)f_{X}(x)=\sum_{x,y}yf_{X,Y}(x,y)=\sum_{y}yf_{Y}(y)=\expect(Y)
        \end{equation*}
    \end{proofing}

    \newpage
    \begin{eg}
        A miner is trapped in a mine with doors, each will lead to a tunnel. Tunnel 1 will help the miner reach safety after 3 hours respectively. However, tunnel 2 and 3 will send the miner back after 5 and 7 hours respectively.\\
        What is the expected amount of time the miner need to reach safety? (Assume that the miner is memoryless)\\
        Let $X$ be the amount of time to reach safety, $Y$ be the door number he chooses for the first time.
        \begin{align*}
            \expect X&=\expect(\expect(X|Y))=\sum_{k=1}^{3}\expect(X|Y=k)\prob(Y=k)=3\left(\frac{1}{3}\right)+(\expect X+5)\left(\frac{1}{3}\right)+(\expect X+7)\left(\frac{1}{3}\right)\\
            \expect X&=15
        \end{align*}
        What is the expected amount of time the miner needed to reach safety after he chose the second door and sent back?\\
        Let $\widetilde{X}$ be the time for the miner to reach safety after the first round. 
        \begin{align*}
            \expect(X|Y=2)&=\sum_{x}xf_{X|Y}(x|2)=\sum_{x}x\frac{\prob(X=x, Y=2)}{\prob(Y=2)}=\sum_{x}x\frac{\prob(\widetilde{X}=x-5,Y=2)}{\prob(Y=2)}=\sum_{\widetilde{x}}(\widetilde{x}+5)\prob(\widetilde{X}=\widetilde{x})=\expect X+5
        \end{align*}
    \end{eg}
    \begin{eg}
        We consider a sum of random number of random variables.\\
        Let $N$ be the number of customers and $X_{i}$ be the amount of money spent by the $i$-th customers.\\
        Assume that $N$ and $X_{i}$'s are all independent and $\expect X_{i}=\expect X$, what is the expected total amount of money spent by all $N$ customers?
        \begin{align*}
            \expect\left(\sum_{i=1}^{N}X_{i}\right)&=\expect\left(\expect\left(\left.\sum_{i=1}^{N}X_{i}\right| N\right)\right)\\
            &=\sum_{n=0}^{\infty}\expect\left(\left.\sum_{i=1}^{N}X_{i}\right|N=n\right)\prob(N=n)\\
            &=\sum_{n=0}^{\infty}\sum_{y}y\left(\frac{\prob\left(\sum_{i=1}^{N}X_{i}=y,N=n\right)}{\prob(N=n)}\right)\prob(N=n)\\
            &=\sum_{n=0}^{\infty}\sum_{y}y\prob\left(\sum_{i=1}^{n}X_{i}=y\right)\prob(N=n)\\
            &=\sum_{n=0}^{\infty}\expect\left(\sum_{i=1}^{n}X_{i}\right)\prob(N=n)\\
            &=\sum_{n=0}^{\infty}n\expect X\prob(N=n)=\expect N\expect X
        \end{align*}
    \end{eg}
    The following theorem is the generalization of Law of total expectation.
    \begin{thm}
        Given two discrete random variables $X$ and $Y$. Conditional expectation $\psi(X)=\expect(Y|X)$ satisfies:
        \begin{equation*}
            \expect(\psi(X)g(X))=\expect(Yg(X))
        \end{equation*}
        for any function $g$ for which both expectations exist.
    \end{thm}
    \begin{proofing}
        By Lemma \ref{Mass function expectation},
        \begin{equation*}
            \expect(\psi(X)g(X))=\sum_{x}\psi(x)g(x)f_{X}(x)=\sum_{x,y}yf_{Y|X}(y|x)g(x)f_{X}(x)=\sum_{x,y}yf_{X,Y}(x,y)g(x)=\expect(Yg(X))
        \end{equation*}
    \end{proofing}
    \newpage

\section{Convolution of discrete random variables}
    Finally, a lot of times, we consider the sum of the two variables. For example, the number of heads in $n$ tosses of a coin. However, there are situations that are more complicated, especially when the summands are dependent. We tries to find a formula for describing the mass function of the sum $Z=X+Y$.
    \begin{thm}
    Given two jointly discrete random variables $X$ and $Y$. The probability of sum of two random variables is given by:
        \begin{equation*}
            \prob(X+Y=z)=\sum_{x}f_{X,Y}(x,z-x)=\sum_{y}f_{X,Y}(z-y,y)
        \end{equation*}
    \end{thm}
    \begin{proofing}
        We have the disjoint union:
        \begin{equation*}
            \{X+Y=z\}=\bigcup_{x}(\{X=x\}\cap\{Y=z-x\})
        \end{equation*}
        At most countably many of its contributions have non-zero probability. Therefore,
        \begin{equation*}
            \prob(X+Y=z)=\sum_{x}\prob(X=x,Y=z-x)=\sum_{x}f(x,z-x)
        \end{equation*}
    \end{proofing}
    \begin{defn}
        \textbf{Convolution} $f_{X+Y}$ ($f_{X}*f_{Y}$) of PMFs of two independent discrete random variables $X$ and $Y$ is the PMF of $X+Y$:
        \begin{equation*}
            f_{X+Y}(z)=\prob(X+Y=z)=\sum_{x}f_{X}(x)f_{Y}(z-x)=\sum_{y}f_{X}(z-y)f_{Y}(y)
        \end{equation*}
    \end{defn}
    There is an important example that has a wide range of applications in real life. However, we will not discuss this here. You can find the example in Appendix \ref{Simple random walk}.
    
\chapter{Continuous random variables}
\section{Introduction to Continuous Random Variables}
	We begin by recalling the definition of continuous random variables.
	\begin{defn}
		A random variable $X$ is \textbf{continuous} if its \textbf{cumulative distribution function} (CDF) $F_{X}(x)$ can be expressed as:
		\begin{equation*}
			F_{X}(x)=\prob(X\leq x)=\intlu{-\infty}{x} f(u)\,du
		\end{equation*}
	for some integrable probability density function (PDF) $f_{X}:\mathbb{R}\to[0,\infty)$.
	\end{defn}
	\begin{rem}
		The PDF $f_{X}$ is not uniquely defined, as two integrable functions that differ only on a set of measure zero yield the same integral. However, if $F_{X}$ is \textbf{differentiable} at $u$, we define $f_{X}(u)=F_{X}'(u)$.
	\end{rem}
	Note that we use the same notation $f$ for both mass functions and density functions, as they serve analogous purposes.
	\begin{rem}
		The value $f_{X}(x)$ is not a probability. However, $f_{X}(x)\,dx=\prob(x<X\leq x+dx)$ can be interpreted as an infinitesimal probability element.
	\end{rem}
	\begin{lem}
		\label{properties of density function}
		If a continuous random variable $X$ has a density function $f_{X}$, then:
		\begin{enumerate}
			\item $\intinfty f_{X}(x)\,dx=1$.
			\item $\prob(X=x)=0$ for all $x\in\mathbb{R}$.
			\item $\prob(a\leq X\leq b)=\intlu{a}{b}f_{X}(x)\,dx$.
		\end{enumerate}
	\end{lem}
	\begin{proofing}
		\begin{enumerate}
			\item 
			\begin{equation*}
				\intinfty f_{X}(x)\,dx=\lim_{x\to\infty}F_{X}(x)=1.
			\end{equation*}
			\item
			\begin{equation*}
				\prob(X=x)=\lim_{h\to 0}\intlu{x-h}{x}f_{X}(x)\,dx=F_{X}(x)-\lim_{h\to\infty}F(x-h)=F_{X}(x)-F_{X}(x)=0.
			\end{equation*}
			\item
			\begin{equation*}
				\prob(a\leq X\leq b)=F(b)-F(a)=\intlu{-\infty}{b}f_{X}(x)\,dx-\intlu{-\infty}{a} f_{X}(x)\,dx=\intlu{a}{b}f_{X}(x)\,dx.
			\end{equation*}
		\end{enumerate}
	\end{proofing}
	\begin{rem}
		More generally, for an interval $B$, we have:
		\begin{equation*}
			\prob(X\in B)=\int_{B}f_{X}(x)\,dx.
		\end{equation*}
	\end{rem}

    \newpage
    We also recall the definition of independence. This definition also works for continuous random variables.
    \begin{defn}
        Two continuous random variables $X$ and $Y$ are called \textbf{independent} if for all $x,y\in\mathbb{R}$,
        \begin{equation*}
            F_{X,Y}(x,y)=F_{X}(x)F_{Y}(y)
        \end{equation*}
    \end{defn}
    \begin{thm}
        Let two continuous random variables $X$ and $Y$ be independent. Suppose $g(X)$ and $h(Y)$ are still continuous random variables, then $g(X)$ and $h(Y)$ are independent.
    \end{thm}

\section{Expectation of continuous random variables}
    In a continuous random variable $X$, the probability in every single point $x$ is $0$. Therefore, in order to make sense of the expectation of continuous random variable, we naturally give the following definition.
    \begin{defn}
        \textbf{Expectation} of a continuous random variable $X$ with density function $f$ is given by:
        \begin{equation*}
            \expect X=\intinfty xf_{X}(x)\,dx
        \end{equation*}
        whenever this integral exists.
    \end{defn}
    \begin{rem}
        We usually can define $\expect X$ only if $\expect|X|$ exists.
    \end{rem}
    We have a special properties in the continuous random variable.
    \begin{lem}(Tail sum formula)
        \label{expectation as integral of additive inverse of cdf}
        If continuous random variable $X$ has a PDF $f_{X}$ with $f_{X}(x)=0$ when $x<0$, and a CDF $F_{X}$, then
        \begin{equation*}
            \expect X=\intlu{0}{\infty}(1-F_{X}(x))\,dx
        \end{equation*}
    \end{lem}
    \begin{proofing}
        \begin{equation*}
            \intlu{0}{\infty}(1-F_{X}(x))\,dx=\intlu{0}{\infty}\prob(X>x)\,dx=\intlu{0}{\infty}\intlu{x}{\infty}f_{X}(y)\,dy\,dx=\intlu{0}{\infty}\intlu{0}{y}f_{X}(y)\,dx\,dy=\intlu{0}{\infty}yf_{X}(y)\,dy=\expect X
        \end{equation*}
    \end{proofing}
    The following lemma is a formula I developed just for proving the next theorem.
    \begin{lem}
        \label{expectation as integral of cdf}
        If continuous random variable $X$ has a PDF $f_{X}$ with $f_{X}(x)=0$ when $x>0$, and a CDF $F_{X}$, then
        \begin{equation*}
            \expect X=\intlu{-\infty}{0}-F_{X}(x)\,dx
        \end{equation*}
    \end{lem}
    \begin{proofing}
        \begin{equation*}
            \intlu{-\infty}{0}-F_{X}(x)\,dx=\intlu{-\infty}{0}\intlu{-\infty}{x}-f_{X}(y)\,dy\,dx=\intlu{-\infty}{0}\intlu{y}{0}-f_{X}(y)\,dx\,dy=\intlu{-\infty}{0}yf_{X}(y)\,dy=\expect X
        \end{equation*}
    \end{proofing}
    Similar to discrete random variable, we can ask what is $\expect g(X)$ for a function $g$.
    \begin{thm}
        \label{expectation of function of continuous random variable}
        If $X$ and $g(X)$ are continuous random variables, then
        \begin{equation*}
            \expect(g(X))=\intinfty g(x)f_{X}(x)\,dx
        \end{equation*}
    \end{thm}

    \newpage
    \begin{proofing}
        We first consider that $g(x)\geq 0$ for all $x$. Let $Y=g(X)$ and $B=\{x:g(x)>y\}$. By Lemma \ref{expectation as integral of additive inverse of cdf},
        \begin{equation*}
            \expect(g(X))=\intlu{0}{\infty}\prob(g(X)>y)\,dy=\intlu{0}{\infty}\int_{B}f_{X}(x)\,dx\,dy=\intlu{0}{\infty}\intlu{0}{g(x)}f_{X}(x)\,dy\,dx=\intlu{0}{\infty}g(x)f_{X}(x)\,dx
        \end{equation*}
        We then consider that $g(x)\leq 0$ for all $x$. Let $Z=g(X)$ and $C=\{x:g(x)<z\}$. By Lemma \ref{expectation as integral of cdf},
        \begin{equation*}
            \expect(g(X))=\intlu{-\infty}{0}-F_{Z}(z)\,dz=\intlu{-\infty}{0}\int_{C}-f_{X}(x)\,dx\,dz=\intlu{-\infty}{0}\intlu{g(x)}{0}-f_{X}(x)\,dz\,dx=\intlu{-\infty}{0}g(x)f_{X}(x)\,dx
        \end{equation*}
        Now we combined both formulas into one. If $g(X)$ is a random variable,
        \begin{equation*}
            \expect(g(X))=\int_{0}^{\infty}g(x)f_{X}(x)\,dx+\intlu{-\infty}{0}g(x)f_{X}(x)\,dx=\intinfty g(x)f_{X}(x)\,dx
        \end{equation*}
    \end{proofing}
    Similar to discrete random variables, this theorem also provided a method to calculate the moments of a continuous distribution.
    \begin{defn}
        Given $k\in\mathbb{N}_{+}$ and a continuous random variable $X$. The \textbf{$k$-th moment} is defined to be
        \begin{equation*}
            \expect X^{k}=\intinfty x^{k}f_{X}(x)\,dx
        \end{equation*}
        The \textbf{$k$-th central moment} is defined to be
        \begin{equation*}
            \expect((X-\expect X)^{k})=\intinfty(x-\expect X)^{k}f_{X}(x)\,dx
        \end{equation*}
        \textbf{Variance} is defined as $\Var(X)=\expect(X^{2})-(\expect X)^{2}$.
    \end{defn}
    We have some important continuous distributions.
    \begin{eg}(\textbf{Uniform distribution}) $X\sim\U[a,b]$\\
        Random variable $X$ is \textbf{uniform} on $[a,b]$ if CDF and PDF of $X$ is
        \begin{align*}
            F_{X}(x)&=\begin{cases}
                0, &x\leq a\\
                \frac{x-a}{b-a}, &a<x\leq b\\
                1, &x>b
            \end{cases} & f_{X}(x)&=\begin{cases}
                \frac{1}{b-a}, &a<x\leq b\\
                0, &\text{Otherwise}
            \end{cases}
        \end{align*}
    \end{eg}
    \begin{eg} (\textbf{Inverse transform sampling})
        If we have an invertible CDF $G(x)$. How can we generate a random variable $Y$ with the given distribution function?\\
        We only need to generate an uniform random variable $U\sim\U[0,1]$. We claim that $Y=G^{-1}(U)$ has the distribution function $G(x)$.
        \begin{equation*}
            F_{Y}(x)=\prob(Y\leq x)=\prob(G^{-1}(U)\leq x)=\prob(U\leq G(x))=F_{U}(G(x))=G(x)
        \end{equation*}
    \end{eg}
    \begin{eg}(\textbf{Exponential distribution}) $X\sim\Exp(\lambda)$\\
        Random variable $X$ is \textbf{exponential} with parameter $\lambda>0$ if CDF and PDF of $X$ is
        \begin{align*}
            F_{X}(x)&=\begin{cases}
                1-e^{-\lambda x}, &x\geq 0\\
                0, &x<0
            \end{cases} & f_{X}(x)&=\begin{cases}
                \lambda e^{-\lambda x}, &x\geq 0\\
                0, &x<0
            \end{cases}
        \end{align*}
    \end{eg}

    \newpage
    \begin{eg}(\textbf{Normal distribution} / \textbf{Gaussian distribution}) $X\sim\N(\mu,\sigma^{2})$\\
        Random variable $X$ is \textbf{normal} if it has two parameters $\mu$ and $\sigma^{2}$, and its PDF and CDF is
        \begin{align*}
            f_{X}(x)&=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right) & F_{X}(x)&=\intlu{-\infty}{x}f_{X}(u)\,du
        \end{align*}
        This distribution is the most important distribution.\\
        The random variable $X$ is \textbf{standard normal} if $\mu=0$ and $\sigma^{2}=1$. ($X\sim\N(0,1)$)
        \begin{align*}
            f_{X}(x)&=\phi(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^{2}}{2}} & F_{X}(x)&=\Phi(x)=\intlu{-\infty}{x}\phi(u)\,du
        \end{align*}
    \end{eg}
    \begin{cla}
        $\phi(x)$ is a probability distribution function.
    \end{cla}
    \begin{proofing}
        Let $I=\intinfty\phi(x)\,dx$.
        \begin{equation*}
            I^{2}=\intinfty\phi(x)\,dx\intinfty\phi(y)\,dy=\frac{1}{2\pi}\intinfty\intinfty e^{-\frac{x^{2}+y^{2}}{2}}\,dx\,dy
        \end{equation*}
        Let $x=r\cos\theta$ and $y=r\sin\theta$ where $r\in[0,\infty)$ and $\theta\in[0,2\pi]$
        \begin{equation*}
            I^{2}=\frac{1}{2\pi}\intlu{0}{2\pi}\intlu{0}{\infty}e^{-\frac{r^{2}}{2}}r\,dr\,d\theta=\frac{1}{2\pi}\intlu{0}{2\pi}\intlu{0}{\infty}e^{-\frac{r^{2}}{2}}\,d\left(\frac{r^{2}}{2}\right)\,d\theta=\frac{1}{2\pi}\intlu{0}{2\pi}\,d\theta=1
        \end{equation*}
    \end{proofing}
    These are some properties that are used frequently.
    \begin{lem}
        The normal distribution has the following properties:
        \begin{enumerate}
            \item Let $X\sim\N(0,1)$. If a random variable $Y=bX+a$ for some $a,b\in\mathbb{R}$ and $b\neq 0$, then $Y\sim\N(a,b^{2})$.
            \item Let $X\sim\N(a,b^{2})$ for some $a,b\in\mathbb{R}$ and $b\neq 0$. If a random variable $Y=\frac{X-a}{b}$, then $Y\sim\N(0,1)$.
            \item If $Y\sim\N(a,b^{2})$, then $\expect Y=a$ and $\Var(Y)=b^{2}$.
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item Let $z=bx+a$.
            \begin{equation*}
                F_{Y}(y)=\prob(Y\leq y)=\prob\left(X\leq\frac{y-a}{b}\right)=\frac{1}{\sqrt{2\pi}}\intlu{-\infty}{\frac{y-a}{b}}e^{-\frac{x^{2}}{2}}\,dx=\frac{1}{\sqrt{2\pi b^{2}}}\intlu{-\infty}{y}e^{-\frac{(z-a)^{2}}{2b^{2}}}\,dz
            \end{equation*}
            Therefore, $Y\sim\N(a,b^{2})$.
            \item Let $x=bz+a$.
            \begin{equation*}
                F_{Y}(y)=\prob(Y\leq y)=\prob(X\leq by+a)=\frac{1}{\sqrt{2\pi b^{2}}}\intlu{-\infty}{by+a}e^{-\frac{(x-a)^{2}}{2b^{2}}}\,dx=\frac{1}{\sqrt{2\pi}}\intlu{-\infty}{y}e^{-\frac{z^{2}}{2}}\,dz
            \end{equation*}
            Therefore, $Y\sim\N(0,1)$.
            \item Let $y=bz+a$.
            \begin{equation*}
                \expect Y=\frac{1}{\sqrt{2\pi b^{2}}}\intinfty ye^{-\frac{(y-a)^{2}}{2b^{2}}}\,dy=\frac{1}{\sqrt{2\pi}}\left(\intinfty bze^{-\frac{z^{2}}{2}}\,dz+\intinfty ae^{-\frac{z^{2}}{2}}\,dz\right)=\frac{a}{\sqrt{2\pi}}\intinfty e^{-\frac{z^{2}}{2}}\,dz=a(1)=a
            \end{equation*}
            \begin{equation*}
                \Var(Y)=\frac{1}{\sqrt{2\pi b^{2}}}\intinfty(y-a)^{2}e^{-\frac{(y-a)^{2}}{2b^{2}}}\,dy=\frac{b^{2}}{\sqrt{2\pi}}\intinfty z^{2}e^{-\frac{z^{2}}{2}}\,dz=\frac{-b^{2}}{\sqrt{2\pi}}\intinfty zd\left(e^{-\frac{z^{2}}{2}}\right)=\frac{b^{2}}{\sqrt{2\pi}}\intinfty e^{-\frac{z^{2}}{2}}\,dz=b^{2}
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    \begin{lem}
        If $X\sim\N(a,b^{2})$, then:
        \begin{equation*}
            \prob(s\leq X\leq t)=\prob\left(\frac{s-a}{|b|}\leq\frac{X-a}{|b|}\leq\frac{t-a}{|b|}\right)=\Phi\left(\frac{t-a}{|b|}\right)-\Phi\left(\frac{s-a}{|b|}\right)
        \end{equation*}
    \end{lem}
    \begin{proofing}
        Just apply Lemma \ref{properties of density function} and you would get the equation.
    \end{proofing}
    \begin{eg}(\textbf{Cauchy distribution}) $X\sim\Cauchy$\\
        Random variable $X$ has a Cauchy distribution if it has a PDF:
        \begin{equation*}
            f_{X}(x)=\frac{1}{\pi(1+x^{2})}
        \end{equation*}
        It has the expectation
        \begin{equation*}
            \expect|X|=\intinfty\frac{|x|}{\pi(1+x^{2})}\,dx=2\intlu{0}{\infty}\frac{x}{\pi(1+x^{2})}\,dx=\infty
        \end{equation*}
    \end{eg}
    There are also plenty of other continuous distributions. For example, Gamma distribution, Beta distribution, Weibull distribution, etc. However, they are too complicated and we will not discuss them here.

\section{Joint distribution function of continuous random variables}
    Again, we recall the definition of joint distribution function.
    \begin{defn}
        \textbf{Joint distribution function} (JCDF) of two continuous  random variables $X$ and $Y$ is the function $F:\mathbb{R}^{2}\to[0,1]$ such that:
        \begin{equation*}
            F_{X,Y}(x,y)=\prob(X\leq x,Y\leq y)
        \end{equation*}
        Two continuous random variables $X$ and $Y$ are \textbf{jointly continuous} if the have a \textbf{joint density function} (JPDF) $f:\mathbb{R}^{2}\to[0,\infty)$ such that:
        \begin{align*}
            F_{X,Y}(x,y)&=\intlu{-\infty}{y}\intlu{-\infty}{x}f_{X,Y}(u,v)\,du\,dv & f_{X,Y}(x,y)&=\pdv*{F_{X,Y}(x,y)}{x,y} & \prob((X,Y)\in D)&=\iint_{D}f_{X,Y}(x,y)\,dx\,dy
        \end{align*}
    \end{defn}
    We also recall the definition of marginal distribution function.
    \begin{defn}
        Given two continuous random variables $X$ and $Y$. Marginal distribution function (Marginal PDF) of $X$ given $Y$ is
        \begin{align*}
            F_{X}(x)&=\prob(X\leq x)=\intinfty\intlu{-\infty}{x}f_{X,Y}(u,v)\,du\,dv=\intlu{-\infty}{x}\intinfty f_{X,Y}(u,v)\,dv\,du\\
            f_{X}(x)&=\intinfty f_{X,Y}(x,u)\,dv
        \end{align*}
    \end{defn}
    Similarly, we have the following extension of Theorem \ref{expectation of function of continuous random variable}. However, we are not going to prove it here.
    \begin{thm}
        If $X$ and $Y$ are jointly continuous random variables and $g(X,Y)$ is continuous random variable, then
        \begin{equation*}
            \expect(g(X,Y))=\intinfty\intinfty g(x,y)f_{X,Y}(x,y)\,dx\,dy
        \end{equation*}
    \end{thm}
    We can obtain the following important lemma.
    \begin{lem}
        If $X$ and $Y$ are jointly continuous random variables, then for any $a,b\in\mathbb{R}$,
        \begin{equation*}
            \expect(aX+bY)=a\expect X+b\expect Y
        \end{equation*}
    \end{lem}
    \begin{proofing}
        \begin{align*}
            \expect(aX+bY)&=\intinfty\intinfty(ax+by)f_{X,Y}(x,y)\,dx\,dy\\
            &=\intinfty axf_{X}(x)\,dx+\intinfty byf_{Y}(y)\,dy\\
            &=a\expect X+b\expect Y
        \end{align*}
    \end{proofing}
    
    \newpage
    \begin{eg}
        Assume that a plane is ruled by horizontal lines separated by $D$ and a needle of length $L\leq D$ is cast randomly on the plane. What is the probability that the needle intersects some lines?\\
        Let $X$ be the distance from center of the needle to the nearest line and $\Theta$ be the acute angle between the needle and vertical line.\\
        We have $\prob(\text{Intersection})=\prob\left(\frac{L}{2}\cos\Theta\geq X\right)$.\\
        Assume that $X\independent\Theta$. We have $X\sim\U\left[0,\frac{D}{2}\right]$ and $\Theta\sim\U\left[0,\frac{\pi}{2}\right]$.
        \begin{align*}
            f_{X,\Theta}(x,\theta)&=\begin{cases}
                \frac{4}{D\pi}, &0\leq x\leq\frac{D}{2},0\leq\theta\leq\frac{\pi}{2}\\
                0, &\text{Otherwise}
            \end{cases}\\
            \prob\left(\frac{L}{2}\cos\Theta\geq X\right)&=\iint_{\frac{L}{2}\cos\theta\geq x}\frac{4}{D\pi}\mathbf{1}_{0\leq x\leq\frac{D}{2}}\mathbf{1}_{0\leq\theta\leq\frac{\pi}{2}}\,dx\,d\theta=\intlu{0}{\frac{\pi}{2}}\intlu{0}{\frac{L}{2}\cos\theta}\frac{4}{D\pi}\,dx\,d\theta=\frac{2L}{D\pi}
        \end{align*}
        Suppose that we throw the needle for $n$ times.
        \begin{equation*}
            \frac{\#\{\text{Intersection}\}}{n}\approx\prob(\text{Intersection})=\frac{2L}{D\pi}
        \end{equation*}
    \end{eg}
    Combining two normal distributions into a joint distribution can be really useful.
    \begin{eg}(\textbf{Standard bivariate normal distribution}) 
        Two continuous random variables $X$ and $Y$ are \textbf{standard bivariate normal} if they have JPDF:
        \begin{equation*}
            f_{X,Y}(x,y)=\frac{1}{2\pi\sqrt{1-\rho^{2}}}\exp\left(-\frac{x^{2}-2\rho xy+y^{2}}{2(1-\rho^{2})}\right)
        \end{equation*}
        where $\rho$ is a constant satisfying $-1<\rho<1$.
    \end{eg}
    \begin{rem}
        If $X\sim\N(0,1)$ and $Y\sim\N(0,1)$,
        \begin{align*}
            f_{Y}(y)&=\intinfty f_{X,Y}(x,y)\,dx\\
            &=\frac{1}{2\pi\sqrt{1-\rho^{2}}}\intinfty\exp\left(-\frac{(x-\rho y)^{2}+(1-\rho^{2})y}{2(1-\rho^{2})}\right)\,dx\\
            &=\frac{1}{\sqrt{2\pi}}e^{-\frac{y^{2}}{2}}\intinfty\frac{1}{\sqrt{2\pi(1-\rho^{2})}}e^{-\frac{(x-\rho y)^{2}}{2(1-\rho^{2})}}\,dx\\
            &=\frac{1}{\sqrt{2\pi}}e^{-\frac{y^{2}}{2}}
        \end{align*}
    \end{rem}
    \begin{rem}
        $\rho$ is the \textbf{correlation coefficient} between $X$ and $Y$ and is given by
        \begin{equation*}
            \rho=\frac{\cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}
        \end{equation*}
    \end{rem}
    \begin{rem}
        If $X\sim\N(0,1)$ and $Y\sim\N(0,1)$,
        \begin{align*}
            \cov(X,Y)&=\expect(XY)-\expect X\expect Y=\expect(XY)\\
            &=\intinfty\intinfty\frac{y}{\sqrt{2\pi}}e^{-\frac{y^{2}}{2}}\frac{x}{\sqrt{2\pi(1-\rho^{2})}}e^{-\frac{(x-\rho y)^{2}}{2(1-\rho^{2})}}\,dx\,dy\\
            &=\intinfty\frac{y}{\sqrt{2\pi}}e^{-\frac{y^{2}}{2}}\rho y\,dy=\rho\intinfty y^{2}\phi(y)\,dy=\rho
        \end{align*}
    \end{rem}
    \begin{eg}(\textbf{Bivariate normal distribution})
        Two continuous random variables $X$ and $Y$ are \textbf{bivariate normal} with means $\mu_{X}$ and $\mu_{Y}$, variance $\sigma_{X}^{2}$ and $\sigma_{Y}^{2}$, and correlation coefficient $\rho$ if JPDF is given by
        \begin{equation*}
            f_{X,Y}(x,y)=\frac{1}{2\pi\sigma_{X}\sigma_{Y}\sqrt{1-\rho^{2}}}\exp\left(-\frac{1}{2(1-\rho^{2})}\left(\left(\frac{x-\mu_{X}}{\sigma_{X}}\right)^{2}-2\rho\left(\frac{x-\mu_{X}}{\sigma_{X}}\right)\left(\frac{y-\mu_{Y}}{\sigma_{Y}}\right)+\left(\frac{y-\mu_{Y}}{\sigma_{Y}}\right)^{2}\right)\right)
        \end{equation*}
    \end{eg}
    There are some remarks that may be important to know about.
    \begin{rem}
        $X$ and $Y$ are bivariate normal and uncorrelated $\iff$ $X$ and $Y$ are independent normal.
    \end{rem}
    \begin{rem}
        $X$ and $Y$ are jointly continuous and they are both normal does not mean they are bivariate normal.
    \end{rem}
    \begin{eg}
        Consider a JPDF of random variables $X$ and $Y$
        \begin{equation*}
            f_{X,Y}(x,y)=\begin{cases}
                \frac{1}{\pi}e^{-\frac{1}{2}(x^{2}+y^{2})}, &xy>0\\
                0, &xy\leq 0
            \end{cases}
        \end{equation*}
        As you can see, this is not a bivariant normal distribution.\\
        However, if you look at their marginal PDF,
        \begin{align*}
            f_{X}(x)&=\intlu{0}{\infty}\frac{1}{\pi}e^{-\frac{1}{2}(x^{2}+y^{2})}\,dy=\frac{1}{2\pi}\intinfty e^{-\frac{1}{2}(x^{2}+y^{2})}\,dy=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^{2}} & x&>0\\
            f_{X}(x)&=\intlu{-\infty}{0}\frac{1}{\pi}e^{-\frac{1}{2}(x^{2}+y^{2})}\,dy=\frac{1}{2\pi}\intinfty e^{-\frac{1}{2}(x^{2}+y^{2})}\,dy=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^{2}} & x&<0
        \end{align*}
        This is the same to $f_{Y}(x)$.\\
        Therefore, $X$ and $Y$ are jointly continuous and they are both normal does not mean they are bivariant normal.
    \end{eg}
    \begin{rem}
        Two random variables $X$ and $Y$ are jointly continuous and uncorrelated Gaussian does not mean they are independent Gaussian.
    \end{rem}
    
\section{Conditional distribution of continuous random variables}
    Recall the definition of conditional distribution function of discrete random variable $Y$ given $X=x$.
    \begin{equation*}
        F_{Y|X}(y|x)=\prob(Y\leq y|X=x)=\frac{\prob(Y\leq y,X=x)}{\prob(X=x)}
    \end{equation*}
    However, for the continuous random variables, $\prob(X=x)=0$ for all $x$. We take a limiting point of view.\\
    Suppose the probability distribution function $f_{X}(x)>0$,
    \begin{align*}
        F_{Y|X}(y|x)=\prob(Y\leq y|x\leq X\leq x+dx)&=\frac{\prob(Y\leq y,x\leq X\leq x+dx)}{\prob(x\leq X\leq x+dx)}\\
        &=\frac{\intlu{-\infty}{y}\intlu{x}{x+dx}f_{X,Y}(u,v)\,du\,dv}{\intlu{x}{x+dx}f_{X}(u)\,du}\\
        &\approx\frac{\intlu{-\infty}{y}f_{X,Y}(x,v)\,dx\,dv}{f_{X}(x)\,dx}\\
        &=\intlu{-\infty}{y}\frac{f_{X,Y}(x,v)}{f_{X}(x)}\,dv
    \end{align*}
    \begin{defn}
        Suppose $X,Y:\Omega\to\mathbb{R}$ are two continuous random variables with PDF $f_{X}(x)>0$ for some 
        $x\in\mathbb{R}$. \textbf{Conditional distribution function} (Conditional CDF) of $Y$ given $X=x$ is defined by
        \begin{equation*}
            F_{Y|X}(y|x)=\prob(Y\leq y|X=x)=\intlu{-\infty}{y}\frac{f_{X,Y}(x,v)}{f_{X}(x)}\,dv
        \end{equation*}
        \textbf{Conditional density function} (Conditional PDF) of $Y$ given $X=x$ is defined by
        \begin{equation*}
            f_{Y|X}(y|x)=\pdv*{F_{Y|X}(y|x)}{y}=\frac{f_{X,Y}(x,y)}{f_{X}(x)}
        \end{equation*}
    \end{defn}
    \begin{rem}
        Since $f_{X}(x)$ can also be computed from $f(x,y)$, we can simply compute
        \begin{equation*}
            f_{Y|X}(y|x)=\frac{f_{X,Y}(x,y)}{\intinfty f_{X,Y}(x,y)\,dy}
        \end{equation*}
    \end{rem}

    \newpage
    \begin{rem}
        More generally, for two continuous random variables $X$ and $Y$ with PDF  $f_{X}(x)>0$ for some $x\in\mathbb{R}$,
        \begin{align*}
            \prob(Y\in A|X=x)&=\int_{A}\frac{f_{X,Y}(x,v)}{f_{X}(x)}\,dv\\
            &=\int_{A}f_{Y|X}(y|x)\,dy
        \end{align*}
    \end{rem}
    \begin{eg}
        \label{j.p.d.f. to con.d.f. example}
        Assume that two jointly continuous random variables $X$ and $Y$ have a JPDF:
        \begin{equation*}
            f_{X,Y}(x,y)=\begin{cases}
                \frac{1}{x}, &0\leq y\leq x\leq 1\\
                0, &\text{Otherwise}
            \end{cases}=\frac{1}{x}\mathbf{1}_{0\leq y\leq x\leq 1}
        \end{equation*}
        We want to compute $f_{X}(x)$ and $f_{Y|X}(y|x)$.
        For $x\in [0,1]$,
        \begin{equation*}
            f_{X}(x)=\intinfty f_{X,Y}(x,y)\,dy=\intinfty\frac{1}{x}\mathbf{1}_{0\leq y\leq x\leq 1}\,dy=\intlu{0}{x}\frac{1}{x}\,dy=1
        \end{equation*}
        Therefore, $X\sim\U[0,1]$.\\
        For $0\leq y\leq x$ and $0\leq x\leq 1$,
        \begin{equation*}
            f_{Y|X}(y|x)=\frac{f_{X,Y}(x,y)}{f_{X}(x)}=\frac{1}{x}
        \end{equation*}
        Therefore, $(Y|X=x)\sim\U[0,x]$.
    \end{eg}
    \begin{eg}
        We want to find $\prob(X^{2}+Y^{2}\leq 1)$ with two jointly continuous random variables $X$ and $Y$ having JPDF in Example \ref{j.p.d.f. to con.d.f. example}. Let $Y\in A_{x}=\{y:|y|\leq\sqrt{1-x^{2}}\}$.
        \begin{align*}
            \prob(X^{2}+Y^{2}\leq 1|X=x)=\prob(|Y|\leq\sqrt{1-x^{2}}|X=x)&=\int_{A_{x}}f_{Y|X}(y|x)\,dy\\
            &=\int_{A_{x}\cap[0,1]}\frac{1}{x}\,dy\\
            &=\intlu{0}{\min\{x,\sqrt{1-x^{2}}\}}\frac{1}{x}\,dy\\
            &=\min\{1,\sqrt{x^{-2}-1}\}
        \end{align*}
        \begin{align*}
            \prob(X^{2}+Y^{2}\leq 1)&=\iiint_{x^{2}+y^{2}\leq 1}f_{X,Y}(x,y)\,dy\,dx\\
            &=\iiint_{x^{2}+y^{2}\leq 1}f_{Y|X}(y|x)\,dy f_{X}(x)\,dx\\
            &=\intlu{0}{1}\min\{1,\sqrt{x^{-2}-1}\}\,dx\\
            &=\intlu{0}{\frac{1}{\sqrt{2}}}\,dx+\intlu{\frac{1}{\sqrt{2}}}{1}\sqrt{x^{-2}-1}\,dx\\
            \tag{$x=\sin\theta$}
            &=\frac{1}{\sqrt{2}}+\intlu{\frac{\pi}{4}}{\frac{\pi}{2}}\left(\frac{1}{\sin\theta}-\sin\theta\right)\,d\theta\\
            &=\left.\ln\left(\tan\frac{\theta}{2}\right)\right|_{\frac{\pi}{4}}^{\frac{\pi}{2}}=\ln(1)-\ln(\sqrt{2}-1)=\ln(1+\sqrt{2})
        \end{align*}
    \end{eg}

    \newpage
    \begin{eg}
        Assume that random variables $X\sim\N(0,1)$ and $Y\sim\N(0,1)$ are standard bivariate normal. For $-1<\rho<1$,
        \begin{equation*}
            f_{X,Y}(x,y)=\frac{1}{2\pi\sqrt{1-\rho^{2}}}\exp\left(-\frac{x^{2}-2\rho xy+y^{2}}{2(1-\rho^{2})}\right)
        \end{equation*}
        We want to find $f_{X|Y}(x|y)$.
        \begin{align*}
            f_{X|Y}(x|y)&=\frac{f_{X,Y}(x,y)}{f_{Y}(y)}\\
            \tag{$C_{1,y}=\sqrt{2\pi}e^{\frac{1}{2}y^{2}}$}
            &=\sqrt{2\pi}e^{\frac{1}{2}y^{2}}f_{X,Y}(x,y)\\
            \tag{$C_{2,y}=\frac{1}{\sqrt{2\pi}\sqrt{1-\rho^{2}}}e^{\left(\frac{1}{2}-\frac{1}{2(1-\rho^{2}}\right)y^{2}}$}
            &=\frac{1}{\sqrt{2\pi}\sqrt{1-\rho^{2}}}e^{\frac{1}{2}y^{2}-\frac{y^{2}}{2(1-\rho^{2})}}\exp\left(-\frac{x^{2}-2\rho xy}{2(1-\rho^{2})}\right)\\
            \tag{$C_{3,y}=\frac{1}{\sqrt{2\pi}\sqrt{1-\rho^{2}}}$}
            &=\frac{1}{\sqrt{2\pi}{\sqrt{1-\rho^{2}}}}e^{\left(\frac{1}{2}-\frac{1}{2(1-\rho^{2})}-\frac{\rho^{2}}{2(1-\rho^{2})}\right)y^{2}}\exp\left(-\frac{(x-\rho y)^{2}}{2(1-\rho^{2})}\right)\\
            &=\frac{1}{\sqrt{2\pi}\sqrt{1-\rho^{2}}}\exp\left(-\frac{(x-\rho y)^{2}}{2(1-\rho^{2})}\right)
        \end{align*}
        Therefore, we have $(X|Y=y)\sim\N(\rho y,1-\rho^{2})$. As $\rho\to 1$, we have $X\to Y$. As $\rho\to -1$, we have $X\to -Y$.\\
        In general, there exists a random variable $Z\sim\N(0,1)$ such that
        \begin{align*}
            X&=\rho Y+\sqrt{1-\rho^{2}}Z & (X|Y=y)&=\rho y+\sqrt{1-\rho^{2}}Z & \begin{pmatrix}
                X\\
                Y
            \end{pmatrix}&=\begin{pmatrix}
                \rho & \sqrt{1-\rho^{2}}\\
                1 & 0
            \end{pmatrix}\begin{pmatrix}
                Y\\
                Z
            \end{pmatrix}
        \end{align*}
        We can see that bivariate normal distribution is a linear transform of two independent normal distribution.\\
        More generally, for any orthogonal matrix $\mathbf{A}$, we have two random variables $W$ and $U$ such that if they can be obtained by:
        \begin{equation*}
            \begin{pmatrix}
                W\\
                U
            \end{pmatrix}=\begin{pmatrix}
                \rho & \sqrt{1-\rho^{2}}\\
                1 & 0
            \end{pmatrix}\mathbf{A}\begin{pmatrix}
                Y\\
                Z
            \end{pmatrix}
        \end{equation*}
        then $W$ and $U$ will also be bivariate normal with $\rho$.
    \end{eg}
    With conditional density function defined, we can now define conditional expectation.
    \begin{defn}
        Given two continuous random variables $X$ and $Y$ and an event $X=x$ for some $x\in\mathbb{R}$. \textbf{Conditional expectation} of $Y$ is defined by:
        \begin{equation*}
            \psi(x)=\expect(Y|X=x)=\intinfty yf_{Y|X}(y|x)\,dy
        \end{equation*}
        Given a continuous random variable $X$. Conditional expectation of $Y$ is defined by:
        \begin{equation*}
            \psi(X)=\expect(Y|X)
        \end{equation*}
    \end{defn}
    Again we also have the same properties of conditional distribution.
    \begin{lem}(Law of total expectation) 
        Conditional expectation $\psi(X)=\expect(Y|X)$ for continuous random variables $X$ and $Y$ satisfies:
        \begin{equation*}
            \expect Y=\expect(\psi(X))
        \end{equation*}
    \end{lem}
    \begin{proofing}
        \begin{align*}
            \expect(\psi(X))&=\intinfty\psi(x)f_{X}(x)\,dx\\
            &=\intinfty\intinfty yf_{Y|X}(y|x)f_{X}(x)\,dy\,dx\\
            &=\intinfty\intinfty yf_{X,Y}(x,y)\,dy\,dx\\
            &=\intinfty y\intinfty f_{X,Y}(x,y)\,dx\,dy\\
            &=\intinfty yf_{Y}(y)\,dy=\expect Y
        \end{align*}
    \end{proofing}
    \begin{lem}
        Conditional expectation $\psi(X)=\expect(Y|X)$ for continuous random variables $X$ and $Y$ satisfies:
        \begin{equation*}
            \expect(Yg(X))=\expect(\psi(X)g(X))
        \end{equation*}
    \end{lem}
    \begin{proofing}
        \begin{align*}
            \expect(\psi(X)g(X))&=\intinfty\psi(x)g(x)f_{X}(x)\,dx\\
            &=\intinfty\intinfty yf_{Y|X}(y|x)f_{X}(x)g(x)\,dy\,dx\\
            &=\intinfty\intinfty yf_{X,Y}(x,y)g(x)\,dy\,dx\\
            &=\expect(Yg(X))
        \end{align*}
    \end{proofing}
    
\section{Functions of continuous random variables}
    Given a continuous random variable $X$ and a function $g$ such that $g(X)$ is still a random variable, we have $\expect g(X)=\intinfty g(x)f_{X}(x)\,dx$. Therefore, we only need $f_{x}(x)$ to compute $\expect g(X)$. However, very often, we want to know the distribution of $g(X)$.
    \begin{eg}
        Assume that $X$ is continuous random variable with PDF $f_{X}(x)$. Let $Y=g(X)$ be a continuous random variable. How do we find the PDF $f_{Y}(y)$? We work with $F_{Y}(y)$ first. Let $g^{-1}(A)=\{x\in\mathbb{R}:g(x)\in A\}$.
        \begin{align*}
            F_{Y}(y)&=\prob(Y\leq y)=\prob(g(X)\in(-\infty,y])=\prob(X\in g^{-1}((-\infty,y]))=\int_{g^{-1}((-\infty,y])}f_{X}(x)\,dx\\
            f_{Y}(y)&=\pdv*{\int_{g^{-1}((-\infty,y])}f_{X}(x)\,dx}{y}
        \end{align*}
    \end{eg}
    \begin{eg}
        Let $X\sim\N(0,1)$. Let $Y=g(X)=X^{2}$. We want to find the PDF $f_{Y}(y)$.
        \begin{align*}
            F_{Y}(y)&=\prob(Y\leq y)=\prob(-\sqrt{y}\leq X\leq \sqrt{y})=\Phi(\sqrt{y})-\Phi(-\sqrt{y})=2\Phi(\sqrt{y})-1\\
            f_{Y}(y)&=F'(y)=2\phi(\sqrt{y})\left(\frac{1}{2\sqrt{y}}\right)=\frac{1}{\sqrt{y}}\phi(\sqrt{y})=\begin{cases}
                \frac{1}{\sqrt{2\pi y}}\exp\left(\frac{-y}{2}\right), &y>0\\
                0, &y<0
            \end{cases}
        \end{align*}
        We have $X^{2}\sim\chi^{2}(1)$. (This is a distribution)
    \end{eg}
    \begin{thm}
        In case that $g(x)$ is strictly monotonic (strictly increasing or strictly decreasing) and differentiable, let $Y=g(X)$. We have
        \begin{equation*}
            f_{Y}(y)=\begin{cases}
                f_{X}(g^{-1}(y))\abs{\pdv*{g^{-1}(y)}{y}}, &\text{if }y=g(x)\text{ for some }x\\
                0, &\text{Otherwise}
            \end{cases}
        \end{equation*}
    \end{thm}
    \begin{proofing}
        If $g(x)$ is a strictly increasing function,
        \begin{align*}
            F_{Y}(y)&=\prob(g(X)\leq y)=\prob(X\leq g^{-1}(y))=F_{X}(g^{-1}(y))\\
            f_{Y}(y)&=F_{Y}'(y)=f_{X}(g^{-1}(y))\pdv*{g^{-1}(y)}{y}=f_{X}(g^{-1}(y))\abs{\pdv*{g^{-1}(y)}{y}}
        \end{align*}
        If $g(x)$ is a strictly decreasing function,
        \begin{align*}
            F_{Y}(y)&=\prob(g(X)\leq y)=\prob(X\geq g^{-1}(y))=1-F_{X}(g^{-1}(y))\\
            f_{Y}(y)&=F_{Y}'(y)=-f_{X}(y^{-1}(y))\pdv*{g^{-1}(y)}{y}=f_{X}(g^{-1}(y))\abs{\pdv*{g^{-1}(y)}{y}}
        \end{align*}
    \end{proofing}

    \newpage
    We can consider the multivariable case.
    \begin{eg}
        Suppose two random variables $X$ and $Y$ are jointly continuous with JPDF $f_{X,Y}$. Given that $U=g(X,Y)$ and $V=h(X,Y)$. What is $f_{U,V}(u,v)$? For simplifying the process, we need to first make some following assumptions.
        \begin{enumerate}
            \item $X,Y$ can be uniquely solved from $U,V$. (There exists only 1 pair of functions $a,b$ such that $X=a(U,V)$ and $Y=b(U,V)$)
            \item The function $g$ and $h$ are differentiable and the Jacobian determinant
            \begin{equation*}
                J(x,y)=\begin{vmatrix}
                    \pdv{g}{x} & \pdv{g}{y}\\
                    \pdv{h}{x} & \pdv{h}{y}
                \end{vmatrix}\neq 0
            \end{equation*}
        \end{enumerate}
        Then
        \begin{equation*}
            f_{U,V}(u,v)=\frac{1}{\abs{J(x,y)}}f_{X,Y}(x,y)=\begin{cases}
                \frac{1}{\abs{J(a(u,v),b(u,v))}}f_{X,Y}(a(u,v),b(u,v)), &(u,v)=(g(x,y),h(x,y))\text{ for some }x,y\\
                0, &\text{Otherwise}
            \end{cases}
        \end{equation*}
    \end{eg}
    \begin{eg}
        Given two jointly continuous random variables $X_{1},X_{2}$ and their JPDF $f_{X_{1},X_{2}}$.\\
        Let $Y_{1}=X_{1}+X_{2}$ and $Y_{2}=X_{1}-X_{2}$.
        \begin{align*}
            X_{1}&=\frac{Y_{1}+Y_{2}}{2}=a(Y_{1},Y_{2}) & X_{2}&=\frac{Y_{1}-Y_{2}}{2}=b(Y_{1},Y_{2}) & J(x_{1},x_{2})&=\begin{vmatrix}
                1 & 1\\
                1 & -1
            \end{vmatrix}=-2
        \end{align*}
        \begin{equation*}
            f_{Y_{1},Y_{2}}(y_{1},y_{2})=\frac{1}{\abs{J(x_{1},x_{2})}}f_{X_{1},X_{2}}(x_{1},x_{2})=\frac{1}{2}f_{X_{1},X_{2}}\left(\frac{y_{1}+y_{2}}{2},\frac{y_{1}-y_{2}}{2}\right)
        \end{equation*}
        More specifically, if $X_{1}\sim\N(0,1)$, $X_{2}\sim\N(0,1)$ and $X_{1}\independent X_{2}$,
        \begin{align*}
            f_{X_{1},X_{2}}(x_{1},x_{2})&=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x_{1}^{2}+x_{2}^{2})}\\
            f_{Y_{1},Y_{2}}(y_{1},y_{2})&=\frac{1}{2}f_{X_{1},X_{2}}\left(\frac{y_{1}+y_{2}}{2},\frac{y_{1}-y_{2}}{2}\right)\\
            &=\frac{1}{4\pi}e^{-\frac{1}{2}\left(\left(\frac{1}{2}(y_{1}+y_{2})\right)^{2}+\left(\frac{1}{2}(y_{1}-y_{2})\right)^{2}\right)}\\
            &=\frac{1}{4\pi}e^{-\frac{1}{4}(y_{1}^{2}+y_{2}^{2})}
        \end{align*}
        Therefore, $Y_{1}\independent Y_{2}$ and we have $Y_{1}\sim\N(0,2)$ and $Y_{2}\sim\N(0,2)$.
    \end{eg}
    \begin{eg}
        Given two random variables $X_{1}\sim\U[0,1]$ and $X_{2}\sim\U[0,1]$. If $X_{1}\independent X_{2}$, for all $x_{1},x_{2}\in\mathbb{R}$,
        \begin{align*}
            f_{X_{1},X_{2}}(x_{1},x_{2})&=\begin{cases}
                1, &x_{1},x_{2}\in[0,1]\\
                0, &\text{Otherwise}
            \end{cases}\\
            f_{Y_{1},Y_{2}}(y_{1},y_{2})&=\frac{1}{2}f_{X_{1},X_{2}}\left(\frac{y_{1}+y_{2}}{2},\frac{y_{1}-y_{2}}{2}\right)\\
            &=\frac{1}{2}\mathbf{1}_{0\leq y_{1}+y_{2}\leq 2,0\leq y_{1}-y_{2}\leq 2}
        \end{align*}
    \end{eg}
    Similar to discrete random variables, we can find the distribution of $X+Y$ when $X$ and $Y$ are jointly continuous.
    \begin{thm}
        If two jointly continuous random variables $X$ and $Y$ have JPDF $f_{X,Y}$, then $X+Y$ has a PDF
        \begin{equation*}
            f_{X+Y}(z)=\intinfty f_{X,Y}(x,z-x)\,dx=\intinfty f_{X,Y}(z-y,y)\,dy
        \end{equation*}
    \end{thm}
    
    \newpage
    \begin{proofing}
        \begin{align*}
            F_{X+Y}(z)&=\prob(X+Y\leq z)\\
            &=\iint_{x+y\leq z}f_{X,Y}(x,y)\,dx\,dy\\
            &=\intinfty\intlu{-\infty}{z-y}f_{X,Y}(x,y)\,dx\,dy\\
            \tag{$v=x+y$}
            &=\intinfty\intlu{-\infty}{z}f_{X,Y}(v-y,y)\,dv\,dy\\
            &=\intlu{-\infty}{z}\intinfty f_{X,Y}(v-y,y)\,dy\,dv\\
            f_{X+Y}(z)&=F_{X+Y}'(z)=\intinfty f_{X,Y}(z-y,y)\,dy=\intinfty f_{X,Y}(x,z-x)\,dx
        \end{align*}
    \end{proofing}
    \begin{defn}
        Given two independent continuous random variables $X$ and $Y$. \textbf{Convolution} $f_{X+Y}$ ($f_{X}*f_{Y}$) of PDFs of $X$ and $Y$ is the PDF of $X+Y$:
        \begin{equation*}
            f_{X+Y}(z)=\intinfty f_{X}(z-y)f_{Y}(y)\,dy=\intinfty f_{X}(x)f_{Y}(z-x)\,dx
        \end{equation*}
    \end{defn}
    \begin{eg}
        If $X\sim\U[0,1]$ and $Y\sim\U[0,1]$. In case of $X\independent Y$,
        \begin{align*}
            f_{X}(t)&=f_{Y}(t)=\begin{cases}
                1, &0\leq t\leq 1\\
                0, &\text{Otherwise}
            \end{cases}\\
            f_{X+Y}(z)&=\intinfty f_{X}(z-y)f_{Y}(y)\,dy\\
            &=\intlu{0}{1}f_{X}(z-y)\,dy\\
            &=\intlu{0}{1}\mathbf{1}_{0\leq z-y\leq 1}\,dy\\
            \tag{$z-1\leq y\leq z$}
            &=\intlu{\max\{0,z-1\}}{\min\{1,z\}}\,dy\\
            &=\min\{1,z\}-\max\{0,z-1\}=\begin{cases}
                z, &0\leq z\leq 1\\
                2-z, &1\leq z\leq 2\\
                0, &\text{Otherwise}
            \end{cases}
        \end{align*}
    \end{eg}
    The following example states that sum of independent normal random variables is still normal.
    \begin{eg}
        If $X_{i}\sim\N(\mu_{i},\sigma_{i}^{2})$ for $i=1,2,\cdots,n$ and they are independent, then $\sum_{i=1}^{n}X_{i}\sim\N\left(\sum_{i=1}^{n}\mu_{i},\sum_{i=1}^{n}\sigma_{i}^{2}\right)$.
    \end{eg}
    
    \newpage
    \begin{cla}
        It suffices to prove for the case $n=2$.
    \end{cla}
    \begin{proofing}
        We first consider a special case when $X\sim\N(0,\sigma^{2})$, $Y\sim\N(0,1)$ and $X\independent Y$.
        \begin{align*}
            f_{X+Y}(z)&=\intinfty f_{X}(z-y)f_{Y}(y)\,dy\\
            &=\intinfty\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(z-y)^{2}}{2\sigma^{2}}\right)\left(\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{y^{2}}{2}\right)\right)\,dy\\
            &=\intinfty\frac{1}{2\pi\sigma}\exp\left(-\frac{z^{2}}{2\sigma^{2}}\right)\exp\left(-\frac{1}{2\sigma^{2}}(-2yz+y^{2}(1+\sigma^{2}))\right)\,dy\\
            &=\intinfty\frac{1}{2\pi\sigma}\exp\left(-\frac{z^{2}}{2\sigma^{2}}+\frac{z^{2}}{2\sigma^{2}(1+\sigma^{2})}\right)\exp\left(-\frac{1+\sigma^{2}}{2\sigma^{2}}\left(\frac{z^{2}}{(1+\sigma^{2})^{2}}-\frac{2yz}{1+\sigma^{2}}+y^{2}\right)\right)\,dy\\
            &=\intinfty\frac{1}{\sqrt{2\pi}\sqrt{1+\sigma^{2}}}\exp\left(-\frac{z^{2}}{2\sigma^{2}}+\frac{z^{2}}{2\sigma^{2}(1+\sigma^{2})}\right)\left(\frac{1}{\sqrt{2\pi}\frac{\sigma}{\sqrt{1+\sigma^{2}}}}\right)\exp\left(-\frac{\left(y-\frac{z}{1+\sigma^{2}}\right)^{2}}{2\left(\frac{\sigma}{\sqrt{1+\sigma^{2}}}\right)^{2}}\right)\,dy\\
            &=\frac{1}{\sqrt{2\pi}\sqrt{1+\sigma^{2}}}\exp\left(-\frac{z^{2}}{2(1+\sigma^{2})}\right)
        \end{align*}
        Therefore, $X+Y\sim\N(0,1+\sigma^{2})$. In general case when $X_{1}\sim\N(\mu_{1},\sigma_{1}^{2})$, $X_{2}\sim\N(\mu_{2},\sigma_{2}^{2})$ and $X_{1}\independent X_{2}$.
        \begin{equation*}
            X_{1}+X_{2}=\sigma_{2}\left(\frac{X_{1}-\mu_{1}}{\sigma_{2}}+\frac{X_{2}-\mu_{2}}{\sigma_{2}}\right)+\mu_{1}+\mu_{2}
        \end{equation*}
        We get $\frac{X_{1}-\mu_{1}}{\sigma_{2}}\sim\N\left(0,\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\right)$. Now we can apply this to special case and we get $\frac{X_{1}-\mu_{1}}{\sigma_{2}}+\frac{X_{2}-\mu_{2}}{\sigma_{2}}\sim\N\left(0,1+\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\right)$.\\
        Therefore, $X_{1}+X_{2}\sim\N(\mu_{1}+\mu_{2},\sigma_{1}^{2}+\sigma_{2}^{2})$. By induction, if $X_{i}\sim\N(\mu_{i},\sigma_{i}^{2})$ for $i=1,2,\cdots,n$ and they are independent, then
        \begin{equation*}
            \sum_{i=1}^{n}X_{i}\sim\N\left(\sum_{i=1}^{n}\mu_{i},\sum_{i=1}^{n}\sigma_{i}^{2}\right)
        \end{equation*}
    \end{proofing}

\addcontentsline{toc}{chapter}{Summary of Chapter 1-4}
\chapter*{Summary}
\section*{Definition}
    \begin{sdefn}
        \textbf{Sample space} $\Omega$ is the set of all possible outcomes $\omega$ of an experiment. It represents the universe of all potential results.
    \end{sdefn}
    \begin{sdefn}
        \textbf{Event} $A$ is a subset of the sample space. Individual outcomes within $A$ are referred to as \textbf{elementary events}.
    \end{sdefn}
    \begin{sdefn}
        The \textbf{complement} of a subset $A$ is the set $A^{\complement}$, which includes all elements in the sample space $\Omega$ that are not part of $A$.
    \end{sdefn}
    \begin{sdefn}
        A \textbf{$\sigma$-field} (or \textbf{$\sigma$-algebra}) $\mathcal{F}$ is a collection of subsets of $\Omega$ that satisfies the following conditions:
        \begin{enumerate}
            \item If $A\in\mathcal{F}$, then $A^{\complement}\in\mathcal{F}$.
            \item If $A_{i}\in\mathcal{F}$ for all $i$, then $\bigcup_{i=1}^{\infty}A_{i}\in\mathcal{F}$.
            \item The empty set $\emptyset\in\mathcal{F}$.
        \end{enumerate}
    \end{sdefn}
    \begin{sdefn}
        A \textbf{measurable space} $(\Omega, \mathcal{F})$ consists of a sample space $\Omega$ and a $\sigma$-field $\mathcal{F}$.
    \end{sdefn}
    \begin{sdefn}
        A \textbf{probability measure} $\prob: \mathcal{F} \to [0,1]$ is a function defined on a measurable space $(\Omega, \mathcal{F})$ that satisfies:
        \begin{enumerate}
            \item $\prob(\emptyset)=0$.
            \item $\prob(\Omega)=1$.
            \item If $A_{i}\in\mathcal{F}$ for all $i$ and the sets $A_{i}$ are disjoint, then:
            \begin{equation*}
                \prob\left(\bigcup_{i=1}^{\infty}A_{i}\right)=\sum_{i=1}^{\infty}\prob(A_{i}).
            \end{equation*}
        \end{enumerate}
    \end{sdefn}
    \begin{sdefn}
        A \textbf{probability space} $(\Omega,\mathcal{F},\prob)$ consists of:
        \begin{enumerate}
            \item A sample space $\Omega$.
            \item A $\sigma$-field $\mathcal{F}$ of subsets of $\Omega$.
            \item A probability measure $\prob$ defined on $(\Omega,\mathcal{F})$.
        \end{enumerate}
    \end{sdefn}
    \begin{sdefn}
        We say a sequence of events $A_{n}$ \textbf{converges} and $\lim_{n\to\infty}A_{n}$ exists if
        \begin{equation*}
            \limsup_{n\to\infty}A_{n}=\liminf_{n\to\infty}A_{n}
        \end{equation*}
        Given a probability space $(\Omega,\mathcal{F},\prob)$. Let $A_{i}\in\mathcal{F}$ for all $i$ such that $A=\lim_{n\to\infty}A_{n}$ exists. Then
        \begin{equation*}
            \lim_{n\to\infty}\prob(A_{n})=\prob\left(\lim_{n\to\infty}A_{n}\right)
        \end{equation*}
    \end{sdefn}
    \begin{sdefn}
        Event $A$ is \textbf{null} if $\prob(A)=0$. This means $A$ has no chance of occurring.
    \end{sdefn}
    \begin{sdefn}
        Event $A$ is \textbf{almost surely} if $\prob(A)=1$. This indicates $A$ occurs with certainty.
    \end{sdefn}
    \begin{sdefn}
        Given $\prob(B)>0$. \textbf{Conditional probability} that $A$ occurs given that $B$ occurs is:
        \begin{equation*}
            \prob(A|B)=\frac{\prob(A\cap B)}{\prob(B)}
        \end{equation*}
    \end{sdefn}
    \begin{sdefn}
        Events $A$ and $B$ are independent ($A\independent B$) if $\prob(A\cap B)=\prob(A)\prob(B)$.\\
        Given $A_{k}$ for all $k\in I$. If for all $i\neq j$, 
        \begin{equation*}
            \prob(A_{i}\cap A_{j})=\prob(A_{i})\prob(A_{j})
        \end{equation*} 
        then they are \textbf{pairwise independent}.\\
        If additionally, for all subsets $J\subseteq I$,
        \begin{equation*}
            \prob\left(\bigcap_{i\in J}A_{i}\right)=\prod_{i\in J}\prob(A_{i})
        \end{equation*}
        then they are \textbf{(mutually) independent}.
    \end{sdefn}
    \begin{sdefn}
        Let $A$ be a collection of subsets of $\Omega$. The \textbf{$\sigma$-field generated by $A$} is:
        \begin{equation*}
            \sigma(A)=\bigcap_{A\subseteq\mathcal{G}}\mathcal{G}
        \end{equation*}
        where $\mathcal{G}$ are also $\sigma$-field. $\sigma(A)$ is the smallest $\sigma$-field containing $A$.
    \end{sdefn}
    \begin{sdefn}
        \textbf{Product space} of two probability spaces $(\Omega_{1},\mathcal{F}_{1},\prob_{1})$ and $(\Omega_{2},\mathcal{F}_{2},\prob_{2})$ is the probability space $(\Omega_{1}\times\Omega_{2},\mathcal{G},\prob_{12})$ comprising:
        \begin{enumerate}
            \item a collection of ordered pairs $\Omega_{1}\times\Omega_{2}=\{(\omega_{1},\omega_{2}):\omega_{1}\in\Omega_{1},\omega_{2}\in\Omega_{2}\}$
            \item a $\sigma$-algebra $\mathcal{G}=\sigma(\mathcal{F}_{1}\times\mathcal{F}_{2})$ where $\mathcal{F}_{1}\times\mathcal{F}_{2}=\{A_{1}\times A_{2}:A_{1}\in\mathcal{F}_{1},A_{2}\in\mathcal{F}_{2}\}$ 
            \item a probability measure $\prob_{12}:\mathcal{F}_{1}\times\mathcal{F}_{2}\to [0,1]$ given by:
            \begin{equation*}
                \prob_{12}(A_{1}\times A_{2})=\prob_{1}(A_{1})\prob_{2}(A_{2})
            \end{equation*}
            for $A_{1}\in\mathcal{F}_{1},A_{2}\in\mathcal{F}_{2}$.
        \end{enumerate}
    \end{sdefn}
    \begin{sdefn}
        \textbf{Random variable} is a function $X:\Omega\to\mathbb{R}$ with the property that:
        \begin{equation*}
            X^{-1}((-\infty,x])=\{\omega\in\Omega:X(\omega)\leq x\}\in\mathcal{F}
        \end{equation*}
        for any $X\in\mathbb{R}$. We say the function is \textbf{$\mathcal{F}$-measurable}.
    \end{sdefn}
    \begin{sdefn}
        \textbf{Borel set} is a set which can be obtained by taking countable union, intersection or complement repeatedly.
    \end{sdefn}
    \begin{sdefn}
        \textbf{Borel $\sigma$-field} $\mathcal{B}(\mathbb{R})$ of $\mathbb{R}$ is a $\sigma$-field that is generated by all open sets. It is a collection of Borel sets.
    \end{sdefn}
    \begin{sdefn}
        \textbf{(Cumulative) distribution function} (CDF) of a random variable $X$ is a function $F_{X}:\mathbb{R}\to[0,1]$ given by
        \begin{equation*}
            F_{X}(x)=\prob(X\leq x)=\prob\circ X^{-1}((-\infty,x])
        \end{equation*}
        In \textbf{discrete} case, \textbf{probabilty mass function} (PMF) of discrete random variable $X$ is the function $f:\mathbb{R}\to[0,1]$ given by:
        \begin{align*}
            f_{X}(x)&=\prob(X=x)=\prob\circ X^{-1}(\{x\}) & F_{X}(x)&=\sum_{i:x_{i}\leq x}f(x_{i}) & f_{X}(x)&=F_{X}(x)-\lim_{y\uparrow x}F_{X}(y)
        \end{align*}
        In \textbf{continuous} case, \textbf{probability density function} (PDF) of continuous random variable $X$ is the function $f:\mathbb{R}\to[0,\infty)$ given by:
        \begin{align*}
            F_{X}(x)&=\intlu{-\infty}{x}f(u)\,du & f_{X}(x)&=\pdv*{F_{X}(x)}{x}
        \end{align*}
    \end{sdefn}

    \newpage
    \begin{sdefn}
        Let $X_{i}:\Omega\to\mathbb{R}$ for all $1\leq i\leq n$ be random variables. \textbf{Random vector} $\vec{X}=(X_{1},X_{2},\cdots,X_{n}):\Omega\to\mathbb{R}^{n}$ with properties:
        \begin{equation*}
            \vec{X}^{-1}(D)=\{\omega\in\Omega:\vec{X}(\omega)=(X_{1}(\omega),X_{2}(\omega),\cdots,X_{n}(\omega))\in D\}\in\mathcal{F}
        \end{equation*}
        for all $D\in\mathcal{B}(\mathbb{R}^{n})$.\\
        We can also say $\vec{X}$ is a random vector if
        \begin{equation*}
            X_{i}^{-1}(B)\in\mathcal{F}
        \end{equation*}
        for all $B\in\mathcal{B}(\mathbb{R})$ and $i$.
    \end{sdefn}
    \begin{sdefn}
        Given a random vector $(X,Y)$. \textbf{Joint distribution function} (JCDF) $F_{X,Y}:\mathbb{R}^{2}\to[0,1]$ is defined as:
        \begin{equation*}
            F_{X,Y}(x,y)=\prob(X\leq x,Y\leq y)=\prob\circ(X,Y)^{-1}((-\infty,x]\times(-\infty,y])
        \end{equation*}
        In discrete case, \textbf{joint probability mass function} (JPMF) of \textbf{jointly discrete} random variable $X$ and $Y$ is the function $f_{X,Y}:\mathbb{R}^{2}\to[0,1]$ given by:
        \begin{align*}
            f_{X,Y}(x,y)&=\prob((X,Y)=(x,y))=\prob\circ(X,Y)^{-1}(\{x,y\}) & F_{X,Y}(x,y)&=\sum_{u\leq x}\sum_{v\leq y}f(u,v)
        \end{align*}
        In continuous case, \textbf{joint probability density function} (JPDF) of \textbf{jointly continuous} random variable $X$ and $Y$ is the function $f_{X,Y}:\mathbb{R}^{2}\to[0,\infty)$ given by:
        \begin{align*}
            f_{X,Y}(x,y)&=\pdv*{F_{X,Y}(x,y)}{x,y} & F_{X,Y}(x,y)&=\intlu{-\infty}{y}\intlu{-\infty}{x}f_{X,Y}(u,v)\,du\,dv
        \end{align*}
    \end{sdefn}
    \begin{sdefn}
        Let $X$ and $Y$ be random variables. \textbf{Marginal distribution function} (Marginal CDF) is given by:
        \begin{equation*}
            F_{X}(x)=\prob(X^{-1}((-\infty,x])\cap Y^{-1}((-\infty,\infty)))=\lim_{y\to\infty}F_{X,Y}(x,y)
        \end{equation*}
        In discrete case, \textbf{marginal mass function} (Marginal PMF) is given by:
        \begin{equation*}
            f_{X}(x)=\sum_{y}f_{X,Y}(x,y)
        \end{equation*}
        In continuous case, \textbf{marginal density function} (Marginal PDF) is given by:
        \begin{equation*}
            f_{X}(x)=\intinfty f_{X,Y}(x,y)\,dy
        \end{equation*}
    \end{sdefn}
    \begin{sdefn}
        Given a random variable $X$. \textbf{Mean value}, \textbf{expectation}, or \textbf{expected value} of $X$ is given by:
        \begin{equation*}
            \expect X=\begin{cases}
                \sum_{x:f_{X}(x)>0}xf_{X}(x), &X\text{ is discrete}\\
                \intinfty xf_{X}(x)\,dx, &X\text{ is continuous}
            \end{cases}
        \end{equation*}
        If it is absolutely convergent.
    \end{sdefn}
    \begin{sdefn}
        Given $k\in\mathbb{N}_{+}$ and a random variable $X$. \textbf{$k$-th moment} $m_{k}$ is defined to be:
        \begin{equation*}
            \expect(X^{k})=\begin{cases}
                \sum_{x}x^{k}f_{X}(x), &X\text{ is discrete}\\
                \intinfty x^{k}f_{X}(x)\,dx, &X\text{ is continuous}
            \end{cases}
        \end{equation*}
        \textbf{$k$-th cnetral moment} $\alpha_{k}$ is defined to be
        \begin{equation*}
            \expect((X-\expect X)^{k})=\begin{cases}
                \sum_{x}(x-\expect X)^{k}f_{X}(x), &X\text{ is discrete}\\
                \intinfty(x-\expect X)^{k}f_{X}(x)\,dx, &X\text{ is continuous}
            \end{cases}
        \end{equation*}
        \textbf{Mean} $\mu$ is the $1$st moment $\mu=m_{1}=\expect X$.\\
        \textbf{Variance} is the $2$nd central moment $\alpha_{2}=\Var(X)=\expect((X-\expect X)^{2})=\expect(X^{2})-(\expect X)^{2}$.\\
        \textbf{Standard deviation} $\sigma$ is defined as $\sigma=\sqrt{\Var(X)}$.
    \end{sdefn}
    \begin{sdefn}
        Two random variables $X$ and $Y$ are \textbf{uncorrelated} if $\expect(XY)=\expect X\expect Y$.
    \end{sdefn}
    \begin{sdefn}
        \textbf{Covariance} of two random variables $X$ and $Y$ is:
        \begin{equation*}
            \cov(X,Y)=\expect((X-\expect X)(Y-\expect Y))=\expect(XY)-\expect X\expect Y
        \end{equation*}
    \end{sdefn}
    \begin{sdefn}
        Given two random variables $X$ and $Y$. \textbf{Conditional distribution function} (Conditional CDF) of $Y$ given $X=x$ for any $x$ is defined by:
        \begin{equation*}
            F_{Y|X}(y|x)=\prob(Y\leq y|X=x)=\begin{cases}
                \frac{\prob(Y\leq y,X=x)}{\prob(X=x)}, &X\text{ is discrete}\\
                \intlu{-\infty}{y}\frac{f_{X,Y}(x,v)}{f_{X}(x)}\,dv, &X\text{ is continuous}
            \end{cases}
        \end{equation*}
        In discrete case, \textbf{conditional mass function} (Conditional PMF) of $Y$ given $X=x$ is defined by:
        \begin{equation*}
            f_{Y|X}(y|x)=\begin{cases}
                \frac{\prob(Y=y,X=x)}{\prob(X=x)}, &X\text{ is discrete}\\
                \pdv*{F_{Y|X}(y|x)}{y}=\frac{f_{X,Y}(x,y)}{f_{X}(x)}, &X\text{ is continuous}
            \end{cases}
        \end{equation*}
    \end{sdefn}
    \begin{sdefn}
        Given two random variables $X$ and $Y$, and an event $X=x$ for some $X$. \textbf{Conditional expectation} of random variable $Y$ is defined by:
        \begin{equation*}
            \psi(x)=\expect(Y|X=x)=\begin{cases}
                \sum_{y}yf_{Y|X}(y|x), &X\text{ and }Y\text{ are discrete}\\
                \intinfty yf_{Y|X}(y|x)\,dy, &X\text{ and }Y\text{ are continuous}
            \end{cases}
        \end{equation*}
        Given a random variable $X$. Conditional expectation of random variable $Y$ is defined by:
        \begin{equation*}
            \psi(X)=\expect(Y|X)=\begin{cases}
                \sum_{x}\psi(x), &X\text{ and }Y\text{ are discrete}\\
                \intinfty\psi(x)\,dx, &X\text{ are continuous}
            \end{cases}
        \end{equation*}
    \end{sdefn}
    \begin{sdefn}
        Given $X\independent Y$. In discrete case, \textbf{convolution} $f_{X+Y}$ ($f_{X}*f_{Y}$) of PMFs of random variables $X$ and $Y$ is the PMF of $X+Y$:
        \begin{equation*}
            f_{X+Y}(z)=\prob(X+Y-z)=\sum_{x}f_{X}(x)f_{Y}(z=x)=\sum_{y}f_{X}(z-y)f_{Y}(y)
        \end{equation*}
        In continuous case, \textbf{convolution} of PDFs of random variables $X$ and $Y$ is the PDF of $X+Y$:
        \begin{equation*}
            f_{X+Y}(z)=\intinfty f_{X}(z-y)f_{Y}(y)\,dy=\intinfty f_{X}(x)f_{Y}(z-x)\,dx
        \end{equation*}
    \end{sdefn}
    
\section*{Named Property}
    \begin{spro}(Inclusion-Exclusion Principle)
    For any finite collection of events $A_{1}, A_{2}, \dots, A_{n}$:
    \begin{equation*}
        \prob\left(\bigcup_{i=1}^{n}A_{i}\right)=\sum_{i}\prob(A_{i})-\sum_{i<j}\prob(A_{i}\cap A_{j})+\cdots+(-1)^{n+1}\prob(A_{1}\cap A_{2}\cap\cdots\cap A_{n}).
    \end{equation*}
    \end{spro}
    \begin{spro}(Law of Total Probability)
    Let $\{B_{1},B_{2},\dots,B_{n}\}$ be a partition of $\Omega$ such that $B_{i}\cap B_{j}=\emptyset$ for all $i\neq j$ and $\bigcup_{i=1}^{n}B_{i} = \Omega$. If $\prob(B_{i})>0$ for all $i$, then:
    \begin{equation*}
        \prob(A)=\sum_{i=1}^{n}\prob(A\mid B_{i})\prob(B_{i}).
    \end{equation*}
    \end{spro}
    \begin{spro}(Law of Total Expectation)
        Let $\psi(X)=\expect(Y|X)$. Conditional expectation satisfies:
        \begin{equation*}
            \expect(\psi(X))=\expect(\expect(Y|X))=\expect(Y)
        \end{equation*}
    \end{spro}
    \begin{spro}(Tail sum formula)
        If $X$ has a PDF $f_{X}$ with $f_{X}(x)=0$ when $x<0$, and a CDF $F_{X}$, then:
        \begin{equation*}
            \expect X=\intlu{0}{\infty}(1-F_{X}(x))\,dx
        \end{equation*}
    \end{spro}
    \newpage

\section*{Distributions}
    For discrete random variables,
    \begin{seg}(Bernoulli distribution) $X\sim\Bern(p)$\\
        Suppose we perform $1$ Bernoulli trial. Let $p$ be probability of success and $X$ be number of successes.
        \begin{align*}
            F_{X}(x)&=\begin{cases}
                0, &x<0\\
                1-p, &0\leq x<1\\
                1, &x\geq 1
            \end{cases} & f_{X}(x)&=\begin{cases}
                1-p, &x=0\\
                p, &x=1\\
                0, &\text{Otherwise}
            \end{cases} & \expect X&=p & \Var(X)&=p(1-p)
        \end{align*}
    \end{seg}
    \begin{seg}(Binomial distribution) $Y\sim\Bin(n,p)$\\
        Suppose we perform $n$ independent Bernoulli trials. Let $p$ be the probability of success and $Y=X_{1}+X_{2}+\cdots+X_{n}$ be total number of successes.
        \begin{align*}
            f_{Y}(k)&=\binom{n}{k}p^{k}(1-p)^{n-k} & F_{Y}(k)&=\sum_{i=0}^{k}\binom{n}{i}p^{i}(1-p)^{n-i} & \expect X&=np & \Var(X)&=np(1-p)
        \end{align*}
    \end{seg}
    \begin{seg}(Trinomial distribution)\\
        Suppose we perform $n$ trials with three outcomes $A$, $B$ and $C$, where the probability of occurrence is $p$, $q$ and $1-p-q$ respectively. Let $X$ be number of occurrence of $A$ and $Y$ be number of occurrence of $B$.\\
        Probability of $x$ $A$'s, $y$ $B$'s and $n-x-y$ $C$'s is:
        \begin{equation*}
            f_{X,Y}(x,y)=\frac{n!}{x!y!(n-x-y)!}p^{x}q^{y}(1-p-q)^{n-x-y}
        \end{equation*}
    \end{seg}
    \begin{seg}(Geometric distribution) $W\sim\Geom(p)$ $X\sim\Geom(p)$\\
        Suppose we keep performing independent Bernoulli trials until the first success shows up. Let $p$ be probability of success.\\
        Let $W$ be the waiting time which elapses before first success. For $k\geq 1$,
        \begin{align*}
            f_{W}(k)&=p(1-p)^{k-1} & F_{W}(k)&=1-(1-p)^{k} & \expect W&=\frac{1}{p} & \Var(W)&=\frac{1-p}{p^{2}}
        \end{align*}
        Let $X$ be number of failures before first success. For $k\geq 0$,
        \begin{align*}
            f_{X}(k)&=p(1-p)^{k} & F_{X}(k)&=1-(1-p)^{k+1} & \expect X&=\frac{1-p}{p} & \Var(X)&=\frac{1-p}{p^{2}}
        \end{align*}
    \end{seg}
    \begin{seg}(Negative Binomial distribution) $W_{r}\sim\NBin(r,p)$ $X\sim\NBin(r,p)$\\
        Suppose we keep performing independent Bernoulli trials until the first success shows up. Let $p$ be the probability of success.\\
        Let $W_{r}$ be the waiting time which elapses before $r$-th success. For any $k\geq r$,
        \begin{align*}
            f_{W_{r}}(k)&=\binom{k-1}{r-1}p^{r}(1-p)^{k-r} & \expect W_{r}&=\frac{r}{p} & \Var(W_{r})&=\frac{r(1-p)}{p^{2}}
        \end{align*}
        Let $X$ be number of failures before the $r$-th success. For any $k\geq 0$,
        \begin{align*}
            f_{X}(k)&=\binom{k+r-1}{r-1}p^{r}(1-p)^{k} & \expect X&=\frac{r(1-p)}{p} & \Var(X)&=\frac{r(1-p)}{p^{2}}
        \end{align*}
    \end{seg}
    \begin{seg}(Poisson distribution) $X\sim\Poisson(\lambda)$\\
        Suppose we perform $n$ independent Bernoulli trials. Let $p$ be the probability of success, $\lambda=np$ and $X\sim\Bin(n,p)$. When $n$ is large, $p$ is small, and $np$ is moderate:
        \begin{align*}
            f_{X}(k)&=\binom{n}{k}p^{k}(1-p)^{n-k}\approx\frac{\lambda^{k}}{k!}e^{-\lambda} & F_{X}(k)&=\sum_{i=0}^{k}\frac{\lambda^{i}}{i!}e^{-\lambda} & \expect X&=\lambda & \Var(X)&=\lambda
        \end{align*}
    \end{seg}

    \newpage
    For continuous random variables,
    \begin{seg}(Uniform distribution) $X\sim\U[a,b]$\\
        Random variable $X$ is uniform on $[a,b]$ is PDF and CDF is:
        \begin{align*}
            f_{X}(x)&=\begin{cases}
                \frac{1}{b-a}, &a\leq x\leq b\\
                0, &\text{Otherwise}
            \end{cases} & F_{X}(x)&=\begin{cases}
                0, &x<a\\
                \frac{x-a}{b-a}, &a\leq x\leq b\\
                1, &x>b
            \end{cases}
        \end{align*}    
    \end{seg}
    \begin{seg}(Exponential distribution) $X\sim\Exp(\lambda)$\\
        Random variable $X$ is exponential with parameter $\lambda>0$ if PDF and CDF is:
        \begin{align*}
            f_{X}(x)&=\begin{cases}
                0, &x<0\\
                \lambda e^{-\lambda x}, &x\geq 0
            \end{cases} & F_{X}(x)&=\begin{cases}
                0, &x<0\\
                1-e^{-\lambda x}, &x\geq 0
            \end{cases}
        \end{align*}
    \end{seg}
    \begin{seg}(Normal distribution / Gaussian distribution) $X\sim\N(\mu,\sigma^{2})$\\
        Random variable $X$ is normal if it has two parameter $\mu$ and $\sigma^{2}$, and its PDF and CDF is:
        \begin{align*}
            f_{X}(x)&=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right) & F_{X}(x)&=\intlu{-\infty}{x}f_{X}(u)\,du & \expect X&=\mu & \Var(X)&=\sigma^{2}
        \end{align*}
        Random variable $X$ is standard normal if $\mu=0$ and $\sigma^{2}=1$. ($X\sim\N(0,1)$)
        \begin{align*}
            f_{X}(x)&=\phi(x)=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x^{2}}{2}\right) & F_{X}(x)&=\Phi(x)=\intlu{-\infty}{x}\phi(u)\,du & \expect X&=0 & \Var(X)&=1
        \end{align*}
    \end{seg}
    \begin{seg}(Cauchy distribution) $X\sim\Cauchy$\\
        Random variable $X$ has a Cauchy distribution if:
        \begin{align*}
            f_{X}(x)&=\frac{1}{\pi(1+x^{2})} & \expect|X|&=\intinfty\frac{|x|}{\pi(1+x^{2})}\,dx=\infty
        \end{align*}
    \end{seg}
    \begin{seg}(Bivariate normal distribution)
        Two random variables $X$ and $Y$ are bivariate normal with $\mu_{X}$ and $\mu_{Y}$, variance $\sigma_{X}^{2}$ and $\sigma_{Y}^{2}$, and correlation coefficient $\rho$ if:
        \begin{equation*}
        f_{X,Y}(x,y)=\frac{1}{2\pi\sigma_{X}\sigma_{Y}\sqrt{1-\rho^{2}}}\exp\left(-\frac{1}{2(1-\rho^{2})}\left(\left(\frac{x-\mu_{X}}{\sigma_{X}}\right)^{2}-2\rho\left(\frac{x-\mu_{X}}{\sigma_{X}}\right)\left(\frac{y-\mu_{Y}}{\sigma_{Y}}\right)+\left(\frac{y-\mu_{Y}}{\sigma_{Y}}\right)^{2}\right)\right)
        \end{equation*}
        Two random variables $X$ and $Y$ are standard bivariate normal if $\mu_{X}=\mu_{Y}=0$ and $\sigma_{X}^{2}=\sigma_{Y}^{2}=1$.
        \begin{equation*}
            f_{X,Y}(x,y)=\frac{1}{2\pi\sqrt{1-\rho^{2}}}\exp\left(-\frac{x^{2}-2\rho xy+y^{2}}{2(1-\rho^{2})}\right)
        \end{equation*}
    \end{seg}

\chapter{Generating function}
\section{Introduction of generating functions}
    A sequence of numbers $a=\{a_{i}:i=0,1,2,\cdots\}$ can hold a significant amount of information. For example, the values of a PMF describe the distribution of discrete random variables.\\
    A concise way to represent this information is by encapsulating the numbers in a generating function. 
    \begin{defn}
        For any sequence $\{a_{n}:n=0,1,2,\cdots\}$, the generating function is defined as
        \begin{equation*}
            G_{a}(s)=\sum_{i=0}^{\infty}a_{i}s^{i}=\lim_{N\uparrow\infty}\sum_{i=0}^{N}a_{i}s^{i}
        \end{equation*}
        for $s\in\mathbb{R}$ if the limit exists. 
    \end{defn}
    \begin{rem}
        It can be observed that
    \begin{equation*}
        a_{i}=\frac{G_{a}^{(i)}(0)}{i!}
    \end{equation*}
    \end{rem}
    \begin{eg}
        In some cases, it is not possible to interchange a countable sum with derivatives.\\
        Let $b_{n}(x)=\frac{\sin{nx}}{n}$ such that $a_{1}(x)=b_{1}(x)$ and $a_{n}(x)=b_{n}(x)-b_{n-1}(x)$.
        \begin{align*}
            \tag{Squeeze Theorem}
            \sum_{n=0}^{\infty}a_{n}(x)&=\lim_{N\uparrow\infty}\sum_{i=0}^{\infty}a_{n}(x)=\lim_{N\uparrow\infty}\frac{\sin{Nx}}{N}=0\\
            \lim_{N\uparrow\infty}\pdv*{\sum_{i=0}^{\infty}a_{i}(x)}{x}&=0\\
            \lim_{N\uparrow\infty}\sum_{i=0}^{N}\pdv*{a_{n}(x)}{x}&=\lim_{N\uparrow\infty}\cos{Nx}\quad \text{does not exist}
        \end{align*}
    \end{eg}
    Convolutions are frequently encountered in probability theory, and generating functions serve as a valuable tool for analyzing them.
    \begin{defn}
        Let $a=\{a_{i}:i\geq 0\}$ and $b=\{b_{i}:i\geq 0\}$ be two sequences of real numbers. The \textbf{convolution} $c=a*b=\{c_{i}:i\geq 0\}$ of $\{a_{i}\}$ and $\{b_{i}\}$ is defined as
        \begin{equation*}
            c_{n}=\sum_{i=0}^{n}a_{i}b_{n-i}
        \end{equation*}
    \end{defn}
    \begin{eg}
        If $a_{n}=f_{X}(n)$ and $b_{n}=f_{Y}(n)$, then $c_{n}=f_{X+Y}(n)$.
    \end{eg}
    \begin{cla}
        If sequences $a$ and $b$ have generating functions $G_{a}(s)$ and $G_{b}(s)$ respectively, then
        \begin{equation*}
            G_{c}(s)=G_{a}(s)G_{b}(s)
        \end{equation*}
    \end{cla}
    \begin{proofing}
        \begin{equation*}
            G_{c}(s)=\sum_{n=0}^{\infty}c_{n}s^{n}=\sum_{n=0}^{\infty}\sum_{i=0}^{n}a_{i}b_{n-i}s^{i}s^{n-i}=\sum_{i=0}^{\infty}a_{i}s^{i}\sum_{n=i}^{\infty}b_{n-i}s^{n-i}=\sum_{i=0}^{\infty}a_{i}s^{i}\sum_{j=0}^{\infty}b_{j}s^{j}=G_{a}(s)G_{b}(s)
        \end{equation*}
    \end{proofing}
    \begin{eg}
        \label{Two independent poisson PGF}
        Suppose that $X\independent Y$. Let $X\sim\Poisson(\lambda)$ and $Y\sim\Poisson(\mu)$. What is the distribution of $Z=X+Y$?\\
        Recall that $f_{Z}=f_{X}*f_{Y}$. We let $a_{n}=f_{X}(n)$ and $b_{n}=f_{Y}(n)$.
        \begin{align*}
            G_{f_{X}}(s)&=\sum_{i=0}^{\infty}\frac{\lambda^{i}e^{-\lambda}}{i!}s^{i}=e^{\lambda(s-1)}\\
            G_{f_{Y}}(s)&=e^{\mu(s-1)}\\
            G_{f_{Z}}(s)&=e^{(\lambda+\mu)(s-1)}
        \end{align*}
    \end{eg}
    Suppose that $X$ is a discrete random variable taking values in the non-negative integers. We can see how the generating function works in probability.
    \begin{defn}
        \textbf{Probability generating function} (PGF) of a non-negative random variable $X$ is
        \begin{equation*}
            G_{X}(s)=\expect s^{X}=\sum_{i=0}^{\infty}s^{i}f_{X}(i)
        \end{equation*}
    \end{defn}
    We can see that the definition is a power series. We may want to know whether the series is convergent.
    \begin{defn}
        \textbf{Radius of convergence} $R$ of power series is the half size of an interval such that the power series $f(s)$ is convergent. If $s\in(-R,R)$, then $f(s)$ is convergent. If $s\in[-R,R]^{\complement}$, then $f(s)$ is divergent.\\
        We can obtain the radius of convergence by applying the root test:
        \begin{equation*}
            R=\frac{1}{\limsup_{n\to\infty}\sqrt[n]{\abs{a_{n}}}}
        \end{equation*}
    \end{defn}
    \begin{rem}
        We need to perform additional tests to find whether the power series converges at $s=-R$ and $s=R$.
    \end{rem}
    \begin{rem}
        Sometimes, it is hard to compute $R$ using the root test. One convenient way to compute $R$ is using the ratio test. If the limit exists,
        \begin{equation*}
            R=\lim_{n\to\infty}\abs{\frac{a_{n}}{a_{n+1}}}
        \end{equation*}
    \end{rem}
    Here are some properties of power series involving the radius of convergence. We will not prove them since the proof is not important.
    \begin{thm}
        If $R$ is the radius of convergence of $G_{a}(s)=\sum_{i=0}^{\infty}a_{i}s^{i}$, then
        \begin{enumerate}
            \item $G_{a}(s)$ converges absolutely for all $\abs{s}<R$ and diverges for all $\abs{s}>R$.
            \item $G_{a}(s)$ can be differentiated or integrated for any fixed number of times term by term if $\abs{s}<R$.
            \begin{equation*}
                \pdv*[order={i}]{\sum_{n=0}^{\infty}a_{n}s^{n}}{s}=\sum_{n=0}^{\infty}\pdv*[order={i}]{a_{n}s^{n}}{s}
            \end{equation*}
            \item If $R>0$ and $G_{a}(s)=G_{b}(s)$ for all $\abs{s}\leq R'$ for some $0<R'\leq R$, then $a_{n}=b_{n}$ for all $n$.
        \end{enumerate}
    \end{thm}
    \begin{rem}
        For any sequence $\{a_{n}:n\geq 0\}$, if the radius of convergence of $G_{a}(s)$ is positive, then $\{a_{n}:n\geq 0\}$ is uniquely determined by $G_{a}(s)$ via
        \begin{equation*}
            a_{n}=\frac{1}{n!}G_{a}^{(n)}(0)
        \end{equation*}
    \end{rem}
    \begin{rem}
        If $a_{n}=f_{X}(n)$ for some random variables $X$, then $R\geq 1$ for $G_{X}(s)=G_{a}(s)$ since
        \begin{equation*}
            \sum_{n=0}^{\infty}f_{X}(n)s^{n}
        \end{equation*}
        converges when $s\in[-1,1]$.
    \end{rem}
    \newpage
    \begin{eg}
        Let $X\sim\Poisson(\lambda)$ and $a_{n}=f_{X}(n)=\frac{\lambda^{n}e^{-\lambda}}{n!}$. By the ratio test,
        \begin{equation*}
            \frac{a_{n}}{a_{n+1}}=\frac{n+1}{\lambda}\to\infty
        \end{equation*}
        Therefore, $R=\infty$.
    \end{eg}
    \begin{eg}
        Let $X$ has a PMF $a_{n}=f_{X}(n)=\frac{c}{n^{2}}$. By the ratio test,
        \begin{equation*}
            \frac{a_{n}}{a_{n+1}}=\frac{(n+1)^{2}}{n}\to 1
        \end{equation*}
        Therefore, $R=1$.
    \end{eg}
    In fact, when $s=1$, we can find the expectation of a distribution.
    \begin{eg}
        \label{Expectation from PGF}
        By having $s=1$,
        \begin{equation*}
            \left.\pdv*{G_{X}(s)}{s}\right|_{s=1}=\left.\pdv*{\sum_{i=0}^{\infty}f_{X}(i)s^{i}}{s}\right|_{s=1}=\left.\sum_{i=0}^{\infty}if_{X}(i)s^{i}\right|=\sum_{i=0}^{\infty}if_{X}(i)=\expect X
        \end{equation*}
    \end{eg}
    There is an important theorem regarding $s=1$. Again, we are not going to prove it.
    \begin{thm}(Abel's Theorem)
        Suppose that $a_{n}\geq 0$ for all $n$. If $a$ has a generating function $G_{a}(s)$ and radius of convergence $R=1$, then if $\sum_{n=0}^{\infty}$ converges in $\mathbb{R}\cup\{\infty\}$, we have
        \begin{equation*}
            \lim_{s\uparrow 1}G_{a}(s)=\sum_{n=0}^{\infty}a_{n}\lim_{s\uparrow 1}s^{n}=\sum_{n=0}^{\infty}a_{n}
        \end{equation*}
    \end{thm}
    \begin{eg}
        We have some PGF of random variable $X$.
        \begin{align*}
            X&\sim\Bern(p) & G_{X}(s)&=ps^{1}+(1-p)s^{0}=1-p+ps\\
            X&\sim\Bin(n,p) & G_{X}(s)&=(1-p+ps)^{n}\\
            X&\sim\Geom(p) & G_{X}(s)&=\sum_{n=1}^{\infty}(1-p)^{n-1}ps^{n}=\frac{ps}{1-s(1-p)}\\
            X&\sim\Poisson(\lambda) & G_{X}(s)&=e^{\lambda(s-1)}
        \end{align*}
    \end{eg}
    We already know that by computing the derivatives of $G$ at $s=0$, we can get the probability sequence. The following theorem shows that we can get the moment sequence by computing the derivatives of $G$ at $s=1$.
    \begin{thm}
        If random variable $X$ has a PGF $G_{X}(s)$, then
        \begin{enumerate}
            \item $\expect X=\lim_{s\uparrow 1}G'(s)=G'(1)$
            \item $\expect(X(X-1)\cdots(X-k+1))=G^{(k)}(1)$
            \item $\Var(X)=G''(1)+G'(1)-(G'(1))^{2}$
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item This is proved in Example \ref{Expectation from PGF}.
            \item Let $s<1$.
            \begin{equation*}
                G^{(k)}(s)=\pdv*[order={k}]{\sum_{n}f_{X}(n)s^{n}}{s}=\sum_{n}n(n-1)\cdots(n-k+1)s^{n-k}f_{X}(n)=\expect(s^{X-k}X(X-1)\cdots(X-k+1))
            \end{equation*}
            By applying Abel's Theorem, we obtain
            \begin{equation*}
                G^{(k)}(1)=\expect(X(X-1)\cdots(X-k+1))
            \end{equation*}
            \item 
            \begin{equation*}
                \Var(X)=\expect(X^{2})-(\expect X)^{2}=\expect(X(X-1))+\expect X-(\expect X)^{2}=G''(1)+G'(1)-(G'(1))^{2}
            \end{equation*}
        \end{enumerate}
    \end{proofing}

    \newpage
    From Example \ref{Two independent poisson PGF}, we can generalize it to study the sum of many other independent discrete random variables.
    \begin{thm}
        \label{Chapter 5 Theorem PGF Sum of random independent variables}
        If $X\independent Y$, then $G_{X+Y}(s)=G_{X}(s)G_{Y}(s)$.
    \end{thm}
    \begin{proofing}
        \begin{equation*}
            G_{X+Y}(s)=\sum_{z=0}^{\infty}\sum_{x=0}^{z}f_{X}(x)f_{Y}(z-x)s^{z}=\sum_{x=0}^{\infty}f_{X}(x)s^{x}\sum_{z=x}^{\infty}f_{Y}(z-x)s^{z-x}=\sum_{x=0}^{\infty}f_{X}(x)\sum_{y=0}^{\infty}f_{Y}(z)s^{y}=G_{X}(s)G_{Y}(s)
        \end{equation*}
    \end{proofing}
    Interestingly, we can also use generating functions to deal with the sum of a random number of independent random variables.
    \begin{thm}
        Let $X_{1},X_{2},\cdots$ be a sequence of independent identically distributed (i.i.d.) random variables with common PGF $G_{X}(s)$ and $N$ be a random variable independent of $X_{i}$ for all $i$ with PGF $G_{N}(s)$. If $T=X_{1}+X_{2}+\cdots+X_{N}$, then
        \begin{equation*}
            G_{T}(s)=G_{N}(G_{X}(s))
        \end{equation*}
    \end{thm}
    \begin{proofing}
        \begin{align*}
            G_{T}(s)=\expect s^{T}=\expect(\expect(s^{T}|N))=\sum_{n}\expect(s^{T}|N=n)\prob(N=n)
            =\sum_{n}\expect(s^{X_{1}+X_{2}+\cdots+X_{n}}|N=n)\prob(N=n)
            &=\sum_{n}(G_{X}(s))^{n}\prob(N=n)\\
            &=G_{N}(G_{X}(s))
        \end{align*}
    \end{proofing}
    \begin{eg}
        The sum of a Poisson number of independent Bernoulli random variables is still Poisson.\\
        Let $G_{N}(t)=e^{\lambda(t-1)}$ and $G_{X}(s)=1-p+ps$.
        \begin{equation*}
            G_{T}(s)=G_{N}(G_{X}(s))=e^{\lambda(1-p+ps-1)}=e^{\lambda p(s-1)}
        \end{equation*}
        Therefore, $T\sim\Poisson(\lambda p)$.
    \end{eg}
    When JPMF exists, there obviously will be a joint PGF.
    \begin{defn}
        Let random variables $X_{1},X_{2}$ be both non-negative integer-valued, jointly discrete with JPMF $f_{X_{1},X_{2}}$.\\
        \textbf{Joint probability generating function} (JPGF) is defined by
        \begin{equation*}
            G_{X_{1},X_{2}}(s_{1},s_{2})=\expect s_{1}^{X_{1}}s_{2}^{X_{2}}=\sum_{i=0}^{\infty}\sum_{j=0}^{\infty}s_{1}^{i}s_{2}^{j}f_{X_{1},X_{2}}(i,j)
        \end{equation*}
    \end{defn}
    \begin{rem}
        We can find that
        \begin{equation*}
            f_{X_{1},X_{2}}(i,j)=\left.\left(\pdv*[order={i}]{\pdv*[order={j}]{\frac{G_{X_{1},X_{2}}(s_{1},s_{2})}{i!j!}}{s_{2}}}{s_{1}}\right)\right|_{(s_{1},s_{2})=(0,0)}
        \end{equation*}
    \end{rem}
    \begin{thm}
        Random variables $X,Y$ are independent if and only if $G_{X,Y}(s,t)=G_{X}(s)G_{Y}(t)$.
    \end{thm}
    \begin{proofing}
        If $X\independent Y$,
        \begin{equation*}
            G_{X,Y}(s,t)=\sum_{i=0}^{\infty}\sum_{j=0}^{\infty}s^{i}t^{j}f_{X,Y}(i,j)=\sum_{i=0}^{\infty}s^{i}f_{X}(i)\sum_{j=0}^{\infty}t^{j}f_{Y}(j)=G_{X}(s)G_{Y}(t)
        \end{equation*}
        If $G_{X,Y}(s,t)=G_{X}(s)G_{Y}(t)$, we consider the coefficient of terms $s^{i}t^{j}$ for all $i\geq 0$ and $j\geq 0$. We can see that
        \begin{equation*}
            f_{X,Y}(i,j)=f_{X}(i)f_{Y}(j)
        \end{equation*}
        Therefore, $X\independent Y$.
    \end{proofing}
    \begin{rem}
        We know that if $X_{1}\independent X_{2}$, then $G_{X_{1}+X_{2}}(s)=\expect s^{X_{1}+X_{2}}=\expect s^{X_{1}}s^{X_{2}}=G_{X_{1}}(s)G_{X_{2}}(s)$. Converse may not be true.
    \end{rem}

\section{Applications of generating functions}
    The following example involves a simple random walk, which is discussed in Appendix \ref{Simple random walk}. Generating functions are particularly valuable when studying random walks. So far, we have only considered random variables $X$ taking finite values. In this application, we encounter variables that can take the value $+\infty$. For such variables $X$, $G_{X}(s)$ converges as long as $\abs{s}<1$ and
    \begin{equation*}
        \lim_{s\uparrow 1}G_{X}(s)=\sum_{k}\prob(X=k)=1-\prob(X=\infty)
    \end{equation*}
    \begin{defn}
        A random variable $X$ is \textbf{defective} if $\prob(X=\infty)>0$.
    \end{defn}
    \begin{rem}
        It is not surprising that the expectation is infinite when the random variable is defective.
    \end{rem}
    With this generalization, we can start discussing random walks.
    \begin{eg}(Recurrence and transience of random walk)
        \label{Simple random walk recurrence and transience}
        Let $S_{n}$ be the position of the particle after $n$ moves, and $X_{i}$ be independent and identically distributed random variables mentioned in Appendix \ref{Simple random walk}. For $n\geq 0$,
        \begin{align*}
            S_{n}&=\sum_{i=1}^{n}X_{i} & S_{0}&=0 & \prob(X_{i}=1)&=p & \prob(X_{i}=-1)&=q=1-p
        \end{align*}
        Let $T_{0}$ be the number of moves until the particle makes its first return to the origin.
        \begin{equation*}
            T_{0}=\min\{i\geq 1:S_{i}=0\}
        \end{equation*}
        Is $T_{0}$ a defective random variable? How do we calculate $\prob(T_{0}=\infty)$?\\
        Let $p_{0}(n)$ be the probability of the particle returning to the origin at $n$ moves, and $P_{0}$ be the generating function of $p_{0}$.\\
        Let $f_{0}(n)$ be the probability of the particle first returning to the origin at $n$ moves, and $F_{0}$ be the generating function of $f_{0}$.
        \begin{align*}
            p_{0}(n)&=\prob(S_{n}=0)=\begin{cases}
                \binom{n}{\frac{n}{2}}p^{\frac{n}{2}}q^{\frac{n}{2}}, &n\text{ is even}\\
                0, &n\text{ is odd}
            \end{cases} & P_{0}(s)&=\lim_{N\uparrow\infty}\sum_{n=0}^{N}p_{0}(n)s^{n}\\
            f_{0}(n)&=\prob(S_{1}\neq 0,S_{2}\neq 0,\cdots,S_{n-1}\neq 0,S_{n}=0)=\prob(T_{0}=n) & F_{0}(s)&=\lim_{N\uparrow\infty}\sum_{n=1}^{N}f_{0}(n)s^{n}
        \end{align*}
    \end{eg}
    \begin{thm}
        \label{Chapter 5 Theorem Simple random walk particle return generating function}
        From the definitions in Example \ref{Simple random walk recurrence and transience}, we have
        \begin{enumerate}
            \item $P_{0}(s)=1+P_{0}(s)F_{0}(s)$
            \item $P_{0}(s)=(1-4pqs^{2})^{-\frac{1}{2}}$
            \item $F_{0}(s)=1-(1-4pqs^{2})^{\frac{1}{2}}$
        \end{enumerate}
    \end{thm}
    \newpage
    \begin{proofing}
        \begin{enumerate}
            \item Let $A_{n}=\{S_{n}=0\}$ and $B_{k}=\{S_{1}\neq 0,S_{2}\neq 0,\cdots,S_{k-1}\neq 0,S_{k}=0\}$. $p_{0}(n)=\prob(A_{n})$ and $f_{0}(k)=\prob(B_{k})$.\\
            By using the Law of total probability, 
            \begin{align*}
                \prob(A_{n})&=\sum_{i=1}^{n}\prob(A_{n}|B_{i})\prob(B_{i})\\
                p_{0}(n)&=\sum_{i=1}^{n}\prob(S_{n}=0|S_{1}\neq 0,S_{2}\neq 0,\cdots,S_{i-1}\neq 0,S_{i}=0)f_{0}(i)\\
                \tag{Markov property in Lemma \ref{Simple random walk properties}}
                &=\sum_{i=1}^{n}\prob(S_{n}=0|S_{i}=0)f_{0}(i)\\
                \tag{Temporarily homogeneous property in Lemma \ref{Simple random walk properties}}
                &=\sum_{i=1}^{n}\prob(S_{n-i}=0)f_{0}(i)\\
                &=\sum_{i=1}^{n}p_{0}(n-k)f_{0}(i)\\
                p_{0}(0)&=1\\
                P_{0}(s)=\sum_{k=0}^{\infty}p_{0}(k)s^{k}=1+\sum_{k=1}^{\infty}p_{0}(k)s^{k}&=1+\sum_{k=1}^{\infty}\sum_{i=1}^{k}p_{0}(k-i)f_{0}(i)s^{k}\\
                &=1+\sum_{i=1}^{\infty}\sum_{k=i}^{\infty}p_{0}(k-i)s^{k-i}f_{0}(i)s^{i}\\
                &=1+P_{0}(s)F_{0}(s)
            \end{align*}
            \item \textit{If you want to understand the proof, search "Central binomial coefficient" in Wikipedia}\\
            We know that $S_{n}=0$ if $n$ is even. Therefore,
            \begin{align*}
                P_{0}(s)=\lim_{N\uparrow\infty}\sum_{n=0}^{N}p_{0}(n)s^{n}&=\lim_{N\uparrow\infty}\sum_{i=0}^{N}\binom{2i}{i}p^{i}q^{i}s^{2i}\\
                \tag{$\binom{\frac{-1}{2}}{i}$ is a generalized binomial coefficient}
                &=\lim_{N\uparrow\infty}\sum_{i=1}^{N}(-1)^{i}4^{i}\binom{\frac{-1}{2}}{i}p^{i}q^{i}s^{2i}\\
                &=\frac{1}{\sqrt{1-4pqs^{2}}}
            \end{align*}
            \item By applying (1) and (2), we can get
            \begin{equation*}
                F_{0}(s)=\frac{P_{0}(s)-1}{P_{0}(s)}=1-\sqrt{1-4pqs^{2}}
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    From this theorem, we can get the following corollary.
    \begin{cor}
        The probability that the particle ever returns to the origin is
        \begin{equation*}
            \sum_{n=1}^{\infty}f_{0}(n)=F_{0}(1)=1-\abs{p-q}
        \end{equation*}
        Probability that the particle will not return to origin ever is
        \begin{equation*}
            \prob(T_{0}=\infty)=\abs{p-q}
        \end{equation*}
    \end{cor}
    \begin{proofing}
        By using Theorem \ref{Chapter 5 Theorem Simple random walk particle return generating function}, since $p+q=1$,
        \begin{equation*}
            F_{0}(1)=1-(1-4pq)^{\frac{1}{2}}=1-(p^{2}-2pq+q^{2})^{\frac{1}{2}}=1-\abs{p-q}
        \end{equation*}
    \end{proofing}
    \begin{rem}
        A random walk is \textbf{recurrent} if it has at least one recurrent point. ($\prob(X<\infty)=1$)\\
        A random walk is \textbf{transient} if it has no recurrent points. ($\prob(X=\infty)>0$)\\
        Notice that when $p=q=\frac{1}{2}$, $\prob(T_{0}=\infty)$ and therefore the random walk is recurrent.\\
        If $p\neq q$, then $\prob(T_{0}=\infty)\neq 0$ and so the random walk is transient.
    \end{rem}
    \begin{eg}
        We use the Example \ref{Simple random walk recurrence and transience} again. How do we calculate $\expect T_{0}$ if $p=q=\frac{1}{2}$?
        \begin{align*}
            F_{0}(s)&=1-\sqrt{1-s^{2}} & F_{0}'(s)&=\frac{s}{\sqrt{1-s^{2}}} & \expect T_{0}&=\lim_{s\uparrow 1}F_{0}'(s)=\infty
        \end{align*}
        This means that although we find that the particle almost certainly returns to the origin, the expectation for the number of steps needed to return to the origin is still infinite.
    \end{eg}
    We move on to our next important application, which is the Branching Process. Many scientists have been interested in reproduction in a population. Accurate models for evolution are extremely difficult to handle, but some non-trivial models are tractable. We will investigate one of the models.
    \begin{eg}(Galton-Watson process) 
        This process investigates a population that evolves in generations.\\
        Let $Z_{n}$ be the number of individuals of the $n$-th generation and $X_{i}^{(m)}$ be the number of offspring of the $i$-th individual of the $m$-th generation. We have:
        \begin{equation*}
            Z_{n+1}=\begin{cases}
                X_{1}^{(n)}+X_{2}^{(n)}+\cdots+X_{Z_{n}}^{(n)}, &Z_{n}\geq 1\\
                0, &Z_{n}=0
            \end{cases}
        \end{equation*}
        We make some following assumptions:
        \begin{enumerate}
            \item Family sizes of the individuals of the branching process form a collection of independent random variables.\\
            ($X_{i}^{(k)}$'s are independent)
            \item All family sizes have the same probability mass function $f$ and generating function $G$. ($X_{i}^{(k)}$'s are identically distributed)
        \end{enumerate}
        Assume that $Z_{0}=1$. Note that $Z_{1}=X_{1}^{(0)}$
    \end{eg}
    \begin{thm}
        \label{Chapter 5 Theorem Galton-Watson process PGF n-fold iterate properties}
        Let $G_{n}(s)=\expect s^{Z_{n}}$ and $G(s)=G_{1}(s)=\expect s^{Z_{1}}=\expect s^{X_{i}^{(m)}}$ for all $i$ and $m$. Then
        \begin{equation*}
            G_{n}(s)=G(G(\cdots(G(s))\cdots))=G(G_{n-1}(s))=G_{n-1}(G(s))
        \end{equation*}
        is the $n$-fold iteration of $G$.\\
        This further implies
        \begin{equation*}
            G_{m+n}(s)=G_{m}(G_{n}(s))=G_{n}(G_{m}(s))
        \end{equation*}
    \end{thm}
    \begin{proofing}
        When $n=2$, 
        \begin{equation*}
            G_{2}(s)=\expect s^{Z_{2}}=\expect s^{X_{1}^{(1)}+X_{2}^{(1)}+\cdots+X_{Z_{1}}^{(1)}}=G_{Z_{1}}\left(G_{X_{1}^{(1)}}(s)\right)=G(G(s))
        \end{equation*}.\\
        When $n=m+1$ for some $m$,
        \begin{equation*}
            G_{m+1}(s)=\expect s^{Z_{m+1}}=\expect s^{X_{1}^{(m)}+X_{2}^{(m)}+\cdots+X_{Z_{m}}^{(m)}}=G_{Z_{m}}\left(G_{X_{1}^{(m)}}(s)\right)=G_{m}(G(s))
        \end{equation*}
    \end{proofing}
    In principle, the above theorem tells us the distribution of $Z_{n}$. However, it may not be easy to compute $G_{n}(s)$.\\
    The moments of $Z_{n}$ can be computed easier.
    \begin{lem}
        Let $\expect Z_{1}=\expect X_{i}^{(m)}=\mu$ and $\Var(Z_{1})=\sigma^{2}$. Then
        \begin{align*}
            \expect Z_{n}&=\mu^{n} & \Var(Z_{n})&=\begin{cases}
                n\sigma^{2}, &\mu=1\\
                \frac{\sigma^{2}(\mu^{n}-1)\mu^{n-1}}{\mu-1}, &\mu\neq 1
            \end{cases}
        \end{align*}
    \end{lem}
    \newpage
    \begin{proofing}
        Using Theorem \ref{Chapter 5 Theorem Galton-Watson process PGF n-fold iterate properties}, we can get
        \begin{align*}
            \expect Z_{2}&=G_{2}'(1)=G'(G(1))G'(1)=G'(1)\mu=\mu^{2}\\
            \expect Z_{n}&=G_{n}'(1)=G'(G_{n-1}(1))G_{n-1}'(1)=G'(1)\mu^{n-1}=\mu^{n}\\
            G_{1}''(1)&=\sigma^{2}+(G'(1))^{2}-G'(1)=\sigma^{2}+\mu^{2}-\mu\\
            G_{2}''(1)&=G''(G(1))(G'(1))^{2}+G'(G(1))G''(1)=G''(1)(\mu^{2}+\mu)\\
            G_{n}''(1)&=G''(G_{n-1}(1))(G_{n-1}'(1))^{2}+G'(G_{n-1}(1))G_{n-1}''(1)\\
            &=(\sigma^{2}+\mu^{2}-\mu)\mu^{2n-2}+\mu G_{n-1}''(1)\\
            &=\mu^{2n-2}(\sigma^{2}+\mu^{2}-\mu)+\mu^{2n-3}(\sigma^{2}+\mu^{2}-\mu)+\cdots+\mu^{n-1}(\sigma^{2}+\mu^{2}-\mu)\\
            &=\frac{\mu^{n-1}(\sigma^{2}+\mu^{2}-\mu)(\mu^{n}-1)}{\mu-1}
        \end{align*}
        If $\mu=1$,
        \begin{equation*}
            \Var(Z_{n})=G_{n}''(1)+G_{n}'(1)-(G_{n}'(1))^{2}=\sigma^{2}+G_{n-1}''(1)+1-1=n\sigma^{2}
        \end{equation*}
        If $\mu\neq 1$,
        \begin{equation*}
            \Var(Z_{n})=G_{n}''(1)+G_{n}'(1)-(G_{n}'(1))^{2}=\frac{\mu^{n-1}(\sigma^{2}+\mu^{2}-\mu)(\mu^{n}-1)}{\mu-1}+\mu^{n}-\mu^{2n}=\frac{\mu^{n-1}\sigma^{2}(\mu^{n}-1)}{\mu-1}
        \end{equation*}
    \end{proofing}
    \begin{eg}
        Does this process eventually lead to extinction?\\
        Note that
        \begin{align*}
            \{\text{ultimate extinction}\}&=\bigcup_{n}\{Z_{n}=0\}=\lim_{n\uparrow\infty}\{Z_{n}=0\}\\
            \prob(\text{ultimate extinction})&=\prob\left(\lim_{n\uparrow\infty}\{Z_{n}=0\}\right)=\lim_{n\uparrow\infty}\prob(Z_{n}=0)=\lim_{n\uparrow\infty}G_{n}(0)
        \end{align*}
        Let $\eta_{n}=G_{n}(0)$ and $\eta=\lim_{n\uparrow\infty}\eta_{n}$.
    \end{eg}
    \begin{thm}
        We have that $\eta$ is the smallest non-negative root of the equation
        \begin{equation*}
            s=G(s)
        \end{equation*}
        Furthermore,
        \begin{enumerate}
            \item $\eta=1$ if $\mu<1$
            \item $\eta<1$ if $\mu>1$
            \item $\eta=1$ if $\mu=1$ and $\sigma^{2}>0$
            \item $\eta=0$ if $\mu=1$ and $\sigma^{2}=0$
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{equation*}
            \eta_{n}=G_{n}(0)=G(G_{n-1}(0))=G(\eta_{n-1})
        \end{equation*}
        We know that $\eta_{n}$ is bounded. Therefore, $\eta_{n}\uparrow\eta$ for some $\eta\in[0,1]$.
        \begin{equation*}
            \eta=\lim_{n\uparrow\infty}\eta_{n}=\lim_{n\uparrow\infty}G(\mu_{n-1})=G\left(\lim_{n\uparrow\infty}\eta_{n-1}\right)=G(\eta)
        \end{equation*}
        Suppose that there exists another non-negative root $\psi$.
        \begin{align*}
            \eta_{1}&=G(0)\leq G(\psi)=\psi\\
            \eta_{2}&=G(\eta_{1})\leq G(\psi)=\psi
        \end{align*}
        By induction, $\eta_{n}\leq\psi$ for all $n$ and therefore $\eta\leq\psi$. Therefore, $\eta$ is the smallest non-negative root of the equation $s=G(s)$.
        \begin{equation*}
            G''(s)=\sum_{i=2}^{\infty}i(i-1)s^{i-2}\prob(Z_{1}=i)\geq 0
        \end{equation*}
        Therefore, $G$ is non-decreasing and also either convex or a straight line.\\
        When $\mu\neq 1$, we can find that two curves $y=G(s)$ and $y=s$ intersect at $s=1$ and $s=k\in\mathbb{R}$.\\
        We know that $\eta\leq 1$ since $\eta$ is the smallest root. In order to intersect at $s=\eta$, $G'(\eta)\leq 1$.\\
        If $\mu=G'(1)<1$, then $\eta=1$. If $\mu=G'(1)>1$, then $\eta=k$ such that $G'(k)\leq 1$.\\
        In the case when $\mu=G'(1)=1$, we need to further analyze whether $y=G(s)$ intersects $y=s$ at $1$ point or infinite points.
        \begin{equation*}
            \sigma^{2}=G''(1)+G'(1)-(G'(1))^{2}=G''(1)
        \end{equation*}
        If $\sigma^{2}=G''(1)>0$, then $\eta=1$. If $\sigma^{2}=G''(1)=0$, then $\eta=0$.
    \end{proofing}

\section{Expectation revisited}
    Recall that the expectations are given respectively by
    \begin{equation*}
        \expect X=\begin{cases}
            \sum xf_{X}(x), &X\text{ is discrete}\\
            \int xf_{X}(x)\,dx, &X\text{ is continuous}
        \end{cases}
    \end{equation*}
    We want a notation which incorporates both these cases. Suppose that $X$ has a CDF $F$. We can rewrite the equations as
    \begin{equation*}
        \expect X=\begin{cases}
            \sum x\,dF_{X}(x), &dF_{X}(x)=F_{X}(x)-\lim_{y\uparrow x}F_{X}(y)=f_{X}(x)\\
            \int x\,dF_{X}(x), &dF_{X}(x)=\pdv{F}{x}\,dx=f_{X}(x)\,dx
        \end{cases}
    \end{equation*}
    Instead of using the regular Riemann integral, which cannot deal with discrete case, we can use the Riemann-Stieltjes integral, which is a generalization of the Riemann integral.
    \begin{align*}
        \intlu{a}{b}g(x)\,dx&=\lim_{\max_{i}\abs{x_{i+1}-x_{i}}}\sum_{i}g(x_{i}^{*})(x_{i+1}-x_{i}) & \intlu{a}{b}g(x)\,dF(x)&=\lim_{\max_{i}\abs{x_{i+1}-x_{i}}}\sum_{i}g(x_{i}^{*})(F(x_{i+1})-F(x_{i}))
    \end{align*}
    if the limit does not depend on the choice of $x_{i}^{*}\in[x_{i},x_{i+1})$.
    \begin{defn}
        \textbf{Expectation} of a random variable $X$ is given by:
        \begin{equation*}
            \expect X=\int x\,dF_{X}
        \end{equation*}
    \end{defn}
    \begin{lem}
        If $g:\mathbb{R}\to\mathbb{R}$ such that $g(X)$ is also a random variable, then
        \begin{equation*}
            \expect(g(X))=\int g(x)\,dF_{X}
        \end{equation*}
    \end{lem}
    \begin{eg}
        If $g$ is regular (differentiable at every point and every values in the domain maps to a value in range), then
        \begin{equation*}
            \sum_{i}g(x_{i}^{*})(F(x_{i+1}-F(x_{i}))\approx\sum_{i}g(x_{i}^{*})f(x_{i}^{*})(x_{i+1}-x_{i})\approx\int g(x)f(x)\,dx
        \end{equation*}
    \end{eg}
    \begin{eg}
        In irregular case, assume that the function $g$ is the Dirichlet function. That is
        \begin{align*}
            \mathbf{1}_{\mathbb{Q}}(x)&=\begin{cases}
                1, &x\in\mathbb{Q}\\
                0, &x\not\in\mathbb{Q}
            \end{cases} & \sum_{i}g(x_{i}^{*})(F(x_{i+1})-F(x_{i}))=\sum_{i}g(x_{i}^{*})(x_{i+1}-x_{i})
        \end{align*}
        Since the limit depends on the choice of $x_{i}^{*}$, Riemann-Stieltjes integral of $\mathbf{1}_{\mathbb{Q}}(x)$ with respect to $F(x)=x$ is not well defined.\\
        Therefore, $\expect\mathbf{1}_{\mathbb{Q}}(X)$ cannot be defined as a Riemann-Stieltjes integral. However, on the other hand,
        \begin{equation*}
            \expect\mathbf{1}_{\mathbb{Q}}(X)=\prob(\mathbf{1}_{\mathbb{Q}}(x)=1)=\prob\circ X^{-1}(\mathbb{Q}\cap[0,1])=0
        \end{equation*}
    \end{eg}
    With this notation, we can also change how we define PGF.
    \begin{defn}
        \textbf{Probability generating function} of a random variable $X$ is given by:
        \begin{equation*}
            \expect s^{X}=\int s^{x}\,dF_{X}
        \end{equation*}
    \end{defn}
    
\section{Moment generating function and Characteristic function}
    Now that we have unified the notations, we can now properly apply the probability generating function.\\
    For a more general variables $X$, it is best if we substitute $s=e^{t}$. We get the following definition.
    \begin{defn}
        \textbf{Moment generating function} (MGF) of a random variable $X$ is the function $M:\mathbb{R}\to[0,\infty)$ given by:
        \begin{equation*}
            M_{X}(t)=\expect(e^{tX})=\int e^{tx}\,dF_{X}
        \end{equation*}
    \end{defn}
    \begin{rem}
        The definition of MGF only requires replacing $s$ by $e^{t}$ in PGF. MGF is easier for computing moments, but less convenient for computing distribution.
    \end{rem}
    \begin{rem}
        MGFs are related to Laplace transforms.
    \end{rem}
    We can easier get the following lemma.
    \begin{lem}
        Given a MGF $M_{X}(t)$ of a random variable $X$. 
        \begin{enumerate}
            \item For any $k\geq 0$,
            \begin{equation*}
                \expect X^{k}=M^{(k)}(0)
            \end{equation*}
            \item The function $M$ can be expanded via Taylor's Theorem within its radius of convergence.
            \begin{equation*}
                M(t)=\sum_{i=0}^{\infty}\frac{\expect X^{k}}{k!}t^{k}
            \end{equation*}
            \item If $X$ and $Y$ are independent, then
            \begin{equation*}
                M_{X+Y}(t)=M_{X}(t)M_{Y}(t)
            \end{equation*}
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item 
            \begin{equation*}
                M^{(k)}(0)=\left.\pdv*[order={k}]{\int e^{tx}\,dF_{X}(x)}{t}\right|_{t=0}=\left.\int x^{k}e^{tx}\,dF_{X}(x)\right|_{t=0}=\int x^{k}\,dF_{X}(x)=\expect X^{k}
            \end{equation*}
            \item Just using (1) and Taylor's Theorem and you get the answer.
            \item This is just Theorem \ref{Chapter 5 Theorem PGF Sum of random independent variables}.
        \end{enumerate}
    \end{proofing}
    \begin{rem}
        $M_{X}(0)=1$ for all random variables $X$.
    \end{rem}
    \begin{eg}
        Let $X\sim\Exp(1)$. For all $x>0$, if $t<1$,
        \begin{align*}
            f_{X}(x)&=e^{-x} & M_{X}(t)&=\intlu{0}{\infty}e^{tx}\,dF_{X}(x)=\intlu{0}{\infty}e^{(t-1)x}\,dx=\frac{1}{1-t}
        \end{align*}
    \end{eg}
    \begin{eg}
        Let $X\sim\Cauchy$.
        \begin{align*}
            f_{X}(x)&=\frac{1}{\pi(1+x^{2})} & M_{X}(t)&=\frac{1}{\pi}\intinfty\frac{e^{tx}}{1+x^{2}}\,dx
        \end{align*}
        $M_{X}(t)$ exists only at $t=0$. We get $M_{X}(0)=1$.
    \end{eg}
    \newpage
    Moment generating functions provide a useful technique but the integrals used to define may not be finite. There is another class of functions which finiteness is guaranteed.
    \begin{defn}
        \textbf{Characteristic function} (CF) of a random variable $X$ is the function $\phi_{X}:\mathbb{R}\to\mathbb{C}$ given by:
        \begin{align*}
            \phi_{X}(t)&=\expect(e^{itX})=\int e^{itx}\,dF_{X}(x)=\expect\cos(tX)+i\expect\sin(tX) & i&=\sqrt{-1}
        \end{align*}
    \end{defn}
    \begin{rem}
        $\phi_{X}(t)$ is essentially a Fourier Transform.
    \end{rem}
    \begin{lem}
        CF $\phi_{X}$ of a random variable $X$ has the following properties:
        \begin{enumerate}
            \item $\phi_{X}(0)=1$. $\abs{\phi_{X}(t)}\leq 1$ for all $t$
            \item $\phi_{X}(t)$ is uniformly continuous
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item For all $t$,
            \begin{align*}
                \phi_{X}(0)&=\int\,dF_{X}(x)=1\\
                \abs{\phi_{X}(t)}&=\abs{\int(\cos(tx)+i\sin(tx))\,dF_{X}(x)}\leq\int\abs{\cos(tx)+i\sin(tx)}\,dF_{X}(x)=\int\,dF_{X}(x)=1
            \end{align*}
            \item
            \begin{equation*}
                \sup_{t}\abs{\phi_{X}(t+c)-\phi_{X}(t)}=\sup_{t}\abs{\int(e^{i(t+c)x}-e^{itx})\,dF_{X}(x)}\leq\sup_{t}\left(\int\abs{e^{itx}}\abs{e^{icx-1}}\,dF_{X}(x)\right)
            \end{equation*}
            When $c\downarrow 0$, the supremum $\to 0$. Therefore, $\phi_{X}(t)$ is uniformly continuous.
        \end{enumerate}
    \end{proofing}
    \begin{thm}
        There are some properties of $\phi_{X}$ of a random variable $X$ regarding derivatives and moments.
        \begin{enumerate}
            \item If $\phi_{X}^{(k)}(0)$ exists, then
            \begin{equation*}
                \begin{cases}
                    \expect\abs{X}^{k}<\infty, &k\text{ is even}\\
                    \expect\abs{X}^{k-1}<\infty, &k\text{ is odd}
                \end{cases}
            \end{equation*}
            \item If $\expect\abs{X}^{k}<\infty$, then $\phi_{X}^{(k)}(0)$ exists. We have
            \begin{equation*}
                \phi_{X}(t)=\sum_{j=0}^{k}\frac{\phi_{X}^{(j)}(0)}{j!}t^{j}+o(t^{k})=\sum_{j=0}^{k}\frac{\expect X^{j}}{j!}(it)^{j}+o(t^{k})
            \end{equation*}
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        We use the Taylor's Theorem.
        \begin{equation*}
            \phi_{X}(t)=\sum_{j=0}^{k}\frac{\phi_{X}^{(j)}(0)}{j!}t^{j}+o(t^{k})=\sum_{j=0}^{k}\frac{\expect X^{j}}{j!}(it)^{j}+o(t^{k})
        \end{equation*}
        \begin{enumerate}
            \item 
            \begin{equation*}
                \phi_{X}^{(k)}(0)=i^{k}\expect X^{k}
            \end{equation*}
            If $k$ is even, we have $\phi_{X}^{(k)}(0)=(-1)^{\frac{k}{2}}\expect X^{k}=(-1)^{\frac{k}{2}}\expect\abs{X}^{k}$ exists. Therefore, $\expect\abs{X}^{k}<\infty$.\\
            If $k$ is odd, we know that $\phi_{X}^{(k-1)}(0)$ exists if $\phi_{X}^{(k)}(0)$ exists.\\
            Therefore, with $\phi_{X}^{(k-1)}(0)=(-1)^{\frac{k-1}{2}}\expect X^{k-1}=(-1)^{\frac{k-1}{2}}\expect\abs{X}^{k-1}$, $\expect\abs{X}^{k-1}<\infty$.
            \item Again using the formula in (1). We have
            \begin{equation*}
                \frac{\phi_{X}^{(k)}(0)}{i^{k}}=\expect X^{k}\leq\expect\abs{X}^{k}<\infty
            \end{equation*}
            Therefore, $\phi_{X}^{(k)}(0)$ exists. The formula can be obtained from the Taylor's theorem formula.
        \end{enumerate}
    \end{proofing}
    \begin{thm}
        If $X\independent Y$, then $\phi_{X+Y}(t)=\phi_{X}(t)\phi_{Y}(t)$
    \end{thm}
    \begin{proofing}
        \begin{equation*}
            \phi_{X+Y}(t)=\expect(e^{it(X+Y)})=\expect(e^{itX})\expect(e^{itY})=\phi_{X}(t)\phi_{Y}(t)
        \end{equation*}
    \end{proofing}
    Again and again, we have a joint characteristic function.
    \begin{defn}
        \textbf{Joint characteristic function} (JCF) $\phi_{X,Y}$ of two random variables $X,Y$ is given by
        \begin{equation*}
            \phi_{X,Y}(s,t)=\expect(e^{i(sX+tY)})
        \end{equation*}
    \end{defn}
    We have another way to prove that two random variables are independent.
    \begin{thm}
        \label{Chapter 5 Theorem Independent via CF}
        Two random variables $X,Y$ are independent if and only if for all $s$ and $t$,
        \begin{equation*}
            \phi_{X,Y}(s,t)=\phi_{X}(s)\phi_{Y}(t)
        \end{equation*}
    \end{thm}
    \begin{proofing}
        If $X\independent Y$,
        \begin{equation*}
            \phi_{X,Y}(s,t)=\expect(e^{i(sX+tY)})=\expect(e^{isX})\expect(e^{itY})=\phi_{X}(s)\phi_{Y}(t)
        \end{equation*}
        Currently, it is not suffice to prove the inverse. We will need to use a theorem later. (Example \ref{Chapter 5 Example Proof of Theorem 5.27})
    \end{proofing}
    \begin{eg}
        Let $X\sim\Bern(p)$. We have
        \begin{equation*}
            \phi_{X}(t)=\expect(e^{itX})=q+pe^{it}
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $X\sim\Bin(n,p)$. We have
        \begin{equation*}
            \phi_{X}(t)=(q+pe^{it})^{n}
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $X\sim\Exp(1)$. We have
        \begin{equation*}
            \phi_{X}(t)=\int e^{(it-1)x}\,dx=\frac{1}{1-it}
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $X\sim\Cauchy$. We have
        \begin{equation*}
            \phi_{X}(t)=e^{-\abs{t}}
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $X\sim\N(\mu,\sigma^{2})$. Using the fact that for any $u\in\mathbb{C}$, not just in $\mathbb{R}$,
        \begin{equation*}
            \frac{1}{\sqrt{2\pi\sigma^{2}}}\intinfty\exp\left(-\frac{(x-u)^{2}}{2\sigma^{2}}\right)\,dx=1
        \end{equation*}
        We have
        \begin{align*}
            \phi_{X}(t)&=\frac{1}{\sqrt{2\pi\sigma^{2}}}\intinfty e^{itx}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right)\,dx\\
            &=\frac{1}{\sqrt{2\pi\sigma^{2}}}\intinfty\exp\left(-\frac{x^{2}-(2\mu+2\sigma^{2}it)x+\mu^{2}}{2\sigma^{2}}\right)\,dx\\
            &=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(\frac{(\mu+\sigma^{2}it)^{2}-\mu^{2}}{2\sigma^{2}}\right)\intinfty\exp\left(-\frac{(x-(\mu+\sigma^{2}it))^{2}}{2\sigma^{2}}\right)\,dx\\
            &=\exp\left(\frac{\mu^{2}+2\sigma^{2}i\mu t-\sigma^{4}t^{2}-\mu^{2}}{2\sigma^{2}}\right)\\
            &=\exp\left(i\mu t-\frac{1}{2}\sigma^{2}t^{2}\right)
        \end{align*}
    \end{eg}
    \begin{rem}
        We have a function called \textbf{cumulant generating function} defined by $\log\phi_{X}(t)$. Normal distribution is the only distribution we have learnt whose cumulant generating function has finite terms, which is:
        \begin{equation*}
            \log\phi_{X}(t)=i\mu t-\frac{1}{2}\sigma^{2}t^{2}
        \end{equation*}
    \end{rem}
    \newpage

\section{Inversion and continuity theorems}
    There are two major ways that characteristic functions are useful. One of them is that we can use characteristic function of a random variable to generate a probability density function of that random variable.
    \begin{thm}(Fourier Inverse Transform for continuous case)
        If a random variable $X$ is continuous with a PDF $f_{X}$ and a CF $\phi_{X}$, then
        \begin{equation*}
            f_{X}(x)=\frac{1}{2\pi}\intinfty e^{-itx}\phi_{X}(t)\,dt
        \end{equation*}
        at all point $x$ which $f_{X}$ is differentiable.\\
        If $X$ has a CDF $F_{X}$, then
        \begin{equation*}
            F_{X}(b)-F_{X}(a)=\frac{1}{2\pi}\intinfty\intlu{a}{b}e^{-itx}\phi_{X}(t)\,dx\,dt
        \end{equation*}
    \end{thm}
    \begin{proofing}
        We give you a non-rigorous proof. Let
        \begin{align*}
            I(x)&=\frac{1}{2\pi}\intinfty e^{itx}\phi_{X}(t)\,dt=\frac{1}{2\pi}\intinfty e^{-itx}\intinfty e^{ity}f_{X}(y)\,dy\,dt\\
            I_{\varepsilon}(x)&=\frac{1}{2\pi}\intinfty e^{-itx}\intinfty e^{ity}f_{X}(y)\,dy\,e^{-\frac{1}{2}\varepsilon^{2}t^{2}}\,dt
        \end{align*}
        We want to show that $I_{\varepsilon}(x)\to I(x)$ when $\varepsilon\downarrow 0$.
        \begin{align*}
            I_{\varepsilon}(x)&=\frac{1}{2\pi}\intinfty\intinfty e^{-\frac{1}{2}\varepsilon^{2}t^{2}+i(y-x)t}f_{X}(y)\,dt\,dy\\
            &=\frac{1}{\sqrt{2\pi\varepsilon^{2}}}\left(\frac{1}{\sqrt{2\pi\frac{1}{\varepsilon^{2}}}}\right)\intinfty\exp\left(-\frac{(y-x)^{2}}{2\varepsilon^{2}}\right)f_{X}(y)\intinfty\exp\left(-\frac{-\left(t-i\frac{y-x}{\varepsilon}\right)^{2}}{2\left(\frac{1}{\varepsilon^{2}}\right)}\right)\,dt\,dy\\
            &=\frac{1}{\sqrt{2\pi\varepsilon^{2}}}\intinfty\exp\left(-\frac{(y-x)^{2}}{2\epsilon}\right)f_{X}(y)\,dy
        \end{align*}
        Let $Z\sim\N(0,1)$ and $Z_{\varepsilon}=\varepsilon Z$. $I_{\varepsilon}(x)$ is the PDF of $\varepsilon Z+X$. Therefore, we can say that $f_{\varepsilon Z+X}(x)\to f_{X}(x)$ when $\varepsilon\downarrow 0$.\\
        Note that this proof is not rigorous.
    \end{proofing}
    \begin{thm}(Inversion Theorem)
        If a random variable $X$ have a CDF $F_{X}$ and a CF $\phi_{X}$, we define $\overline{F}_{X}:\mathbb{R}\to[0,1]$ by
        \begin{equation*}
            \overline{F}_{X}(x)=\frac{1}{2}\left(F_{X}(x)+F_{X}(x^{-})\right)
        \end{equation*}
        Then for all $a\leq b$,
        \begin{equation*}
            \overline{F}_{X}(b)-\overline{F}_{X}(a)=\intinfty\frac{e^{-iat}-e^{-ibt}}{2\pi it}\phi_{X}(t)\,dt
        \end{equation*}
    \end{thm}
    \begin{rem}
        We can say $\overline{F}_{X}$ represents the average of limit going from two directions.
    \end{rem}
    \begin{eg}
        \label{Chapter 5 Example Proof of Theorem 5.27}
        With the Inversion Theorem, we can now prove Theorem \ref{Chapter 5 Theorem Independent via CF}.\\
        Given two random variables $X,Y$. We want to first extend the Fourier Inverse Transform into multivariable case.\\
        If $\phi_{X,Y}(s,t)=\phi_{X}(s)\phi_{Y}(t)$, then for any $a\leq b$ and $c\leq d$,
        \begin{align*}
            \overline{F}_{X,Y}(b,d)-\overline{F}_{X,Y}(b,c)-\overline{F}_{X,Y}(a,d)+\overline{F}_{X,Y}(a,c)&=\intinfty\intinfty\frac{(e^{-ias}-e^{-ibs})(e^{-ict}-e^{-idt})}{-4\pi^{2}t^{2}}\phi_{X}(s)\phi_{Y}(t)\,ds\,dt\\
            &=(\overline{F}_{X}(b)-\overline{F}_{X}(a))\intinfty\frac{e^{-ict}-e^{-idt}}{2\pi it}\phi_{Y}(t)\,dt\\
            &=(\overline{F}_{X}(b)-\overline{F}_{X}(a))(\overline{F}_{Y}(d)-\overline{F}_{Y}(c))\\
            &=\overline{F}_{X}(b)\overline{F}_{Y}(d)-\overline{F}_{X}(b)\overline{F}_{Y}(c)-\overline{F}_{X}(a)\overline{F}_{Y}(d)+\overline{F}_{X}(a)\overline{F}_{Y}(c)
        \end{align*}
        From the definition of independent random variables, we prove that $X\independent Y$ if $\phi_{X,Y}(s,t)=\phi_{X}(s)\phi_{Y}(t)$.
    \end{eg}
    \newpage
    Another way is to evaluate the convergence of a sequence of cumulative distribution function.
    \begin{defn}(Convergence of distribution function sequence [Weak convergence])
        A sequence of CDF 
        $F_{1},F_{2},\cdots$ \textbf{converges} to a CDF $F$, written as $F_{n}\to F$, if at each point $x$ where $F$ is continuous,
        \begin{equation*}
            F_{n}(x)\to F(x)
        \end{equation*}
    \end{defn}
    \begin{eg}
        Assume we have two sequences of CDF.
        \begin{align*}
            F_{n}(x)&=\begin{cases}
                0, &x<\frac{1}{n}\\
                1, & x\geq\frac{1}{n}
            \end{cases} & G_{n}(x)&=\begin{cases}
                0, &x<-\frac{1}{n}\\
                1, & x\geq-\frac{1}{n}
            \end{cases}
        \end{align*}
        If we have $n\to\infty$, we get
        \begin{align*}
            F(x)&=\begin{cases}
                0, &x\leq 0\\
                1, &x>0
            \end{cases} & G(x)&=\begin{cases}
                0, &x<0\\
                1, &x\geq 0
            \end{cases}
        \end{align*}
        This is problematic because $F(x)$ in this case is not a distribution function because it is not right-continuous.\\
        Therefore, it is needed to define the convergence so that both sequences $\{F_{n}\}$ and $\{G_{n}\}$ have the same limit.
    \end{eg}
    We can modify a bit on the definition to say each distribution function in the sequence represents a different random variable.
    \begin{defn}(Convergence in distribution for random variables)
        Let $X,X_{1},X_{2},\cdots$ be a family of random variables with PDF $F,F_{1},F_{2},\cdots$, we say $X_{n}\to X$, written as $X_{n}\xrightarrow{D}X$ or $X_{n}\Rightarrow X$, if $F_{n}\to F$.
    \end{defn}
    \begin{rem}
        For this convergence definition, we do not care about the closeness of $X_{n}$ and $X$ as functions of $\omega$.
    \end{rem}
    \begin{rem}
        Sometimes, we also write $X_{n}\Rightarrow F$ or $X_{n}\xrightarrow{D}F$.
    \end{rem}
    With the definition, sequence of characteristic functions can be used to determine whether the sequence of cumulative distribution function converges.
    \begin{thm}(L\'evy continuity theorem)
        \label{Chapter 5 Theorem Levy continuity theorem}
        Suppose that $F_{1},F_{2},\cdots$ is a sequence of CDF with CF $\phi_{1},\phi_{2},\cdots$, then
        \begin{enumerate}
            \item If $F_{n}\to F$ for some CDF $F$ with CF $\phi$, then $\phi_{n}\to\phi$ pointwise.
            \item If $\phi_{n}\to\phi$ pointwise for some CF $\phi$, and $\phi$ is continuous at $O$ ($t=0$), then $\phi$ is the CF of some CDF $F$ and $F_{n}\to F$.
        \end{enumerate}
    \end{thm}
    We have a more general definition of convergence.
    \begin{defn}(\textbf{Vague convergence})
        Given a sequence of CDF $F_{1},F_{2},\cdots$. Suppose that $F_{n}(x)\to G(x)$ at all continuity point of $G$ but $G$ may not be a CDF. Then we say $F_{n}\to G$ \textbf{vaguely}, written as $F_{n}\xrightarrow{v}G$.
    \end{defn}
    \begin{eg}
    If
        \begin{align*}
            F_{n}(x)&=\begin{cases}
                0, &x<\frac{1}{n}\\
                \frac{1}{2}, &\frac{1}{n}\leq x<n\\
                1, &x\geq n
            \end{cases} & G(x)&=\begin{cases}
                0, &x<0\\
                \frac{1}{2}, &x\geq 0
            \end{cases}
        \end{align*}
        We can see that $F_{n}\xrightarrow{v}G$ if $n\to\infty$ and $G$ is not a CDF.
    \end{eg}
    \begin{rem}
        In Theorem \ref{Chapter 5 Theorem Levy continuity theorem} (2), the statement that $\phi$ is continuous at $O$ can be replaced by any of the following statements:
        \begin{enumerate}
            \item $\phi(t)$ is a continuous function of $t$
            \item $\phi(t)$ is a CF of some CDF
            \item The sequence $\{F_{n}\}_{n=1}^{\infty}$ is tight, i.e. for all $\epsilon>0$, there exists $M_{\epsilon}>0$ such that
            \begin{equation*}
                \sup_{n}(F_{n}(-M_{\epsilon})+1-F_{n}(M_{\epsilon}))\leq\epsilon
            \end{equation*}
        \end{enumerate}
    \end{rem}
    \begin{eg}
        Let $X_{n}\sim\N(0,n^{2})$ and let $\phi_{n}$ be the CF of $X_{n}$. Then
        \begin{equation*}
            \phi_{n}(t)=\exp\left(-\frac{1}{2}n^{2}t^{2}\right)\to\phi(t)=\begin{cases}
                0, &t\neq 0\\
                1, &t=0
            \end{cases}
        \end{equation*}
    \end{eg}

\section{Two limit theorems}
    In this section, we introduce two fundamental theorems in probability theory: the Law of Large Numbers and the Central Limit Theorem.
    \begin{thm}(\textbf{Weak Law of Large Numbers} [WLLN])
        Let $X_{1},X_{2},\cdots$ be i.i.d. random variables. Assume that $\expect\abs{X_{1}}<\infty$ and $\expect X_{1}=\mu$. Let $S_{n}=\sum_{i=1}^{n}X_{i}$. Then
        \begin{equation*}
            \frac{1}{n}S_{n}\xrightarrow{D}\mu
        \end{equation*}
    \end{thm}
    \begin{proofing}
        We recall the Taylor expansion of $\phi_{\xi}(s)$ at $0$. If $\expect\abs{\xi}^{k}<\infty$ and $s$ is small, then
        \begin{equation*}
            \phi_{\zeta}(s)=\sum_{j=0}^{k}\frac{\expect\xi^{j}}{j!}(is)^{j}+o(s^{k})
        \end{equation*}
        For any $t\in\mathbb{R}$, let $\phi_{X_{1}}(s)=\expect(e^{isX_{1}})$.
        \begin{align*}
            \phi_{n}(t)=\expect\left(\exp\left(\frac{it}{n}S_{n}\right)\right)=\expect\left(\prod_{i=1}^{n}\exp\left(\frac{itX_{i}}{n}\right)\right)=\left(\expect\left(\exp\left(\frac{itX_{1}}{n}\right)\right)\right)^{n}=\left(\phi_{X_{1}}\left(\frac{t}{n}\right)\right)^{n}&=\left(1+\frac{it}{n}\expect X_{1}+o\left(\frac{t}{n}\right)\right)^{n}\\
            &=\left(1+\frac{i\mu t}{n}+o\left(\frac{t}{n}\right)\right)^{n}\\
            &\to e^{i\mu t}
        \end{align*}
        By L\'evy continuity theorem, we get that $\frac{1}{n}S_{n}\xrightarrow{D}\mu$.
    \end{proofing}
    \begin{thm}(\textbf{Central Limit Theorem} [CLT])
        Let $X_{1},X_{2},\cdots$ be i.i.d. random variables with $\expect\abs{X_{1}}^{2}<\infty$ and $\expect X_{1}=\mu$, $\Var(X_{1})=\sigma^{2}$, $S_{n}=\sum_{i=1}^{n}X_{i}$. Then
        \begin{equation*}
            \frac{1}{\sigma}\sqrt{n}\left(\frac{1}{n}S_{n}-\mu\right)=\frac{S_{n}-n\mu}{\sqrt{n}\sigma}\xrightarrow{D}\N(0,1)
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Let $Y_{i}=\frac{X_{i}-\mu}{\sigma}$. We have $\expect Y_{i}=0$ and $\Var(Y_{i})=1$.
        \begin{align*}
            \frac{S_{n}-n\mu}{\sqrt{n}\sigma}&=\sum_{i=1}^{n}\frac{1}{\sqrt{n}}\frac{X_{i}-\mu}{\sigma}=\sum_{i=1}^{n}\frac{Y_{i}}{\sqrt{n}}\\
            \phi_{n}(t)&=\expect\left(\exp\left(it\sum_{\ell-1}^{n}\frac{Y_{\ell}}{\sqrt{n}}\right)\right)\\
            &=\left(\expect\left(\exp\left(\frac{itY_{1}}{\sqrt{n}}\right)\right)\right)^{n}\\
            &=\left(\phi_{Y_{1}}\left(\frac{t}{\sqrt{n}}\right)\right)^{n}\\
            \tag{Taylor expansion}
            &=\left(1+\frac{it}{\sqrt{n}}\expect Y_{1}+\frac{1}{2}\left(\frac{it}{\sqrt{n}}\right)^{2}\expect(Y_{i}^{2})+o\left(\frac{t^{2}}{n}\right)\right)^{n}\\
            &=\left(1-\frac{t^{2}}{2n}+o\left(\frac{t^{2}}{n}\right)\right)^{n}\\
            &\to e^{-\frac{1}{2}t^{2}}
        \end{align*}
        By L\'evy continuity theorem, $\frac{S_{n}-n\mu}{\sqrt{n}\sigma}\xrightarrow{D}\N(0,1)$.
    \end{proofing}
    Central Limit Theorem can be generalized in several directions, one of which concerns independent random variables instead of i.i.d. random variables.
    \begin{thm}
        Let $X_{1},X_{2},\cdots$ be independent random variables satisfying $\expect X_{i}=0$, $\Var(X_{i})=\sigma_{i}^{2}$, $\expect\abs{X_{i}}^{3}<\infty$ and such that
        \begin{equation*}
            \tag{*}
            \frac{1}{(\sigma(n))^{3}}\sum_{i=1}^{n}\expect\abs{X_{i}^{3}}\to 0 \text{ as }n\to\infty
        \end{equation*}
        where $(\sigma(n))^{2}=\Var(\sum_{i=1}^{n}X_{i})=\sum_{i=1}^{n}\sigma_{i}^{2}$. Then
        \begin{equation*}
            \frac{1}{\sigma(n)}\sum_{i=1}^{n}X_{i}\xrightarrow{D}\N(0,1)
        \end{equation*}
    \end{thm}
    \begin{rem}
        The condition (*) means that none of the random variables $X_{i}$ can be significant in the sum $S_{n}$.
        \begin{equation*}
            \frac{1}{(\sigma(n))^{3}}\sum_{i=1}^{n}\abs{X_{i}}^{3}\lesssim\frac{1}{\sigma(n)}\max_{i=1,2,\cdots,n}\abs{X_{i}}\left(\frac{1}{(\sigma(n))^{2}}\right)\sum_{i=1}^{n}(X_{i})^{2}\approx\frac{1}{\sigma(n)}\max_{i=1,2,\cdots,n}\abs{X_{i}}\to 0
        \end{equation*}
    \end{rem}

\chapter{Markov chains (Skipped, read the book for reference)}

\chapter{Convergence of Random Variables}
    In Chapter $5$, we discussed convergence in distribution. However, this is not the only significant mode of convergence for random variables. In this chapter, we will explore other modes of convergence.
\section{Modes of Convergence}
    We will discuss various modes of convergence for a sequence of random variables.\\
    Let us first recall the convergence modes for real functions. Let $f,f_{1},f_{2},\cdots:[0,1]\to\mathbb{R}$.
    \begin{enumerate}
        \item Pointwise Convergence\\
        We say $f_{n}\to f$ pointwise if, for all $x\in[0,1]$,
        \begin{equation*}
            f_{n}(x)\to f(x)\text{ as }n\to\infty
        \end{equation*}
        \item Convergence in Norm $\norm{\cdot}$\\
        We say $f_{n}\to f$ in norm $\norm{\cdot}$ if
        \begin{equation*}
            \norm{f_{n}-f}\to 0\text{ as }n\to\infty
        \end{equation*}
        \item Convergence in Lebesgue (Uniform) Measure\\
        We say $f_{n}\to f$ in uniform measure $\mu$ if, for all $\epsilon>0$,
        \begin{equation*}
            \mu\left(\{x\in[0,1]:\abs{f_{n}(x)-f(x)}>\epsilon\}\right)\to 0\text{ as }n\to\infty
        \end{equation*}
    \end{enumerate}
    These definitions can be extended to define convergence modes for random variables.
    \begin{defn}(\textbf{Almost Sure Convergence})
        We say $X_{n}\to X$ \textbf{almost surely}, denoted as $X_{n}\xrightarrow{\text{a.s.}}X$, if
        \begin{align*}
            \prob(\{\omega\in\Omega:X_{n}(\omega)\to X(\omega)\text{ as }n\to\infty\})&=1 & &\text{or} & \prob(\{\omega\in\Omega:X_{n}(\omega)\not\to X(\omega)\text{ as }n\to\infty\})&=0
        \end{align*}
    \end{defn}
    \begin{rem}
        $X_{n}\xrightarrow{\text{a.s.}}X$ is an adaptation of pointwise convergence for functions.
    \end{rem}
    \begin{rem}
        Almost sure convergence is often referred to as:
        \begin{enumerate}
            \item $X_{n}\to X$ almost everywhere ($X_{n}\xrightarrow{\text{a.e.}}X$)
            \item $X_{n}\to X$ with probability $1$ ($X_{n}\to X$ w.p. $1$)
        \end{enumerate}
    \end{rem}
    \begin{defn}(Convergence in $r$-th Mean)
        Let $r\geq 1$. We say $X_{n}\to X$ \textbf{in $r$-th mean}, denoted as $X_{n}\xrightarrow{r}X$, if
        \begin{equation*}
            \expect\abs{X_{n}-X}^{r}\to 0\text{ as }n\to\infty
        \end{equation*}
    \end{defn}
    \begin{eg}
        If $r=1$, we say $X_{n}\to X$ in mean or expectation.\\
        If $r=2$, we say $X_{n}\to X$ in mean square.
    \end{eg}
    \begin{defn}(Convergence in Probability)
        We say $X_{n}\to X$ \textbf{in probability}, denoted as $X_{n}\xrightarrow{\prob}X$, if, for all $\varepsilon>0$,
        \begin{equation*}
            \prob(\abs{X_{n}-X}>\varepsilon)\to 0\text{ as }n\to\infty
        \end{equation*}
    \end{defn}
    \begin{defn}(Convergence in Distribution)
        We say $X_{n}\to X$ \textbf{in distribution}, denoted as $X_{n}\xrightarrow{D}X$, if, at continuity points of $\prob(X\leq x)$,
        \begin{equation*}
            F_{n}(x)=\prob(X_{n}\leq x)\to\prob(X\leq x)=F(x)\text{ as }n\to\infty
        \end{equation*}
    \end{defn}
    Before exploring the relationships between different convergence modes, we first introduce some key formulas.
    \begin{lem}(Markov's Inequality)
        If $X$ is any random variable with a finite mean, then for all $a>0$,
        \begin{equation*}
            \prob(\abs{X}\geq a)\leq\frac{\expect\abs{X}}{a}
        \end{equation*}
    \end{lem}
    \begin{proofing}
        \begin{equation*}
            \prob(\abs{X}\geq a)=\expect(\mathbf{1}_{\abs{X}\geq a})\leq\expect\left(\frac{\abs{X}}{a}\mathbf{1}_{\abs{X}>a}\right)\leq\frac{\expect\abs{X}}{a}
        \end{equation*}
    \end{proofing}
    \begin{rem}
        For any non-negative function $\varphi$ that is increasing on $[0,\infty)$,
        \begin{equation*}
            \prob(\abs{X}\geq a)=\prob(\varphi(\abs{X})\geq\varphi(a))\leq\frac{\expect(\varphi(\abs{X}))}{\varphi(a)}
        \end{equation*}
    \end{rem}
    The following inequality requires H\"older's inequality (see Appendix $C$) for its proof. Therefore, we will not prove it here.
    \begin{lem}(Lyapunov's Inequality)
        Let $Z$ be any random variable. For all $r\geq s>0$,
        \begin{equation*}
            (\expect\abs{Z}^{s})^{\frac{1}{s}}\leq(\expect\abs{Z}^{r})^{\frac{1}{r}}
        \end{equation*}
    \end{lem}
    We also need to understand how to achieve almost sure convergence.
    \begin{lem}
        \label{Chapter 7 Lemma Obtaining almost sure convergence}
        Let
        \begin{align*}
            A_{n}(\varepsilon)&=\{\omega\in\Omega:\abs{X_{n}(\omega)-X(\omega)}>\varepsilon\} & B_{m}(\varepsilon)&=\bigcup_{n=m}^{\infty}A_{n}(\varepsilon)
        \end{align*}
        We have
        \begin{enumerate}
            \item $X_{n}\xrightarrow{\text{a.s.}}X$ if and only if $\lim_{m\uparrow\infty}\prob(B_{m}(\varepsilon))=0$ for all $\varepsilon>0$.
            \item $X_{n}\xrightarrow{\text{a.s.}}X$ if $\sum_{n=1}^{\infty}\prob(A_{n}(\varepsilon))<\infty$ for all $\varepsilon>0$.
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item We denote $C=\{\omega\in\Omega:X_{n}(\omega)\to X(\omega)\text{ as }n\to\infty\}$.\\
            If $\omega\in C$, it means that for all $\varepsilon>0$, there exists $n_{0}>0$ such that $\abs{X_{n}(\omega)-X(\omega)}\leq\varepsilon$ for all $n\geq n_{0}$.\\
            This also implies that for all $\varepsilon>0$, $\abs{X_{n}(\omega)-X(\omega)}>\varepsilon$ for finitely many $n$.\\
            If $\omega\in C^{\complement}$, it means that for all $\varepsilon>0$, $\abs{X_{n}(\omega)-X(\omega)}>\varepsilon$ for infinitely many $n$. ($\omega\in\bigcap_{m=1}^{\infty}\bigcup_{n=m}^{\infty}A_{n}(\varepsilon)$)\\
            Therefore,
            \begin{equation*}
                C^{\complement}=\bigcup_{\varepsilon>0}\bigcap_{m=1}^{\infty}\bigcup_{n=m}^{\infty}A_{n}(\varepsilon)
            \end{equation*}
            If $\prob(C^{\complement})=0$, then for all $\varepsilon>0$,
            \begin{equation*}
                \prob\left(\bigcap_{m=1}^{\infty}\bigcup_{n=m}^{\infty}A_{n}(\varepsilon)\right)=0
            \end{equation*}
            We can also find that
            \begin{align*}
                \prob\left(\bigcap_{m=1}^{\infty}\bigcup_{n=m}^{\infty}A_{n}(\varepsilon)\right)&=0 & &\implies & \prob(C^{\complement})&=\prob\left(\bigcup_{\varepsilon>0}\bigcap_{m=1}^{\infty}\bigcup_{n=m}^{\infty}A_{n}(\varepsilon)\right)=\prob\left(\bigcup_{k=1}^{\infty}\bigcap_{m=1}^{\infty}\bigcup_{n=m}^{\infty}A_{n}\left(\frac{1}{k}\right)\right)=0
            \end{align*}
            Therefore, $X_{n}\xrightarrow{\text{a.s.}}X$ if and only if $\lim_{m\uparrow\infty}\prob(B_{m}(\varepsilon))=0$ for all $\varepsilon>0$.
            \item From (1), for all $\varepsilon>0$,
            \begin{equation*}
                \sum_{n=1}^{\infty}\prob(A_{n}(\varepsilon))<\infty\implies\lim_{m\to\infty}\sum_{n=m}^{\infty}\prob(A_{n}(\varepsilon))=0\implies\lim_{m\to\infty}\prob(B_{m}(\varepsilon))=0\implies(X_{n}\xrightarrow{\text{a.s.}}X)
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    \begin{lem}
        \label{Chapter 7 Lemma Non-relationship between almost surely convergence and mean}
        There exist sequences that
        \begin{enumerate}
            \item converge almost surely but not in mean.
            \item converge in mean but not almost surely.
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item We consider
            \begin{equation*}
                X_{n}=\begin{cases}
                    n^{3}, &\text{Probability }=n^{-2}\\
                    0, &\text{Probability }=1-n^{-2}
                \end{cases}
            \end{equation*}
            By applying Lemma \ref{Chapter 7 Lemma Obtaining almost sure convergence}, for some $\varepsilon>0$.
            \begin{align*}
                \prob(\abs{X_{n}(\omega)-X(\omega)}>\varepsilon)&=\frac{1}{n^{2}} & \sum_{n=1}^{\infty}\prob(\abs{X_{n}(\omega)-X(\omega)}>\varepsilon)&<\infty
            \end{align*}
            Therefore, the sequence converges almost surely. However,
            \begin{equation*}
                \expect\abs{X_{n}-X}=n^{3}\left(\frac{1}{n^{2}}\right)=n\to\infty
            \end{equation*}
            Therefore, the sequence does not converge in mean.
            \item We consider
            \begin{equation*}
                X_{n}=\begin{cases}
                    1, &\text{Probability }=n^{-1}\\
                    0, &\text{Probability }=1-n^{-1}
                \end{cases}
            \end{equation*}
            In mean, as $n\to\infty$ we have
            \begin{equation*}
                \expect\abs{X_{n}-X}=1\left(\frac{1}{n}\right)=\frac{1}{n}\to 0
            \end{equation*}
            However, by applying Lemma \ref{Chapter 7 Lemma Obtaining almost sure convergence}, if $\varepsilon\in(0,1)$, for all $n$
            \begin{align*}
                \prob(B_{m}(\varepsilon))&=1-\lim_{r\to\infty}\prob(X_{n}=0\text{ for all }n\text{ such that }m\leq n\leq r)\\
                &=1-\lim_{r\to\infty}\prod_{i=m}^{r}\frac{i-1}{i}\\
                &=1-\lim_{r\to\infty}\frac{m-1}{r}\to 1\neq 0
            \end{align*}
            Therefore, the sequence does not converge almost surely.
        \end{enumerate}
    \end{proofing}
    We can now deduce the following implications. Roughly speaking, convergence in distribution is the weakest among all convergence modes, since it only cares about the distribution of $X_{n}$.
    \begin{thm}
        \label{Chapter 7 Theorem implications of different convergence modes}
        The following implications hold:
        \begin{enumerate}
            \item \begin{enumerate}
                \item $(X_{n}\xrightarrow{\text{a.s.}}X)\implies(X_{n}\xrightarrow{\prob}X)$
                \item $(X_{n}\xrightarrow{r}X)\implies(X_{n}\xrightarrow{\prob}X)$
                \item $(X_{n}\xrightarrow{\prob}X)\implies(X_{n}\xrightarrow{D}X)$
            \end{enumerate}
            \item If $r\geq s\geq 1$, then $(X_{n}\xrightarrow{r}X)\implies(X_{n}\xrightarrow{s}X)$
            \item No other implications holds in general.
        \end{enumerate}
    \end{thm}
    \newpage
    \begin{proofing}
        \begin{enumerate}
            \item \begin{enumerate}
                \item From Lemma \ref{Chapter 7 Lemma Obtaining almost sure convergence}, for all $\varepsilon>0$,
                \begin{equation*}
                    \prob(A_{m}(\varepsilon))\leq\prob\left(\bigcup_{n=m}^{\infty}A_{n}(\varepsilon)\right)=\prob(B_{m}(\varepsilon))\to 0
                \end{equation*}
                Therefore, $(X_{n}\xrightarrow{\text{a.s.}}X)\implies(X_{n}\xrightarrow{\prob}X)$
                \item From Markov's inequality, since $r\geq 1$,
                \begin{equation*}
                    0\leq\prob(\abs{X-X_{n}}>\varepsilon)=\prob(\abs{X-X_{n}}^{r}>\varepsilon^{r})\leq\frac{\expect\abs{X_{n}-X}^{r}}{\varepsilon^{r}}
                \end{equation*}
                Therefore, if $X_{n}\xrightarrow{r}X$, then $\expect\abs{X_{n}-X}^{r}\to 0$. We have $\prob(\abs{X-X_{n}}>\varepsilon)\to 0$ and thus $X_{n}\xrightarrow{\prob}X$.
                \item 
                \begin{align*}
                    \prob(X_{n}\leq x)&=\prob(X_{n}\leq x,X\leq x+\varepsilon)+\prob(X_{n}\leq x,X>x+\varepsilon)\leq\prob(X\leq x+\varepsilon)+\prob(\abs{X_{n}-X}>\varepsilon)\\
                    \prob(X\leq y)&\leq\prob(X_{n}\leq y+\varepsilon)+\prob(\abs{X_{n}-X}>\varepsilon)\\
                    \tag{$y=x-\varepsilon$}
                    \prob(X_{n}\leq x)
                    &\geq\prob(X\leq x-\varepsilon)-\prob(\abs{X_{n}-X}>\varepsilon)
                \end{align*}
                Since $X_{n}\xrightarrow{\prob}X$, $\prob(\abs{X_{n}-X}>\varepsilon)\to 0$ for all $\varepsilon>0$. Therefore,
                \begin{equation*}
                    \prob(X\leq x-\varepsilon)\leq\liminf_{n\to\infty}\prob(X_{n}\leq x)\leq\limsup_{n\to\infty}\prob(X_{n}\leq x)\leq\prob(X\leq x+\varepsilon)
                \end{equation*}
                By having $\varepsilon\downarrow 0$,
                \begin{equation*}
                    \prob(X\leq x)\leq\liminf_{n\to\infty}\prob(X_{n}\leq x)\leq\limsup_{n\to\infty}\prob(X_{n}\leq x)\leq\prob(X\leq x)
                \end{equation*}
                Therefore, $\lim_{n\to\infty}\prob(X_{n}\leq x)=\prob(X\leq x)$ and thus $X_{n}\xrightarrow{D}X$.
            \end{enumerate}
            \item Since $X_{n}\xrightarrow{r}X$, $\expect\abs{X_{n}-X}\to 0$ as $n\to\infty$. By Lyapunov's inequality, if $r\geq s$,
            \begin{equation*}
                \expect\abs{X_{n}-X}^{s}\leq(\expect\abs{X_{n}-X}^{r})^{\frac{s}{r}}\to 0
            \end{equation*}
            \item Let $\Omega=\{H,T\}$ and $\prob(H)=\prob(T)=\frac{1}{2}$. Let
            \begin{align*}
                X_{2m}(\omega)&=\begin{cases}
                    1, &\omega=H\\
                    0, &\omega=T
                \end{cases} & X_{2m+1}(\omega)=\begin{cases}
                    0, &\omega=H\\
                    1, &\omega=T
                \end{cases}
            \end{align*}
            Since $F(x)$ and $F_{n}(x)$ for all $n$ are all the same, $X_{n}\xrightarrow{D}X$. However, for $\varepsilon\in[0,1]$, $\prob(\abs{X_{n}-X}>\varepsilon)\not\to 0$.\\
            Therefore, $(X_{n}\xrightarrow{D}X)\centernot\implies(X_{n}\xrightarrow{\prob}X)$.\\
            Let $r=1$ and
            \begin{align*}
                X_{n}&=\begin{cases}
                    n, &\text{probability }=\frac{1}{n}\\
                    0, &\text{probability }=1-\frac{1}{n}
                \end{cases} & X&=0
            \end{align*}
            We get that $\prob(\abs{X_{n}-X}>\varepsilon)=\frac{1}{n}\to 0$. However, $\expect\abs{X_{n}-X}=n\left(\frac{1}{n}\right)=1\not\to 0$. Therefore, $(X_{n}\xrightarrow{\prob}X)\centernot\implies(X_{n}\xrightarrow{r}X)$.\\
            Let $\Omega=[0,1]$, $\mathcal{F}=\mathcal{B}([0,1])$ and $\prob$ be uniform.\\
            Let $I_{i}$ be such that $I_{\frac{1}{2}m(m-1)+1}, I_{\frac{1}{2}m(m-1)+2}, \cdots, I_{\frac{1}{2}m(m-1)+m}$ is a partition of $[0,1]$ for all $m$.\\
            We have $I_{1}=[0,1], I_{2}\cup I_{3}=[0,1],\cdots$. Let
            \begin{align*}
                X_{n}(\omega)&=\mathbf{1}_{I_{n}(\omega)}=\begin{cases}
                    1, &\omega\in I_{n}\\
                    0, &\omega\in I_{n}^{\complement}
                \end{cases} & X(\omega)&=0\text{ for all }\omega\in\Omega
            \end{align*}
            For all $\varepsilon\in[0,1]$, $\prob(\abs{X_{n}-X}>\varepsilon)=\prob(I_{n})=\frac{1}{n}\to 0$ for some $n$ if $n\to\infty$.\\
            However, for any given $\omega\in\Omega$, although $1$ becomes less often due to decreasing probability, it never dies out.\\
            Therefore, $X_{n}(\omega)\not\to 0=X(\omega)$ and $\prob(\{\omega\in\Omega:X_{n}(\omega)\to X(\omega)\text{ as }n\to\infty\})=0$, and thus, $(X_{n}\xrightarrow{\prob}X)\centernot\implies(X_{n}\xrightarrow{\text{a.s.}}X)$.\\
            If $r\geq s\geq 1$, let
            \begin{align*}
                X_{n}&=\begin{cases}
                    n, &\text{probability }=n^{-\left(\frac{r+s}{2}\right)}\\
                    0, &\text{probability }=1-n^{-\left(\frac{r+s}{2}\right)}
                \end{cases} & X&=0
            \end{align*}

            \begin{align*}
                \expect\abs{X_{n}-X}^{s}&=n^{s}\left(n^{-\left(\frac{r+s}{2}\right)}\right)=n^{\frac{s-r}{2}}\to 0 & \expect\abs{X_{n}-X}^{r}&=n^{r}\left(n^{-\left(\frac{r+s}{2}\right)}\right)=n^{\frac{r-s}{2}}\to\infty
            \end{align*}
            Therefore, if $r\geq s\geq 1$, $(X_{n}\xrightarrow{s}X)\centernot\implies(X_{n}\xrightarrow{r}X)$.\\
            We have proven that $(X_{n}\xrightarrow{\text{a.s.}}X)\centernot\implies(X_{n}\xrightarrow{r}X)$ and $(X_{n}\xrightarrow{r}X)\centernot\implies(X_{n}\xrightarrow{\text{a.s.}}X)$ in Lemma \ref{Chapter 7 Lemma Non-relationship between almost surely convergence and mean}.
        \end{enumerate}
    \end{proofing}
    By applying Theorem \ref{Chapter 7 Theorem implications of different convergence modes}, we can easily obtain this lemma.
    \begin{lem}
        The following implications hold:
        \begin{enumerate}
            \item $(X_{n}\xrightarrow{1}X)\implies(X_{n}\xrightarrow{\prob}X)$
        \end{enumerate}
    \end{lem}
    Some of the implications do not hold in general but they hold if we apply some restrictions.
    \begin{thm}(Partial Converse Statements)
        The following implications hold:
        \begin{enumerate}
            \item If $X_{n}\xrightarrow{D}c$, where $c$ is a constant, then $X_{n}\xrightarrow{\prob}c$.
            \item If $X_{n}\xrightarrow{\prob}X$ and $\prob(\abs{X_{n}}\leq k)=1$ for all $n$ with some fixed constant $k>0$, then $X_{n}\xrightarrow{r}X$ for all $r\geq 1$.
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item Since $X_{n}\xrightarrow{D}X$, $\prob(X_{n}\leq x)\to\prob(c\leq x)$ as $n\to\infty$. For all $\varepsilon>0$,
            \begin{equation*}
                \prob(\abs{X_{n}-c}\geq\varepsilon)=\prob(X_{n}\leq c-\varepsilon)+\prob(X_{n}\geq c+\varepsilon)=\prob(X_{n}\leq c-\varepsilon)+1-\prob(X_{n}<c+\varepsilon)
            \end{equation*}
            We can get that $\prob(X_{n}\leq c-\varepsilon)\to\prob(c\leq c-\varepsilon)=0$. For $\prob(X_{n}<c+\varepsilon)$,
            \begin{equation*}
                \prob\left(X_{n}\leq c+\frac{\varepsilon}{2}\right)\leq\prob(X_{n}<c+\varepsilon)\leq\prob(X_{n}\leq c+2\varepsilon)
            \end{equation*}
            \begin{align*}
                \prob\left(X_{n}\leq c+\frac{\varepsilon}{2}\right)&\to\prob\left(c\leq c+\frac{\varepsilon}{2}\right)=1 & \prob(X_{n}\leq c+2\varepsilon)&\to\prob(c\leq c+2\varepsilon)=1
            \end{align*}
            Therefore, $\prob(X_{n}<c+\varepsilon)\to 1$. We have
            \begin{equation*}
                \prob(\abs{X_{n}-c}\geq\varepsilon)\to 0+1-1 = 0
            \end{equation*}
            Therefore, $X_{n}\xrightarrow{\prob}c$.
            \item Since $X_{n}\xrightarrow{\prob}X$, $X_{n}\xrightarrow{D}X$. We have $\prob(\abs{X_{n}}\leq k)\to\prob(\abs{X}\leq k)=1$.\\
            Therefore, for all $\varepsilon>0$, if $\abs{X_{n}-X}\leq\varepsilon$, $\abs{X_{n}-X}\leq\abs{X_{n}}+\abs{X}\leq 2k$.
            \begin{align*}
                \expect\abs{X_{n}-X}^{r}&=\expect\left(\abs{X_{n}-X}^{r}\mathbf{1}_{\abs{X_{n}-X}\leq\varepsilon}\right)+\expect\left(\abs{X_{n}-X}^{r}\mathbf{1}_{\abs{X_{n}-X}>\varepsilon}\right)\\
                &\leq\varepsilon^{r}\expect\left(\mathbf{1}_{\abs{X_{n}-X}\leq\varepsilon}\right)+(2k)^{r}\expect\left(\mathbf{1}_{\abs{X_{n}-X}>\varepsilon}\right)\\
                &\leq\varepsilon^{r}+((2k)^{r}-\varepsilon^{r})\prob(\abs{X_{n}-X}>\varepsilon)
            \end{align*}
            Since $X_{n}\xrightarrow{\prob}X$, as $n\to\infty$, $\expect\abs{X_{n}-X}^{r}\to\varepsilon^{r}$. If we send $\varepsilon\downarrow 0$, $\expect\abs{X_{n}-X}^{r}\to 0$ and therefore $X_{n}\xrightarrow{r}X$.
        \end{enumerate}
    \end{proofing}
    Note that any sequence $\{X_{n}\}$ which satisfies $X_{n}\xrightarrow{\prob}X$ necessarily contains a subsequence $\{X_{n_{i}}:1\leq i<\infty\}$ which converges almost surely.
    \begin{thm}
        If $X_{n}\xrightarrow{\prob}X$, then there exists a non-random increasing sequence of integers $n_{1},n_{2},\cdots$ such that as $i\to\infty$,
        \begin{equation*}
            X_{n_{i}}\xrightarrow{\text{a.s.}}X
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Since $X_{n}\xrightarrow{\prob}X$, $\prob(\abs{X_{n}-X}>\varepsilon)\to 0$ as $n\to\infty$ for all $\varepsilon>0$.\\
        We can pick an increasing sequence $n_{1},n_{2},\cdots$ of positive integers such that
        \begin{equation*}
            \prob(\abs{X_{n_{i}}-X}>i^{-1})\leq i^{-2}
        \end{equation*}
        For any $\varepsilon>0$,
        \begin{equation*}
            \sum_{i>\varepsilon^{-1}}\prob(\abs{X_{n_{i}}-X}>\varepsilon)\leq\sum_{i>\varepsilon^{-1}}\prob(\abs{X_{n_{i}}-X}>i^{-1})\leq\sum_{i}i^{-2}<\infty
        \end{equation*}
        By Lemma \ref{Chapter 7 Lemma Obtaining almost sure convergence}, we get the $X_{n_{i}}\xrightarrow{\text{a.s.}}X$ as $i\to\infty$
    \end{proofing}
    \newpage

\section{Other Versions of the Weak Law of Large Numbers}
    Let us revisit and introduce additional versions of the Weak Law of Large Numbers (WLLN) and their applications.
    \begin{thm}($L^{2}$-WLLN) 
        Let $X_{1},X_{2},\cdots,X_{n}$ be uncorrelated random variables with $\expect X_{i}=\mu$ and $\Var(X_{i})\leq c<\infty$ for all $i$. Let $S_{n}=\sum_{i=1}^{n}X_{i}$. Then
        \begin{equation*}
            \frac{S_{n}}{n}\xrightarrow{2}\mu
        \end{equation*}
    \end{thm}
    \begin{proofing}
        \begin{equation*}
            \expect\left(\frac{S_{n}}{n}-\mu\right)^{2}=\frac{\expect(S_{n}-\expect S_{n})^{2}}{n^{2}}=\frac{1}{n^{2}}\Var(S_{n})=\frac{1}{n^{2}}\sum_{i=1}^{n}\Var(X_{i})\leq\frac{c}{n}\to 0
        \end{equation*}
        Therefore, $\frac{S_{n}}{n}\xrightarrow{2}\mu$.
    \end{proofing}
    \begin{rem}
        From this theorem, we can immediately conclude that
        \begin{equation*}
            \left(\frac{S_{n}}{n}\xrightarrow{2}\mu\right)\implies\left(\frac{S_{n}}{n}\xrightarrow{\prob}\mu\right)
        \end{equation*}
    \end{rem}
    \begin{rem}
        Note that in the i.i.d. case, the existence of variance is not required.
    \end{rem}
    There are numerous applications of the Weak Law of Large Numbers.
    \begin{eg}(Bernstein Approximation)
        Let $f$ be a continuous function on $[0,1]$, and define
        \begin{equation*}
            \tag{Bernstein Polynomial}
            f_{n}(x)=\sum_{m=0}^{n}\binom{n}{m}x^{m}(1-x)^{n-m}f\left(\frac{m}{n}\right).
        \end{equation*}
        We aim to show that as $n\to\infty$,
        \begin{equation*}
            \sup_{x\in[0,1]}\abs{f_{n}(x)-f(x)}\to 0.
        \end{equation*}
    \end{eg}
    \begin{rem}
        Let $x\in[0,1]$. To better approach this question, we can let $X_{1,x},X_{2,x},\cdots,X_{n,x}\sim\Bern(x)$ be i.i.d. random variables. Let $S_{n,x}=\sum_{i=1}^{n}X_{i,x}\sim\Bin(n,x)$.
        \begin{align*}
            \prob(S_{n,x}=m)&=\binom{n}{m}x^{m}(1-x)^{n-m},\\
            f_{n}(x)&=\sum_{m=0}^{n}\prob(S_{n,x}=m)f\left(\frac{m}{n}\right)=\expect\left(f\left(\frac{S_{n,x}}{n}\right)\right).
        \end{align*}
        By the WLLN, $\frac{S_{n,x}}{n}\xrightarrow{\prob}x$.
    \end{rem}
    \begin{rem}(Continuous Mapping Theorem)
        Let $f$ be a uniformly continuous function. For all $\varepsilon>0$, there exists $\delta_{\varepsilon}$ such that
        \begin{equation*}
            \text{if}\quad\abs{\frac{S_{n,x}}{n}-x}\leq\delta_{\varepsilon},\quad\text{then}\quad\abs{f\left(\frac{S_{n,x}}{n}\right)-f(x)}\leq\varepsilon.
        \end{equation*}
        By the contrapositive,
        \begin{equation*}
            \prob\left(\omega\in\Omega:\abs{f\left(\frac{S_{n,x}(\omega)}{n}\right)-f(x)}>\varepsilon\right)\leq\prob\left(\omega\in\Omega:\abs{\frac{S_{n,x}(\omega)}{n}-x}>\delta_{\varepsilon}\right)\to 0.
        \end{equation*}
        From this, we conclude that $f\left(\frac{S_{n,x}}{n}\right)\xrightarrow{\prob}f(x)$.\\
        \textit{Note: For non-uniformly continuous functions, the analysis is more complex. Further research is recommended.}
    \end{rem}
    \newpage
    \begin{eg}
        By establishing that $f\left(\frac{S_{n,x}}{n}\right)\xrightarrow{\prob}f(x)$, and noting that there exists a constant $M$ such that $\norm{f}_{\infty}\leq M$ (due to $f$ being continuous on $[0,1]$), we have
        \begin{align*}
            \abs{\expect\left(f\left(\frac{S_{n,x}}{n}\right)\right)-f(x)}\leq\expect\abs{f\left(\frac{S_{n,x}}{n}\right)-f(x)}&=\expect\left(\abs{f\left(\frac{S_{n,x}}{n}\right)-f(x)}\mathbf{1}_{\abs{\frac{S_{n,x}}{n}-x}\leq\delta_{\varepsilon}}\right)+\expect\left(\abs{f\left(\frac{S_{n,x}}{n}\right)-f(x)}\mathbf{1}_{\abs{\frac{S_{n,x}}{n}-x}>\delta_{\varepsilon}}\right)\\
            &\leq\varepsilon+2M\prob\left(\abs{\frac{S_{n,x}}{n}-x}>\delta_{\varepsilon}\right)
        \end{align*}
        \begin{align*}
            \sup_{x\in[0,1]}\abs{\expect\left(f\left(\frac{S_{n,x}}{n}\right)\right)-f(x)}&=\varepsilon+2M\sup_{x\in [0,1]}\left(\prob\left(\abs{\frac{S_{n,x}-nx}{n}}>\delta_{\varepsilon}\right)\right)\\
            \tag{Markov's Inequality and Lyapunov's Inequality}
            &\leq\varepsilon+2M\sup_{x\in[0,1]}\left(\frac{\expect\abs{S_{n,x}-nx}^{2}}{n^{2}\delta_{\varepsilon}^{2}}\right)\\
            \tag{$\expect S_{n,x}=nx$}
            &\leq\varepsilon+2M\sup_{x\in[0,1]}\left(\frac{\Var(S_{n,x})}{n^{2}\delta_{\varepsilon}^{2}}\right)=\varepsilon+2M\sup_{x\in[0,1]}\left(\frac{x(1-x)}{n\delta_{\varepsilon}^{2}}\right)\\
            &\leq\varepsilon+\frac{M}{2n\delta_{\varepsilon}^{2}}.\\
            \limsup_{n\to\infty}\sup_{x\in[0,1]}\abs{\expect\left(f\left(\frac{S_{n,x}}{n}\right)\right)-f(x)}&\leq\varepsilon\to 0.
        \end{align*}
        Therefore, we conclude that $\sup_{x\in[0,1]}\abs{f_{n}(x)-f(x)}\to 0$ as $n\to\infty$.
    \end{eg}
    \begin{eg}(Borel's Geometric Concentration)
        Let $\mu_{n}$ be the uniform probability measure on the $n$-dimensional cube $[-1,1]^{n}$. Let $\mathcal{H}$ be a hyperplane that is orthogonal to a principal diagonal of $[-1,1]$ ($\mathcal{H}=(1,\cdots,1)^{\perp}$).\\
        Let $\mathcal{H}_{r}=\{x\in[-1,1]^{n}:\dist(x:\mathcal{H})\leq r\}$. Then for any given $\varepsilon>0$, $\mu_{n}(\mathcal{H}_{\varepsilon\sqrt{n}})\to 1$ as $n\to\infty$.\\
        We can prove this by letting $X_{1},X_{2},\cdots\sim\U[-1,1]$ be i.i.d. random variables and $\expect X_{i}=0$. Let $X=(X_{1},X_{2},\cdots,X_{n})$.\\
        For all $B\in[-1,1]^{n}$, $\mu_{n}(B)=\prob(X\in B)=\prob\circ X^{-1}(B)$.
        \begin{align*}
            \mu_{n}(\mathcal{H}_{\varepsilon\sqrt{n}})&=\prob(\dist(X,\mathcal{H})\leq\varepsilon\sqrt{n})\\
            &=\prob\left(\frac{\abs{\left<X,(1,\cdots,1)\right>}}{\norm{(1,\cdots,1)}_{2}}\leq\varepsilon\sqrt{n}\right)\\
            &=\prob\left(\abs{\frac{\sum_{i=1}^{n}X_{i}}{n}}\leq\varepsilon\right)\\
            &=\prob\left(\abs{\frac{S_{n}}{n}-\expect X_{1}}\leq\varepsilon\right)\\
            \tag{WLLN}
            &\to 1
        \end{align*}
    \end{eg}
    We do not necessarily need to stick to a given sequence of random variables $X_{1},X_{2},\cdots$ in the Law of Large Numbers.
    \begin{thm}(WLLN for Triangular Array)
        Let $\{X_{n,j}\}_{1\leq j\leq n<\infty}$ be a triangular array. Let $S_{n}=\sum_{i=1}^{n}X_{n,i}$, $\mu_{n}=\expect S_{n}$ and $\sigma_{n}^{2}=\Var(S_{n})$. Suppose that for some sequence $b_{n}$,
        \begin{equation*}
            \frac{\sigma_{n}^{2}}{b_{n}^{2}}=\expect\left(\frac{S_{n}-\mu_{n}}{b_{n}}\right)^{2}\to 0
        \end{equation*}
        Then we have
        \begin{equation*}
            \frac{S_{n}-\mu_{n}}{b_{n}}\xrightarrow{\prob}0
        \end{equation*}
    \end{thm}
    \begin{proofing}
        \begin{equation*}
            \expect\left(\frac{S_{n}-\mu_{n}}{b_{n}}\right)^{2}=\frac{\Var(S_{n})}{b_{n}^{2}}\to 0
        \end{equation*}
        Therefore, $\frac{S_{n}-\mu_{n}}{b_{n}}\xrightarrow{2}0$ and thus $\frac{S_{n}-\mu_{n}}{b_{n}}\xrightarrow{\prob}0$.
    \end{proofing}
    \begin{rem}
        We should choose $b_{n}$ that is no larger than $\expect S_{n}$ if possible.
    \end{rem}
    \begin{eg}(Coupon Collector's Problem) 
        Let $X_{1},X_{2},\cdots$ be i.i.d. uniform random variables on $\{1,2,\cdots,n\}$.\\
        Let $\tau_{k}^{n}=\inf\{m:\abs{\{X_{1},X_{2},\cdots,X_{m}\}}=k\}$ be the waiting time for picking $k$ distinct types.\\
        What is the asymptotic behavior of $\tau_{n}^{n}$?\\
        It is easy to see that $\tau_{1}^{n}=1$. By convention, $\tau_{0}^{n}=0$.\\
        For $1\leq k\leq n$, let $X_{n,k}=\tau_{k}^{n}-\tau_{k-1}^{n}$ be the additional waiting time for picking $k$ distinct types when we have $k-1$ types.\\
        Notice that
        \begin{equation*}
            \tau_{n}^{n}=\sum_{k=1}^{n}X_{n,k}
        \end{equation*}
        We know that
        \begin{align*}
            \prob(X_{n,k}&=\ell)=\left(\frac{k-1}{n}\right)^{\ell-1}\left(1-\frac{k-1}{n}\right) & &\implies & X_{n,k}&\sim\Geom\left(1-\frac{k-1}{n}\right)
        \end{align*}
        We claim that $X_{n,k}$ are independent for all $k$. For a constant $c$,
        \begin{align*}
            \expect\tau_{n}^{n}&=\sum_{k=1}^{n}\expect X_{n,k}=\sum_{k=1}^{n}\left(1-\frac{k-1}{n}\right)^{-1}=\sum_{m=1}^{n}\frac{n}{m}\sim n\log n\\
            \Var(\tau_{n}^{n})&=\sum_{k=1}^{n}\Var(X_{n,k})=\sum_{k=1}^{n}\left(\left(1-\frac{k-1}{n}\right)^{-2}-\left(1-\frac{k-1}{n}\right)^{-1}\right)\leq\sum_{k=1}^{n}\left(1-\frac{k-1}{n}\right)^{-2}=\sum_{m=1}^{n}\frac{n^{2}}{m^{2}}\leq cn^{2}
        \end{align*}
        By WLLN, if we choose $b_{n}=n\log n$, then we have
        \begin{equation*}
            \frac{\Var(\tau_{n}^{n})}{b_{n}^{2}}\to 0\implies\frac{\tau_{n}^{n}-\sum_{m=1}^{n}\frac{n}{m}}{n\log n}\xrightarrow{\prob}0
        \end{equation*}
        Therefore, $\frac{\tau_{n}^{n}}{n\log n}\xrightarrow{\prob}1$
    \end{eg}
    \begin{eg}(An Occupancy Problem)
        $r$ balls are put at random into $n$ bins. All $n^{r}$ configurations are equally likely.\\
        Let $A_{i}$ be the event that the $i$-th bin is empty, and let $N_{n}$ be the number of empty bins $=\sum_{i=1}^{n}\mathbf{1}_{A_{i}}$.\\
        How can we prove that if $\frac{r}{n}\to c$ as $n\to\infty$,
        \begin{equation*}
            \frac{N_{n}}{n}\xrightarrow{\prob}e^{-c}?
        \end{equation*}
        We can see that
        \begin{align*}
            \frac{\expect N_{n}}{n}&=\frac{1}{n}\sum_{i=1}^{n}\expect\mathbf{1}_{A_{i}}=\prob(A_{i})=\left(1-\frac{1}{n}\right)^{r}\to e^{-c},\\
            \Var(N_{n})&=\expect(N_{n}^{2})-(\expect N_{n})^{2}\\
            &=\expect\left(\sum_{i=1}^{n}\mathbf{1}_{A_{i}}\right)^{2}-\left(\expect\left(\sum_{i=1}^{n}\mathbf{1}_{A_{i}}\right)\right)^{2}\\
            &=\sum_{i=1}^{n}(\prob(A_{1})-(\prob(A_{1}))^{2})+\sum_{i\neq j}(\prob(A_{i}\cap A_{j})-(\prob(A_{1}))^{2})\\
            &=n\left(\left(1-\frac{1}{n}\right)^{r}-\left(1-\frac{1}{n}\right)^{2r}\right)+n(n-1)\left(\left(1-\frac{2}{n}\right)^{r}-\left(1-\frac{1}{n}\right)^{2r}\right)\\
            &=o(n^{2})
        \end{align*}
        By using the WLLN, let $b_{n}=n$,
        \begin{equation*}
            \frac{\Var(N_{n})}{b_{n}^{2}}\to 0\implies\frac{N_{n}-\expect N_{n}}{n}\xrightarrow{\prob}0
        \end{equation*}
        Therefore, $\frac{N_{n}}{n}\xrightarrow{\prob}e^{-c}$.
    \end{eg}
    \newpage

\section{Borel-Cantelli Lemmas}
    Let $A_{1},A_{2},\dots$ be a sequence of events in $(\Omega,\mathcal{F})$. We are particularly interested in
    \begin{equation*}
        \limsup_{n\to\infty}A_{n}=\{A_{n}\text{ i.o.}\}=\bigcap_{m}\bigcup_{n=m}^{\infty} A_{n}
    \end{equation*}
    \begin{thm}(Borel-Cantelli Lemmas)
        For any sequence of events $A_{n}\in\mathcal{F}$,
        \begin{enumerate}
            \item (BCI) If $\sum_{n=1}^{\infty}\prob(A_{n})<\infty$, then
            \begin{equation*}
                \prob(A_{n}\text{ i.o.})=0
            \end{equation*}
            \item (BCII) If $\sum_{n=1}^{\infty}\prob(A_{n})=\infty$ and $A_{n}$ are independent, then
            \begin{equation*}
                \prob(A_{n}\text{ i.o.})=1
            \end{equation*}
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item If $\sum_{n=1}^{\infty}\prob(A_{n})<\infty$,
            \begin{equation*}
                \prob(A_{n}\text{ i.o.})=\lim_{m\to\infty}\prob\left(\bigcup_{n=m}^{\infty}A_{n}\right)\leq\lim_{m\to\infty}\sum_{n=m}^{\infty}\prob(A_{n})=0
            \end{equation*}
            \item If $\sum_{n=1}^{\infty}\prob(A_{n})=\infty$ and $A_{n}$ are independent, we have
            \begin{align*}
                \prob\left(\bigcup_{m=1}^{\infty}\bigcap_{n=m}^{\infty}A_{n}^{\complement}\right)&=\lim_{m\uparrow\infty}\prob\left(\bigcap_{n=m}^{\infty}A_{n}^{\complement}\right)=\lim_{m\uparrow\infty}\lim_{r\uparrow\infty}\prob\left(\bigcap_{n=m}^{r}A_{n}^{\complement}\right)=\lim_{m\uparrow\infty}\lim_{r\uparrow\infty}\prod_{n=m}^{r}\prob(A_{n}^{\complement})=\lim_{m\uparrow\infty}\prod_{n=m}^{\infty}(1-\prob(A_{n}))\\
                \tag{$1-x\leq e^{-x}$ if $x\geq 0$}
                &\leq\lim_{m\uparrow\infty}\prod_{n=m}^{\infty}e^{-\prob(A_{n})}=\lim_{m\uparrow\infty}\exp\left(-\sum_{n=m}^{\infty}\prob(A_{n})\right)=0\\
                \prob(A_{n}\text{ i.o.})&=\prob\left(\bigcap_{m=1}^{\infty}\bigcup_{n=m}^{\infty}A_{n}\right)=1-\prob\left(\bigcup_{m=1}^{\infty}\bigcap_{n=m}^{\infty}A_{n}^{\complement}\right)=1
            \end{align*}
        \end{enumerate}
    \end{proofing}
    \begin{rem}
        BCII can be considered a partial converse of BCI.
    \end{rem}
    \begin{rem}
        i.o. stands for "infinitely often," while f.o. stands for "finitely often."
    \end{rem}
    We will now explore applications of the Borel-Cantelli Lemmas.
    \begin{eg}(Infinite Monkey Problem)
        Assume there is a keyboard with $N$ keys, each representing a distinct letter. Given a string of letters $S$ of length $m$, a monkey randomly hits any key at each round.\\
        How can we prove that, almost surely, the monkey will type the given string $S$ infinitely many times?\\
        Let $E_{k}$ be the event that the $m$-string $S$ is typed starting from the $k$-th hit. Note that $E_{k}$ are not independent.\\
        To produce an independent sequence, consider $E_{mk+1}$, where each string is $m$ letters apart from the next.\\
        For any $i$, $\prob(E_{i})=\left(\frac{1}{N}\right)^{m}$. By BCII,
        \begin{equation*}
            \sum_{k=0}^{\infty}\prob(E_{mk+1})=\infty\implies\prob(E_{mk+1}\text{ i.o.})=1
        \end{equation*}
        Therefore, $\prob(E_{k}\text{ i.o.})=1$.
    \end{eg}
    \newpage
    Recall that if $X_{n}\xrightarrow{\prob}X$, there exists a non-random increasing sequence of integers $n_{1},n_{2},\dots$ such that $X_{n_{i}}\xrightarrow{\text{a.s.}}X$ as $i\to\infty$.\\
    We can use the Borel-Cantelli Lemmas to prove a similar theorem.
    \begin{thm}
        $X_{n}\xrightarrow{\prob}X$ if and only if for all subsequences $X_{n(m)}$, there exists a further subsequence
        \begin{equation*}
            X_{n(m_{k})}\xrightarrow{\text{a.s.}}X
        \end{equation*}
    \end{thm}
    \begin{proofing}
        \begin{figure}[h!]
            \begin{subfigure}[b]{0.05\textwidth}
                ($\Longrightarrow$)
            \end{subfigure}
            \begin{subfigure}[t]{0.9\textwidth}
                Let $\varepsilon_{k}$ be a sequence of positive numbers such that $\varepsilon_{k}\downarrow 0$ as $k\uparrow\infty$. For any $k$, there exists an $n(m_{k})>n(m_{k-1})$ such that
                \begin{equation*}
                    \tag{$X_{n}\xrightarrow{\prob}X$}
                    \prob(\abs{X_{n(m_{k})}-X}>\varepsilon_{k})\leq 2^{-k}
                \end{equation*}
                Since $\sum_{k=1}^{\infty}\prob(\abs{X_{n(m_{k})}-X}>\varepsilon_{k})<\infty$, by BCI,
                \begin{align*}
                    \prob(\abs{X_{n(m_{k})}-X}>\varepsilon_{k}\text{ i.o.})&=0 & \prob(\abs{X_{n(m_{k})}-X}>\varepsilon_{k}\text{ f.o.})&=1
                \end{align*}
                For all $\varepsilon>0$, $\varepsilon_{k}\leq\varepsilon$ for all $k\geq k_{0}$. If $\varepsilon_{k}\leq\varepsilon$,
                \begin{equation*}
                    \{\abs{X_{n(m_{k})}-X}>\varepsilon_{k}\}\supseteq\{\abs{X_{n(m_{k})}-X}>\varepsilon\}
                \end{equation*}
                If $\omega\in\{\abs{X_{n(m_{k})}-X}>\varepsilon_{k}\}$ for finitely many $k$, then $\omega\in\{\abs{X_{n(m_{k})}-X}>\varepsilon\}$ for finitely many $k$. Therefore, for all $\varepsilon >0$
                \begin{equation*}
                    \prob(\abs{X_{n(m_{k})}-X}>\varepsilon\text{ i.o.})=0
                \end{equation*}
            \end{subfigure}
        \end{figure}
        \begin{figure}[h!]
            \begin{subfigure}[b]{0.05\textwidth}
                ($\Longleftarrow$)
            \end{subfigure}
            \begin{subfigure}[t]{0.9\textwidth}
                For all $\varepsilon>0$, let $a_{n}=\prob(\abs{X_{n}-X}>\varepsilon)$.\\
                For all $n(m)$, there exists $n(m_{k})$ such that $X_{n(m_{k})}\xrightarrow{\text{a.s.}}X$. We have
                \begin{equation*}
                    (X_{n(m_{k})}\xrightarrow{\text{a.s.}}X)\implies(X_{n(m_{k})}\xrightarrow{\prob}X)\implies a_{n(m_{k})}\to 0
                \end{equation*}
                Therefore, for any $a_{n}$ and $a_{n(m)}$, there exists further $a_{n(m_{k})}\to 0$.\\
                We have $a_{n}\to 0\implies(X_{n}\xrightarrow{\prob}X)$.
            \end{subfigure}
        \end{figure}
    \end{proofing}
    We have a theorem that has conditions quite similar to the Law of Large Numbers. However, notice that $\expect\abs{X_{1}}=\infty$ here.
    \begin{thm}
        If $X_{1},X_{2},\cdots$ are i.i.d. random variables with $\expect\abs{X_{i}}=\infty$. Then
        \begin{equation*}
            \prob(\abs{X_{n}}\geq n\text{ i.o.})=1
        \end{equation*}
        Let $S_{n}=\sum_{i=1}^{n}X_{i}$. Then
        \begin{equation*}
            \prob\left(\lim_{n\to\infty}\frac{S_{n}}{n}\text{ exists in }(-\infty,\infty)\right)=0
        \end{equation*}
    \end{thm}
    \begin{proofing}
        \begin{equation*}
            \expect\abs{X_{1}}=\int_{0}^{\infty}\prob(\abs{X_{1}}>t)\,dt\leq\sum_{n=0}^{\infty}\prob(\abs{X_{1}}>n)
        \end{equation*}
        Since $\{\abs{X_{n}}>n\}$ is a collection of independent events, by BCII, $\prob(\abs{X_{n}}>n\text{ i.o.})=1$.\\
        For the second statement, let $C=\{\omega\in\Omega:\lim_{n\to\infty}\frac{S_{n}(\omega)}{n}\text{ exists in }\mathbb{R}\}$.\\
        Assume that $\omega\in C$, then
        \begin{equation*}
            \frac{S_{n}(\omega)}{n}-\frac{S_{n+1}(\omega)}{n+1}=\frac{S_{n}(\omega)}{n(n+1)}-\frac{X_{n+1}(\omega)}{n+1}
        \end{equation*}
        Since $\frac{S_{n}}{n}$ converges, $\frac{S_{n}(\omega)}{n}-\frac{S_{n+1}(\omega)}{n+1}\to 0$ and $\frac{S_{n}(\omega)}{n(n+1)}\to 0$. We get that $\frac{X_{n+1}(\omega)}{n+1}\to 0$.\\
        However, that means $\abs{X_{n+1}}<n+1$ for an arbitrary large $n$. Therefore, $\omega\not\in\{\abs{X_{n}}\geq n\text{ i.o.}\}$.\\
        From that, we get that $\prob(C)=0$ since $\prob(\abs{X_{n}}\geq n\text{ i.o.})=1$.
    \end{proofing}
    \newpage
    The next result extends BCII.
    \begin{thm}
        If $A_{1},A_{2},\cdots$ are pairwise independent and $\sum_{n=1}^{\infty}\prob(A_{n})=\infty$, then as $n\to\infty$,
        \begin{equation*}
            \frac{\sum_{m=1}^{n}\mathbf{1}_{A_{m}}}{\sum_{m=1}^{n}\prob(A_{m})}\xrightarrow{\text{a.s.}}1
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Let $X_{n}=\mathbf{1}_{A_{n}}$, $S_{n}=\sum_{i=1}^{n}X_{i}$ and $\expect S_{n}=\sum_{m=1}^{n}\prob(A_{m})$.\\
        Notice that pairwise independence is already enough for $\cov(X_{i},X_{j})=0$ for all $i\neq j$.\\
        Using Markov's inequality, for any $\varepsilon>0$, we get as $n\to\infty$
        \begin{align*}
            \prob\left(\abs{\frac{S_{n}-\expect S_{n}}{\expect S_{n}}}>\varepsilon\right)&\leq\frac{\expect(S_{n}-\expect S_{n})^{2}}{\varepsilon^{2}(\expect S_{n})^{2}}=\frac{\Var(S_{n})}{\varepsilon^{2}(\expect S_{n})^{2}}=\sum_{m=1}^{n}\frac{\Var(\mathbf{1}_{A_{m}})}{\varepsilon^{2}(\expect S_{n})^{2}}=\sum_{m=1}^{n}\frac{\expect\mathbf{1}_{A_{m}}}{\varepsilon^{2}(\expect S_{n})^{2}}=\frac{1}{\varepsilon^{2}\expect S_{n}}\to 0
        \end{align*}
        Therefore, we get that $\frac{S_{n}-\expect S_{n}}{\expect S_{n}}\xrightarrow{\prob}0$.\\
        Now, we can choose a desirable subsequence to prove almost surely convergence. Let $n_{k}=\inf\{n:\expect S_{n}\geq k^{2}\}$.\\
        We can get that $\expect S_{n_{k}}\geq k^{2}$ and $\expect S_{n_{k}}=\expect S_{n_{k}-1}+\expect\mathbf{1}_{A_{n_{k}}}<k^{2}+1$. Again by Markov's inequality,
        \begin{equation*}
            \sum_{k=1}^{\infty}\prob\left(\abs{\frac{S_{n_{k}}-\expect S_{n_{k}}}{\expect S_{n_{k}}}}>\varepsilon\right)\leq\sum_{k=1}^{\infty}\frac{1}{\varepsilon^{2}\expect S_{n_{k}}}\leq\sum_{k=1}^{\infty}\frac{1}{\varepsilon^{2}(k^{2}+1)}<\infty
        \end{equation*}
        By BCI, we have that as $k\to\infty$,
        \begin{align*}
            \frac{S_{n_{k}}}{\expect S_{n_{k}}}&\xrightarrow{\text{a.s.}}1 & \prob\left(\frac{S_{n_{k}}}{\expect S_{n_{k}}}\to 1\text{ as }k\to\infty\right)=1
        \end{align*}
        Let $C=\{\omega\in\Omega:\frac{S_{n_{k}}(\omega)}{\expect S_{n_{k}}}\to 1\text{ as }k\to\infty\}$. For $\omega\in C$, for all $n_{k}\leq n<n_{k+1}$, we have $S_{n_{k}}(\omega)\leq S_{n}(\omega)\leq S_{n_{k+1}}(\omega)$.
        \begin{equation*}
            \frac{S_{n_{k}}(\omega)}{\expect S_{n_{k}+1}}\leq\frac{S_{n}(\omega)}{\expect S_{n}}\leq\frac{S_{n_{k}+1}(\omega)}{\expect S_{n_{k}}}
        \end{equation*}
        Since $\frac{S_{n_{k}}(\omega)}{\expect S_{n_{k}+1}}=\frac{S_{n_{k}}(\omega)}{\expect S_{n_{k}}}\left(\frac{\expect S_{n_{k}}}{\expect S_{n_{k}+1}}\right)\to 1$ and $\frac{S_{n_{k}+1}(\omega)}{\expect S_{n_{k}+1}}=\frac{S_{n_{k}+1}(\omega)}{\expect S_{n_{k}+1}}\left(\frac{\expect S_{n_{k}+1}}{\expect S_{n_{k}}}\right)\to 1$, we get that for any $\omega\in C$,
        \begin{equation*}
            \frac{S_{n}(\omega)}{\expect S_{n}}\to 1
        \end{equation*}
        Therefore, we have
        \begin{equation*}
            \prob\left(\frac{S_{n}}{\expect S_{n}}\to 1\right)\geq\prob\left(\frac{S_{n_{k}}}{\expect S_{n_{k}}}\to 1\text{ as }k\to\infty\right)=1
        \end{equation*}
        As a result, we get that
        \begin{equation*}
            \frac{S_{n}}{\expect S_{n}}\xrightarrow{\text{a.s.}}1
        \end{equation*}
    \end{proofing}
    If the events $A_{1}, A_{2},\cdots$ in the Borel-Cantelli Lemmas are independent, then $\prob(A)$ is either $0$ or $1$ depending on whether $\sum\prob(A_{n})$ converges. The following is a simple version.
    \begin{thm}(Borel Zero-one Law)
        Let $A_{1},A_{2},\cdots\in\mathcal{F}$ and $\mathcal{A}=\sigma(A_{1},A_{2},\cdots)$. Suppose that
        \begin{enumerate}
            \item $A\in\mathcal{A}$
            \item $A$ is independent with any finite collection of $A_{1},A_{2},\cdots$
        \end{enumerate}
        Then $\prob(A)=0$ or $1$.
    \end{thm}
    \begin{proofing}[Proof (Non-rigorous)]
        Suppose that $A_{1},A_{2},\cdots$ are independent. Let $A=\limsup_{n}A_{n}$.\\
        We know that $A=\bigcap_{m=1}^{\infty}\bigcup_{n=m}^{\infty}A_{n}$. Therefore, $A\in\mathcal{A}=\sigma(A_{1},A_{2},\cdots)$.\\
        For all $k$, we can also have $A=\bigcap_{m=k+1}^{\infty}\bigcup_{n=m}^{\infty}A_{n}$. Therefore, $A$ is independent with any $A_{i}\in\sigma(A_{1},A_{2},\cdots,A_{k})$.\\
        Setting $k\to\infty$, we have that $A$ is independent of all elements in $\mathcal{A}$, which also include itself.\\
        Therefore, $\prob(A)=\prob(A\cap A)=(\prob(A))^{2}\implies\prob(A)=0$ or $1$.
    \end{proofing}
    \newpage
    Let $X_{1},X_{2},\cdots$ be a collection of random variables. For any subcollection $\{X_{i}:i\in I\}$, write $\sigma(X_{i}:i\in I)$ for the smallest $\sigma$-field with reference to which each of $X_{i}$ is measurable.
    \begin{defn}
        Let $\mathcal{H}_{n}=\sigma(X_{n+1},X_{n+2},\cdots)$. We have $\mathcal{H}_{n}\supseteq\mathcal{H}_{n+1}\supseteq\cdots$. \textbf{Tail $\sigma$-field} is defined as
        \begin{equation*}
            \mathcal{H}_{\infty}=\bigcap_{n}\mathcal{H}_{n}
        \end{equation*}
    \end{defn}
    \begin{rem}
        If $E\in\mathcal{H}_{\infty}$, then $E$ is called \textbf{tail event}.
    \end{rem}
    \begin{eg}
        $\{\limsup_{n\to\infty}X_{n}=\infty\}$ is a tail event.
    \end{eg}
    \begin{eg}
        $\{\sum_{n}X_{n}\text{ converges}\}$ is a tail event.
    \end{eg}
    \begin{eg}
        $\{\sum_{n}X_{n}\text{ converges to }1\}$ is not a tail event.
    \end{eg}
    We get another version of zero-one law.
    \begin{thm}(Kolmogorov's zero-one law)
        If $H\in\mathcal{H}_{\infty}$, then $\prob(H)=0$ or $1$.
    \end{thm}
    We continue to explore more into tail events.
    \begin{defn}
        We define \textbf{tail function} to be $Y:\Omega\to\mathbb{R}\cup\{-\infty,\infty\}$, which is a generalized random variables that is a function of $X_{1},X_{2},\cdots$. It is independent of any finite collection of $X_{i}$'s and is $\mathcal{H}_{\infty}$-measurable.
    \end{defn}
    \begin{eg}
        Let $Y(\omega)=\limsup_{n\to\infty}X_{n}(\omega)$ for all $\omega\in\Omega$. $F_{Y}(y)=\prob(Y\leq y)=0$ or $1$ for all $y\in\mathbb{R}\cup\{-\infty,\infty\}$.\\
        $\{Y\leq y\}$ is a tail event.
    \end{eg}
    \begin{thm}
        If $Y$ is a tail function of independent sequence of random variables $X_{1},X_{2},\cdots$, then there exists $-\infty\leq k\leq\infty$,
        \begin{equation*}
            \prob(Y=k)=1
        \end{equation*}
    \end{thm}
    Again let $X_{1},X_{2},\cdots$ be i.i.d. random variables and let $S_{n}=\sum_{i=1}^{n}X_{i}$.\\
    Recall that if $\expect\abs{X_{1}}<\infty$,
    \begin{equation*}
        \prob\left(\lim_{n\to\infty}\frac{S_{n}}{n}=\expect X_{1}\right)=1
    \end{equation*}
    If $\expect\abs{X_{1}}=\infty$,
    \begin{equation*}
        \prob\left(\lim_{n\to\infty}\frac{S_{n}}{n}\text{ exists}\right)=0
    \end{equation*}
    Using tail function, the random variables are not necessarily identically distributed.
    \begin{thm}
        Let $X_{1},X_{2},\cdots$ be independent random variables. Let $S_{n}=\sum_{i=1}^{n}X_{i}$. Then
        \begin{equation*}
            \prob\left(\lim_{n\to\infty}\frac{S_{n}}{n}\text{ exists}\right)=0\text{ or }1
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Let $Z_{1}=\limsup_{n\to\infty}\frac{S_{n}}{n}$ and $Z_{2}=\liminf_{n\to\infty}\frac{S_{n}}{n}$. We claim that both $Z_{1}$ and $Z_{2}$ are tail functions of $X_{i}$'s. For any $k$,
        \begin{align*}
            Z_{1}(\omega)&=\limsup_{n\to\infty}\left(\frac{1}{n}\sum_{i=1}^{k}X_{i}(\omega)+\frac{1}{n}\sum_{i=k+1}^{n}X_{i}(\omega)\right) & Z_{2}(\omega)&=\liminf_{n\to\infty}\left(\frac{1}{n}\sum_{i=1}^{k}X_{i}(\omega)+\frac{1}{n}\sum_{i=k+1}^{n}X_{i}(\omega)\right)
        \end{align*}
        Therefore, both $Z_{1}$ and $Z_{2}$ do not depend on any finite collection of $X_{i}$. We say that $\{Z_{1}=Z_{2}\}$ is a tail event.\\
        Therefore, by Kolmogorov's zero-one law.
        \begin{equation*}
            \prob\left(\lim_{n\to\infty}\frac{S_{n}}{n}\text{ exists}\right)=\prob(Z_{1}=Z_{2})=0\text{ or }1
        \end{equation*}
    \end{proofing}
    \begin{eg}(Random power series)
        Let $X_{1},X_{2},\cdots$ be i.i.d. exponential random variables with parameter $\lambda=1$. We consider a random power series
        \begin{equation*}
            p(z;\omega)=\sum_{n=0}^{\infty}X_{n}(\omega)z^{n}
        \end{equation*}
        The formula for radius of convergence is
        \begin{equation*}
            R(\omega)=\frac{1}{\limsup_{n\to\infty}\abs{X_{n}(\omega)}^{\frac{1}{n}}}
        \end{equation*}
        We can get that $R(\omega)$ is a tail function of $X_{i}$'s. Therefore, there exists $C$ such that $\prob(R=C)=1$ ($R=C$ almost surely)\\
        We want to find the value of $C$.\\
        We claim that $C=1$.
        \begin{equation*}
            \prob\left(\limsup_{n\to\infty}\abs{X_{n}}^{\frac{1}{n}}=1\right)=1
        \end{equation*}
        It suffices to show that for all $\varepsilon>0$,
        \begin{align*}
            \prob\left(\limsup_{n\to\infty}\abs{X_{n}}^{\frac{1}{n}}\leq 1+\varepsilon\right)&=1 & \prob\left(\limsup_{n\to\infty}\abs{X_{n}}^{\frac{1}{n}}\geq 1-\varepsilon\right)&=1
        \end{align*}
        We first prove the first one.
        \begin{equation*}
            \sum_{n=1}^{\infty}\prob\left(\abs{X_{n}}^{\frac{1}{n}}>1+\varepsilon\right)=\sum_{n=1}^{\infty}\prob(\abs{X_{n}}>(1+\varepsilon)^{n})=\sum_{n=1}^{\infty}e^{-(1+\varepsilon)^{n}}<\infty
        \end{equation*}
        Therefore, by BCI,
        \begin{equation*}
            \prob(\abs{X_{n}}^{\frac{1}{n}}>1+\varepsilon\text{ i.o.})=0\implies\prob\left(\limsup_{n\to\infty}\abs{X_{n}}^{\frac{1}{n}}\leq 1+\varepsilon\right)=1
        \end{equation*}
        Similarly,
        \begin{equation*}
            \sum_{n=1}^{\infty}\prob\left(\abs{X_{n}}^{\frac{1}{n}}>1-\varepsilon\right)=\sum_{n=1}^{\infty}\prob(\abs{X_{n}}>(1-\varepsilon)^{n})=\sum_{n=1}^{\infty}e^{-(1-\varepsilon)^{n}}=\infty
        \end{equation*}
        Therefore, by BCII,
        \begin{equation*}
            \prob(\abs{X_{n}}^{\frac{1}{n}}>1-\varepsilon\text{ i.o.})=1\implies\prob\left(\limsup_{n\to\infty}\abs{X_{n}}^{\frac{1}{n}}\geq 1-\varepsilon\right)=1
        \end{equation*}
        By sending $\varepsilon\downarrow 0$, we get
        \begin{equation*}
            \prob\left(\limsup_{n\to\infty}\abs{X_{n}}^{\frac{1}{n}}=1\right)=1
        \end{equation*}
        Therefore, $C=1$.
    \end{eg}

\section{Strong Law of Large Numbers}
    Let us revisit the Weak Law of Large Numbers (WLLN). Consider $X_{1},X_{2},\dots$ as a sequence of i.i.d. random variables with $\expect(X_{1})=\mu$. Define $S_{n}=\sum_{i=1}^{n}X_{i}$. Then, as $n\to\infty$,
    \begin{align*}
        \frac{S_{n}}{n}&\xrightarrow{D}\mu & \frac{S_{n}}{n}&\xrightarrow{\prob}\mu
    \end{align*}
    The Strong Law of Large Numbers (SLLN) is a more robust version of WLLN. Below, we prove one version of SLLN.
    \begin{thm}(\textbf{Strong Law of Large Numbers} [SLLN])
        Let $X_{1},X_{2},\dots$ be i.i.d. random variables with $\expect X_{1}=\mu$ and $\expect\abs{X_{1}}<\infty$. Define $S_{n}=\sum_{i=1}^{n}X_{i}$. Then,
        \begin{equation*}
            \frac{S_{n}}{n}\xrightarrow{\text{a.s.}}\mu
        \end{equation*}
    \end{thm}
    Note that the proof of SLLN is intricate and will not be covered here. Instead, we present a simpler version of SLLN.
    \newpage
    \begin{thm}(SLLN with $\expect X_{i}^{4}<\infty$)
        Let $X_{1},X_{2},\dots$ be i.i.d. random variables with $\expect X_{1}=0$ and $\expect(X_{1}^{4})<\infty$. Define $S_{n}=\sum_{i=1}^{n}X_{i}$. Then,
        \begin{equation*}
            \frac{S_{n}}{n}\xrightarrow{\text{a.s.}}0
        \end{equation*}
    \end{thm}
    \begin{proofing}
        \begin{equation*}
            \expect S_{n}^{4}=\expect\left(\sum_{i=1}^{n}X_{i}\right)^{4}=\sum_{i,j,k,\ell=1}^{n}\expect X_{i}X_{j}X_{k}X_{\ell}
        \end{equation*}
        The expectation is non-zero only if there are two pairs of random variables with identical values.
        \begin{equation*}
            \expect S_{n}^{4}=3\sum_{i\neq j}\expect X_{i}^{2}\expect X_{j}^{2}+\sum_{i}\expect X_{i}^{4}=O(n^{2})
        \end{equation*}
        \begin{equation*}
            \prob\left(\abs{\frac{S_{n}}{n}}\geq\varepsilon\right)\leq\frac{\expect S_{n}^{4}}{(n\varepsilon)^{4}}=O\left(\frac{1}{n^{2}}\right)
        \end{equation*}
        Consequently, for all $\varepsilon>0$,
        \begin{equation*}
            \sum_{n=1}^{\infty}\prob\left(\abs{\frac{S_{n}}{n}}>\varepsilon\right)<\infty
        \end{equation*}
        Thus, $\frac{S_{n}}{n}\xrightarrow{\text{a.s.}}0$.
    \end{proofing}
    \begin{thm}(SLLN with $\expect X_{1}^{2}<\infty$) Let $X_{1},X_{2},\dots$ be i.i.d. random variables with $\expect X_{1}^{2}<\infty$ and $\expect X_{i}=\mu$. Define $S_{n}=\sum_{i=1}^{n}X_{i}$. Then,
        \begin{align*}
            \frac{S_{n}}{n}&\xrightarrow{2}\mu & \frac{S_{n}}{n}&\xrightarrow{\text{a.s.}}\mu
        \end{align*}
    \end{thm}
    \begin{proofing}
        We first demonstrate convergence in mean square. Since $\expect X_{1}^{2}<\infty$, as $n\to\infty$,
        \begin{equation*}
            \expect\left(\frac{S_{n}}{n}-\mu\right)^{2}=\frac{\expect(S_{n}-n\mu)^{2}}{n^{2}}=\frac{\Var(S_{n})}{n^{2}}=\frac{\Var(X_{1})}{n}\to 0
        \end{equation*}
        For almost sure convergence, we know that convergence in probability implies the existence of almost sure convergence for some subsequence of $\frac{S_{n}}{n}$ to $\mu$. Let $n_{i}=i^{2}$. Using Markov's inequality, for all $\varepsilon>0$,
        \begin{equation*}
            \sum_{i}\prob\left(\frac{\abs{S_{i^{2}}-i^{2}\mu}}{i^{2}}>\varepsilon\right)\leq\sum_{i}\frac{\expect\abs{S_{i^{2}}-i^{2}\mu}^{2}}{i^{4}\varepsilon^{2}}=\sum_{i}\frac{\Var(S_{i^{2}})}{i^{4}\varepsilon^{2}}=\sum_{i}\frac{\Var(X_{1})}{i^{2}\varepsilon^{2}}<\infty
        \end{equation*}
        Therefore, $\frac{S_{i^{2}}}{i^{2}}\xrightarrow{\text{a.s.}}\mu$. However, we need to address the gaps.\\
        Assume $X_{i}$ are non-negative. Then $S_{i^{2}}\leq S_{n}\leq S_{(i+1)^{2}}$ if $i^{2}\leq n\leq (i+1)^{2}$.\\
        We can deduce that
        \begin{equation*}
            \frac{S_{i^{2}}}{(i+1)^{2}}\leq\frac{S_{n}}{n}\leq\frac{S_{(i+1)^{2}}}{i^{2}}
        \end{equation*}
        Since $\frac{S_{i^{2}}}{i^{2}}\xrightarrow{\text{a.s.}}\mu$, and $\frac{i^{2}}{(i+1)^{2}}\to 1$ as $i\to\infty$, we conclude that for non-negative $X_{i}$, as $n\to\infty$,
        \begin{equation*}
            \frac{S_{n}}{n}\xrightarrow{\text{a.s.}}\mu
        \end{equation*}
        For general $X_{i}$, we can write $X_{n}=X_{n}^{+}-X_{n}^{-}$ where
        \begin{align*}
            X_{n}^{+}(\omega)&=\max\{X_{n}(\omega),0\} & X_{n}^{-}(\omega)&=-\min\{X_{n}(\omega),0\}
        \end{align*}
        Both $X_{n}^{+}(\omega)$ and $X_{n}^{-}(\omega)$ are non-negative.\\
        Furthermore, $X_{n}^{+}\leq\abs{X_{n}}$ and $X_{n}^{-}\leq\abs{X_{n}}$. Thus, $\expect(X_{n}^{+})^{2}<\infty$ and $\expect(X_{n}^{-})^{2}<\infty$. By the earlier conclusion for non-negative random variables, we find that as $n\to\infty$,
        \begin{equation*}
            \frac{S_{n}}{n}=\frac{1}{n}\left(\sum_{i=1}^{n}X_{i}^{+}-\sum_{i=1}^{n}X_{i}^{-}\right)\xrightarrow{\text{a.s.}}\expect X_{1}^{+}-\expect X_{1}^{-}=\expect X_{1}
        \end{equation*}
        Therefore, $\frac{S_{n}}{n}\xrightarrow{\text{a.s.}}\mu$.
    \end{proofing}
    \newpage
    Why do we need SLLN? There are a lot of applications that specifically need SLLN.
    \begin{eg}(Renewal Theory) Assume that we have a light bulb. We change it immediately when it burnt out.\\
    Let $X_{i}$ be the lifetime of $i$-th bulb and $T_{n}=X_{1}+X_{2}+\cdots+X_{n}$ be the time to replace the $n$-th bulb.\\
    Let $N_{t}=\sup\{n:T_{n}\leq t\}$ be number of bulbs that have burnt out by time $t$. $T_{N_{t}}$ is the exact time that $N_{t}$'s bulb burnt out.\\
    Since we are dealing with practical bulb, assume that $X_{1},X_{2},\cdots$ are i.i.d. random variables with $0<X_{i}<\infty$ and $\expect X_{1}<\infty$.
    \end{eg}
    \begin{thm}
        Let $\expect X_{1}=\mu$. As $t\to\infty$,
        \begin{equation*}
            \frac{t}{N_{t}}\xrightarrow{\text{a.s.}}\mu
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Since $T_{N_{t}}\leq t<T_{N_{t}+1}$,
        \begin{equation*}
            \frac{T_{N_{t}}}{N_{t}}\leq\frac{t}{N_{t}}<\frac{T_{N_{t}+1}}{N_{t}+1}\left(\frac{N_{t}+1}{N_{t}}\right)
        \end{equation*}
        By SLLN, we know that $\frac{T_{n}}{n}\xrightarrow{\text{a.s.}}\mu$. Since $\frac{T_{n}}{n}$ and $\frac{T_{N_{t}}}{N_{t}}$ are the same sequence, we get that
        \begin{align*}
            \frac{T_{N_{t}}}{N_{t}}&\xrightarrow{\text{a.s.}}\mu & \frac{T_{N_{t}+1}}{N_{t}+1}&\xrightarrow{\text{a.s.}}\mu
        \end{align*}
        For all $\omega\in\Omega$, $t<T_{N_{t}+1}=X_{1}(\omega)+X_{2}(\omega)+\cdots+X_{N_{t}(\omega)+1}(\omega)$.\\
        As $t\to\infty$, it forces $N_{t}(\omega)\to\infty$. Therefore, $\frac{N_{t}+1}{N_{t}}\xrightarrow{\text{a.s.}}1$.
        Combining all of this, we get $\frac{t}{N_{t}}\xrightarrow{\text{a.s.}}\mu$.
    \end{proofing}
    \begin{cla}
        If $X_{n}\xrightarrow{\prob}X_{\infty}$, then $N_{m}\xrightarrow{\text{a.s.}}\infty$ as $m\to\infty$.
    \end{cla}
    \begin{rem}
        For this claim, it is not necessary that $X_{N_{m}}\xrightarrow{\text{a.s}}X_{\infty}$ or $X_{N_{m}}\xrightarrow{\prob}X_{\infty}$.
    \end{rem}
    \begin{eg}
        Recall the example that we use in Theorem \ref{Chapter 7 Theorem implications of different convergence modes} to prove $(X_{n}\xrightarrow{\prob}X)\centernot\implies(X_{n}\xrightarrow{\text{a.s.}}X)$. Let $\Omega=[0,1]$. Let
        \begin{equation*}
            Y_{m,k}=\mathbf{1}_{I_{m,k}}=\begin{cases}
                1, &\omega\in\left[\frac{k-1}{m},\frac{k}{m}\right]\\
                0, &\text{Otherwise}
            \end{cases}
        \end{equation*}
        Let $X_{n}$ be the enumeration of $Y_{m,k}$. i.e. $X_{1}=Y_{1,1}$, $X_{2}=Y_{2,1}$, $X_{3}=Y_{2,2}$, $\cdots$.\\
        From the proof of the theorem, we got that $X_{n}\xrightarrow{\prob}X_{\infty}=0$ but $X_{n}\centernot{\xrightarrow{\text{a.s.}}}X_{\infty}$.\\
        For each $\omega\in\Omega$, and each $m\geq 1$, there exists $k$ such that $\omega\in\left[\frac{k-1}{m},\frac{k}{m}\right]$. We denote these as $k_{m}(\omega)$.\\
        Let $N_{m}(\omega)=\sum_{i=1}^{m-1}i+k_{m}(\omega)$. We get that $X_{N_{m}(\omega)}(\omega)=Y_{m,k_{m}(\omega)}(\omega)=1$.\\
        However, $X_{\infty}=0$. That means, $X_{N_{m}}\centernot{\xrightarrow{\prob}}X_{\infty}$ and $X_{N_{m}}\centernot{\xrightarrow{\text{a.s.}}}X_{\infty}$.
    \end{eg}
    We move to our next examples, which is the Glivenko-Cantelli Theorem. It is also called the Fundamental Theorem of Statistics.
    \begin{thm}(Glivenko-Cantelli Theorem)
        Assume that $X\sim F(x)$ where $F(x)$ is unknown. Let $X_{1},X_{2},\cdots$ be i.i.d. random samples of $X$. We define the empirical distribution function, which is also a distribution function of a histogram.
        \begin{align*}
            F_{N}(x)&=\frac{1}{N}\sum_{i=1}^{N}\mathbf{1}_{X_{i}\leq x} & F_{N}(x;\omega)&=\frac{1}{N}\sum_{i=1}^{N}\mathbf{1}_{X_{i}(\omega)\leq x}
        \end{align*}
        We have that
        \begin{equation*}
            \sup_{x}\abs{F_{n}(x)-F(x)}\xrightarrow{\text{a.s.}}0
        \end{equation*}
    \end{thm}
    \begin{proofing}
        We only proof for the case when $F(x)$ is continuous.\\
        For each $m$, there exists $-\infty=x_{0}<x_{1}<\cdots<x_{m}=\infty$ such that $F(x_{i})-F(x_{i-1})=\frac{1}{m}$.\\
        For all $x\in[x_{i-1},x_{i})$,
        \begin{align*}
            F_{N}(x)-F(x)&\leq F_{N}(x_{i})-F(x_{i-1})=F_{N}(x_{i})-F(x_{i})+\frac{1}{m}\\
            F_{N}(x)-F(x)&\geq F_{N}(x_{i-1})-F(x_{i})=F_{N}(x_{i-1})-F(x_{i-1})-\frac{1}{m}
        \end{align*}
        From this, we get
        \begin{equation*}
            -\sup_{i}\abs{F_{N}(x_{i})-F(x_{i})}-\frac{1}{m}\leq F_{N}(x)-F(x)\leq\sup_{i}\abs{F_{N}(x_{i})-F(x_{i})}+\frac{1}{m}\implies\sup_{x}\abs{F_{N}(x)-F(x)}\leq\sup_{i}\abs{F_{N}(x_{i})-F(x_{i})}+\frac{1}{m}
        \end{equation*}
        By SLLN, when we fix $x$, we get
        \begin{align*}
            F_{N}(x)=\frac{1}{N}\sum_{i=1}^{N}\mathbf{1}_{X_{i}\leq x}&\xrightarrow{\text{a.s.}}\expect\mathbf{1}_{X_{1}\leq x}=\prob(X_{1}\leq x)=F(x) & \prob(\{\omega\in\Omega:F_{N}(x;\omega)\to F(x)\text{ as }N\to\infty\})&=1
        \end{align*}
        Let $C_{x}=\{\omega\in\Omega:F_{N}(x;\omega)\to F(x)\text{ as }N\to\infty\}$. Notice that if $\omega\in\bigcap_{i=1}^{\infty}C_{x_{i}}$, $\sup_{i}\abs{F_{N}(x_{i};\omega)-F(x_{i})}\to 0$.
        \begin{equation*}
            \limsup_{N}\sup_{x}\abs{F_{N}(x)-F(x)}\leq\frac{1}{m}
        \end{equation*}
        If $\omega\in\bigcap_{m=1}^{\infty}\bigcap_{i=1}^{m}C_{x_{i}}$,
        \begin{equation*}
            \limsup_{N}\sup_{x}\abs{F_{N}(x)-F(x)}=0
        \end{equation*}
        Therefore, since $\bigcap_{m=1}^{\infty}\bigcap_{i=1}^{m}C_{x_{i}}\subseteq\{\omega\in\Omega:\sup_{x}\abs{F_{N}(x;\omega)-F(x)}\to 0\text{ as }N\to\infty\}$ and $\prob(C_{x_{i}})=1$ by SLLN,
        \begin{equation*}
            \prob(\{\omega\in\Omega:\sup_{x}\abs{F_{N}(x;\omega)-F(x)}\to 0\text{ as }N\to\infty\})=1
        \end{equation*}
    \end{proofing}
    We will end here. Of course, there are still a lot of examples that we haven't explored (including some mentioned during the lectures that I'm too lazy to include here). We also skipped a lot of proofs in some of the theorems. It is up to you to explore further, either in other courses or in the future world of mathematics.

\appendix
\renewcommand{\thechapter}{\Alph{chapter}}
    
\chapter{Random walk}
    \label{Simple random walk}
    \begin{eg}(\textbf{Simple random walk})
        Consider a particle moving along the real line. At each step, it moves either one unit to the right with probability $p$ or one unit to the left with probability $q=1-p$.\\
        Let $S_{n}$ represent the particle's position after $n$ steps, with $S_{0}=a$. Then:
        \begin{equation*}
            S_{n}=a+\sum_{i=1}^{n}X_{i}
        \end{equation*}
        where $X_{1},X_{2},\dots$ are independent random variables taking the value $1$ with probability $p$ and $-1$ with probability $q$.\\
        The random walk is called \textbf{symmetric} if $p=q=\frac{1}{2}$.
    \end{eg}
    \begin{lem}
        \label{Simple random walk properties}
        A simple random walk has the following properties:
        \begin{enumerate}
            \item It is \textbf{spatially homogeneous}: $\prob(S_{n}=j|S_{0}=a)=\prob(S_{n}=j+b|S_{0}=a+b)$.
            \item It is \textbf{temporally homogeneous}: $\prob(S_{n}=j|S_{0}=a)=\prob(S_{m+n}=j|S_{m}=a)$.
            \item It satisfies the \textbf{Markov property}: $\prob(S_{m+n}=j|S_{0},S_{1},\dots,S_{m})=\prob(S_{m+n}=j|S_{m})$, $n\geq 0$.
        \end{enumerate}
    \end{lem}
    \begin{proofing}
            \begin{enumerate}
                \item $\prob(S_{n}=j|S_{0}=a)=\prob(\sum_{i=1}^{n}X_{i}=j-a)=\prob(S_{n}=j+b|S_{0}=a+b)$.
                \item 
                \begin{equation*}
                    \prob(S_{n}=j|S_{0}=a)=\prob\left(\sum_{i=1}^{n}X_{i}=j-a\right)=\prob\left(\sum_{i=m+1}^{m+n}X_{i}=j-a\right)=\prob(S_{m+n}=j|S_{m}=a).
                \end{equation*}
                \item If $S_{m}$ is known, the distribution of $S_{m+n}$ depends only on $X_{m+1},X_{m+2},\dots,X_{m+n}$, and is independent of $S_{0},S_{1},\dots,S_{m-1}$.
            \end{enumerate}
    \end{proofing}
    \begin{eg}(Probability via sample path counting)
        Define a \textbf{sample path} $\vec{s}=(s_{0},s_{1},\dots,s_{n})$ as the outcome or realization of the random walk, where $s_{0}=a$ and $s_{i+1}-s_{i}=\pm 1$.
        \begin{align*}
            \prob((S_{0},S_{1},\dots,S_{n})=(s_{0},s_{1},\dots,s_{n}))&=p^{r}q^{\ell}, & r&=\#\{i:s_{i+1}-s_{i}=1\}, & \ell&=\#\{i:s_{i+1}-s_{i}=-1\}.
        \end{align*}
    \end{eg}
    \begin{eg}
        Let $M_{n}^{r}(a,b)$ denote the number of paths $(s_{0},s_{1},\dots,s_{n})$ with $s_{0}=a$, $s_{n}=b$, and exactly $r$ steps to the right.
        \begin{equation*}
            \prob(S_{n}=b)=\sum_{r} M_{n}^{r}(a,b)p^{r}q^{n-r}.
        \end{equation*}
        Using the equations $r+\ell=n$ and $r-\ell=b-a$, we find $r=\frac{1}{2}(n+b-a)$ and $\ell=(n-b+a)$. If $\frac{1}{2}(n+b-a)\in\{0,1,\dots,n\}$,
        \begin{equation*}
            \prob(S_{n}=b)=\binom{n}{\frac{1}{2}(n+b-a)}p^{\frac{1}{2}(n+b-a)}q^{\frac{1}{2}(n-b+a)}.
        \end{equation*}
        Otherwise, $\prob(S_{n}=b)=0$.
    \end{eg}
    
    \newpage
    \begin{thm}(Reflection principle)
        Let $N_{n}(a,b)$ represent the number of possible paths from $(0,a)$ to $(n,b)$, and let $N_{n}^{0}(a,b)$ denote the number of such paths that pass through some point $(k,0)$ on the $x$-axis. If $a,b>0$, then:
        \begin{equation*}
            N_{n}^{0}(a,b)=N_{n}(-a,b).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Each path from $(0,-a)$ to $(n,b)$ intersects the $x$-axis at some earliest point $(k,0)$.\\
        Reflect the segment of the path with $0\leq x\leq k$ across the $x$-axis to obtain a path joining $(0,a)$ to $(n,b)$ that intersects the $x$-axis.\\
        This operation establishes a one-to-one correspondence between these collections of paths.
    \end{proofing}
    \begin{lem}
        \label{Number of paths calculation}
        \begin{equation*}
            N_{n}(a,b)=\binom{n}{\frac{1}{2}(n+b-a)}.
        \end{equation*}
    \end{lem}
    \begin{proofing}
        Consider a path from $(0,a)$ to $(n,b)$, and let $\alpha$ and $\beta$ represent the number of positive and negative steps, respectively.\\
        Then $\alpha+\beta=n$ and $\alpha-\beta=b-a$, which implies $\alpha=\frac{1}{2}(n+b-a)$.\\
        The number of such paths corresponds to the number of ways to choose $\alpha$ positive steps from $n$ available steps. Thus,
        \begin{equation*}
            N_{n}(a,b)=\binom{n}{\alpha}=\binom{n}{\frac{1}{2}(n+b-a)}.
        \end{equation*}
    \end{proofing}
    \begin{eg}
        We aim to determine the probability that the walk does not revisit its starting point during the first $n$ steps.\\
        Without loss of generality, assume $S_{0}=0$, so that $S_{1},S_{2},\dots,S_{n}\neq 0$ if and only if $S_{1}S_{2}\cdots S_{n}\neq 0$.\\
        The event $S_{1}S_{2}\cdots S_{n}\neq 0$ occurs if and only if the path of the walk does not intersect the $x$-axis during the interval $[1,n]$.\\
        If $b>0$, the first step must be $(1,1)$. By Lemma \ref{Number of paths calculation} and the Reflection Principle, the number of such paths is:
        \begin{align*}
            N_{n-1}(1,b)-N_{n-1}^{0}(1,b)&=N_{n-1}(1,b)-N_{n-1}(-1,b)\\
            &=\binom{n-1}{\frac{1}{2}(n+b-2)}-\binom{n-1}{\frac{1}{2}(n+b)}\\
            &=\left(\frac{n+b}{2n}-\frac{n-b}{2n}\right)\binom{n}{\frac{1}{2}(n+b)}\\
            &=\frac{b}{n}\binom{n}{\frac{1}{2}(n+b)}.
        \end{align*}
        There are $\frac{1}{2}(n+b)$ rightward steps and $\frac{1}{2}(n-b)$ leftward steps. Therefore,
        \begin{equation*}
            \prob(S_{1}S_{2}\cdots S_{n}\neq 0,S_{n}=b)=\frac{b}{n}N_{n}(0,b)p^{\frac{1}{2}(n+b)}q^{\frac{1}{2}(n-b)}=\frac{b}{n}\prob(S_{n}=b).
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $M_{n}=\max\{S_{i}:0\leq i\leq n\}$ denote the maximum value attained by the random walk up to time $n$. Suppose $S_{0}=0$, so $M_{n}\geq 0$. Clearly, $M_{n}\geq S_{n}$.
    \end{eg}
    \begin{thm}
        Suppose $S_{0}=0$. Then, for $r\geq 1$,
        \begin{equation*}
            \prob(M_{n}\geq r,S_{n}=b)=\begin{cases}
                \prob(S_{n}=b), &\text{if }b\geq r\\
                \left(\frac{q}{p}\right)^{r-b}\prob(S_{n}=2r-b), &\text{if }b<r.
            \end{cases}
        \end{equation*}
        Consequently, for $r\geq 1$,
        \begin{equation*}
            \prob(M_{n}\geq r)=\prob(S_{n}\geq r)+\sum_{b=-\infty}^{r-1}\left(\frac{q}{p}\right)^{r-b}\prob(S_{n}=2r-b)=\prob(S_{n}=r)+\sum_{c=r+1}^{\infty}\left(1+\left(\frac{q}{p}\right)^{c-r}\right)\prob(S_{n}=c).
        \end{equation*}
        For the symmetric case where $p=q=\frac{1}{2}$,
        \begin{equation*}
            \prob(M_{n}\geq r)=2\prob(S_{n}\geq r+1)+\prob(S_{n}=r).
        \end{equation*}
    \end{thm}

    \newpage
    \begin{proofing}
        Assume $r\geq 1$ and $b<r$. Let $N_{n}^{r}(0,b)$ denote the number of paths from $(0,0)$ to $(n,b)$ that include at least one point with height $r$ (i.e., some point $(i,r)$ with $0<i<n$).\\
        For a path $\pi$, let $(i_{\pi},r)$ be the earliest such point.\\
        Reflect the segment of the path with $i_{\pi}\leq x\leq n$ across the line $y=r$ to obtain a path $\pi'$ joining $(0,0)$ to $(n,2r-b)$.\\
        Thus, $N_{n}^{r}(0,b)=N_{n}(0,2r-b)$.
        \begin{equation*}
            \prob(M_{n}\geq r,S_{n}=b)=N_{n}^{r}(0,b)p^{\frac{1}{2}(n+b)}q^{\frac{1}{2}(n-b)}=\left(\frac{q}{p}\right)^{r-b}N_{n}(0,2r-b)p^{\frac{1}{2}(n+2r-b)}q^{\frac{1}{2}(n-2r+b)}=\left(\frac{q}{p}\right)^{r-b}\prob(S_{n}=2r-b).
        \end{equation*}
    \end{proofing}

\chapter{Terminologies in other fields of mathematics}
    \begin{defn}
        The \textbf{supremum} of a subset $S$ is the smallest upper bound $x$ such that $x\geq a$ for all $a \in S$. It is denoted as:
        \begin{equation*}
            x=\sup S
        \end{equation*}
    \end{defn}
    \begin{defn}
        The \textbf{infimum} of a subset $S$ is the greatest lower bound $x$ such that $x\leq b$ for all $b \in S$. It is denoted as:
        \begin{equation*}
            x=\inf S
        \end{equation*}
    \end{defn}
    \begin{defn}
        The \textbf{limit superior} and \textbf{limit inferior} of a sequence $x_{1},x_{2},\dots$ are defined as:
        \begin{align*}
            \limsup_{n\to\infty}x_{n}&=\lim_{n\to\infty}\sup_{m\geq n}x_{m}, & \liminf_{n\to\infty}x_{n}&=\lim_{n\to\infty}\inf_{m\geq n}x_{m}.
        \end{align*}
    \end{defn}
    \begin{defn}
        An infinite series $\sum_{n=0}^{\infty}a_{n}$ is \textbf{absolutely convergent} if there exists a real number $L$ such that:
        \begin{equation*}
            \sum_{n=0}^{\infty}\abs{a_{n}}=L.
        \end{equation*}
        Rearranging or grouping terms in an absolutely convergent series does not change its sum.\\
        A series is \textbf{conditionally convergent} if it converges but does not satisfy the condition of absolute convergence.
    \end{defn}
    \begin{defn}(\textbf{Monotonicity})
        A \textbf{monotonic} function is one that is either entirely non-increasing or entirely non-decreasing.\\
        A \textbf{strictly monotonic} function is one that is either entirely strictly increasing or strictly decreasing.
    \end{defn}
    \begin{defn}
        The \textbf{arguments of the maxima} are the input values at which a function achieves its maximum output. It is defined as:
        \begin{equation*}
            \argmax_{x\in S}f(x)=\{x\in S:f(x)\geq f(s)\text{ for all }s\in S\}.
        \end{equation*}
    \end{defn}
    \begin{defn}
        The \textbf{arguments of the minima} are the input values at which a function achieves its minimum output. It is defined as:
        \begin{equation*}
            \argmin_{x\in S}f(x)=\{x\in S:f(x)\leq f(s)\text{ for all }s\in S\}.
        \end{equation*}
    \end{defn}
    \begin{defn}(\textbf{Linearity})
        A \textbf{linear} function $f$ satisfies the following two properties:
        \begin{enumerate}
            \item $f(x+y)=f(x)+f(y)$.
            \item $f(ax)=af(x)$ for all $a$.
        \end{enumerate}
    \end{defn}
    \begin{defn}
        A \textbf{regular} function $f$ satisfies the following conditions:
        \begin{enumerate}
            \item It is single-valued (each input in the domain maps to exactly one output).
            \item It is analytic ($f$ can be expressed as a convergent power series).
        \end{enumerate}
    \end{defn}
    \begin{defn}
        Let $V$ be the space of all real functions on $[0,1]$. A \textbf{norm} $\norm{\cdot}:V\to\mathbb{R}$ of a function $f$ satisfies:
        \begin{enumerate}
            \item $\norm{f}\geq 0$ for all $f\in V$.
            \item If $\norm{f}=0$, then $f=0$.
            \item $\norm{af}=\abs{a}\norm{f}$ for all $f\in V$ and $a\in\mathbb{R}$.
            \item (Triangle inequality) $\norm{f+g}\leq\norm{f}+\norm{g}$ for all $f,g\in V$.
        \end{enumerate}
        The $L_{p}$ norm for $p\geq 1$ is defined as:
        \begin{equation*}
            \norm{f}_{p}=\left(\intlu{0}{1}\abs{f(x)}^{p}\,dx\right)^{\frac{1}{p}}.
        \end{equation*}
        The \textbf{infinity norm} of a function $f\in V$ is defined as:
        \begin{equation*}
            \norm{f}_{\infty}=\max_{0\leq x\leq 1}\abs{f(x)}.
        \end{equation*}
    \end{defn}
    \begin{defn}
        Two functions $f$ and $g$ are \textbf{asymptotically equivalent} ($f\sim g$) if and only if:
        \begin{equation*}
            \lim_{x\to\infty}\frac{f(x)}{g(x)}=1.
        \end{equation*}
    \end{defn}
	
\chapter{Some useful inequalities}
    \begin{thm}(Triangle inequality)
        Let $X$ and $Y$ be random variables. Then:
        \begin{equation*}
            \abs{X+Y}\leq\abs{X}+\abs{Y}.
        \end{equation*}
    \end{thm}
    \begin{thm}(Reverse triangle inequality)
        Let $X$ and $Y$ be random variables. Then:
        \begin{equation*}
            \abs{X-Y}\geq\abs{\abs{X}-\abs{Y}}.
        \end{equation*}
    \end{thm}
    \begin{thm}(Cauchy-Schwarz inequality)
        Let $X$ and $Y$ be random variables. Then:
        \begin{equation*}
            \abs{\expect(XY)}^{2}\leq\expect(X^{2})\expect(Y^{2}).
        \end{equation*}
    \end{thm}
    \begin{thm}(Covariance inequality)
        Let $X$ and $Y$ be random variables. Then:
        \begin{equation*}
            \abs{\cov(X,Y)}^{2}\leq\Var(X)\Var(Y).
        \end{equation*}
    \end{thm}
    \begin{thm}(Markov's inequality)
        Let $X$ be a random variable with a finite mean. For all $k>0$ and any non-negative function $\gamma$ that is increasing on $[0,\infty)$:
        \begin{equation*}
            \prob(\abs{X}\geq k)\leq\frac{\expect(\gamma(\abs{X}))}{\gamma(k)}.
        \end{equation*}
    \end{thm}
    \begin{thm}(Chebyshev's inequality)
        Let $X$ be a random variable with $\expect X=\mu$ and $\Var(X)=\sigma^{2}$. For all $k>0$:
        \begin{equation*}
            \prob(\abs{X-\mu}\geq k\sigma)\leq\frac{1}{k^{2}}.
        \end{equation*}
    \end{thm}
    \begin{thm}(H\"older's inequality)
        Let $X$ and $Y$ be random variables. For any $p>1$, let $q=\frac{p}{p-1}$. Then:
        \begin{equation*}
            \expect\abs{XY}\leq(\expect\abs{X}^{p})^{\frac{1}{p}}(\expect\abs{Y}^{q})^{\frac{1}{q}}.
        \end{equation*}
    \end{thm}
    \begin{thm}(Lyapunov's inequality)
        Let $X$ be a random variable. For all $0<s\leq r$:
        \begin{equation*}
            (\expect\abs{X}^{s})^{\frac{1}{s}}\leq(\expect\abs{X}^{r})^{\frac{1}{r}}.
        \end{equation*}
    \end{thm}
    \begin{thm}(Minkowski inequality)
        Let $X$ and $Y$ be random variables. For any $r\geq 1$:
        \begin{equation*}
            (\expect\abs{X+Y}^{r})^{\frac{1}{r}}\leq(\expect\abs{X}^{r})^{\frac{1}{r}}+(\expect\abs{Y}^{r})^{\frac{1}{r}}.
        \end{equation*}
    \end{thm}
    \begin{thm}(Jensen's inequality)
        Let $X$ be a random variable and $\gamma$ a convex function. Then:
        \begin{equation*}
            \gamma(\expect X)\leq\expect(\gamma(X)).
        \end{equation*}
    \end{thm}
    For better memorization:\\
    Triangle inequality $\implies$ Reverse triangle inequality\\
    Markov's inequality $\implies$ Chebyshev's inequality\\
    H\"older's inequality $\implies$ Cauchy-Schwarz inequality $\implies$ Covariance inequality

\chapter{Some other distributions}
    \begin{eg}(Gamma distribution) $X\sim\Gamma(\alpha,\beta)$\\
        A random variable $X$ follows a gamma distribution with parameters $\alpha$ and $\beta$ if:
        \begin{align*}
            f(x)&=\frac{x^{\alpha-1}e^{-\beta x}\beta^{\alpha}}{\Gamma(\alpha)}, & \expect X&=\frac{\alpha}{\beta}, & \Var(X)&=\frac{\alpha}{\beta^{2}}, & M_{X}(t)&=\left(1-\frac{t}{\beta}\right)^{-\alpha}, & G_{X}(t)&=\left(1-\frac{it}{\beta}\right)^{-\alpha},
        \end{align*}
        where $\Gamma(\alpha)$ is the gamma function. If $\alpha$ is a positive integer, $\Gamma(\alpha)=(\alpha-1)!$.
    \end{eg}
    \begin{eg}(Chi-squared distribution) $Y\sim\chi^{2}(k)$\\
        Suppose $X_{1},X_{2},\dots,X_{n}$ are independent standard normal random variables. Let $Y=\sum_{i=1}^{n}X_{i}^{2}$. Then $Y$ follows a $\chi^{2}$-distribution with parameter $k$ if:
        \begin{align*}
            f(x)&=\begin{cases}
                \frac{x^{\frac{k}{2}-1}e^{-\frac{x}{2}}}{2^{\frac{k}{2}}\Gamma\left(\frac{k}{2}\right)}, &x\geq 0,\\
                0, &x<0,
            \end{cases} & \expect Y&=k, & \Var(Y)&=2k, & M_{Y}(t)&=(1-2t)^{-\frac{k}{2}}, & G_{Y}(t)&=(1-2it)^{-\frac{k}{2}}.
        \end{align*}
    \end{eg}

\end{document}
