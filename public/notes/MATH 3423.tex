\documentclass{huhtakm-template-book-v2}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\prob}{\mathbb{P}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\Poisson}{Poisson}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\Cauchy}{Cauchy}
\DeclareMathOperator{\NBin}{NBin}
\DeclareMathOperator{\U}{U}
\DeclareMathOperator{\Gam}{Gamma}
\DeclareMathOperator{\Bet}{Beta}
\setlength{\parindent}{0pt}
\title{
	\Huge MATH 3423: Statistical Inference
}
\author{
	HU-HTAKM\\
	\small Website: \url{https://htakm.github.io/htakm_test/}
}
\date{
	Last major change: December 7, 2024\\
	Last small update: September 16, 2025
}
\begin{document}
\maketitle
This is a lecture note for MATH 3423, created by me. Please note that some notations differ slightly from those used in the course, as I have adjusted them for greater clarity and to match my own preferences. For example:
\begin{table}[h]
	\centering
	\begin{tabular}{||c|c|c||}
		\hline
		Name & My notation & Dr. YU Chi Wai's notation\\
		\hline
		Transpose & $\mathbf{A}^{T}$ &  $\mathbf{A}'$\\
		Set of real numbers & $\mathbb{R}$ & $R$\\
		Fisher Information & $\mathcal{I}_{X}(\theta)$ & $I_{X}(\theta)$\\
		Convergence in distribution & $X_{n}\xrightarrow{D}X$ & $X_{n}\xrightarrow{d}X$\\
		Probability & $\prob$ & $P$\\
		Expected value & $\E$ & $E$ or $E_{X}$\\
		Indicator function & $\mathbf{1}_{A}$ & $I_{A}$\\
		chisq-value & $\chi_{\alpha,n}^{2}$ & $\chi_{\alpha}^{2}(n)$\\
		t-value & $t_{\alpha,n}$ & $t_{\alpha}(n)$\\
		f-value & $f_{\alpha,(n,m)}$ & $F_{\alpha}(n,m)$ or $f_{\alpha}(n,m)$\\
		\hline
	\end{tabular}
\end{table}

Some things to note about:
\begin{enumerate}
    \item Simply following all the examples in this lecture note may not be sufficient to excel in Dr. YU's exams, but it will help you understand the material (according to an anonymous reader who got A+ in this course (wtf)).
    \item Some topics covered here may or may not be included in the exam. This lecture note also contains material from MATH 2421/2431 and some supplementary notes that may not be tested.
    \item If you are preparing for Dr. YU's exams, please use his notation instead of mine.
    \item There may be typos in these notes. Please read with caution.
\end{enumerate}
\tableofcontents
\chapter{Preliminary}
    Statistical inference is the process of investigating how to use the information from the data and using data to make inferences about the distribution of a random variable of interest. In MATH 3423, we focus on two core concepts of statistical inference: point estimation and hypothesis testing.

\section{Random variables}
    In a particular event, it usually results in the outcome $\omega$. All possible outcomes are grouped into a sample space $\Omega$. To perform numerical analysis from the sample space, we map these outcomes to numerical values, which we define as random variables.
    \begin{defn}
        Given a sample space $\Omega$:
        \begin{enumerate}
            \item A \textbf{random variable} $X$ is a function $X:\Omega\to\mathbb{R}$ that maps outcomes in the sample space $\Omega$ to real numbers.
            \item The \textbf{probability} of $X$ taking a value in a set $A$ is defined as:
            \begin{equation*}
                \prob(X \in A) = \prob(\{\omega \in \Omega : X(\omega) \in A\}).
            \end{equation*}
            \item The \textbf{cumulative distribution function} (CDF) of $X$ is given by:
            \begin{equation*}
                F_{X}(x) = \prob(X \leq x).
            \end{equation*}
            \item The random variable $X$ is \textbf{discrete} if it has a \textbf{probability mass function} (PMF) $p_{X}$ defined as:
            \begin{equation*}
                p_{X}(x) = \prob(X = x).
            \end{equation*}
            \item The random variable $X$ is \textbf{continuous} if its CDF can be expressed using a \textbf{probability density function} (PDF) $f_{X}$ as:
            \begin{equation*}
                F_{X}(x) = \int_{-\infty}^{x} f_{X}(u) \, du.
            \end{equation*}
        \end{enumerate}
    \end{defn}
    \begin{defn}
        Two random variables $X$ and $Y$ are \textbf{independent} if either:
        \begin{equation*}
            f_{X,Y}(x, y) = f_{X}(x)f_{Y}(y) \quad \text{or} \quad F_{X,Y}(x, y) = F_{X}(x)F_{Y}(y).
        \end{equation*}
    \end{defn}
    This theorem is very important and will be used frequently later.
    \begin{thm}
        If $X$ and $Y$ are independent, then $f(X)$ and $g(Y)$ are independent for any functions $f$ and $g$.
    \end{thm}
    \newpage

\section{Random sample and parametric distribution}
    To make statistical inferences about the distribution of the random variable $X$, we need to collect a sample of data.
    \begin{defn}
        Denote the first observation by $X_{1}$, the second by $X_{2}$, and so on. A set of random variables $\{X_{1},\cdots,X_{n}\}$ is called a \textbf{random sample} of size $n$ from the common distribution of $X$ with a PMF $p_{X}(x)$ or PDF $f_{X}(x)$ if they are independent and identically distributed (i.i.d.).
    \end{defn}
    \begin{rem}
        The random variables $X_{1},\cdots,X_{n}$ are assumed to be observable, with known actual values $x_{1},\cdots,x_{n}$, respectively.
    \end{rem}
    Under the random sampling setting, the following lemma is straightforward.
    \begin{lem}
        Given a random sample $\{X_{1},\cdots,X_{n}\}$ from a common distribution $X$:
        \begin{enumerate}
            \item If the random sample is discrete with a common PMF $p_{X}(x)$, then the joint PMF of the random sample is:
            \begin{equation*}
                p_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}) = \prod_{i=1}^{n} p_{X_{i}}(x_{i}).
            \end{equation*}
            \item If the random sample is continuous with a common PDF $f_{X}(x)$, then the joint PDF of the random sample is:
            \begin{equation*}
                f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}) = \prod_{i=1}^{n} f_{X_{i}}(x_{i}).
            \end{equation*}
        \end{enumerate}
    \end{lem}
    In practice, the underlying distribution of $X$ is assumed to be unknown or partially known. In most situations, it is reasonable to assume that the form of the PMF $p_{X}$ or PDF $f_{X}$ of the distribution is known but contains some unknown parameters $\theta$.
    \begin{defn}
        A \textbf{parametric distribution} is a distribution where the PMF $p_{X}$ or PDF $f_{X}$ contains some unknown parameters $\theta$. Such a PMF or PDF is said to be \textbf{parametric}.
    \end{defn}
    \begin{rem}
        Instead of assuming parametric distributions for the data, we may assume that the form of the distribution is unknown but has certain properties. For example, a distribution may be continuous. Such a distribution is called a \textbf{non-parametric distribution}, and the corresponding statistical method is called a \textbf{non-parametric statistical approach}. If parameters are involved but the form of the distribution is unknown, the distribution is called a \textbf{semi-parametric distribution}, and the corresponding method is called a \textbf{semi-parametric statistical approach}.
    \end{rem}
    \begin{eg}
        Data are often assumed to follow a normal distribution with mean $\mu$ and variance $\sigma^{2}$, where the parameter:
        \begin{equation*}
            \theta = \begin{pmatrix}
                \mu\\
                \sigma^{2}
            \end{pmatrix}
        \end{equation*}
        is unknown but fixed.
    \end{eg}
    \begin{lem}
        Given a random sample $\{X_{1},\cdots,X_{n}\}$ from a common distribution $X$:
        \begin{enumerate}
            \item If the random sample is discrete with a common parametric PMF $p_{X}(x|\theta)$, then the joint PMF of the random sample is:
            \begin{equation*}
                p_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta) = \prod_{i=1}^{n} p_{X}(x_{i}|\theta).
            \end{equation*}
            \item If the random sample is continuous with a common parametric PDF $p_{X}(x|\theta)$, then the joint PDF of the random sample is:
            \begin{equation*}
                f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta) = \prod_{i=1}^{n} f_{X}(x_{i}|\theta).
            \end{equation*}
        \end{enumerate}
    \end{lem}
    \newpage

    Under the parametric setting, the uncertainty of the distribution is reduced to the uncertainty of its parameters. One of the central problems in statistics is determining which function of the data is the best estimator for $\theta$.
    \begin{defn}
        Let $\mathbf{X}=(X_{1}\ \cdots\ X_{n})^{T}$ be a random vector.
        \begin{enumerate}
            \item If $T(\cdot)$ is a real-valued or vector-valued function such that for all $\mathbf{X}\in\Omega$, $T(\mathbf{X})$ does not contain any unknown parameters, then $T(\mathbf{X})$ is called a \textbf{statistic}.
            \item If we use the statistic $T(\mathbf{X})$ to estimate an unknown parameter $\theta$, then $T(\mathbf{X})$ and $T(\mathbf{x})$ are called an \textbf{estimator} and an \textbf{estimate} of $\theta$, respectively, where $\mathbf{x}$ is an observed value of $\mathbf{X}$.
        \end{enumerate}
    \end{defn}
    \begin{rem}
        We usually denote an estimator of $\theta$ by $\hat{\theta}(\mathbf{X})$ or simply $\hat{\theta}$.
    \end{rem}
    \begin{rem}
        Since $T(\mathbf{X})$ is also random, it has a distribution called the \textbf{sampling distribution}.
    \end{rem}

\section{Moments}
    The population moments of a distribution play a significant role in both theoretical and applied statistics. Let us first define what a moment is.
    \begin{defn}
        Given a random variable $X$:
        \begin{enumerate}
            \item If the random variable is discrete with a PMF $p_{X}(x)$, the \textbf{expectation} or \textbf{population mean} of $X$ is defined as:
            \begin{equation*}
                \mu = \E(X) = \sum_{x} x p_{X}(x).
            \end{equation*}
            \item If the random variable is continuous with a PDF $f_{X}(x)$, the \textbf{expectation} or \textbf{population mean} of $X$ is defined as:
            \begin{equation*}
                \mu = \E(X) = \int_{-\infty}^{\infty} x f_{X}(x) \, dx.
            \end{equation*}
        \end{enumerate}
    \end{defn}
    \begin{rem}
        The expression $\E[(X - a)^{k}]$ can be simplified to $\E(X - a)^{k}$. However, it should not be confused with $[\E(X - a)]^{k}$.
    \end{rem}
    \begin{lem}\named{Linearity of expectation}
        Given a set of random variables $\{X_{1}, \cdots, X_{n}\}$, for any constants $a_{i}$, the following holds:
        \begin{equation*}
            \E\left(\sum_{i=1}^{n} a_{i} X_{i}\right) = \sum_{i=1}^{n} a_{i} \E(X_{i}).
        \end{equation*}
    \end{lem}
    \begin{lem}
        Given a set of \textbf{independent} random variables, the following holds:
        \begin{equation*}
            \E\left(\prod_{i=1}^{n} X_{i}\right) = \prod_{i=1}^{n} \E(X_{i}).
        \end{equation*}
    \end{lem}
    \begin{defn}
        Given a random variable $X$, the \textbf{population variance} of $X$ is defined as:
        \begin{equation*}
            \sigma^{2} = \Var(X) = \E[(X - \E(X))^{2}] = \E(X^{2}) - [\E(X)]^{2}.
        \end{equation*}
    \end{defn}
    \begin{lem}
        Given a set of \textbf{independent} random variables $\{X_{1}, \cdots, X_{n}\}$, for any constants $a_{i}$, the following holds:
        \begin{equation*}
            \Var\left(\sum_{i=1}^{n} a_{i} X_{i}\right) = \sum_{i=1}^{n} a_{i}^{2} \Var(X_{i}).
        \end{equation*}
    \end{lem}
    \begin{defn}
        Given two random variables $X$ and $Y$, the \textbf{covariance} of $X$ and $Y$ is defined as:
        \begin{equation*}
            \cov(X, Y) = \E[(X - \E(X))(Y - \E(Y))] = \E(XY) - \E(X)\E(Y).
        \end{equation*}
    \end{defn}
    \newpage

    From these definitions, we can generalize the concept to higher-order population moments.
    \begin{defn}
        For each positive integer $k$:
        \begin{enumerate}
            \item The \textbf{$k$-th population moment} of $X$ about $0$, denoted by $\mu_{k}'$, is defined as:
            \begin{equation*}
                \mu_{k}' = \E(X^{k}),
            \end{equation*}
            if the expectation exists.
            \item The \textbf{$k$-th population central moment} of $X$, denoted by $\mu_{k}$, is defined as:
            \begin{equation*}
                \mu_{k} = \E[(X - \mu)^{k}],
            \end{equation*}
            if the expectation exists.
        \end{enumerate}
    \end{defn}
    \begin{rem}
        Do not confuse the population mean $\mu$ with the $k$-th population central moment $\mu_{k}$!
    \end{rem}
    \begin{eg}
        Some useful population moments have specific terminologies:
        \begin{enumerate}
            \item \textbf{Skewness}: $\mu_{3}$, which measures asymmetry or skewness.
            \begin{enumerate}
                \item If $\mu_{3} < 0$, the distribution is left-skewed (the tail is on the left).
                \item If $\mu_{3} > 0$, the distribution is right-skewed (the tail is on the right).
                \item If $\mu_{3} = 0$, the distribution is symmetric.
            \end{enumerate}
            The ratio $\frac{\mu_{3}}{\sigma^{3}}$ is called the \textbf{coefficient of skewness}.
            \item \textbf{Kurtosis}: $\mu_{4}$, which measures the degree of peakedness or flatness of a distribution near its center. The term $\frac{\mu_{4}}{\sigma^{4}} - 3$ is called the \textbf{coefficient of kurtosis}.
            \begin{enumerate}
                \item If $\frac{\mu_{4}}{\sigma^{4}} - 3 > 0$, the distribution has a sharper peak than the normal distribution.
                \item If $\frac{\mu_{4}}{\sigma^{4}} - 3 < 0$, the distribution has a flatter peak than the normal distribution.
            \end{enumerate}
        \end{enumerate}
    \end{eg}
    Sample moments are often used to estimate population moments.
    \begin{defn}
        Let $X_{1}, \cdots, X_{n}$ be a random sample of size $n$. For each positive integer $k$:
        \begin{enumerate}
            \item The \textbf{$k$-th sample moment} about $0$, denoted by $\overline{X^{k}}$, is defined as:
            \begin{equation*}
                \overline{X^{k}} = \frac{1}{n} \sum_{i=1}^{n} X_{i}^{k}.
            \end{equation*}
            When $k = 1$, $\overline{X}$ is called the \textbf{sample mean} of $X$.
            \item The \textbf{$k$-th sample moment} about $\overline{X}$, denoted by $S_{n}^{k}$, is defined as:
            \begin{equation*}
                S_{n}^{k} = \frac{1}{n} \sum_{i=1}^{n} (X_{i} - \overline{X})^{k}.
            \end{equation*}
        \end{enumerate}
    \end{defn}
    \begin{eg}
        When $k = 2$, $S_{n}^{2}$ is called the \textbf{sample variance}. However, we typically use an alternative version of the sample variance, denoted by $S_{n-1}^{2}$, which is defined as:
        \begin{equation*}
            S_{n-1}^{2} = \frac{1}{n - 1} \sum_{i=1}^{n} (X_{i} - \overline{X})^{2}.
        \end{equation*}
        This version is preferred because $S_{n-1}^{2}$ is an unbiased estimator, whereas $S_{n}^{2}$ is not.
    \end{eg}
    \newpage

    \begin{lem}
        Let $\{X_{1}, \cdots, X_{n}\}$ be a random sample of size $n$. Then:
        \begin{equation*}
            \E(\overline{X^{k}}) = \mu_{k}',
        \end{equation*}
        if $\mu_{k}'$ exists. Additionally:
        \begin{equation*}
            \Var(\overline{X^{k}}) = \frac{1}{n} \left[\mu_{2k}' - (\mu_{k}')^{2}\right].
        \end{equation*}
    \end{lem}
    \begin{proofing}
        Since $X_{1}, \cdots, X_{n}$ have the same distribution:
        \begin{equation*}
            \E(X_{1}^{k}) = \cdots = \E(X_{n}^{k}) = \E(X^{k}) = \mu_{k}'.
        \end{equation*}
        Therefore:
        \begin{equation*}
            \E(\overline{X^{k}}) = \E\left(\frac{1}{n} \sum_{i=1}^{n} X_{i}^{k}\right) = \frac{1}{n} \sum_{i=1}^{n} \E(X_{i}^{k}) = \mu_{k}'.
        \end{equation*}
        Since $X_{1}^{k}, \cdots, X_{n}^{k}$ are independent:
        \begin{equation*}
            \Var(\overline{X^{k}}) = \Var\left(\frac{1}{n} \sum_{i=1}^{n} X_{i}^{k}\right) = \frac{1}{n^{2}} \sum_{i=1}^{n} \Var(X_{i}^{k}) = \frac{1}{n^{2}} \sum_{i=1}^{n} \left[\E(X_{i}^{2k}) - [\E(X_{i}^{k})]^{2}\right] = \frac{1}{n} \left[\mu_{2k}' - (\mu_{k}')^{2}\right].
        \end{equation*}
    \end{proofing}

\section{Conditional distribution}
    Sometimes, we deal with cases where certain information is given. 
    \begin{defn}
        Suppose $X$ and $Y$ are two random variables. The \textbf{conditional distribution function} of $Y$ given $X = x$, for any $x$ such that the PMF $p_{X}(x) > 0$ or the PDF $f_{X}(x) > 0$, is defined as:
        \begin{equation*}
            F_{Y|X}(y|x) = \prob(Y \leq y | X = x).
        \end{equation*}
        The \textbf{conditional PDF/PMF} of $Y$ given $X = x$, for any $x$ such that the PMF $p_{X}(x) > 0$ or the PDF $f_{X}(x) > 0$, is defined as:
        \begin{equation*}
            \begin{cases}
                p_{Y|X}(y|x) = \frac{p_{X,Y}(x, y)}{p_{X}(x)}, &\text{Discrete case},\\
                f_{Y|X}(y|x) = \frac{f_{X,Y}(x, y)}{f_{X}(x)}, &\text{Continuous case}.
            \end{cases}
        \end{equation*}
    \end{defn}
    The conditional distribution has a corresponding expectation.
    \begin{defn}
        Suppose $X$ and $Y$ are two random variables. The \textbf{conditional expectation} of $Y$ given $X = x$, for any $x$ such that the PMF $p_{X}(x) > 0$ or the PDF $f_{X}(x) > 0$, is defined as:
        \begin{equation*}
            \E(Y | X = x) = \begin{cases}
                \sum_{y} y p_{Y|X}(y|x), &\text{Discrete case},\\
                \int_{-\infty}^{\infty} y f_{Y|X}(y|x) \, dy, &\text{Continuous case}.
            \end{cases}
        \end{equation*}
    \end{defn}
    \begin{rem}
        $\E(Y | X = x)$ is a function of $x$. Similarly, $\E(Y | X)$ is a function of $X$.
    \end{rem}
    \begin{eg}
        Suppose the joint PDF of $X$ and $Y$ is given by:
        \begin{equation*}
            f_{X,Y}(x, y) = \begin{cases}
                \frac{1}{y} e^{-\frac{x}{y}}, &x > 0, y > 0,\\
                0, &\text{Otherwise}.
            \end{cases}
        \end{equation*}
        We want to compute $\E(X | Y = y)$. We find that:
        \begin{align*}
            f_{Y}(y) &= \int_{0}^{\infty} f_{X,Y}(x, y) \, dx = \int_{0}^{\infty} \frac{1}{y} e^{-\frac{x}{y}} \, dx = 1, & f_{X|Y}(x|y) &= \frac{f_{X,Y}(x, y)}{f_{Y}(y)} = \frac{1}{y} e^{-\frac{x}{y}}.
        \end{align*}
        We see that $(X | Y = y) \sim \Exp\left(\frac{1}{y}\right)$. Therefore, $\E(X | Y = y) = y$.
    \end{eg}
    \newpage

    Conditional expectation has the following properties.
    \begin{lem}
        \label{Chapter 1 (Lemma) Properties of conditional expectation}
        Suppose $X$, $Y$, and $Z$ are three random variables. The conditional expectation satisfies the following properties:
        \begin{enumerate}
            \item $\E(aY + bZ | X) = a\E(Y | X) + b\E(Z | X)$ for $a, b \in \mathbb{R}$.
            \item $\E(Y | X) \geq 0$ if $Y \geq 0$.
            \item If $X$ and $Y$ are independent, then $\E(Y | X) = \E(Y)$.
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        The proof for the discrete case is similar to the continuous case.
        \begin{enumerate}
            \item 
            \begin{align*}
                \E(aY + bZ | X) &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (ay + bz) f_{Y,Z|X}(y, z | X) \, dy \, dz \\
                &= a \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y f_{Y,Z|X}(y, z | X) \, dy \, dz + b \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} z f_{Y,Z|X}(y, z | X) \, dy \, dz \\
                &= a \int_{-\infty}^{\infty} y f_{Y|X}(y | X) \, dy + b \int_{-\infty}^{\infty} z f_{Z|X}(z | X) \, dz = a\E(Y | X) + b\E(Z | X).
            \end{align*}
            \item If $Y \geq 0$, then since $f_{Y|X}(y | x) \geq 0$ for any $x$ such that $f_{X}(x) > 0$:
            \begin{equation*}
                \E(Y | X) = \int_{0}^{\infty} y f_{Y|X}(y | X) \, dy \geq 0.
            \end{equation*} 
            \item If $X$ and $Y$ are independent, then:
            \begin{equation*}
                \E(Y | X) = \int_{-\infty}^{\infty} y f_{Y|X}(y | X) \, dy = \int_{-\infty}^{\infty} y f_{Y}(y) \, dy = \E(Y).
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    If $\E(Y | X)$ is a function of $X$, what is its expectation?
    \begin{thm}\named{Law of total expectation}
        Given two random variables $X$ and $Y$, we have:
        \begin{equation*}
            \E(Y) = \E(\E(Y | X)),
        \end{equation*}
        if both expectations exist. 
    \end{thm}
    \begin{proofing}
        We prove this for the continuous case. The discrete case works similarly.
        \begin{align*}
            \E(\E(Y | X)) &= \int_{-\infty}^{\infty} \E(Y | X = x) f_{X}(x) \, dx \\
            &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y f_{Y|X}(y | x) f_{X}(x) \, dy \, dx \\
            &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y f_{X,Y}(x, y) \, dx \, dy \\
            &= \int_{-\infty}^{\infty} y f_{Y}(y) \, dy = \E(Y).
        \end{align*}
    \end{proofing}
    The following theorem generalizes the Law of Total Expectation. We omit the proof.
    \begin{lem}
        \label{Chapter 1 (Lemma) Generalization of Law of total expectation}
        Given two random variables $X$ and $Y$, for any function $g$, we have:
        \begin{equation*}
            \E(\E(Y | X) g(X)) = \E(Y g(X)),
        \end{equation*}
        if both expectations exist.
    \end{lem}
    \newpage

    Similarly, we define conditional variance.
    \begin{defn}
        Given two random variables $X$ and $Y$, the \textbf{conditional variance} of $Y$ given $X$ is defined as:
        \begin{equation*}
            \Var(Y | X) = \E\left[(Y - \E(Y | X))^{2} | X\right].
        \end{equation*}
    \end{defn}
    \begin{lem}
        \label{Chapter 1 (Lemma) Conditional variance decomposition}
        Given two random variables $X$ and $Y$, we have:
        \begin{equation*}
            \Var(Y | X) = \E(Y^{2} | X) - [\E(Y | X)]^{2}.
        \end{equation*}
    \end{lem}
    \begin{proofing}
        By Lemma \ref{Chapter 1 (Lemma) Properties of conditional expectation} and Lemma \ref{Chapter 1 (Lemma) Generalization of Law of total expectation},
        \begin{align*}
            \Var(Y | X) &= \E\left[(Y - \E(Y | X))^{2} | X\right] \\
            &= \E(Y^{2} | X) - 2\E(Y\E(Y | X) | X) + \E((\E(Y | X))^{2} | X) \\
            \tag{$\E(Y | X)$ is a function of $X$}
            &= \E(Y^{2} | X) - [\E(Y | X)]^{2}.
        \end{align*}
    \end{proofing}
    We have the Law of Total Variance.
    \begin{thm}\named{Law of total variance}
        Given two random variables $X$ and $Y$, we have:
        \begin{equation*}
            \Var(Y) = \E[\Var(Y | X)] + \Var(\E(Y | X)),
        \end{equation*}
        if the expectations and variances exist.
    \end{thm}
    \begin{proofing}
        By Lemma \ref{Chapter 1 (Lemma) Conditional variance decomposition} and the Law of Total Expectation,
        \begin{align*}
            \E[\Var(Y | X)] + \Var(\E(Y | X)) &= \E[\E(Y^{2} | X)] - \E[\E(Y | X)]^{2} + \E[\E(Y | X)]^{2} - \left[\E(\E(Y | X))\right]^{2} \\
            &= \E(Y^{2}) - [\E(Y)]^{2} = \Var(Y).
        \end{align*}
    \end{proofing}

\section{Commonly used distribution}
    The indicator function is highly important and will be used later.
    \begin{defn}
        The indicator function of a set $A$ is a function $\mathbf{1}_{A}$ defined as:
        \begin{equation*}
            \mathbf{1}_{A}(x) = \begin{cases}
                1, &x \in A,\\
                0, &x \not\in A.
            \end{cases}
        \end{equation*}
    \end{defn}
    Let us recall some useful distributions.
    \begin{eg}\named{Bernoulli distribution} $X \sim \Bern(p)$\\
        A random variable $X$ is a Bernoulli random variable with parameter $p \in [0, 1]$ if it has the PMF:
        \begin{align*}
            p_{X}(x) &= \begin{cases}
                p^{x}(1 - p)^{1 - x}, &x \in \{0, 1\},\\
                0, &\text{Otherwise}.
            \end{cases} & \E(X) &= p, & \Var(X) &= p(1 - p).
        \end{align*}
    \end{eg}
    \begin{eg}\named{Binomial distribution} $X \sim \Bin(n, p)$\\
        A random variable $X$ is a Binomial random variable with parameters $n \in \mathbb{N}$ and $p \in [0, 1]$ if it has the PMF for $x = 0, \cdots, n$:
        \begin{align*}
            p_{X}(x) &= \binom{n}{x} p^{x}(1 - p)^{n - x}, & \E(X) &= np, & \Var(X) &= np(1 - p).
        \end{align*}
    \end{eg}
    \newpage

    \begin{eg}\named{Geometric distribution} $X \sim \Geom(p)$\\
        A random variable $X$ is geometric with parameter $p \in [0, 1]$ if it has the PMF for $x = 1, 2, \cdots$:
        \begin{align*}
            p_{X}(x) &= p(1 - p)^{x - 1}, & \E(X) &= \frac{1}{p}, & \Var(X) &= \frac{1 - p}{p^{2}}.
        \end{align*}
    \end{eg}
    \begin{eg}\named{Poisson distribution} $X \sim \Poisson(\lambda)$\\
        A random variable $X$ is a Poisson random variable with parameter $\lambda$ if it has the PMF for $x = 0, 1, \cdots$:
        \begin{align*}
            p_{X}(x) &= \frac{\lambda^{x}}{x!} e^{-\lambda}, & \E(X) &= \lambda, & \Var(X) &= \lambda.
        \end{align*}
    \end{eg}
    \begin{eg}\named{Negative Binomial distribution} $X \sim \NBin(r, p)$\\
        Assume $X_{1}, \cdots, X_{r}$ are independent and $X_{i} \sim \Geom(p)$ for $i = 1, \cdots, r$. Let $Y = \sum_{i=1}^{r} X_{i}$. The random variable $Y$ is negative Binomial with parameters $r > 0$ and $p \in [0, 1]$ if for $x > r$:
        \begin{align*}
            p_{X}(x) &= \binom{x - 1}{r - 1} (1 - p)^{x - r} p^{r}, & \E(X) &= \frac{r}{p}, & \Var(X) &= \frac{r(1 - p)}{p^{2}}.
        \end{align*}
    \end{eg}
    \begin{eg}\named{Cauchy distribution} $X \sim \Cauchy(\theta)$\\
        A random variable $X$ is a Cauchy random variable with parameter $\theta$ if it has the PDF:
        \begin{align*}
            f_{X}(x) &= \frac{1}{\pi(1 + (x - \theta)^{2})}, & \E(X) &\text{ DNE}, & \Var(X) &\text{ DNE}.
        \end{align*}
    \end{eg}
    \begin{eg}\named{Uniform distribution} $X \sim \U[a, b]$\\
        A random variable $X$ is uniform if, given $a < b$, it has the PDF:
        \begin{align*}
            f_{X}(x) &= \begin{cases}
                \frac{1}{b - a}, &x \in [a, b],\\
                0, &\text{Otherwise}.
            \end{cases} & \E(X) &= \frac{a + b}{2}, & \Var(X) &= \frac{(b - a)^{2}}{12}.
        \end{align*}
    \end{eg}
    \begin{eg}\named{Exponential distribution} $X \sim \Exp(\lambda)$\\
        A random variable $X$ is exponential with parameter $\lambda$ if it has the PDF:
        \begin{align*}
            f_{X}(x) &= \begin{cases}
                \lambda e^{-\lambda x}, &x \geq 0,\\
                0, &x < 0.
            \end{cases} & \E(X) &= \frac{1}{\lambda}, & \Var(X) &= \frac{1}{\lambda^{2}}.
        \end{align*}
    \end{eg}
    \begin{eg}\named{Normal distribution / Gaussian distribution} $X \sim \N(\mu, \sigma^{2})$\\
        A random variable $X$ is normal if it has two parameters $\mu$ and $\sigma^{2}$, and its PDF and CDF are:
        \begin{align*}
            f_{X}(x) &= \frac{1}{\sqrt{2\pi\sigma^{2}}} \exp\left(-\frac{(x - \mu)^{2}}{2\sigma^{2}}\right), & F_{X}(x) &= \int_{-\infty}^{x} f_{X}(u) \, du, & \E(X) &= \mu, & \Var(X) &= \sigma^{2}.
        \end{align*}
        A random variable $Z$ is standard normal if it is normal with $\mu = 0$ and $\sigma^{2} = 1$ ($Z \sim \N(0, 1)$):
        \begin{align*}
            f_{Z}(z) &= \phi(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{z^{2}}{2}\right), & F_{Z}(z) &= \Phi(z) = \int_{-\infty}^{z} \phi(u) \, du, & \E(Z) &= 0, & \Var(Z) &= 1.
        \end{align*}
        We define $z_{\alpha}$ by:
        \begin{equation*}
            \prob(Z \geq z_{\alpha}) = \alpha.
        \end{equation*}
    \end{eg}
    \newpage

    \begin{eg}\named{Gamma distribution} $X \sim \Gam(\alpha, \beta)$\\
        A random variable $X$ is a gamma random variable with parameters $\alpha > 0$ and $\beta > 0$ if its PDF is:
        \begin{align*}
            f_{X}(x) &= \begin{cases}
                \frac{1}{\Gamma(\alpha)} \beta^{\alpha} x^{\alpha - 1} e^{-\beta x}, &x \geq 0,\\
                0, &\text{Otherwise}.
            \end{cases} & \E(X) &= \frac{\alpha}{\beta}, & \Var(X) &= \frac{\alpha}{\beta^{2}}.
        \end{align*}
    \end{eg}
    \begin{rem}
        For any $z$, the gamma function $\Gamma(z)$ has the following properties:
        \begin{enumerate}
            \item $\Gamma(z + 1) = z\Gamma(z)$. If $z$ is a positive integer, then $\Gamma(z) = (z - 1)!$.
            \item If $\Re(z) > 0$, then $\Gamma(z) = \int_{0}^{\infty} t^{z - 1} e^{-t} \, dt$.
            \item $\Gamma\left(\frac{1}{2}\right) = \sqrt{\pi}$.
        \end{enumerate}
    \end{rem}
    \begin{eg}\named{Beta distribution} $X \sim \Bet(\alpha, \beta)$\\
        A random variable $X$ is a beta random variable with parameters $\alpha > 0$ and $\beta > 0$ if its PDF is: 
        \begin{align*}
            f_{X}(x) &= \begin{cases}
                \frac{x^{\alpha - 1} (1 - x)^{\beta - 1}}{B(\alpha, \beta)}, &x \in (0, 1),\\
                0, &\text{Otherwise}.
            \end{cases} & \E(X) &= \frac{\alpha}{\alpha + \beta}, & \Var(X) &= \frac{\alpha\beta}{(\alpha + \beta)^{2} (\alpha + \beta + 1)}.
        \end{align*}
    \end{eg}
    \begin{rem}
        For any $z_{1}, z_{2}$, the beta function $B(z_{1}, z_{2})$ has the following properties:
        \begin{enumerate}
            \item $B(z_{1}, z_{2}) = \frac{\Gamma(z_{1})\Gamma(z_{2})}{\Gamma(z_{1} + z_{2})}$.
            \item $B(z_{1}, z_{2}) = \int_{0}^{1} t^{z_{1} - 1} (1 - t)^{z_{2} - 1} \, dt$.
        \end{enumerate}
    \end{rem}
    We have some more distributions that are associated with the normal distribution. For example, the Chi-squared distribution, which is a special case of the gamma distribution.
    \begin{eg}\named{Chi-squared distribution} $Y \sim \chi^{2}(n)$\\
        Assume that $X_{1}, X_{2}, \cdots, X_{n}$ are independent and $X_{i} \sim \N(0, 1)$ for $i = 1, \cdots, n$. Let $Y = \sum_{i=1}^{n} X_{i}^{2}$. The random variable $Y$ has a $\chi^{2}$-distribution with $n$ degrees of freedom if:
        \begin{align*}
            f_{Y}(x) &= \begin{cases}
                \frac{1}{\Gamma\left(\frac{n}{2}\right)} 2^{-\frac{n}{2}} x^{\frac{n}{2} - 1} e^{-\frac{x}{2}}, &x > 0,\\
                0, &\text{Otherwise}.
            \end{cases} & \E(Y) &= n, & \Var(Y) &= 2n.
        \end{align*}
        We define $\chi^{2}_{\alpha, n}$ by:
        \begin{equation*}
            \prob(Y \geq \chi^{2}_{\alpha, n}) = \alpha.
        \end{equation*}
    \end{eg} 
    \begin{thm}
        \label{Chapter 1 (Theorem) chi-square distribution with 1 degree of freedom}
        If a random variable $X \sim \N(\mu, \sigma^{2})$, where $\sigma^{2} > 0$, then the random variable $V = \frac{(X - \mu)^{2}}{\sigma^{2}} \sim \chi^{2}(1)$.
    \end{thm}
    \begin{proofing}
        By the properties of the normal distribution, we get that:
        \begin{equation*}
            \frac{X - \mu}{\sigma} \sim \N(0, 1).
        \end{equation*}
        Therefore, by the definition of the chi-squared distribution,
        \begin{equation*}
            V = \left(\frac{X - \mu}{\sigma}\right)^{2} \sim \chi^{2}(1).
        \end{equation*}
    \end{proofing}
    \begin{thm}
        Given a set of random variables $\{X_{1}, \cdots, X_{k}\}$. Let $Y = \sum_{i=1}^{k} X_{i}$ and $X_{i} \sim \chi^{2}(r_{i})$ for all $i = 1, \cdots, k$. If they are independent, then $Y \sim \chi^{2}(r_{1} + \cdots + r_{k})$.
    \end{thm}
    \begin{proofing}
        It suffices to prove that if $Z_{1} \sim \chi^{2}(n_{1})$ and $Z_{2} \sim \chi^{2}(n_{2})$, then $Z_{1} + Z_{2} \sim \chi^{2}(n_{1} + n_{2})$. By repeatedly applying the relation for two random variables, one can easily derive the desired relation for $k$ random variables.
        From the definition, $Z_{1} = X_{11}^{2} + \cdots + X_{1n_{1}}^{2}$ and $Z_{2} = X_{21}^{2} + \cdots + X_{2n_{2}}^{2}$, where $X_{1i} \sim \N(0, 1)$ and $X_{2j} \sim \N(0, 1)$ for $i = 1, \cdots, n_{1}$ and $j = 1, \cdots, n_{2}$. Therefore,
        \begin{equation*}
            Z_{1} + Z_{2} = X_{11}^{2} + \cdots + X_{1n_{1}}^{2} + X_{21}^{2} + \cdots + X_{2n_{2}}^{2} \sim \chi^{2}(n_{1} + n_{2}).
        \end{equation*}
    \end{proofing}
	\newpage

    \begin{thm}
        \label{Chapter 1 (Theorem) Normal and chi-squared distribution related to sample mean and variance}
        If $\{X_{1}, \cdots, X_{n}\}$ is a random sample of size $n > 1$ of a random variable $X \sim \N(\mu, \sigma^{2})$, then we have:
        \begin{enumerate}
            \item The sample mean $\overline{X} \sim \N\left(\mu, \frac{\sigma^{2}}{n}\right)$.
            \item The sample mean $\overline{X}$ and the sample variance $S_{n-1}^{2}$ are independent.
            \item 
            \begin{equation*}
                \frac{(n - 1) S_{n-1}^{2}}{\sigma^{2}} = \frac{n S_{n}^{2}}{\sigma^{2}} = \frac{1}{\sigma^{2}} \sum_{i=1}^{n} (X_{i} - \overline{X})^{2} \sim \chi^{2}(n - 1).
            \end{equation*}
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item From the definition,
            \begin{equation*}
                \overline{X} = \frac{1}{n} \sum_{i=1}^{n} X_{i}.
            \end{equation*}
            Since $X_{i} \sim \N(\mu, \sigma^{2})$ for $i = 1, \cdots, n$, we find that $\overline{X} \sim \N\left(\mu, \frac{\sigma^{2}}{n}\right)$.
            \item Let $\mathbf{X} = (X_{1}\ \cdots\ X_{n})^{T}$. We may find that:
            \begin{align*}
                \begin{pmatrix}
                    \overline{X}\\ X_{1} - \overline{X}\\ X_{2} - \overline{X}\\ \vdots\\ X_{n} - \overline{X}
                \end{pmatrix}=\begin{pmatrix}
                    \frac{1}{n}X_{1} + \frac{1}{n}X_{2} + \cdots + \frac{1}{n}X_{n}\\
                    \left(1 - \frac{1}{n}\right)X_{1} - \frac{1}{n}X_{2} - \cdots - \frac{1}{n}X_{n}\\
                    -\frac{1}{n}X_{1} + \left(1 - \frac{1}{n}\right)X_{2} - \cdots - \frac{1}{n}X_{n}\\
                    \vdots\\
                    -\frac{1}{n}X_{1} - \frac{1}{n}X_{2} - \cdots + \left(1 - \frac{1}{n}\right)X_{n}
                \end{pmatrix} &= \mathbf{AX}, & \mathbf{A} &= \begin{pmatrix}
                    \frac{1}{n} & \frac{1}{n} & \hdots & \frac{1}{n}\\
                    1 - \frac{1}{n} & -\frac{1}{n} & \hdots & -\frac{1}{n}\\
                    -\frac{1}{n} & 1 - \frac{1}{n} & \ddots & \vdots\\
                    \vdots & \ddots & \ddots & -\frac{1}{n}\\
                    -\frac{1}{n} & \hdots & -\frac{1}{n} & 1 - \frac{1}{n}
                \end{pmatrix}.
            \end{align*}
            By Lemma \ref{Chapter 1 (Lemma) Distribution of matrix multiplication with random vector}, we have $\mathbf{AX} \sim \N_{n+1}(\mathbf{A}\boldsymbol{\mu}, \mathbf{A\sigma^{2}I_{n\times n}A}^{T})$, where $\boldsymbol{\mu} = (\mu\ \cdots\ \mu)^{T}$.\\
            Let $\mathbf{X^{*}} = (X_{1} - \overline{X}\ \cdots\ X_{n} - \overline{X})^{T}$ and $\mathbf{\Sigma^{*}}$ be the variance-covariance matrix of $\mathbf{X}^{*}$. We can notice that:
            \begin{equation*}
                \mathbf{A\sigma^{2}I_{n\times n}A}^{T} = \begin{pmatrix}\begin{array}{c|c}
                    \Var(\overline{X}) & \cov(\mathbf{X^{*}}, \overline{X})\\
                    \hline
                    \cov(\mathbf{X^{*}}, \overline{X}) & \mathbf{\Sigma^{*}}
                \end{array}\end{pmatrix}.
            \end{equation*}
            Since $X_{i}$ are independent for all $i$,
            \begin{equation*}
                \cov(X_{i} - \overline{X}, \overline{X}) = \cov(X_{i}, \overline{X}) - \Var(\overline{X}) = \frac{1}{n} \Var(X_{i}) - \frac{\sigma^{2}}{n} = 0.
            \end{equation*}
            Therefore, we find that $\cov(\mathbf{X^{*}}, \overline{X}) = 0$. By Lemma \ref{Chapter 1 (Lemma) Independence iff uncorrelated and bivariate}, $\overline{X}$ and $\mathbf{X^{*}}$ are independent.\\
            Since $S_{n-1}^{2}$ is a function of $\mathbf{X^{*}}$, we conclude that $\overline{X}$ and $S_{n-1}^{2}$ are independent.
            \item We have:
            \begin{equation*}
                \frac{1}{\sigma^{2}} \sum_{i=1}^{n} (X_{i} - \overline{X})^{2} = \frac{1}{\sigma^{2}} \sum_{i=1}^{n} (X_{i} - \mu + \mu - \overline{X})^{2} = \frac{1}{\sigma^{2}} \sum_{i=1}^{n} (X_{i} - \mu)^{2} - \frac{n(\overline{X} - \mu)^{2}}{\sigma^{2}}.
            \end{equation*}
            Let $U = \sum_{i=1}^{n} \left(\frac{X_{i} - \mu}{\sigma}\right)^{2}$ and $V = \left(\frac{\sqrt{n}(\overline{X} - \mu)}{\sigma}\right)^{2}$. The distribution we are finding is $U - V$.\\
            From the definition, we know that $U \sim \chi^{2}(n)$. From Theorem \ref{Chapter 1 (Theorem) chi-square distribution with 1 degree of freedom}, we find that $V \sim \chi^{2}(1)$.\\
            From Part 2, since functions of $\mathbf{X}^{*}$ and $\overline{X}$ are independent, 
            \begin{equation*}
                M_{U-V}(t) = \frac{M_{U}(t)}{M_{V}(t)} = \frac{(1 - 2t)^{-\frac{n}{2}}}{(1 - 2t)^{-\frac{1}{2}}} = (1 - 2t)^{-\frac{n - 1}{2}}.
            \end{equation*}
            Therefore, we conclude that:
            \begin{equation*}
                \frac{1}{\sigma^{2}} \sum_{i=1}^{n} (X_{i} - \overline{X})^{2} \sim \chi^{2}(n - 1).
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    \begin{rem}
        From the same proof of the above theorem part 2, we can also find that $\overline{X}$ and $S_{n}^{2}$ are independent.
    \end{rem}
	\newpage
	
    \begin{eg}\named{Student's t-distribution} $T \sim t(r)$\\
        Assume that $X \sim \N(0, 1)$ and $Y \sim \chi^{2}(r)$. Let:
        \begin{equation*}
            T = \frac{X}{\sqrt{\frac{Y}{r}}}.
        \end{equation*}
        Then $T$ has a t-distribution with $r$ degrees of freedom, and:
        \begin{align*}
            f_{T}(t) &= \frac{\Gamma\left(\frac{r + 1}{2}\right)}{\sqrt{r\pi} \Gamma\left(\frac{r}{2}\right)} \left(1 + \frac{t^{2}}{r}\right)^{-\frac{r + 1}{2}}, & \E(T) &= \begin{cases}
                \text{Undefined}, &r \leq 1,\\
                0, &r > 1,
            \end{cases} & \Var(T) &= \begin{cases}
                \text{Undefined}, &r \leq 1,\\
                \infty, &1 < r \leq 2,\\
                \frac{r}{r - 2}, &r > 2.
            \end{cases}
        \end{align*}
        We define $t_{\alpha, r}$ by:
        \begin{equation*}
            \prob(T \geq t_{\alpha, r}) = \alpha.
        \end{equation*}
    \end{eg}
    \begin{rem}
        As $r \to \infty$, $T \to \N(0, 1)$ by the Central Limit Theorem (CLT).
    \end{rem}
    \begin{rem}
        If we fix $Y = y$, then we find that $T \sim \N\left(0, \frac{r}{y}\right)$.
    \end{rem}
    The t-distribution has the following properties.
    \begin{thm}
        If $\{X_{1}, \cdots, X_{n}\}$ is a random sample of size $n > 1$ of a random variable $X \sim \N(\mu, \sigma^{2})$, then:
        \begin{equation*}
            \frac{\sqrt{n}(\overline{X} - \mu)}{S_{n-1}} \sim t(n - 1).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        From Theorem \ref{Chapter 1 (Theorem) Normal and chi-squared distribution related to sample mean and variance}, $\overline{X}$ and $S_{n-1}^{2}$ are independent, and:
        \begin{align*}
            \frac{\sqrt{n}(\overline{X} - \mu)}{\sigma} &\sim \N(0, 1), & \frac{(n - 1) S_{n-1}^{2}}{\sigma^{2}} &\sim \chi^{2}(n - 1).
        \end{align*}
        Therefore, from the definition:
        \begin{equation*}
            \frac{\sqrt{n}(\overline{X} - \mu)}{S_{n-1}} = \frac{\frac{\sqrt{n}(\overline{X} - \mu)}{\sigma}}{\sqrt{\frac{1}{n - 1} \left(\frac{(n - 1) S_{n-1}^{2}}{\sigma^{2}}\right)}} \sim t(n - 1).
        \end{equation*}
    \end{proofing}
    \begin{eg}
        Assume that we want to find the $95\%$ confidence interval of $\mu$ without knowing the population variance $\sigma^{2}$. Then we find:
        \begin{align*}
            0.95 &= \prob\left(-t_{0.025, n - 1} \leq \frac{\sqrt{n}(\overline{X} - \mu)}{S_{n-1}} \leq t_{0.025, n - 1}\right)\\
            &= \prob\left(\overline{X} - t_{0.025, n - 1} \frac{S_{n-1}}{\sqrt{n}} \leq \mu \leq \overline{X} + t_{0.025, n - 1} \frac{S_{n-1}}{\sqrt{n}}\right).
        \end{align*}
        Therefore, the $95\%$ confidence interval is:
        \begin{equation*}
            \left(\overline{x} - t_{0.025, n - 1} \frac{s_{n-1}}{\sqrt{n}}, \overline{x} + t_{0.025, n - 1} \frac{s_{n-1}}{\sqrt{n}}\right).
        \end{equation*}
        Usually, when $n > 30$, $t_{0.025, n - 1} \approx z_{0.025}$. Therefore, the $95\%$ confidence interval becomes:
        \begin{equation*}
            \left(\overline{x} - z_{0.025} \frac{s_{n-1}}{\sqrt{n}}, \overline{x} + z_{0.025} \frac{s_{n-1}}{\sqrt{n}}\right).
        \end{equation*}
    \end{eg}
    \newpage
    
    \begin{eg}\named{F distribution} $F \sim F(r_{1}, r_{2})$\\
        Assume that $X$ and $Y$ are independent random variables with $X \sim \chi^{2}(r_{1})$ and $Y \sim \chi^{2}(r_{2})$. Let:
        \begin{equation*}
            F = \frac{\frac{X}{r_{1}}}{\frac{Y}{r_{2}}}.
        \end{equation*}
        Then $F$ has an F-distribution with $r_{1}$ and $r_{2}$ degrees of freedom, and:
        \begin{equation*}
            f_{F}(w) = \frac{\Gamma\left(\frac{r_{1} + r_{2}}{2}\right)}{\Gamma\left(\frac{r_{1}}{2}\right) \Gamma\left(\frac{r_{2}}{2}\right)} \left(\frac{r_{1}}{r_{2}}\right)^{\frac{r_{1}}{2}} w^{\frac{r_{1}}{2} - 1} \left(1 + \frac{r_{1}w}{r_{2}}\right)^{-\frac{r_{1} + r_{2}}{2}},
        \end{equation*}
        where $0 < w < \infty$. We define $f_{\alpha, (r_{1}, r_{2})}$ by:
        \begin{equation*}
            \prob(F \geq f_{\alpha, (r_{1}, r_{2})}) = \alpha.
        \end{equation*}
    \end{eg}
    \begin{lem}
        Let $U \sim F(r_{1}, r_{2})$. The F-distribution has the following properties:
        \begin{enumerate}
            \item $\frac{1}{U} \sim F(r_{2}, r_{1})$.
            \item If $f_{\alpha, (r_{1}, r_{2})}$ is defined by $\prob(U \geq f_{\alpha, (r_{1}, r_{2})}) = \alpha$, then:
            \begin{equation*}
                \frac{1}{f_{\alpha, (r_{1}, r_{2})}} = f_{1 - \alpha, (r_{2}, r_{1})}.
            \end{equation*}
        \end{enumerate}
    \end{lem}
    \begin{proofing}
        \begin{enumerate}
            \item By definition:
            \begin{equation*}
                U = \frac{\frac{X}{r_{1}}}{\frac{Y}{r_{2}}},
            \end{equation*}
            where $X \sim \chi^{2}(r_{1})$ and $Y \sim \chi^{2}(r_{2})$. Therefore:
            \begin{equation*}
                \frac{1}{U} = \frac{\frac{Y}{r_{2}}}{\frac{X}{r_{1}}} \sim F(r_{2}, r_{1}).
            \end{equation*}
            \item With $\prob(U \geq f_{\alpha, (r_{1}, r_{2})}) = \alpha$, since $f_{U}(w)$ is only defined for $w > 0$, we have:
            \begin{align*}
                \prob\left(\frac{1}{U} \leq \frac{1}{f_{\alpha, (r_{1}, r_{2})}}\right) &= \alpha,\\
                \prob\left(\frac{1}{U} > \frac{1}{f_{\alpha, (r_{1}, r_{2})}}\right) &= 1 - \alpha.
            \end{align*}
            From Part 1, we find that $\frac{1}{U} \sim F(r_{2}, r_{1})$. Therefore:
            \begin{equation*}
                \prob\left(\frac{1}{U} \geq f_{1 - \alpha, (r_{2}, r_{1})}\right) = 1 - \alpha.
            \end{equation*}
            Thus, we conclude that:
            \begin{equation*}
                \frac{1}{f_{\alpha, (r_{1}, r_{2})}} = f_{1 - \alpha, (r_{2}, r_{1})}.
            \end{equation*}
        \end{enumerate}
    \end{proofing}
    \newpage

    \begin{eg}
        Assume that we want to compare two populations. Let $X_{1} \sim \N(\mu_{1}, \sigma_{1}^{2})$ represent the random variable of the first population, and $X_{2} \sim \N(\mu_{2}, \sigma_{2}^{2})$ represent the random variable of the second population. We aim to find a confidence interval for their ratio of variances $\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}$.\\
        Let $\{X_{11}, \cdots, X_{1n}\}$ be a random sample of size $n$ from $X_{1}$, and $\{X_{21}, \cdots, X_{2m}\}$ be a random sample of size $m$ from $X_{2}$. We find that:
        \begin{align*}
            \frac{(n - 1) S_{n-1,1}^{2}}{\sigma_{1}^{2}} &\sim \chi^{2}(n - 1), & \frac{(m - 1) S_{m-1,2}^{2}}{\sigma_{2}^{2}} &\sim \chi^{2}(m - 1).
        \end{align*}
        We also find that $S_{n-1,1}$ and $S_{m-1,2}$ are independent since they are from different populations. Therefore, we have:
        \begin{equation*}
            \frac{\sigma_{1}^{2}}{\sigma_{2}^{2}} \left(\frac{S_{m-1,2}^{2}}{S_{n-1,1}^{2}}\right) = \frac{\frac{1}{m - 1} \left(\frac{(m - 1) S_{m-1,2}^{2}}{\sigma_{2}^{2}}\right)}{\frac{1}{n - 1} \left(\frac{(n - 1) S_{n-1,1}^{2}}{\sigma_{1}^{2}}\right)} \sim F(m - 1, n - 1).
        \end{equation*}
        Then we can find the $95\%$ confidence interval as:
        \begin{align*}
            0.95 &= \prob\left(f_{0.975, (m - 1, n - 1)} \leq \frac{\sigma_{1}^{2}}{\sigma_{2}^{2}} \left(\frac{S_{m-1,2}^{2}}{S_{n-1,1}^{2}}\right) \leq f_{0.025, (m - 1, n - 1)}\right)\\
            &= \prob\left(\frac{S_{n-1,1}^{2}}{S_{m-1,2}^{2}} f_{0.975, (m - 1, n - 1)} \leq \frac{\sigma_{1}^{2}}{\sigma_{2}^{2}} \leq \frac{S_{n-1,1}^{2}}{S_{m-1,2}^{2}} f_{0.025, (m - 1, n - 1)}\right).
        \end{align*}
    \end{eg}

\section{Moment generating function}
    It is useful to have a function that can generate all moments of a random variable.
    \begin{defn}
        The \textbf{moment generating function} (MGF) of a random variable $X$, denoted by $M_{X}(t)$, is defined as:
        \begin{equation*}
            M_{X}(t) = \E(e^{tX}),
        \end{equation*}
        if the expectation exists for $t$ in some neighborhood of $0$.
    \end{defn}
    \begin{rem}
        More precisely, there exists $h > 0$ such that for all $t$ in $(-h, h)$, $\E(e^{tX})$ exists.
    \end{rem}
    \begin{rem}
        The MGF of $X$ may not always exist. However, if it does exist, then $M_{X}(t)$ is continuously differentiable in some neighborhood of the origin.
    \end{rem}
    \begin{rem}
        If we replace $e^{tX}$ with its Taylor series, we obtain:
        \begin{equation*}
            M_{X}(t) = \E\left(\sum_{i=0}^{\infty} \frac{(tX)^{i}}{i!}\right) = \sum_{i=0}^{\infty} \frac{t^{i}}{i!} \E(X^{i}) = \sum_{i=0}^{\infty} \frac{t^{i}}{i!} \mu_{i}'.
        \end{equation*}
    \end{rem}
    \begin{lem}
        If $M_{X}(t)$ is the MGF of a random variable $X$, then:
        \begin{equation*}
            \left.\odv*[order={k}]{M_{X}(t)}{t}\right|_{t=0} = \E(X^{k}) = \mu_{k}'.
        \end{equation*}
    \end{lem}
    \begin{proofing}
        From the Taylor series expansion of the MGF, we see that:
        \begin{equation*}
            \left.\odv*[order={k}]{M_{X}(t)}{t}\right|_{t=0} = \left.\sum_{i=k}^{\infty} \frac{t^{i-k}}{(i-k)!} \E(X^{i})\right|_{t=0} = \E(X^{k}).
        \end{equation*}
    \end{proofing}
    \begin{eg}
        What is the MGF of $X \sim \Bern(p)$? We have:
        \begin{equation*}
            M_{X}(t) = \E(e^{tX}) = e^{t(0)}(1 - p) + e^{t(1)}(p) = p e^{t} + 1 - p.
        \end{equation*}
    \end{eg}
	\newpage

    \begin{lem}
        Random variables $X$ and $Y$ are independent if and only if:
        \begin{equation*}
            M_{X,Y}(s, t) = M_{X}(s) M_{Y}(t).
        \end{equation*}
    \end{lem}
    \begin{lem}
        If random variables $X$ and $Y$ are independent, then:
        \begin{equation*}
            M_{X+Y}(t) = M_{X}(t) M_{Y}(t).
        \end{equation*}
    \end{lem}
    \begin{proofing}
        Since $X$ and $Y$ are independent:
        \begin{equation*}
            M_{X+Y}(t) = \E(e^{t(X + Y)}) = \E(e^{tX}) \E(e^{tY}) = M_{X}(t) M_{Y}(t).
        \end{equation*}
    \end{proofing}
    \begin{eg}
        By definition, if $Y = \Bin(n, p)$, then $Y = X_{1} + \cdots + X_{n}$, where $X_{i} \sim \Bern(p)$ for all $i$, and they are independent. Therefore:
        \begin{equation*}
            M_{Y}(t) = \prod_{i=1}^{n} M_{X_{i}}(t) = (pe^{t} + 1 - p)^{n}.
        \end{equation*}
        Alternatively, we can solve it without using the definition:
        \begin{equation*}
            M_{Y}(t) = \E(e^{tY}) = \sum_{i=0}^{n} \binom{n}{i} (pe^{t})^{i} (1 - p)^{n - i} = (pe^{t} + 1 - p)^{n}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Consider $X \sim \Poisson(\lambda)$. The MGF of $X$ can be obtained as:
        \begin{equation*}
            M_{X}(t) = \sum_{k=0}^{\infty} e^{tk} \frac{\lambda^{k} e^{-\lambda}}{k!} = e^{-\lambda} \sum_{k=0}^{\infty} \frac{(\lambda e^{t})^{k}}{k!} = e^{\lambda(e^{t} - 1)}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Consider $X \sim \Exp(\lambda)$. If $t < \lambda$, we have:
        \begin{equation*}
            M_{X}(t) = \E(e^{tX}) = \int_{0}^{\infty} e^{tx} \lambda e^{-\lambda x} \, dx = \lambda \int_{0}^{\infty} e^{-(\lambda - t)x} \, dx = \frac{\lambda}{\lambda - t}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        What is the MGF of $X \sim \N(\mu, \sigma^{2})$? We may first find the MGF of $Z \sim \N(0, 1)$:
        \begin{align*}
            M_{Z}(t) &= \E(e^{tZ}) = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}(z^{2} - 2tz)} \, dz \\
            &= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}((z - t)^{2} - t^{2})} \, dz = e^{\frac{t^{2}}{2}} \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}(z - t)^{2}} \, dz = e^{\frac{t^{2}}{2}}.
        \end{align*}
        Therefore, by having $X = \sigma Z + \mu$, we have:
        \begin{equation*}
            M_{X}(t) = \E(e^{tX}) = e^{\mu t} \E(e^{t\sigma Z}) = e^{\mu t + \frac{1}{2} \sigma^{2} t^{2}}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Consider $X \sim \U[a, b]$, where $a < b$. We have:
        \begin{equation*}
            M_{X}(t) = \E(e^{tX}) = \int_{a}^{b} \frac{e^{tx}}{b - a} \, dx = \left[\frac{e^{tx}}{t(b - a)}\right]_{a}^{b} = \frac{e^{bt} - e^{at}}{t(b - a)}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        If $X \sim \NBin(r, p)$, then for $t < -\ln(1 - p)$:
        \begin{equation*}
            M_{X}(t) = \left(\frac{pe^{t}}{1 - (1 - p)e^{t}}\right)^{r}.
        \end{equation*}
        If $X \sim \Gam(\alpha, \beta)$, then for $t < \beta$:
        \begin{equation*}
            M_{X}(t) = \left(\frac{\beta}{\beta - t}\right)^{\alpha}.
        \end{equation*}
    \end{eg}
    \newpage

    \begin{eg}
        Given $Y \sim \chi^{2}(r)$. How do we find the MGF of $Y$?\\
        Note that the chi-squared distribution is a special case of the gamma distribution. We have $\chi^{2}(r) = \Gamma\left(\frac{r}{2}, \frac{1}{2}\right)$. Therefore, by substitution, for $t < \frac{1}{2}$, we get:
        \begin{equation*}
            M_{Y}(t) = \left(\frac{\frac{1}{2}}{\frac{1}{2} - t}\right)^{\frac{r}{2}} = (1 - 2t)^{-\frac{r}{2}}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Given that $Y \sim \chi^{2}(r)$. How do we find $\E(Y)$ without using the MGF of $Y$?\\
        By definition, let $Y = \sum_{i=1}^{r} X_{i}^{2}$, where $X_{i} \sim \N(0, 1)$. Therefore:
        \begin{equation*}
            \E(Y) = \sum_{i=1}^{r} \E(X_{i}^{2}) = \sum_{i=1}^{r} \left.\odv*[order={2}]{}{t} e^{\frac{1}{2}t^{2}}\right|_{t=0} = \left.r(1 + t^{2})e^{\frac{1}{2}t^{2}}\right|_{t=0} = r.
        \end{equation*}
    \end{eg}
    Ultimately, the reason why we use the moment generating function is the following fact.
    \begin{thm}\named{Uniqueness of MGF}
        Let $X$ and $Y$ be two random variables. Suppose that their MGFs exist and are equal for all $t \in (-h, h)$ for some $h > 0$, then the distribution functions $F_{X}$ and $F_{Y}$ are equal.
    \end{thm}
    This means that by knowing the MGF of a particular random variable $X$, we can determine its distribution.
    \begin{eg}
        Assume that $X_{1}, \cdots, X_{n}$ are independent and $X_{i} \sim \Bin(m_{i}, p)$ for all $i = 1, \cdots, n$. Then we have:
        \begin{equation*}
            M_{X_{1} + \cdots + X_{n}}(t) = \prod_{i=1}^{n} M_{X_{i}}(t) = \prod_{i=1}^{n} (pe^{t} + 1 - p)^{m_{i}} = (pe^{t} + 1 - p)^{\sum_{i=1}^{n} m_{i}}.
        \end{equation*}
        Therefore, we have $X_{1} + \cdots + X_{n} \sim \Bin\left(\sum_{i=1}^{n} m_{i}, p\right)$.
    \end{eg}
    \begin{eg}
        Assume that $X_{1}, \cdots, X_{n}$ are independent and $X_{i} \sim \Poisson(\lambda_{i})$ for all $i = 1, \cdots, n$. Then we have:
        \begin{equation*}
            M_{X_{1} + \cdots + X_{n}}(t) = \prod_{i=1}^{n} M_{X_{i}}(t) = \prod_{i=1}^{n} e^{\lambda_{i}(e^{t} - 1)} = e^{\sum_{i=1}^{n} \lambda_{i}(e^{t} - 1)}.
        \end{equation*}
        Therefore, we have $X_{1} + \cdots + X_{n} \sim \Poisson\left(\sum_{i=1}^{n} \lambda_{i}\right)$.
    \end{eg}
    \begin{eg}
        Similarly, given a set of independent random variables $\{X_{1}, \cdots, X_{n}\}$:
        \begin{enumerate}
            \item If $X_{i} \sim \NBin(r_{i}, p)$, then $X_{1} + \cdots + X_{n} \sim \NBin\left(\sum_{i=1}^{n} r_{i}, p\right)$.
            \item If $X_{i} \sim \N(\mu_{i}, \sigma_{i}^{2})$, then $X_{1} + \cdots + X_{n} \sim \N\left(\sum_{i=1}^{n} \mu_{i}, \sum_{i=1}^{n} \sigma_{i}^{2}\right)$.
            \item If $X_{i} \sim \Gam(\alpha_{i}, \beta)$, then $X_{1} + \cdots + X_{n} \sim \Gam\left(\sum_{i=1}^{n} \alpha_{i}, \beta\right)$ and $cX_{i} \sim \Gam\left(\alpha_{i}, \frac{\beta}{c}\right)$ for $c \neq 0$.
        \end{enumerate}
    \end{eg}
    \begin{rem}
        Not all sums of distributions will result in the same type of distribution.
    \end{rem}
    More generally, we deal with problems of limiting distributions.
    \begin{thm}
        Suppose $\{X_{n}\}$ is a sequence of random variables, each with MGF $M_{X_{n}}(t)$. If:
        \begin{equation*}
            \lim_{n \to \infty} M_{X_{n}}(t) = M_{Y}(t),
        \end{equation*}
        for all $t$ in a neighborhood of $0$, where $M_{Y}(t)$ is the MGF of some random variable $Y$, then there is a unique distribution function $F_{Y}$ with corresponding $M_{Y}(t)$ such that:
        \begin{equation*}
            \lim_{n \to \infty} F_{X_{n}}(y) = F_{Y}(y),
        \end{equation*}
        for all $y$ where $F_{Y}(y)$ is continuous. We denote this as $X_{n} \to Y$ or $X_{n} \xrightarrow{D} Y$.
    \end{thm}
    \begin{rem}
        Simply put, the limiting distribution of $X_{n}$ is equal to the distribution of $Y$.
    \end{rem}
    \newpage
    
    We may define limiting convergence in a more theoretical way.
    \begin{defn}
        A sequence of random variables $\{X_{n}\}$ \textbf{converges in distribution} to a random variable $X$, denoted by $X_{n} \xrightarrow{D} X$, if for all continuity points $x$ of $F_{X}$, as $n \to \infty$:
        \begin{equation*}
            F_{X_{n}}(x) \to F_{X}(x).
        \end{equation*}
    \end{defn}
    We also define a stricter form of convergence.
    \begin{defn}
        A sequence of random variables $\{X_{n}\}$ \textbf{converges in probability} to a random variable $X$, denoted by $X_{n} \xrightarrow{\prob} X$, if for any $\varepsilon > 0$, as $n \to \infty$:
        \begin{align*}
            \prob(\abs{X_{n} - X} < \varepsilon) &\to 1, & \prob(\abs{X_{n} - X} \geq \varepsilon) &\to 0.
        \end{align*}
    \end{defn}
    \begin{rem}
        If $X_{n} \xrightarrow{\prob} X$, then $X_{n} \xrightarrow{D} X$. The converse is not necessarily true.
    \end{rem}
    \begin{rem}
        After this point, we primarily use $X_{n} \xrightarrow{D} X$ in most cases. For simplicity, we may write it as $X_{n} \to X$.
    \end{rem}

\section{Limit Theorems}
    Using the last two theorems, the following two theorems are highly useful in both statistics and probability theory as they provide approximate distributions of averages without requiring strong distributional assumptions.
    \begin{thm}\named{Weak Law of Large Numbers (WLLN)}
        Let $\{X_{n}\}$ be a sequence of i.i.d. random variables. Let $\E(X_{i}) = \mu$ for all $i = 1, 2, \cdots$. Define $\overline{X}$ as the sample mean of the random variables. Then, as $n \to \infty$:
        \begin{equation*}
            \overline{X} \xrightarrow{D} \mu.
        \end{equation*}
    \end{thm}
    \begin{thm}\named{Classical Central Limit Theorem (CLT)}
        Let $\{X_{n}\}$ be a sequence of i.i.d. random variables whose MGFs exist in a neighborhood of $0$. Let $\E(X_{i}) = \mu$ and $\Var(X_{i}) = \sigma^{2} > 0$ for all $i = 1, 2, \cdots$. Define $\overline{X}$ as the sample mean of the random variables. Then, as $n \to \infty$:
        \begin{equation*}
            \frac{\sqrt{n}(\overline{X} - \mu)}{\sigma} = \frac{\sum_{i=1}^{n} X_{i} - n\mu}{\sqrt{n}\sigma} \xrightarrow{D} \N(0, 1).
        \end{equation*}
    \end{thm}
    \begin{rem}
        This is a common abuse of notation.
    \end{rem}
    This works generally for most distributions. However, it is often tedious to find the MGF. We can apply the following version of the CLT instead.
    \begin{thm}\named{L\'evy-Lindeberg Central Limit Theorem}
        Let $\{X_{n}\}$ be a sequence of i.i.d. random variables with a common population mean $\mu$ and a common population variance $\sigma^{2}$. Assume that $0 < \sigma^{2} < \infty$. Define $\overline{X}$ as the sample mean of the random variables. Then, as $n \to \infty$:
        \begin{equation*}
            \frac{\sqrt{n}(\overline{X} - \mu)}{\sigma} = \frac{\sum_{i=1}^{n} X_{i} - n\mu}{\sqrt{n}\sigma} \xrightarrow{D} \N(0, 1).
        \end{equation*}
    \end{thm}
    Sometimes, we deal with functions of multiple random variables, and we must establish how they converge.
    \begin{thm}\named{Slutsky's Theorem}
        If $X_{n} \xrightarrow{D} X$ and $Y_{n} \xrightarrow{\prob} c$, then:
        \begin{enumerate}
            \item $X_{n} + Y_{n} \xrightarrow{D} X + c$,
            \item $X_{n} Y_{n} \xrightarrow{D} cX$,
            \item $\frac{X_{n}}{Y_{n}} \xrightarrow{D} \frac{X}{c}$ if $c \neq 0$.
        \end{enumerate}
    \end{thm}
    \newpage

    \begin{eg}
        Assume that $X_{i} \sim \Bern(p)$ for all $i$. We want to estimate the unknown $p$. We have a common mean $\mu = p$ and a common variance $\sigma^{2} = p(1 - p)$. By applying the CLT, as $n \to \infty$:
        \begin{equation*}
            \overline{X} \to \N\left(p, \frac{p(1 - p)}{n}\right).
        \end{equation*}
        Therefore, we can use the normal distribution to approximate the unknown parameter. We want an estimate that we can be confident about, and commonly we use a probability of $0.95$:
        \begin{equation*}
            0.95 = \prob\left(-z_{0.025} \leq \frac{\overline{X} - p}{\sqrt{\frac{p(1 - p)}{n}}} \leq z_{0.025}\right) = \prob\left((\overline{X} - p)^{2} \leq z_{0.025}^{2} \frac{p(1 - p)}{n}\right).
        \end{equation*} 
        Solving the inequality, we would find an interval that estimates the parameter $p$. However, this is highly inconvenient. We may use another method. Let us replace $\sqrt{\frac{p(1 - p)}{n}}$ with $\sqrt{\frac{\overline{X}(1 - \overline{X})}{n}}$. As $n \to \infty$:
        \begin{equation*}
            \sqrt{\frac{\overline{X}(1 - \overline{X})}{n}} = \sqrt{\frac{p(1 - p)}{n}} \sqrt{\frac{p(1 - p)}{\overline{X}(1 - \overline{X})}} \to \sqrt{\frac{p(1 - p)}{n}},
        \end{equation*}
        since, by Slutsky's Theorem, $\sqrt{\frac{p(1 - p)}{\overline{X}(1 - \overline{X})}} \to 1$ as $\overline{X} \to p$ by the WLLN. We have:
        \begin{equation*}
            0.95 = \prob\left(-z_{0.025} \leq \frac{\overline{X} - p}{\sqrt{\frac{\overline{X}(1 - \overline{X})}{n}}} \leq z_{0.025}\right) = \prob\left(\overline{X} - z_{0.025} \sqrt{\frac{\overline{X}(1 - \overline{X})}{n}} \leq p \leq \overline{X} + z_{0.025} \sqrt{\frac{\overline{X}(1 - \overline{X})}{n}}\right).
        \end{equation*} 
    \end{eg}
    \begin{eg}
        In a survey before an election, a poll was taken of $300$ potential voters. Among them, $120$ said that they would vote for candidate A. Determine a $95\%$ confidence interval for the population proportion $p_{A}$ of voters who would vote for candidate A in the election.\\
        From the poll, we have a point estimate $\overline{x} = \hat{p}_{A} = \frac{120}{300} = 0.4$. From the last example, we have found that the $95\%$ confidence interval is:
        \begin{equation*}
            \left(\overline{x} - z_{0.025} \sqrt{\frac{\overline{x}(1 - \overline{x})}{n}}, \overline{x} + z_{0.025} \sqrt{\frac{\overline{x}(1 - \overline{x})}{n}}\right) \approx (0.3446, 0.4554).
        \end{equation*}
        Equivalently, the percentage of voters for candidate A would be from $34.46\%$ to $45.54\%$, with a margin of error of $5.54\%$.
    \end{eg}
    \begin{eg}
        Following the previous example, assume that we have been given a margin of error $D$. How many data points should we collect in order to achieve this margin of error?\\
        From how we find the margin of error:
        \begin{equation*}
            z_{0.025} \sqrt{\frac{\overline{x}(1 - \overline{x})}{n}} = D \implies n = p(1 - p) \frac{z_{0.025}^{2}}{D^{2}}.
        \end{equation*}
        Since $p(1 - p) \leq \frac{1}{4}$, if we specify that $D = 0.05$, we have:
        \begin{equation*}
            n \leq \frac{z_{0.025}^{2}}{4D^{2}} = \frac{1.96^{2}}{4(0.05)^{2}} \leq \frac{2^{2}}{4(0.05)^{2}} = 400.
        \end{equation*}
        We may use this to determine whether we have obtained enough data.\\
        Assume that we have $n^{*}$ respondents. Is it enough? The number of required respondents is obtained by:
        \begin{equation*}
            n_{\text{required}} = \frac{\overline{x^{*}}(1 - \overline{x^{*}}) z_{0.025}^{2}}{D^{2}}.
        \end{equation*}
        If $n^{*} < n_{\text{required}}$, then the current number of data points is not enough, and we would need to find more respondents. If $n^{*} \geq n_{\text{required}}$, then the current number of data points is sufficient.
    \end{eg}
    \newpage

    \begin{eg}
        We aim to use Poisson random variables to prove that as $n \to \infty$:
        \begin{equation*}
            e^{-n} \sum_{k=0}^{n} \frac{n^{k}}{k!} \to \frac{1}{2}.
        \end{equation*}
        Let $\{X_{n}\}$ be a sequence of i.i.d. random variables where $X_{i} \sim \Poisson(1)$ for $i = 1, 2, \cdots$. Let $Y_{n} = \sum_{i=1}^{n} X_{i}$. By the CLT, we have:
        \begin{equation*}
            \frac{Y_{n} - n}{\sqrt{n}} \to \N(0, 1).
        \end{equation*}
        Since $Y_{n} \sim \Poisson(n)$, we have:
        \begin{equation*}
            e^{-n} \sum_{k=0}^{n} \frac{n^{k}}{k!} = \prob(Y_{n} \leq n) = \prob\left(\frac{Y_{n} - n}{\sqrt{n}} \leq 0\right) \to \frac{1}{2}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Given a sequence of i.i.d. random variables $\{X_{n}\}$, we want to find the asymptotic distribution for the $k$-th sample moment $\overline{X^{k}}$ as $n \to \infty$. Notice that $X_{i}^{k}$ are independent for $i = 1, 2, \cdots$. By the CLT:
        \begin{equation*}
            \frac{\sqrt{n}(\overline{X^{k}} - \mu_{k}')}{\sqrt{\mu_{2k}' - (\mu_{k}')^{2}}} \to \N(0, 1).
        \end{equation*}
        Therefore, the asymptotic distribution for $\overline{X^{k}}$ when $n \to \infty$ is $\N\left(\mu_{k}', \frac{1}{n}[\mu_{2k}' - (\mu_{k}')^{2}]\right)$.
    \end{eg}
    The Central Limit Theorem provides us with a limiting standard normal distribution for the sample mean. However, we often deal with functions of the sample mean.
    \begin{thm}\named{Continuous Mapping Theorem}
        Let $\{X_{n}\}$ be a sequence of random variables and $X$ be a random variable. Suppose there is a function $g$ with a set of discontinuity points $D_{g}$ such that $\prob(X \in D_{g}) = 0$. Then:
        \begin{enumerate}
            \item If $X_{n} \xrightarrow{D} X$, then $g(X_{n}) \xrightarrow{D} g(X)$.
            \item If $X_{n} \xrightarrow{\prob} X$, then $g(X_{n}) \xrightarrow{\prob} g(X)$.
        \end{enumerate}
    \end{thm}
    \begin{thm}\named{Delta Method}
        Let $\{X_{n}\}$ be a sequence of random variables such that for constants $a$ and $b > 0$, as $n \to \infty$:
        \begin{equation*}
            \sqrt{n}(X_{n} - a) \xrightarrow{D} \N(0, b^{2}).
        \end{equation*}
        Then for a given function $g$, suppose that $g'(a)$ exists and is not $0$. As $n \to \infty$:
        \begin{equation*}
            \sqrt{n}(g(X_{n}) - g(a)) \xrightarrow{D} \N(0, [g'(a)b]^{2}).
        \end{equation*}
    \end{thm}
    \begin{cor}
        \label{Chapter 1 (Corollary) CLT for functions of random variables}
        If $\overline{X}$ is the sample mean of a random sample $X_{1}, \cdots, X_{n}$ of size $n$ from a distribution with a finite mean $\mu$ and finite variance $\sigma^{2} > 0$, then for a given function $g$, suppose that $g'(\mu)$ exists and is not $0$. As $n \to \infty$:
        \begin{equation*}
            \sqrt{n}(g(\overline{X}) - g(\mu)) \xrightarrow{D} \N(0, [g'(\mu)\sigma]^{2}).
        \end{equation*}  
    \end{cor}
    \begin{proofing}
        By the Central Limit Theorem, we have:
        \begin{equation*}
            \sqrt{n}(\overline{X} - \mu) \xrightarrow{D} \N(0, \sigma^{2}).
        \end{equation*}
        Therefore, by the Delta Method, for any function $g$ such that $g'(\mu)$ exists and is not $0$:
        \begin{equation*}
            \sqrt{n}(g(\overline{X}) - g(\mu)) \xrightarrow{D} \N(0, [g'(\mu)\sigma]^{2}).
        \end{equation*}
    \end{proofing}
    \newpage

    \begin{eg}
        Assume that there are $70$ respondents, $68$ of whom would vote for one candidate.\\
        If we use the same process from previous examples, we find that the $95\%$ confidence interval is $(0.9324, 1.0105)$, which is out of range. In fact, if the point estimate $\hat{p}$ is quite close to $0$ or $1$, the resulting interval may include values that are outside the range of $p$. This is a poor interval estimate.\\
        We take a transformation, say $g(p)$, such that $g(p) \in (-\infty, \infty)$. Since $0 < p < 1$, $\ln{p} < 0$. Therefore, we find that:
        \begin{equation*}
            g(p) = \ln(-\ln{p}) \in (-\infty, \infty).
        \end{equation*}
        By the Delta method:
        \begin{equation*}
            \frac{g(\overline{X}) - g(p)}{g'(p) \sqrt{\frac{\overline{X}(1 - \overline{X})}{n}}} \to \N(0, 1).
        \end{equation*}
        By the WLLN and the Continuous Mapping Theorem, we can replace $g'(p)$ with $g'(\overline{X})$. Therefore, we have:
        \begin{align*}
            0.95 = \prob\left(-z_{0.025} \leq \frac{g(\overline{X}) - g(p)}{g'(\overline{X}) \sqrt{\frac{\overline{X}(1 - \overline{X})}{n}}} \leq z_{0.025}\right).
        \end{align*}
        Solving the formula gives a good $95\%$ confidence interval for $p$.
    \end{eg}
    \begin{eg}
        Let $\{X_{n}\}$ be a sequence of i.i.d. random variables where $X_{i} \sim \Bern(\theta)$ for $i = 1, 2, \cdots$. Show that:
        \begin{equation*}
            Z_{n} = 2\sqrt{n}\left(\sin^{-1}{\sqrt{\overline{X}}} - \sin^{-1}{\sqrt{\theta}}\right) \to \N(0, 1).
        \end{equation*}
        Let $g(t) = \sin^{-1}{\sqrt{t}}$. We obtain:
        \begin{equation*}
            g'(t) = \frac{1}{2\sqrt{t}\sqrt{1 - t}}.
        \end{equation*}
        The derivative is well-defined and non-zero for $0 < \theta < 1$ by substituting $t = \theta$. Note that $\E(X_{i}) = \theta$ and $\Var(X_{i}) = \theta(1 - \theta)$ for $i = 1, \cdots, n$. By Corollary \ref{Chapter 1 (Corollary) CLT for functions of random variables}:
        \begin{equation*}
            \sqrt{n}(g(\overline{X}) - g(\theta)) \to \N\left(0, \frac{1}{4}\right).
        \end{equation*}
        Since $Z_{n} = 2\sqrt{n}(g(\overline{X}) - g(\theta))$, we find that as $n \to \infty$:
        \begin{equation*}
            Z_{n} \to \N(0, 1).
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $\{X_{n}\}$ be a sequence of i.i.d. random variables where $X_{i} \sim \Exp(\theta)$ for $i = 1, 2, \cdots$. We want to find a variance-stabilizing transformation, which is a function $g(x)$ such that the limiting distribution of:
        \begin{equation*}
            Y_{n} = \sqrt{n}[g(\overline{X_{n}}) - g(\theta)]
        \end{equation*}
        does not depend on $\theta$. We find that $\E(X_{i}) = \frac{1}{\theta}$ and $\Var(X_{i}) = \frac{1}{\theta^{2}}$ for $i = 1, 2, \cdots$. 
        
        We claim that $g(x) = \ln{x}$ is the desired transformation. We have:
        \begin{equation*}
            g'(x) = \frac{1}{x}.
        \end{equation*}
        By substituting $x = \frac{1}{\theta}$, we see that the derivative is non-zero. Applying Corollary \ref{Chapter 1 (Corollary) CLT for functions of random variables}:
        \begin{equation*}
            \sqrt{n}\left(g(\overline{X}) - g\left(\frac{1}{\theta}\right)\right) \to \N(0, 1).
        \end{equation*}
        Therefore, $g(x) = \ln{x}$ is the variance-stabilizing transformation.
    \end{eg}
    \newpage
    
    However, we usually deal with more than one variable. Before extending the theorems to the multivariate case, we must first introduce the multivariate normal distribution.
    \begin{eg}\named{Multivariate Normal Distribution} $\mathbf{X} \sim \N_{k}(\boldsymbol{\mu}, \mathbf{\Sigma})$\\
        Given a random vector $\mathbf{X}$, let the $k \times 1$ vector $\boldsymbol{\mu}$ be the expected value of $\mathbf{X}$ and the $k \times k$ matrix $\mathbf{\Sigma}$ be its variance-covariance matrix. Assume that $\mathbf{\Sigma}$ is positive-definite (for all non-zero vectors $\mathbf{z}$ with real entries, we have $\mathbf{z}^{T} \mathbf{\Sigma z} > 0$). The random vector $\mathbf{X}$ is $k$-dimensional normal if its PDF is:
        \begin{equation*}
            f_{\mathbf{X}}(\mathbf{x}) = (2\pi)^{-\frac{k}{2}} \abs{\mathbf{\Sigma}}^{-\frac{1}{2}} e^{-\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^{T} \mathbf{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})}.
        \end{equation*}
    \end{eg}
    \begin{rem}
        The $i$-th row and $j$-th column of the $k \times k$ variance-covariance matrix $\mathbf{\Sigma}$ is the element $a_{ij}$, given by:
        \begin{equation*}
            a_{ij} = \cov(X_{i}, X_{j}).
        \end{equation*}
        Note that if $i = j$, then $\cov(X_{i}, X_{i}) = \Var(X_{i})$.
    \end{rem}
    \begin{eg}
        If $k = 2$, then $X \sim \N_{2}(\boldsymbol{\mu}, \mathbf{\Sigma})$ is bivariate normal.
    \end{eg}
    \begin{lem}
        \label{Chapter 1 (Lemma) Distribution of matrix multiplication with random vector}
        If $\mathbf{X} \sim \N_{p}(\boldsymbol{\mu}, \mathbf{\Sigma})$, then for any $q \times p$ matrix $\mathbf{A}$, we have:
        \begin{equation*}
            \mathbf{AX} \sim \N_{q}(\mathbf{A} \boldsymbol{\mu}, \mathbf{A\Sigma A}^{T}).
        \end{equation*}
    \end{lem}
    \begin{eg}
        Using this lemma, one can isolate some of the random variables that make up the random vector $\mathbf{X} = (X_{1}\ \cdots\ X_{p})^{T} \sim \N_{p}(\boldsymbol{\mu}, \mathbf{\Sigma})$. For example, setting the $(p-1) \times p$ matrix $\mathbf{A}$ as:
        \begin{equation*}
            \mathbf{A} = \begin{pmatrix}
                0 & 1 & 0 & \hdots & 0\\
                \vdots & \ddots & \ddots & \ddots & \vdots\\
                \vdots & & \ddots & \ddots & 0\\
                0 & \hdots & \hdots & 0 & 1
            \end{pmatrix} = \begin{pmatrix}\begin{array}{c|c}
                0 & \\
                \vdots & \mathbf{I}_{(p-1) \times (p-1)}\\
                0 &
            \end{array}\end{pmatrix}.
        \end{equation*}
        We find that:
        \begin{equation*}
            \mathbf{AX} = \begin{pmatrix}
            X_{2}\\
            \vdots\\
            X_{p}
            \end{pmatrix} \sim \N_{p-1}(\mathbf{A} \boldsymbol{\mu}, \mathbf{A\Sigma A}^{T}),
        \end{equation*}
        where $\mathbf{A} \boldsymbol{\mu}$ is the mean vector of $(X_{2}\ \cdots\ X_{p})^{T}$ and $\mathbf{A\Sigma A}^{T}$ is the variance-covariance matrix of $(X_{2}\ \cdots\ X_{p})^{T}$.
    \end{eg}
    \begin{lem}
        \label{Chapter 1 (Lemma) Independence iff uncorrelated and bivariate}
        If:
        \begin{equation*}
            \begin{pmatrix}X_{1}\\X_{2}\end{pmatrix} \sim \N_{2}\left(\begin{pmatrix}\mu_{1}\\\mu_{2}\end{pmatrix}, \begin{pmatrix}\sigma_{11}^{2} & \sigma_{12}^{2}\\\sigma_{21}^{2} & \sigma_{22}^{2}\end{pmatrix}\right),
        \end{equation*}
        then $X_{1}$ and $X_{2}$ are independent if and only if $\sigma_{12}^{2} = \sigma_{21}^{2} = 0$.
    \end{lem}
    \begin{proofing}
        From the properties of covariance:
        \begin{equation*}
            \sigma_{12}^{2} = \cov(X_{1}, X_{2}) = \cov(X_{2}, X_{1}) = \sigma_{21}^{2}.
        \end{equation*}
        Suppose that $X_{1}$ and $X_{2}$ are independent. We have:
        \begin{equation*}
            \cov(X_{1}, X_{2}) = \E[(X_{1} - \E(X_{1}))(X_{2} - \E(X_{2}))] = \E(X_{1}X_{2}) - \E(X_{1})\E(X_{2}) = 0.
        \end{equation*}
        Therefore, $\sigma_{12}^{2} = \sigma_{21}^{2} = 0$.\\
        Conversely, suppose that $\sigma_{12}^{2} = \sigma_{21}^{2} = 0$. We have $\cov(X_{1}, X_{2}) = 0$. Therefore:
        \begin{align*}
            f_{X_{1}, X_{2}}(x_{1}, x_{2}) &= \frac{1}{2\pi\sigma_{11}\sigma_{22}} \exp\left(-\frac{1}{2}\left(\frac{(x_{1} - \mu_{1})^{2}}{\sigma_{11}^{2}} + \frac{(x_{2} - \mu_{2})^{2}}{\sigma_{22}^{2}}\right)\right)\\
            &= \frac{1}{\sqrt{2\pi\sigma_{11}^{2}}} \exp\left(-\frac{(x_{1} - \mu_{1})^{2}}{2\sigma_{11}^{2}}\right) \frac{1}{\sqrt{2\pi\sigma_{22}^{2}}} \exp\left(-\frac{(x_{2} - \mu_{2})^{2}}{2\sigma_{22}^{2}}\right) = f_{X_{1}}(x_{1})f_{X_{2}}(x_{2}).
        \end{align*}
        Therefore, $X_{1}$ and $X_{2}$ are independent.
    \end{proofing}
	\newpage

    \begin{rem}
        Two random variables being uncorrelated does not imply that they are independent. This is only true if they are bivariate normal.
    \end{rem}
    We may extend the CLT to the multivariate case.
    \begin{thm}\named{Multivariate Central Limit Theorem}
        Let $\{\mathbf{X}_{n} = (X_{n1}\ \cdots\ X_{nk})^{T} \in \mathbb{R}^{k}\}$ be a sequence of i.i.d. random vectors with a variance-covariance matrix $\mathbf{\Sigma}$. We assume that $\E(X_{ij}^{2}) < \infty$ for $i = 1, 2, \cdots$ and $j = 1, \cdots, k$. Define $\mathbf{\overline{X}}$ as the sample mean of the random vectors. Then, as $n \to \infty$:
        \begin{equation*}
            \sqrt{n}(\mathbf{\overline{X}} - \boldsymbol{\mu}) \xrightarrow{D} \N_{k}(\mathbf{0}, \mathbf{\Sigma}).
        \end{equation*}
    \end{thm}
    We may extend the Delta method to multivariate cases.
    \begin{thm}\named{Multivariate 1st-Order Delta Method}
        Let $\{\mathbf{X}_{n} \in \mathbb{R}^{k}\}$ be a sequence of random vectors such that for a constant vector $\mathbf{a} \in \mathbb{R}^{k}$, as $n \to \infty$:
        \begin{equation*}
            \sqrt{n}(\mathbf{X}_{n} - \mathbf{a}) \xrightarrow{D} \mathbf{U},
        \end{equation*}
        where $\mathbf{U}$ is a random vector in $\mathbb{R}^{k}$. If a function $h: \mathbb{R}^{k} \to \mathbb{R}$ has a derivative $\nabla h(\mathbf{a}) \neq \mathbf{0}$, then as $n \to \infty$:
        \begin{equation*}
            \sqrt{n}(h(\mathbf{X}_{n}) - h(\mathbf{a})) \xrightarrow{D} \nabla h(\mathbf{a}) \mathbf{U},
        \end{equation*}
        where:
        \begin{equation*}
            \nabla h = \left(\pdv*[order=1]{h(t_{1}, \cdots, t_{k})}{t_{1}}, \cdots, \pdv*[order=1]{h(t_{1}, \cdots, t_{k})}{t_{k}}\right).
        \end{equation*}
    \end{thm}

\chapter{Point Estimation}
    In this chapter, we will study two general approaches to estimate unknown parameters of any given parametric distribution.\\
    The basic idea of point estimation is to use a statistic $T$, an estimate $T(\mathbf{x})$, or an estimator $T(\mathbf{X})$ to estimate the unknown parameter $g(\theta)$, where $\mathbf{x} = (x_{1}\ \cdots\ x_{n})^{T}$ is a realization of the random vector $\mathbf{X} = (X_{1}\ \cdots\ X_{n})^{T}$ with a PDF $f(x | \theta)$ or PMF $p(x | \theta)$, and $\theta$ lies in the parameter space $\Theta$.
    \begin{rem}
        Most often, the parameters of interest to be estimated (\textbf{estimand}) are functions of the unknown distribution parameters $\theta$, e.g., $\mu^{2}$ or $\frac{\sigma}{\mu}$.
    \end{rem}
    \begin{rem}
        We only estimate unknown parameters. There is no point in estimating an already known parameter.
    \end{rem}
    \begin{defn}
        An estimator or estimate $\hat{\theta}$ is \textbf{unbiased} or \textbf{mean-unbiased} for $\theta$ if $\E(\hat{\theta}) = \theta$.
    \end{defn}
    
\section{Methods of Moments Estimation}
    The method of moments estimation is one of the most widely used techniques in statistics for estimating unknown parameters. As the name suggests, it is based on moments. The motivation behind this method is that, in some cases, the parameter of interest can be expressed as a function of population moments about $0$.
    \begin{defn}
        Suppose there are $k$ unknown parameters $\theta_{1}, \cdots, \theta_{k}$. If these parameters can be expressed in terms of $k$ or more moments, i.e.:
        \begin{equation*}
            \begin{cases}
                \theta_{1} = g_{1}(\mu_{1}', \mu_{2}', \cdots, \mu_{k}', \cdots),\\
                \theta_{2} = g_{2}(\mu_{1}', \mu_{2}', \cdots, \mu_{k}', \cdots),\\
                \vdots\\
                \theta_{k} = g_{k}(\mu_{1}', \mu_{2}', \cdots, \mu_{k}', \cdots),
            \end{cases}
        \end{equation*}
        then the \textbf{method of moments estimator} (MME) of $(\theta_{1}, \theta_{2}, \cdots, \theta_{k})$, denoted by $(\widetilde{\theta}_{1}, \widetilde{\theta}_{2}, \cdots, \widetilde{\theta}_{k})$, is given by:
        \begin{equation*}
            \begin{cases}
                \widetilde{\theta}_{1} = g_{1}(\overline{X}, \overline{X^{2}}, \cdots, \overline{X^{k}}, \cdots),\\
                \widetilde{\theta}_{2} = g_{2}(\overline{X}, \overline{X^{2}}, \cdots, \overline{X^{k}}, \cdots),\\
                \vdots\\
                \widetilde{\theta}_{k} = g_{k}(\overline{X}, \overline{X^{2}}, \cdots, \overline{X^{k}}, \cdots).
            \end{cases}
        \end{equation*}
    \end{defn}
    \begin{rem}
        The method of moments estimate is obtained by substituting sample moments:
        \begin{equation*}
        	\widetilde{\theta}_{i} = g_{i}(\overline{x}, \overline{x^{2}}, \cdots, \overline{x^{k}}, \cdots)
        \end{equation*}
        for $i = 1, \cdots, k$.
    \end{rem}
    \begin{rem}
        This method is quick and straightforward, but the MMEs obtained are often biased and heavily depend on the existence of the required population moments.
    \end{rem}
    \newpage
    
    \begin{rem}
        Do not confuse the method of moments estimator with the method of moments estimate.
    \end{rem}
    \begin{rem}
        Do not write the MME as $(\theta_{1}, \theta_{2}, \cdots, \theta_{k})$. This is incorrect.
    \end{rem}
	\begin{eg}
        Consider a random sample of size $n$ from $X \sim \N(10, \sigma^{2})$. We want to estimate $\sigma^{2}$.\\
        We have $k = 1$, $\theta_{1} = \sigma^{2}$. We can express it in terms of moments:
        \begin{equation*}
            \sigma^{2} = \E(X^{2}) - 100.
        \end{equation*}
        Therefore, the MME of $\sigma^{2}$ is:
        \begin{equation*}
            \widetilde{\sigma}^{2} = \overline{X^{2}} - 100.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Consider a random sample of size $n$ from $X \sim \N(\mu, \sigma^{2})$. We want to estimate $\mu$ and $\sigma^{2}$.\\
        We have $k = 2$, $(\theta_{1}, \theta_{2}) = (\mu, \sigma^{2})$. We can express them in terms of moments: 
        \begin{equation*}
            \begin{cases}
                \mu = \E(X),\\
                \sigma^{2} = \E(X^{2}) - [\E(X)]^{2}.
            \end{cases}
        \end{equation*}
        Therefore, the MME of $\mu$ and $\sigma^{2}$ are:
        \begin{equation*}
            \begin{cases}
                \widetilde{\mu} = \overline{X},\\
                \widetilde{\sigma}^{2} = \overline{X^{2}} - (\overline{X})^{2} = \frac{1}{n} \sum_{i=1}^{n} (X_{i} - \overline{X})^{2}.
            \end{cases}
        \end{equation*}
    \end{eg}
    \begin{rem}
        The MME may not be unique because the parameter can be expressed as different functions of moments. To address this issue, we usually prefer using fewer or lower moments to obtain the MME.
    \end{rem}
    \begin{eg}
        Consider a random sample of size $n$ from $X \sim \Poisson(\lambda)$. We want to estimate $\lambda$. We have $k = 1$, $\theta_{1} = \lambda$. There are multiple ways to express it in terms of moments. For example, $\lambda = \E(X)$, $\lambda = \E(X^{2}) - [\E(X)]^{2}$, or other combinations. Based on the remark, we choose the one with fewer or lower moments:
        \begin{equation*}
            \lambda = \E(X).
        \end{equation*}
        Therefore, the MME of $\lambda$ is:
        \begin{equation*}
            \lambda = \overline{X}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Consider a random sample of size $n$ from $X \sim \Gam(\alpha, \beta)$. Assume that we know $\E(X) = 3423$. We have $k = 2$, $(\theta_{1}, \theta_{2}) = (\alpha, \beta)$. We can express them in terms of moments:
        \begin{equation*}
            \begin{cases}
                3423 = \frac{\alpha}{\beta},\\
                \E(X^{2}) = \frac{\alpha}{\beta^{2}} + 3423^{2}.
            \end{cases} \implies \begin{cases}
                \alpha = \frac{3423^{2}}{\E(X^{2}) - 3423^{2}},\\
                \beta = \frac{3423}{\E(X^{2}) - 3423^{2}}.
            \end{cases}
        \end{equation*}
        Therefore, the MME of $\alpha$ and $\beta$ is:
        \begin{equation*}
            \begin{cases}
                \widetilde{\alpha} = \frac{3423^{2}}{\overline{X^{2}} - 3423^{2}},\\
                \widetilde{\beta} = \frac{3423}{\overline{X^{2}} - 3423^{2}}.
            \end{cases}
        \end{equation*}
    \end{eg}
    \begin{lem}\named{Invariance Property of MME}
        If $\widetilde{\theta}_{i}$ is the MME for $\theta_{i}$ for $i = 1, \cdots, k$, then $h(\widetilde{\theta}_{1}, \cdots, \widetilde{\theta}_{k})$ is the MME for $h(\theta_{1}, \cdots, \theta_{k})$, where $h$ is a known function.
    \end{lem}
    \begin{thm}
        \label{Chapter 2 (Theorem) Sequence of MME is asympt. normal}
        A sequence of MMEs $\{\widetilde{\theta}_{n} \in \mathbb{R}^{k}\}$ is consistent, asymptotically unbiased for $\theta$, and asymptotically normally distributed. More precisely, under certain assumptions like $\E\abs{X}^{2k} < \infty$, as $n \to \infty$, we have:
        \begin{equation*}
            \sqrt{n}(\widetilde{\theta}_{n} - \theta) \to \N_{k}(\mathbf{0}, \mathbf{GHG}^{T}),
        \end{equation*}
        where $\mathbf{G}$ is a $k \times k$ matrix with $\pdv{g_{i}}{\mu_{j}'}$ as its $(i, j)$-th entry, and $\mathbf{H}$ is a $k \times k$ matrix with $\mu_{i+j}' - \mu_{i}'\mu_{j}'$ as its $(i, j)$-th entry, for $i = 1, \cdots, k$ and $j = 1, \cdots, k$.
    \end{thm}
    \newpage
    
    \begin{rem}
        In the theorem, "consistent" means convergence in probability. For any $\varepsilon > 0$, as $n \to \infty$:
        \begin{equation*}
            \prob(|\widetilde{\theta}_{n} - \theta| > \varepsilon) \to 0.
        \end{equation*}
    \end{rem}
    \begin{rem}
        Also in the theorem, "asymptotically unbiased" means that:
        \begin{equation*}
            \lim_{n \to \infty} \E(\widetilde{\theta}_{n}) = \theta.
        \end{equation*}
        Note that it may be true that $\E(\widetilde{\theta}_{n}) \neq \theta$ for some $n$.
    \end{rem}
    \begin{eg}
        Consider a random sample of size $n$ from a random variable $X$ with $\E\abs{X}^{4} < \infty$. We take:
        \begin{equation*}
            \theta = \begin{pmatrix}
                \mu\\ \sigma^{2}
            \end{pmatrix} = \begin{pmatrix}
                \mu_{1}'\\ \mu_{2}' - (\mu_{1}')^{2}
            \end{pmatrix}.
        \end{equation*}
        We have:
        \begin{align*}
            \mathbf{G} &= \begin{pmatrix}
                1 & 0\\
                -2\mu_{1}' & 1
            \end{pmatrix}, & \mathbf{H} &= \begin{pmatrix}
                \mu_{2}' - (\mu_{1}')^{2} & \mu_{3}' - \mu_{1}'\mu_{2}'\\
                \mu_{3}' - \mu_{2}'\mu_{1}' & \mu_{4}' - (\mu_{2}')^{2}
            \end{pmatrix}.
        \end{align*}
        Therefore:
        \begin{align*}
            \mathbf{GHG}^{T} &= \begin{pmatrix}
                1 & 0\\
                -2\mu_{1}' & 1
            \end{pmatrix} \begin{pmatrix}
                \mu_{2}' - (\mu_{1}')^{2} & \mu_{3}' - \mu_{1}'\mu_{2}'\\
                \mu_{3}' - \mu_{2}'\mu_{1}' & \mu_{4}' - (\mu_{2}')^{2}
            \end{pmatrix} \mathbf{G}^{T}\\
            &=\begin{pmatrix}
                \mu_{2}' - (\mu_{1}')^{2} & \mu_{3}' - \mu_{1}'\mu_{2}'\\
                \mu_{3}' - 3\mu_{1}'\mu_{2}' + 2(\mu_{1}')^{3} & \mu_{4}' - 2\mu_{1}'\mu_{3}' - (\mu_{2}')^{2} + 2(\mu_{1}')^{2}\mu_{2}'
            \end{pmatrix}\begin{pmatrix}
                1 & -2\mu_{1}'\\
                0 & 1
            \end{pmatrix}\\
            &= \begin{pmatrix}
                \mu_{2}' - (\mu_{1}')^{2} & \mu_{3}' - 3\mu_{1}'\mu_{2}' + 2(\mu_{1}')^{3}\\
                \mu_{3}' - 3\mu_{1}'\mu_{2}' + 2(\mu_{1}')^{3} & \mu_{4}' - 4\mu_{1}'\mu_{3}' - (\mu_{2}')^{2} + 8\mu_{2}'(\mu_{1}')^{2} - 4(\mu_{1}')^{4}
            \end{pmatrix}.
        \end{align*}
        Using the fact that:
        \begin{align*}
            \mu_{3} &= \mu_{3}' - 3\mu_{2}'\mu_{1}' + 2(\mu_{1}')^{3},\\ 
            \mu_{4} &= \mu_{4}' - 4\mu_{3}'\mu_{1}' + 6\mu_{2}'(\mu_{1}')^{2} - 3(\mu_{1}')^{4},\\
            \sigma^{4} &= (\mu_{2}')^{2} - 2\mu_{2}'(\mu_{1}')^{2} + (\mu_{1}')^{4},
        \end{align*}
        we find the resultant matrix:
        \begin{equation*}
            \mathbf{GHG}^{T} = \begin{pmatrix}
                \sigma^{2} & \mu_{3}\\
                \mu_{3} & \mu_{4} - \sigma^{4}
            \end{pmatrix}.
        \end{equation*}
        Using Theorem \ref{Chapter 2 (Theorem) Sequence of MME is asympt. normal}, denote:
        \begin{equation*}
            \widetilde{\theta}_{n} = \begin{pmatrix}
                \overline{X}_{n}\\ S_{n}^{2}
            \end{pmatrix}.
        \end{equation*}
        As $n \to \infty$:
        \begin{equation*}
            \sqrt{n}\left[\begin{pmatrix}
                \overline{X}_{n}\\ S_{n}^{2}
            \end{pmatrix} - \begin{pmatrix}
                \mu\\ \sigma^{2}
            \end{pmatrix}\right] \to \N_{2}\left(\begin{pmatrix}
                0\\ 0
            \end{pmatrix}, \begin{pmatrix}
                \sigma^{2} & \mu_{3}\\
                \mu_{3} & \mu_{4} - \sigma^{4}
            \end{pmatrix}\right).
        \end{equation*}
        Based on the properties of the variance-covariance matrix, we find that as $n \to \infty$:
        \begin{equation*}
            \sqrt{n}(S_{n}^{2} - \sigma^{2}) \to \N(0, \mu_{4} - \sigma^{4}).
        \end{equation*}
        By the Delta Method, under the condition that $\sigma^{2} > 0$:
        \begin{equation*}
            \sqrt{n}(S_{n} - \sigma) \to \N\left(0, \frac{\mu_{4} - \sigma^{4}}{4\sigma^{2}}\right).
        \end{equation*}
    \end{eg}
    \newpage

\section{Maximum Likelihood Estimation}
    The method of maximum likelihood is by far the most popular technique for deriving estimators, popularized by Ronald Aylmer Fisher in 1922. Currently, there is still a lot of research studying the properties of this estimation method.
    \begin{defn}
        Consider a random sample of size $n$ from a population with a PDF $f(\mathbf{x} | \theta)$ or a PMF $p(\mathbf{x} | \theta)$. Given a realization $\mathbf{x} = (x_{1}\ \cdots\ x_{n})^{T}$, the \textbf{likelihood function} is defined as:
        \begin{equation*}
            L(\theta) = L(\theta_{1}, \cdots, \theta_{k} | \mathbf{x}) = \begin{cases}
                \prod_{i=1}^{n} f(x_{i} | \theta), &\text{Continuous case},\\
                \prod_{i=1}^{n} p(x_{i} | \theta), &\text{Discrete case}.
            \end{cases}
        \end{equation*}
        The likelihood function quantifies how likely the observed data is to occur.
    \end{defn}
    \begin{rem}
        The likelihood function $L(\theta)$ is a function of $\theta$ with fixed $\mathbf{x}$.
    \end{rem}
    \begin{rem}
        Do not replace $x_{i}$ with $x$:
        \begin{equation*}
            L(\theta) = \begin{cases}
                \prod_{i=1}^{n} f(x_{i} | \theta) \neq \prod_{i=1}^{n} f(x | \theta), &\text{Continuous case},\\
                \prod_{i=1}^{n} p(x_{i} | \theta) \neq \prod_{i=1}^{n} p(x | \theta), &\text{Discrete case}.
            \end{cases}
        \end{equation*}
    \end{rem}
    The idea is that for each realization of $\mathbf{x}$, we want to estimate a value of $\theta \in \Theta$ at which $L(\theta)$ attains its maximum.
    \begin{defn}
        The \textbf{maximum likelihood estimate} (MLE), denoted by $\hat{\theta}$, is obtained as:
        \begin{equation*}
            \hat{\theta} = \argmax_{\theta \in \Theta} L(\theta).
        \end{equation*}
    \end{defn}
    \begin{rem}
        In some cases, especially when differentiation is used, it is easier to work with the \textbf{log-likelihood}, defined as:
        \begin{equation*}
            l(\theta) = \ln{L(\theta)}.
        \end{equation*}
        We can do this because $l(\theta)$ and $L(\theta)$ are strictly increasing and have the same maxima.
    \end{rem}
    \begin{eg}
        Consider a random sample of size $n = 10$ from $\Bern(\theta)$, where $\theta$ is unknown. Therefore:
        \begin{equation*}
            L(\theta) = \prod_{i=1}^{n} p(x_{i} | \theta) = \theta^{n\overline{x}} (1 - \theta)^{n - n\overline{x}}.
        \end{equation*}
        Suppose that there are only two possible values of $\theta$: $\theta = 0.1$ or $\theta = 0.5$.\\
        From the observed data, assume that $\overline{x} = 0.4$. Substituting gives:
        \begin{align*}
            L(0.1) &= (0.1)^{4} (0.9)^{6} = 0.0000531441, & L(0.5) &= (0.5)^{4} (0.5)^{6} = 0.0009765625.
        \end{align*}
        Therefore, the MLE of $\theta$ is $\hat{\theta} = 0.5$.
    \end{eg}
    \begin{eg}
        In the case where $L(\theta)$ is differentiable on the interior of $\Theta$, one possible way of finding an MLE of $\theta = (\theta_{1}\ \cdots\ \theta_{k})^{T}$ is to solve the first-order equations for $i = 1, \cdots, k$:
        \begin{equation*}
            \pdv*{L(\theta)}{\theta_{i}} = 0 \quad \text{or} \quad \pdv*{l(\theta)}{\theta_{i}} = 0,
        \end{equation*}
        and check all the extrema.
    \end{eg}
    \begin{rem}
        Solving the first-order likelihood equations only gives you the maxima at critical points. You also need to check the extreme values.
    \end{rem}
    \newpage

    \begin{eg}
        Consider a random sample of size $n$ from $\N(\theta, 1)$, where $\theta$ is unknown. We may obtain the log-likelihood:
        \begin{equation*}
            l(\theta) = \ln\left(\prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}(x_{i} - \theta)^{2}}\right) = -\frac{1}{2} \sum_{i=1}^{n} (x_{i} - \theta)^{2} - \frac{n}{2} \ln(2\pi).
        \end{equation*}
        We find the critical points by solving:
        \begin{equation*}
            0 = \pdv*{l(\theta)}{\theta} = \sum_{i=1}^{n} (x_{i} - \theta).
        \end{equation*}
        This has the solution $\hat{\theta} = \overline{x}$. To check that the solution is indeed a global maximum, we verify:
        \begin{equation*}
            \pdv*[order={2}]{l(\theta)}{\theta} = -n < 0.
        \end{equation*}
        Therefore, the MLE of $\theta$ is $\hat{\theta} = \overline{x}$.
    \end{eg}
    \begin{eg}
        Continuing the previous example, we may alternatively find that for any $\theta \in \Theta$:
        \begin{equation*}
            \sum_{i=1}^{n} (x_{i} - \theta)^{2} \geq \sum_{i=1}^{n} (x_{i} - \overline{x})^{2}.
        \end{equation*}
        Thus, for any $\theta \in \Theta$:
        \begin{equation*}
            L(\theta) \leq L(\overline{x}).
        \end{equation*}
        Therefore, the MLE of $\theta$ is $\hat{\theta} = \overline{x}$.
    \end{eg}
    \begin{eg}
        Consider a random sample of size $n$ from $\N(\theta, 1)$, where $\theta$ is unknown. Previously, we found that $\hat{\theta} = \overline{x}$, which maximizes the log-likelihood. Let us now restrict $\theta \geq 0$.\\
        If $\overline{x} \geq 0$, then it satisfies the constraint $\theta \geq 0$. Therefore, the MLE is:
        \begin{equation*}
            \hat{\theta} = \overline{x}.
        \end{equation*}
        If $\overline{x} < 0$, then it does not satisfy the constraint $\theta \geq 0$. We analyze the log-likelihood again:
        \begin{equation*}
            l(\theta) = -\frac{1}{2} \sum_{i=1}^{n} (x_{i} - \theta)^{2} - \frac{n}{2} \ln(2\pi) = -\frac{1}{2} \sum_{i=1}^{n} (x_{i} - \overline{x})^{2} - \frac{n}{2} (\overline{x} - \theta)^{2} - \frac{n}{2} \ln(2\pi).
        \end{equation*}
        The term $(\overline{x} - \theta)^{2}$ is minimized while satisfying the constraint when $\theta = 0$.\\
        Therefore, if we restrict $\theta \geq 0$, the MLE of $\theta$ is:
        \begin{equation*}
            \hat{\theta} = \max\{\overline{x}, 0\}.
        \end{equation*}
    \end{eg}
    \begin{rem}
        Remember, when we estimate a parameter, we must use the data we have obtained.
    \end{rem}
    \begin{eg}
        Consider a random sample of size $n$ from $\U[0, \theta]$, where $\theta \in (0, \infty)$ is unknown. The likelihood function is:
        \begin{equation*}
            L(\theta) = \frac{1}{\theta^{n}} \mathbf{1}_{0 \leq x_{(1)} \leq \cdots \leq x_{(n)} \leq \theta},
        \end{equation*}
        where $x_{(i)}$ represents the $i$-th smallest data point for $i = 1, \cdots, n$. Therefore, the MLE is:
        \begin{equation*}
            \hat{\theta} = x_{(n)}.
        \end{equation*}
    \end{eg}
    \begin{rem}
        The MLE may be biased, and it may not exist in $\Theta$, especially when $\Theta$ is an open set.
    \end{rem}
    \newpage

    \begin{rem}
        The MLE defined may not be unique.
    \end{rem}
    \begin{eg}
        Consider a random sample of size $n$ from $\U[\theta - 1, \theta + 1]$, where $\theta$ is unknown. The likelihood function is:
        \begin{equation*}
            L(\theta) = \frac{1}{2^{n}} \mathbf{1}_{\theta - 1 \leq x_{(1)} \leq \cdots \leq x_{(n)} \leq \theta + 1} = \frac{1}{2^{n}} \mathbf{1}_{x_{(n)} - 1 \leq \theta \leq x_{(1)} + 1},
        \end{equation*}
        where $x_{(i)}$ represents the $i$-th smallest data point for $i = 1, \cdots, n$. We find that any estimate in $[x_{(n)} - 1, x_{(1)} + 1]$ maximizes $L(\theta)$. Therefore, there are infinitely many MLEs of $\theta$.
    \end{eg}
    \begin{lem}\named{Invariance Property of MLE}
        If $\hat{\theta}_{i}$ is the MLE of $\theta_{i}$ for $i = 1, \cdots, k$, then $h(\hat{\theta}_{1}, \cdots, \hat{\theta}_{k})$ is the MLE for $h(\theta_{1}, \cdots, \theta_{k})$, where $h$ is a known function.
    \end{lem}
    \begin{thm}
        \label{Chapter 2 (Theorem) Sequence of MLE is asympt. normal}
        A sequence of MLEs $\{\hat{\theta}_{n} \in \mathbb{R}^{k}\}$ is consistent, asymptotically unbiased for $\theta$, asymptotically efficient, and asymptotically normally distributed. More precisely, under regularity assumptions, as $n \to \infty$, we have:
        \begin{equation*}
            \sqrt{n}(\hat{\theta}_{n} - \theta) \to \N_{k}(\mathbf{0}, \mathcal{I}_{X}^{-1}(\theta)),
        \end{equation*}
        where $\mathcal{I}_{X}(\theta)$ is known as the \textbf{Fisher Information matrix} and is a $k \times k$ matrix with the $(i, j)$-th entry defined as:
        \begin{equation*}
            \begin{cases}
                \E\left[\left(\pdv*{\ln{f_{X}(X | \theta)}}{\theta_{i}}\right)\left(\pdv*{\ln{f_{X}(X | \theta)}}{\theta_{j}}\right)\right], &\text{Continuous case},\\
                \E\left[\left(\pdv*{\ln{p_{X}(X | \theta)}}{\theta_{i}}\right)\left(\pdv*{\ln{p_{X}(X | \theta)}}{\theta_{j}}\right)\right], &\text{Discrete case},
            \end{cases}
        \end{equation*}
        for $i = 1, \cdots, k$ and $j = 1, \cdots, k$.
    \end{thm}
    \begin{rem}
        In the theorem, "asymptotically efficient" means that the limiting variance is the smallest possible. This will be further discussed in Chapter 3.
    \end{rem}
    Notice that we have used a special matrix called the "Fisher Information Matrix." What is Fisher Information?
    \begin{defn}
        Given a set of random variables $\{X_{1}, \cdots, X_{n}\}$, the \textbf{Fisher Information}, or \textbf{Fisher Information matrix} if more than one unknown parameter is considered, of the set is defined as:
        \begin{equation*}
            \mathcal{I}_{X_{1}, \cdots, X_{n}}(\theta) = \begin{cases}
                \E\left[\odv*{}{\theta} \ln{f_{X_{1}, \cdots, X_{n}}(X_{1}, \cdots, X_{n} | \theta)}\right]^{2}, &\text{Continuous case},\\
                \E\left[\odv*{}{\theta} \ln{p_{X_{1}, \cdots, X_{n}}(X_{1}, \cdots, X_{n} | \theta)}\right]^{2}, &\text{Discrete case}.
            \end{cases}
        \end{equation*}
    \end{defn}
    \begin{rem}
        Fisher Information is a measure of the amount of information about an unknown parameter $\theta$ that a random variable or data carries. It is very important because it quantifies this amount appropriately.
    \end{rem}
    \begin{eg}
        If $X \sim \N(\mu, \sigma^{2})$, where $\sigma^{2}$ is known but $\mu \in (-\infty, \infty)$ is unknown, then the Fisher Information about $\mu$ contained in $X$ is:
        \begin{equation*}
            \mathcal{I}_{X}(\mu) = \E\left[\odv*{\ln{f_{X}(X | \mu)}}{\mu}\right]^{2} = \E\left[\odv*{}{\mu}\left(-\frac{1}{2\sigma^{2}}(X - \mu)^{2} - \frac{1}{2}\ln(2\pi\sigma^{2})\right)\right]^{2} = \E\left[\frac{1}{\sigma^{2}}(X - \mu)\right]^{2} = \frac{1}{\sigma^{2}}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        If $X \sim \Bern(p)$, where $p \in (0, 1)$ is unknown, then the Fisher Information about $p$ contained in $X$ is:
        \begin{align*}
            \mathcal{I}_{X}(p) &= \E\left[\odv*{\ln{f_{X}(X | p)}}{p}\right]^{2}\\
            &= \E\left[\odv*{}{p}\left(X\ln{p} + (1 - X)\ln(1 - p)\right)\right]^{2}\\
            &= \E\left[\frac{X}{p} - \frac{1 - X}{1 - p}\right]^{2}\\
            &= \E\left[\frac{X - p}{p(1 - p)}\right]^{2}\\
            &= \frac{p(1 - p)}{p^{2}(1 - p)^{2}} = \frac{1}{p(1 - p)}.
        \end{align*}
    \end{eg}
    \newpage

    We will see some properties of the Fisher Information. \textit{For simplicity, we will only discuss continuous random variables.} Notice that we used something called the "regularity assumption"? The following are the regularity conditions that we need:
    \begin{enumerate}
        \item $\odv*{}{\theta} \ln{f_{X_{1}, \cdots, X_{n}}}(x_{1}, \cdots, x_{n} | \theta)$ exists for all $x_{1}, \cdots, x_{n}$ and all $\theta \in \Theta$.
        \item For any statistic $T(x_{1}, \cdots, x_{n})$:
        \begin{align*}
            &\odv*{}{\theta} \int \cdots \int T(x_{1}, \cdots, x_{n}) f_{X_{1}, \cdots, X_{n}}(x_{1}, \cdots, x_{n} | \theta) \, dx_{1} \cdots dx_{n}\\
            =& \int \cdots \int T(x_{1}, \cdots, x_{n}) \odv*{}{\theta} f_{X_{1}, \cdots, X_{n}}(x_{1}, \cdots, x_{n} | \theta) \, dx_{1} \cdots dx_{n}.
        \end{align*}
        \item $0 < \mathcal{I}_{X_{1}, \cdots, X_{n}}(\theta) < \infty$ for all $\theta \in \Theta$.
    \end{enumerate}
    Condition 2 can be satisfied when the support of $X$ does not depend on $\theta$, where the support of $X$ is defined below:
    \begin{defn}
        Suppose $X$ is a random variable with a PMF $p(x)$ or a PDF $f(x)$. The \textbf{support} of $X$ is defined as:
        \begin{equation*}
            \supp(X) = \begin{cases}
                \{x : p_{X}(x) > 0\}, &\text{Discrete case},\\
                \{x : f_{X}(x) > 0\}, &\text{Continuous case}.
            \end{cases}
        \end{equation*}
    \end{defn}
    \begin{lem}
        \label{Chapter 2 (Lemma) Expectation of Fisher Information but with first moment is zero}
        Suppose $X$ is a random variable with PDF $f_{X}$. Under the regularity conditions, we have:
        \begin{equation*}
            \E\left[\odv*{}{\theta} \ln{f_{X}(X | \theta)}\right] = 0.
        \end{equation*}
    \end{lem}
    \begin{proofing}
        \begin{equation*}
            0 = \odv*{}{\theta} \intinfty f_{X}(x | \theta) \, dx = \intinfty \odv*{}{\theta} f_{X}(x | \theta) \, dx = \intinfty \left(\odv*{}{\theta} \ln{f_{X}(x | \theta)}\right) f_{X}(x | \theta) \, dx = \E\left[\odv*{}{\theta} \ln{f_{X}(X | \theta)}\right].
        \end{equation*}
    \end{proofing}
    \begin{rem}
        Using this lemma, we can find that:
        \begin{equation*}
            \mathcal{I}_{X}(\theta) = \Var\left(\odv*{}{\theta} \ln{f_{X}(X | \theta)}\right).
        \end{equation*}
    \end{rem}
    \begin{lem}
        \label{Chapter 2 (Lemma) Relationship between expectation of l'' and I}
        Suppose that $\{X_{1}, \cdots, X_{n}\}$ is a set of random variables. Under the regularity conditions and the assumption that $\odv*[order={2}]{}{\theta} \ln{f_{X_{1}, \cdots, X_{n}}(x_{1}, \cdots, x_{n} | \theta)}$ exists for all $x_{1}, \cdots, x_{n}$ and all $\theta \in \Theta$, we have:
        \begin{equation*}
            \E\left[\odv*{}{\theta} \ln{f_{X}(X | \theta)}\right]^{2} = -\E\left[\odv*[order={2}]{}{\theta} \ln{f_{X}(X | \theta)}\right].
        \end{equation*}
    \end{lem}
    \begin{proofing}
        From the proof of the last lemma:
        \begin{align*}
            0 &= \odv*{}{\theta} \intinfty \left(\odv*{}{\theta} \ln{f_{X}(x | \theta)}\right) f_{X}(x | \theta) \, dx\\
            &= \intinfty \odv*{}{\theta} \left[\left(\odv*{}{\theta} \ln{f_{X}(x | \theta)}\right) f_{X}(x | \theta)\right] \, dx\\
            &= \intinfty \left(\odv*[order=2]{}{\theta} \ln{f_{X}(x | \theta)}\right) f_{X}(x | \theta) \, dx + \intinfty \left(\odv*{}{\theta} \ln{f_{X}(x | \theta)}\right) \odv*{}{\theta} f_{X}(x | \theta) \, dx\\
            &= \intinfty \left(\odv*[order=2]{}{\theta} \ln{f_{X}(x | \theta)}\right) f_{X}(x | \theta) \, dx + \intinfty \left(\odv*{}{\theta} \ln{f_{X}(x | \theta)}\right)^{2} f_{X}(x | \theta) \, dx\\
            &= \E\left[\odv*[order={2}]{}{\theta} \ln{f_{X}(X | \theta)}\right] + \E\left[\odv*{}{\theta} \ln{f_{X}(X | \theta)}\right]^{2},\\
            -\E\left[\odv*[order={2}]{}{\theta} \ln{f_{X}(X | \theta)}\right] &= \E\left[\odv*{}{\theta} \ln{f_{X}(X | \theta)}\right]^{2}.
        \end{align*}
    \end{proofing}
    \newpage

    Assume that we consider two independent random variables $X$ and $Y$. We can find the Fisher Information about $\theta$ contained in $(X, Y)$ by finding the Fisher Information about $\theta$ contained in each of them.
    \begin{lem}
        If $X$ and $Y$ are independent and their PDFs satisfy the regularity conditions, then:
        \begin{equation*}
            \mathcal{I}_{X, Y}(\theta) = \mathcal{I}_{X}(\theta) + \mathcal{I}_{Y}(\theta).
        \end{equation*}
    \end{lem}
    \begin{proofing}
        Since $X$ and $Y$ are independent:
        \begin{align*}
            \mathcal{I}_{X, Y}(\theta) &= \E\left[\odv*{}{\theta} \ln{f_{X, Y}(X, Y | \theta)}\right]^{2}\\
            &= \E\left[\odv*{}{\theta} \ln{f_{X}(X | \theta)} + \odv*{}{\theta} \ln{f_{Y}(Y | \theta)}\right]^{2}\\
            &= \E\left[\odv*{}{\theta} \ln{f_{X}(X | \theta)}\right]^{2} + 2\E\left[\left(\odv*{}{\theta} \ln{f_{X}(X | \theta)}\right)\left(\odv*{}{\theta} \ln{f_{Y}(Y | \theta)}\right)\right] + \E\left[\odv*{}{\theta} \ln{f_{Y}(Y | \theta)}\right]^{2}\\
            \tag{Lemma \ref{Chapter 2 (Lemma) Expectation of Fisher Information but with first moment is zero}}
            &= \E\left[\odv*{}{\theta} \ln{f_{X}(X | \theta)}\right]^{2} + \E\left[\odv*{}{\theta} \ln{f_{Y}(Y | \theta)}\right]^{2}\\
            &= \mathcal{I}_{X}(\theta) + \mathcal{I}_{Y}(\theta).
        \end{align*}
    \end{proofing}
    By applying the same result to a random sample of size $n$, we can obtain the following property.
    \begin{lem}
        Suppose $\{X_{1}, \cdots, X_{n}\}$ is a random sample of size $n$ from a distribution. Then:
        \begin{equation*}
            \mathcal{I}_{X_{1}, \cdots, X_{n}}(\theta) = \sum_{i=1}^{n} \mathcal{I}_{X_{i}}(\theta) = n\mathcal{I}_{X_{1}}(\theta).
        \end{equation*}
    \end{lem}
    \begin{rem}
        For any $i \neq j$, $\mathcal{I}_{X_{i}}(\theta) = \mathcal{I}_{X_{j}}(\theta)$ only means that $X_{i}$ and $X_{j}$ carry the same amount of information about $\theta$. It does not mean they carry identical information.
    \end{rem}
    \begin{eg}
        Consider a set of i.i.d. random variables $\{X_{1}, \cdots, X_{n}\}$ where for all $i = 1, \cdots, n$, $X_{i} \sim \Cauchy(\theta)$ and has a PDF:
        \begin{equation*}
            f_{X_{i}}(x | \theta) = \frac{1}{\pi(1 + (x - \theta)^{2})}.
        \end{equation*}
        We may find that:
        \begin{align*}
            \mathcal{I}_{X_{i}}(\theta) &= \E\left[\odv*{}{\theta} \ln{f_{X_{i}}(X_{i} | \theta)}\right]^{2}\\
            &= \E\left(\frac{2(X_{i} - \theta)}{1 + (X_{i} - \theta)^{2}}\right)^{2}\\
            &= \intinfty \left(\frac{2(x - \theta)}{1 + (x - \theta)^{2}}\right)^{2} \frac{1}{\pi(1 + (x - \theta)^{2})} \, dx\\
            \tag{$u = x - \theta$, $du = dx$}
            &= \frac{4}{\pi} \intinfty \frac{u^{2}}{(1 + u^{2})^{3}} \, du\\
            &= \frac{8}{\pi} \int_{0}^{\infty} \frac{u^{2}}{(1 + u^{2})^{3}} \, du\\
            \tag{$y = \frac{1}{1 + u^{2}}$, $dy = -\frac{2u}{(1 + u^{2})^{2}} \, du$}
            &= \frac{4}{\pi} \int_{0}^{1} \sqrt{y}\sqrt{1 - y} \, dy\\
            \tag{Beta integral}
            &= \frac{4}{\pi} \int_{0}^{1} (y)^{\frac{3}{2} - 1}(1 - y)^{\frac{3}{2} - 1} \, dy\\
            \tag{$\frac{\Gamma(z_{1})\Gamma(z_{2})}{\Gamma(z_{1} + z_{2})} = \int_{0}^{1} t^{z_{1} - 1}(1 - t)^{z_{2} - 1} \, dt$}
            &= \frac{4\Gamma(\frac{3}{2})\Gamma(\frac{3}{2})}{\pi\Gamma(3)} = \frac{4(0.5\sqrt{\pi})^{2}}{\pi(2!)} = \frac{1}{2}.
        \end{align*}
        Therefore, $\mathcal{I}_{X_{1}, \cdots, X_{n}}(\theta) = n\mathcal{I}_{X_{1}}(\theta) = \frac{n}{2}$.
    \end{eg}
    \newpage

    Note that a statistic or an estimator can be considered as a function for data condensation because it condenses a random sample into a lower-dimensional quantity.
    \begin{lem}
        Suppose that $\mathbf{X}$ is a random vector. Under the regularity conditions, for any statistic $T(\mathbf{X})$ for $\theta$, we have:
        \begin{equation*}
            \mathcal{I}_{T(\mathbf{X})}(\theta) \leq \mathcal{I}_{\mathbf{X}}(\theta).
        \end{equation*}
    \end{lem}
    \begin{rem}
        The Fisher Information of $T(\mathbf{X})$ is defined as:
        \begin{equation*}
            \mathcal{I}_{T(\mathbf{X})}(\theta) = \E\left[\odv*{}{\theta} \ln{f_{T(\mathbf{X})}(T(\mathbf{X}) | \theta)}\right]^{2}.
        \end{equation*}
    \end{rem}
    We may prove Theorem \ref{Chapter 2 (Theorem) Sequence of MLE is asympt. normal} in the one-parameter case:
    \begin{thm}
        Consider a random sample $\{X_{1}, \cdots, X_{n}\}$ of size $n$ from a parametric distribution with a PDF $f_{X}$. Then, under the regularity and some other conditions, for $\theta \in \mathbb{R}$, a sequence of MLEs $\{\hat{\theta}_{n} \in \mathbb{R}\}$ satisfies:
        \begin{equation*}
            \sqrt{n}(\hat{\theta}_{n} - \theta) \to \N\left(0, \frac{1}{\mathcal{I}_{X}(\theta)}\right).
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Since the MLE $\hat{\theta}_{n}$ is the solution to $l'(\theta) = 0$, we can apply a Taylor expansion of $l'(\hat{\theta}_{n})$ at $\theta$ to find:
        \begin{align*}
            0 &= l'(\theta) + l''(\theta)(\hat{\theta}_{n} - \theta) + o(\hat{\theta}_{n} - \theta),\\
            \sqrt{n}(\hat{\theta}_{n} - \theta) &= \frac{\frac{1}{\sqrt{n}}l'(\theta)}{-\frac{1}{n}l''(\theta)} - o(\hat{\theta}_{n} - \theta).
        \end{align*}
        First, consider the numerator. Note that $\odv*{}{\theta} \ln{f_{X}(X_{1} | \theta)}, \cdots, \odv*{}{\theta} \ln{f_{X}(X_{n} | \theta)}$ are i.i.d. By the CLT, we have:
        \begin{equation*}
            \sqrt{n}\left(\frac{1}{n} \sum_{i=1}^{n} \odv*{}{\theta} \ln{f_{X}(X_{i} | \theta)} - \E\left[\odv*{}{\theta} \ln{f_{X}(X_{1} | \theta)}\right]\right) \to \N\left(0, \Var\left[\odv*{}{\theta} \ln{f_{X}(X_{1} | \theta)}\right]\right).
        \end{equation*}
        By Lemma \ref{Chapter 2 (Lemma) Expectation of Fisher Information but with first moment is zero}, we have:
        \begin{equation*}
            \frac{1}{\sqrt{n}}l'(\theta) = \frac{1}{\sqrt{n}} \sum_{i=1}^{n} \odv*{}{\theta} \ln{f_{X}(X_{i} | \theta)} \to \N(0, \mathcal{I}_{X}(\theta)).
        \end{equation*}
        Now consider the denominator. By the WLLN and Lemma \ref{Chapter 2 (Lemma) Relationship between expectation of l'' and I}, since $\odv*[order={2}]{}{\theta} \ln{f_{X}(X_{1} | \theta)}, \cdots, \odv*[order={2}]{}{\theta} \ln{f_{X}(X_{n} | \theta)}$ are i.i.d.:
        \begin{equation*}
            -\frac{1}{n}l''(\theta) = -\frac{1}{n} \sum_{i=1}^{n} \odv*[order={2}]{}{\theta} \ln{f_{X}(X_{i} | \theta)} \to -\E\left[\odv*[order={2}]{}{\theta} \ln{f_{X}(X | \theta)}\right] = \E\left[\odv*{}{\theta} \ln{f_{X}(X | \theta)}\right]^{2} = \mathcal{I}_{X}(\theta).
        \end{equation*}
        Consequently, we have:
        \begin{equation*}
            \sqrt{n}(\hat{\theta}_{n} - \theta) = \frac{\frac{1}{\sqrt{n}}l'(\theta)}{-\frac{1}{n}l''(\theta)} - o((\hat{\theta}_{n} - \theta)) \to \N\left(0, \frac{1}{\mathcal{I}_{X}(\theta)}\right).
        \end{equation*}
    \end{proofing}
    \begin{rem}
        \label{Chapter 2 (Remark) Cases when Fisher Information cannot be determined easily}
        Sometimes, $\mathcal{I}_{X}(\theta)$ cannot be determined easily. We replace it with the observed Fisher Information defined as $-\frac{1}{n}l''(\hat{\theta}_{n})$. Since $\hat{\theta}_{n}$ is consistent for $\theta$, $-\frac{1}{n}l''(\hat{\theta}_{n})$ is also consistent for $\mathcal{I}_{X}(\theta)$ by the Continuous Mapping Theorem. Therefore:
        \begin{equation*}
            \sqrt{-l''(\hat{\theta}_{n})}(\hat{\theta}_{n} - \theta) = \sqrt{n}\sqrt{-\frac{1}{n}l''(\hat{\theta}_{n})}(\hat{\theta}_{n} - \theta) \to \N(0, 1).
        \end{equation*}
    \end{rem}
    \newpage
    
    \begin{eg}\named{Principle of Numerical Solution to Likelihood Equations}
        Consider a random sample of size $n$ from $X \sim \Cauchy(\theta)$, similar to the previous example. We want to find the MLE of $\theta$. We have:
        \begin{equation*}
            l(\theta) = -n\ln{\pi} - \sum_{i=1}^{n} \ln(1 + (x_{i} - \theta)^{2}).
        \end{equation*}
        We want to find the solution of $l'(\theta) = 0$, which is the MLE. Setting:
        \begin{equation*}
            \sum_{i=1}^{n} \frac{2(x_{i} - \theta)}{1 + (x_{i} - \theta)^{2}} = 0.
        \end{equation*}
        However, this is extremely hard to solve explicitly. We need a numerical method to solve this.
    \end{eg}
    \begin{eg}\named{Newton-Raphson Algorithm}
        By Taylor expansion, we can write:
        \begin{equation*}
            0 = \frac{1}{n}l'(\hat{\theta}) \approx \frac{1}{n}l'(\theta) + \frac{1}{n}(\hat{\theta} - \theta)l''(\theta).
        \end{equation*}
        Rearranging gives:
        \begin{equation*}
            \hat{\theta} \approx \theta - \frac{l'(\theta)}{l''(\theta)}.
        \end{equation*}
        We may initially guess a number, say $\theta_{0}$. By iteratively applying the procedure for $j = 0, 1, \cdots$:
        \begin{equation*}
            \theta_{j+1} = \theta_{j} - \frac{l'(\theta_{j})}{l''(\theta_{j})},
        \end{equation*}
        and stopping at a certain criterion, say $\abs{\theta_{j+1} - \theta_{j}} < K$ for some chosen constant $K$ (e.g., $K = 10^{-5}$), we can approximate the MLE of $\theta$ using this algorithm.
    \end{eg}
    \begin{eg}
        Consider a random sample of size $n$ from $X \sim \Gam(\alpha, \beta)$, where $\beta = 3423$ and $\alpha$ is unknown. The PDF is defined as:
        \begin{equation*}
            f_{X}(x | \alpha) = \begin{cases}
                \frac{3423^{\alpha}}{\Gamma(\alpha)} x^{\alpha - 1} e^{-3423x}, &x > 0,\\
                0, &\text{Otherwise}.
            \end{cases}
        \end{equation*}
        We can find the log-likelihood:
        \begin{equation*}
            l(\alpha) = \sum_{i=1}^{n} \ln{f_{X}(x_{i} | \alpha)} = n\alpha\ln{3423} - n\ln(\Gamma(\alpha)) + (\alpha - 1)\sum_{i=1}^{n} \ln{x_{i}} - 3423\sum_{i=1}^{n} x_{i}.
        \end{equation*}
        To find the MLE, we solve the equation:
        \begin{equation*}
            0 = \odv*{}{\alpha}l(\alpha) = n\ln{3423} - n\odv*{}{\alpha}\ln(\Gamma(\alpha)) + \sum_{i=1}^{n} \ln{x_{i}}.
        \end{equation*}
        However, since $\odv*{}{\alpha}\ln(\Gamma(\alpha))$ is difficult to compute explicitly, we use numerical methods to approximate the MLE.
    \end{eg}
    
\chapter{Uniformly Minimum Variance Unbiased Estimator}
    We usually want to find the best estimator that can approximate some parameters. However, there are many estimators we can provide based on the information given. In this chapter, we will try to find the best among them.
\section{Introduction to UMVUE}
    Consider a class $M$ defined as all the estimators for $\theta$. If there exists an estimator $\hat{\theta}^{*} \in M$ that is uniformly better than any other estimator in $M$, then we say $\hat{\theta}^{*}$ is the best estimator of $\theta$ in $M$. However, in general, this estimator does not exist, partly because there are too many estimators to consider, and some of them are poor or not reasonable. To avoid this problem, we only consider a particular class of estimators, which is the mean-unbiased estimators.
    \begin{rem}
        In this context, $\hat{\theta}^{*}$ being "uniformly better" means that $\Var(\hat{\theta}^{*}) < \Var(\hat{\theta})$ for any other $\hat{\theta} \in M$.
    \end{rem}
    Recall the definition of a mean-unbiased estimator. If an estimator $\hat{\theta}$ satisfies:
    \begin{equation*}
        \E(\hat{\theta}) = \theta,
    \end{equation*}
    for all $\theta \in \Theta$, then it is mean-unbiased or simply unbiased for $\theta$. Otherwise, it is biased.\\
    From past experiences, we may note the following remarks.
    \begin{rem}
        Unbiasedness means that by repeated sampling, $\hat{\theta} = \theta$ on average. The underestimation and overestimation will balance out in the long run.
    \end{rem}
    \begin{rem}
        Sample variance $S_{n-1}^{2}$ is unbiased for $\sigma^{2}$, but $S_{n}^{2}$ is not. This is why we use $S_{n-1}^{2}$ to estimate $\sigma^{2}$ instead of $S_{n}^{2}$.
    \end{rem}
    \begin{rem}
        MME and MLE are usually biased, but they are asymptotically unbiased.
    \end{rem}
    \begin{rem}
        It is possible to have infinitely many different unbiased estimators for $\theta$.
    \end{rem}
    \begin{eg}
        Consider $\{X_{1}, \cdots, X_{n}\}$ as a random sample of size $n$ from a distribution with a finite mean $\theta$.\\
        Any estimator $\hat{\theta}$ in the form of:
        \begin{equation*}
            \hat{\theta} = \frac{\sum_{i=1}^{n} a_{i} X_{i}}{\sum_{i=1}^{n} a_{i}},
        \end{equation*}
        where $a_{i} \in \mathbb{R}$ for $i = 1, \cdots, n$ and $\sum_{i=1}^{n} a_{i} \neq 0$, is unbiased for $\theta$.
    \end{eg}
    \begin{rem}
        It is possible to have no unbiased estimators for $\theta$.
    \end{rem}
    \begin{eg}
        Consider a random sample of size $n$ from a random variable $X \sim \Bin(1, \theta)$ with $g(\theta) = \frac{\theta}{1 - \theta}$ as the parameter being estimated. There does not exist an unbiased estimator for $g(\theta)$.
    \end{eg}
    \begin{rem}
        Unbiasedness does not have an invariance property. If $\hat{\theta}$ is unbiased for $\theta$, it does not mean $h(\hat{\theta})$ is unbiased for $h(\theta)$.
    \end{rem}
    \begin{eg}
        We have $\overline{X}$ as unbiased for $\mu$, but $(\overline{X})^{2}$ is not unbiased for $\mu^{2}$ when $\sigma > 0$.
    \end{eg}
    The best unbiased estimator is the unbiased estimator with the smallest variance.
    \begin{defn}
        The \textbf{Uniformly Minimum Variance Unbiased Estimator (UMVUE)} $\hat{\theta}^{*}$ for $\theta$ is an unbiased estimator such that for all other unbiased estimators $\hat{\theta}$ for $\theta$:
        \begin{equation*}
            \Var(\hat{\theta}^{*}) \leq \Var(\hat{\theta}),
        \end{equation*}
        for all $\theta \in \Theta$.
    \end{defn}
    \begin{lem}\named{Uniqueness of UMVUE}
        Assume that the UMVUE for $\theta$ exists. Then, it is unique.
    \end{lem}
    \begin{proofing}
        Assume that there are two distinct UMVUEs, $\hat{\theta}^{*}$ and $\hat{\theta}^{**}$, for $\theta$. We may find that for all $\theta \in \Theta$ and any unbiased estimator $\hat{\theta}$ of $\theta$:
        \begin{equation*}
            \Var(\hat{\theta}^{*}) = \Var(\hat{\theta}^{**}) \leq \Var(\hat{\theta}).
        \end{equation*}
        Let $\hat{\theta}' = \frac{1}{2}(\hat{\theta}^{*} + \hat{\theta}^{**})$. We can easily find that $\hat{\theta}'$ is unbiased for $\theta$. We have:
        \begin{align*}
            \Var(\hat{\theta}') &= \frac{1}{4}\Var(\hat{\theta}^{*}) + \frac{1}{4}\Var(\hat{\theta}^{**}) + \frac{1}{2}\cov(\hat{\theta}^{*}, \hat{\theta}^{**})\\
            &\leq \frac{1}{2}\Var(\hat{\theta}^{*}) + \frac{1}{2}\sqrt{\Var(\hat{\theta}^{*})\Var(\hat{\theta}^{**})}\\
            &\leq \Var(\hat{\theta}^{*}) \leq \Var(\hat{\theta}').
        \end{align*}
        Thus, $\Var(\hat{\theta}') = \Var(\hat{\theta}^{*})$ and $\cov(\hat{\theta}^{*}, \hat{\theta}^{**}) = \sqrt{\Var(\hat{\theta}^{*})\Var(\hat{\theta}^{**})}$.\\
        Recall the Pearson correlation coefficient. We find that:
        \begin{equation*}
            \rho = \frac{\cov(\hat{\theta}^{*}, \hat{\theta}^{**})}{\sqrt{\Var(\hat{\theta}^{*})\Var(\hat{\theta}^{**})}} = 1.
        \end{equation*}
        This means $\hat{\theta}^{*}$ and $\hat{\theta}^{**}$ have a perfectly linear positive relationship. We say $\hat{\theta}^{*} = a\hat{\theta}^{**} + b$ where $a > 0$ and $b \in \mathbb{R}$.\\
        Solving the equations:
        \begin{equation*}
            \begin{cases}
                \Var(\hat{\theta}^{**}) = \Var(\hat{\theta}^{*}) = a^{2}\Var(\hat{\theta}^{**}),\\
                \E(\hat{\theta}^{**}) = \theta = \E(\hat{\theta}^{*}) = a\E(\hat{\theta}^{**}) + b,
            \end{cases}
        \end{equation*}
        we find that $a = 1$ and $b = 0$. Therefore, $\hat{\theta}^{*} = \hat{\theta}^{**}$, and the UMVUE is unique.
    \end{proofing}
    
\section{Sufficient Statistic}
    It is not easy to find the UMVUE for a parameter being estimated. However, we have the Rao-Blackwell Theorem (we will discuss it later), which tells us that the UMVUE must be a function of a sufficient statistic. Let us discuss sufficient statistics.\\
    Note that a statistic or an estimator can be considered as a function for data condensation because it condenses a random sample into a lower-dimensional quantity. However, in the process, we may lose some information about the parameter $\theta$.\\
    Recall this lemma: under regularity conditions, for any statistic $T = T(\mathbf{X})$ for $\theta$, we have:
    \begin{equation*}
        \mathcal{I}_{T}(\theta) \leq \mathcal{I}_{\mathbf{X}}(\theta).
    \end{equation*}
    Most statistics lose some information about $\theta$, but there exist some statistics that can substantially reduce the dimension without losing any information. We call these sufficient statistics.
    \begin{defn}
        Under regularity conditions, the \textbf{sufficient statistic} for $\theta$, denoted by $S = S(\mathbf{X})$, is a statistic that satisfies:
        \begin{equation*}
            \mathcal{I}_{S}(\theta) = \mathcal{I}_{\mathbf{X}}(\theta).
        \end{equation*}
    \end{defn}
    \begin{rem}
        If the conditional distribution of the sample given a statistic $T$ depends on $\theta$, then there is still some information about $\theta$ contained in the sample that $T$ does not carry. Therefore, $T$ is not sufficient.
    \end{rem}
    \begin{eg}
        Let $\{X_{1}, X_{2}\}$ be a random sample of size $2$ from $X \sim \Bin(m, \theta)$. We show that $T = T(X_{1}, X_{2}) = X_{1} + X_{2}$ is sufficient.
        \begin{align*}
            p_{X_{1}, X_{2} | T}(x_{1}, x_{2} | t) &= \frac{p_{X_{1}, X_{2}, T}(x_{1}, x_{2}, t)}{p_{T}(t)}\\
            &= \frac{p_{X_{1}, X_{2}, T}(x_{1}, t - x_{1}, t)}{p_{T}(t)}\\
            &= \frac{p_{X_{1}}(x_{1})p_{X_{2}}(t - x_{1})}{p_{T}(t)}\\
            &= \frac{\binom{m}{x_{1}}\binom{m}{t - x_{1}}\theta^{t}(1 - \theta)^{n - t}}{\binom{2m}{t}\theta^{t}(1 - \theta)^{n - t}} = \frac{\binom{m}{x_{1}}\binom{m}{t - x_{1}}}{\binom{2m}{t}}.
        \end{align*}
        Therefore, since the conditional distribution of the sample given a statistic $T$ does not depend on $\theta$, we find that $T$ is a sufficient statistic.
    \end{eg}
    We may rewrite the definition of a sufficient statistic as follows:
    \begin{defn}
        Let $\mathbf{X} = \{X_{1}, \cdots, X_{n}\}$ be a random vector of a random sample of size $n$ from a PDF $f(x | \theta)$ or PMF $p(x | \theta)$, where $\theta \in \Theta \subset \mathbb{R}^{k}$ for integer $k > 1$. A set of statistics $\{S_{1}, S_{2}, \cdots, S_{r}\}$, where $r \geq k$ and $S_{i} = S_{i}(\mathbf{X})$ for $i = 1, \cdots, r$, is said to be \textbf{jointly sufficient} if and only if the conditional distribution:
        \begin{equation*}
            \begin{cases}
                p_{\mathbf{X} | S_{1}, \cdots, S_{r}}(\mathbf{x} | S_{1} = s_{1}, \cdots, S_{r} = s_{r}, \theta), &\text{Discrete case},\\
                f_{\mathbf{X} | S_{1}, \cdots, S_{r}}(\mathbf{x} | S_{1} = s_{1}, \cdots, S_{r} = s_{r}, \theta), &\text{Continuous case},
            \end{cases}
        \end{equation*}
        does not depend on $\theta$, for all values $s_{1}$ of $S_{1}, \cdots, s_{r}$ of $S_{r}$.
    \end{defn}
    or in the one-parameter case:
    \begin{defn}
        Let $\mathbf{X} = \{X_{1}, \cdots, X_{n}\}$ be a random sample of size $n$ from a PDF $f(x | \theta)$ or PMF $p(x | \theta)$, where $\theta \in \Theta \subset \mathbb{R}$. A statistic $S = S(\mathbf{X})$ is said to be \textbf{sufficient} if and only if the conditional distribution:
        \begin{equation*}
            \begin{cases}
                p_{\mathbf{X} | S}(\mathbf{x} | S = s, \theta), &\text{Discrete case},\\
                f_{\mathbf{X} | S}(\mathbf{x} | S = s, \theta), &\text{Continuous case},
            \end{cases}
        \end{equation*}
        does not depend on $\theta$, for all values $s$ of $S$.
    \end{defn}
    \begin{thm}\named{Fisher-Neyman Factorization Theorem}
        Let $\mathbf{X} = \{X_{1}, \cdots, X_{n}\}$ be a random sample of size $n$ from a PDF $f(x | \theta)$ or a PMF $p(x | \theta)$, where $\theta \in \Theta \subset \mathbb{R}^{k}$. A set of statistics $\{S_{1}, \cdots, S_{r}\}$, where $r \geq k$ and $S_{i} = S_{i}(\mathbf{X})$ for $i = 1, \cdots, r$, is jointly sufficient if and only if:
        \begin{equation*}
            \begin{cases}
                p_{\mathbf{X}}(\mathbf{x} | \theta) = g(S_{1}(\mathbf{x}), \cdots, S_{r}(\mathbf{x}) | \theta)h(\mathbf{x}), &\text{Discrete case},\\
                f_{\mathbf{X}}(\mathbf{x} | \theta) = g(S_{1}(\mathbf{x}), \cdots, S_{r}(\mathbf{x}) | \theta)h(\mathbf{x}), &\text{Continuous case},
            \end{cases}
        \end{equation*}
        where $g$ is a non-negative function of $x_{1}, \cdots, x_{n}$ only through the statistics $S_{1}, \cdots, S_{r}$ and depends on $\theta$, and $h$ is a non-negative function of $x_{1}, \cdots, x_{n}$ not depending on $\theta$.
    \end{thm}
    \begin{proofing}
        We shall prove it in the discrete case with only one statistic. The proof in the continuous case or for more than one statistic is out of our scope.\\
        Note that if $\mathbf{X} \in B$ for a set $B$, then $S(\mathbf{X}) \in S(B)$. Therefore, for $i = 1, \cdots, n$:
        \begin{equation*}
            \{\mathbf{X} \in B\} \cap \{S(\mathbf{X}) \in S(B)\} = \{\mathbf{X} \in B\}.
        \end{equation*}
        \begin{align*}
            \prob(\mathbf{X} \in B | \theta) &= \prob(\mathbf{X} \in B, S(\mathbf{X}) \in S(B) | \theta)\\
            \tag{$\prob(A \cap B | D) = \prob(A | B \cap D)\prob(B | D)$}
            &= \prob(\mathbf{X} \in B | S(\mathbf{X}) \in S(B), \theta)\prob(S(\mathbf{X}) \in S(B) | \theta).
        \end{align*}
        \newpage

        Suppose that $S$ is sufficient. Then by definition:
        \begin{equation*}
            \prob(\mathbf{X} \in B | S(\mathbf{X}) \in S(B), \theta) = \prob(X_{i} \in B | S(X_{i}) \in S(B)).
        \end{equation*}
        Substituting $B = \{\mathbf{x}\}$, we get:
        \begin{equation*}
            p_{\mathbf{X}}(\mathbf{x} | \theta) = p_{\mathbf{X} | S}(\mathbf{x} | S(\mathbf{x}))p_{S}(S(\mathbf{x}) | \theta).
        \end{equation*}
        We find that $g(S(\mathbf{x}) | \theta) = p_{S}(S(\mathbf{x}) | \theta)$ and $h(\mathbf{x}) = p_{\mathbf{X} | S}(\mathbf{x} | S(\mathbf{x}))$. Suppose that $p_{X}(\mathbf{x} | \theta) = g(T(\mathbf{x}) | \theta)h(\mathbf{x})$. Then:
        \begin{equation*}
            p_{\mathbf{X} | T}(\mathbf{x} | t) = \frac{p_{\mathbf{X}, T}(\mathbf{x}, t | \theta)}{p_{T}(t | \theta)} = \begin{cases}
                0, &t \neq T(\mathbf{x}),\\
                \frac{p_{\mathbf{X}}(\mathbf{x} | \theta)}{p_{T}(t | \theta)}, &t = T(\mathbf{x}).
            \end{cases}
        \end{equation*}
        Considering only the case where $t = T(\mathbf{x})$, we have:
        \begin{equation*}
            p_{\mathbf{X} | T}(\mathbf{x} | t) = \frac{p_{\mathbf{X}}(\mathbf{x} | \theta)}{\sum_{\mathbf{x} : T(\mathbf{x}) = t}p_{\mathbf{x}}(\mathbf{x} | \theta)} = \frac{g(t | \theta)h(\mathbf{x})}{\sum_{\mathbf{x} : T(\mathbf{x}) = t}g(t | \theta)h(\mathbf{x})} = \frac{h(\mathbf{x})}{\sum_{\mathbf{x} : T(\mathbf{x}) = t}h(\mathbf{x})}.
        \end{equation*}
        We find that $p_{\mathbf{X} | T}(\mathbf{x} | t)$ does not depend on $\theta$. Therefore, by definition, $T$ is sufficient.
    \end{proofing}
    \begin{eg}
        \label{Chapter 1 (Example) Sufficient statistics of Bern(theta)}
        Let $\{X_{1}, \cdots, X_{n}\}$ be a random sample from $\Bern(\theta)$, where $\theta \in [0, 1]$ is unknown. The joint PMF of the random sample is:
        \begin{align*}
            p_{X_{1}, \cdots, X_{n}}(x_{1}, \cdots, x_{n} | \theta) &= \prod_{i=1}^{n} \theta^{x_{i}}(1 - \theta)^{1 - x_{i}}\\
            &= \theta^{\sum_{i=1}^{n} x_{i}}(1 - \theta)^{n - \sum_{i=1}^{n} x_{i}} \times 1\\
            &= g\left(\left.\sum_{i=1}^{n} x_{i}\right| \theta\right) \times h(x_{1}, \cdots, x_{n}).
        \end{align*}
        Therefore, $S = \sum_{i=1}^{n} X_{i}$ is a sufficient statistic.
    \end{eg}
    \begin{eg}
        Let $\{X_{1}, \cdots, X_{n}\}$ be a random sample from $\N(\mu, \sigma^{2})$, where $\mu$ and $\sigma^{2} > 0$ are unknown. The joint PDF of the random sample is:
        \begin{align*}
            f_{X_{1}, \cdots, X_{n}}(x_{1}, \cdots, x_{n} | \mu, \sigma^{2}) &= \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^{2}}} \exp\left(-\frac{(x_{i} - \mu)^{2}}{2\sigma^{2}}\right)\\
            &= \frac{1}{(2\pi\sigma^{2})^{\frac{n}{2}}} \exp\left(-\frac{1}{2\sigma^{2}} \sum_{i=1}^{n} (x_{i} - \mu)^{2}\right)\\
            &= \sigma^{-n} \exp\left[-\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n} x_{i}^{2} - 2\mu \sum_{i=1}^{n} x_{i} + n\mu^{2}\right)\right] \times \frac{1}{(2\pi)^{\frac{n}{2}}}\\
            &= g\left(\left.\sum_{i=1}^{n} x_{i}, \sum_{i=1}^{n} x_{i}^{2}\right| \mu, \sigma^{2}\right) \times h(x_{1}, \cdots, x_{n}).
        \end{align*}
        Therefore, $S_{1} = \sum_{i=1}^{n} X_{i}$ and $S_{2} = \sum_{i=1}^{n} X_{i}^{2}$ are jointly sufficient.
    \end{eg}
    \begin{eg}
        \label{Chapter 3 (Example) Sufficient statistic for U[0,theta]}
        Let $\{X_{1}, \cdots, X_{n}\}$ be a random sample of size $n$ from $\U[0, \theta]$, where $\theta > 0$ is unknown. The joint PDF of the random sample is:
        \begin{align*}
            f_{X_{1}, \cdots, X_{n}}(x_{1}, \cdots, x_{n} | \theta) &= \frac{1}{\theta^{n}} \prod_{i=1}^{n} \mathbf{1}_{0 \leq x_{i} \leq \theta}\\
            \tag{$x_{(i)}$ is the $i$-th smallest sample}
            &= \frac{1}{\theta^{n}} \mathbf{1}_{0 \leq x_{(1)} < x_{(n)} \leq \theta}\\
            &= \frac{1}{\theta^{n}} \mathbf{1}_{x_{(n)} \leq \theta} \times \mathbf{1}_{x_{(1)} \geq 0}\\
            &= g(x_{(n)} | \theta) \times h(x_{1}, \cdots, x_{n}).
        \end{align*}
        Therefore, $S = X_{(n)} = \max\{X_{1}, \cdots, X_{n}\}$ is sufficient.
    \end{eg}
    \begin{eg}
        Let $\{X_{1}, \cdots, X_{n}\}$ be a random sample of size $n$ from $\U[\theta - \frac{1}{2}, \theta + \frac{1}{2}]$, where $\theta$ is unknown. The joint PDF of the random sample is:
        \begin{align*}
            f_{X_{1}, \cdots, X_{n}}(x_{1}, \cdots, x_{n} | \theta) &= \prod_{i=1}^{n} \mathbf{1}_{\theta - \frac{1}{2} \leq x_{i} \leq \theta + \frac{1}{2}}\\
            \tag{$x_{(i)}$ is the $i$-th smallest sample}
            &= \mathbf{1}_{\theta - \frac{1}{2} \leq x_{(1)} < x_{(n)} \leq \theta + \frac{1}{2}}\\
            &= \mathbf{1}_{x_{(n)} - \frac{1}{2} \leq \theta \leq x_{(1)} + \frac{1}{2}} \times 1\\
            &= g(x_{(1)}, x_{(n)} | \theta) \times h(x_{1}, \cdots, x_{n}).
        \end{align*}
        Therefore, $S_{1} = X_{(1)} = \min\{X_{1}, \cdots, X_{n}\}$ and $S_{2} = X_{(n)} = \max\{X_{1}, \cdots, X_{n}\}$ are jointly sufficient.
    \end{eg}
    \begin{eg}
        Let $\{X_{1}, \cdots, X_{n}\}$ be a random sample of size $n$ from $\U[\theta_{1}, \theta_{2}]$, where $\theta_{1}, \theta_{2}$ are unknown with $\theta_{1} < \theta_{2}$ and $\theta_{1}$ is not a function of $\theta_{2}$. The joint PDF of the random sample is:
        \begin{align*}
            f_{X_{1}, \cdots, X_{n}}(x_{1}, \cdots, x_{n} | \theta_{1}, \theta_{2}) &= \frac{1}{(\theta_{2} - \theta_{1})^{n}} \prod_{i=1}^{n} \mathbf{1}_{\theta_{1} \leq x_{i} \leq \theta_{2}}\\
            \tag{$x_{(i)}$ is the $i$-th smallest sample}
            &= \frac{1}{(\theta_{2} - \theta_{1})^{n}} \mathbf{1}_{\theta_{1} \leq x_{(1)} < x_{(n)} \leq \theta_{2}} \times 1\\
            &= g(x_{(1)}, x_{(n)} | \theta_{1}, \theta_{2}) \times h(x_{1}, \cdots, x_{n}).
        \end{align*}
        Therefore, $S_{1} = X_{(1)} = \min\{X_{1}, \cdots, X_{n}\}$ and $S_{2} = X_{(n)} = \max\{X_{1}, \cdots, X_{n}\}$ are jointly sufficient.
    \end{eg}
    \begin{rem}
        Sufficient statistics may not be unique because we may have more than one factorization.
    \end{rem}
    \begin{eg}
        \label{Chapter 3 (Example) Sufficient statistic for normal distribution with unit variance}
        Let $\{X_{1}, \cdots, X_{n}\}$ be a random sample of size $n$ from $\N(\mu, 1)$, where $\mu$ is unknown. The joint PDF of the random sample is:
        \begin{equation*}
            f_{X_{1}, \cdots, X_{n}}(x_{1}, \cdots, x_{n} | \mu) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2}(x_{i} - \mu)^{2}\right) = \frac{1}{(2\pi)^{\frac{n}{2}}} \exp\left(-\frac{1}{2} \sum_{i=1}^{n} (x_{i} - \mu)^{2}\right).
        \end{equation*}
        One way to factorize is:
        \begin{align*}
            \frac{1}{(2\pi)^{\frac{n}{2}}} \exp\left(-\frac{1}{2} \sum_{i=1}^{n} (x_{i} - \mu)^{2}\right) &= \frac{1}{(2\pi)^{\frac{n}{2}}} \exp\left[-\frac{1}{2}\left(\sum_{i=1}^{n} x_{i}^{2} - 2\mu \sum_{i=1}^{n} x_{i} + n\mu^{2}\right)\right]\\
            &= \frac{1}{(2\pi)^{\frac{n}{2}}} \exp\left(\mu \sum_{i=1}^{n} x_{i} - \frac{n}{2}\mu^{2}\right) \times \exp\left(-\frac{1}{2} \sum_{i=1}^{n} x_{i}^{2}\right)\\
            &= g\left(\left.\sum_{i=1}^{n} x_{i}\right| \mu\right) \times h(x_{1}, \cdots, x_{n}).
        \end{align*}
        Therefore, we find that $S_{1} = \sum_{i=1}^{n} X_{i}$ is sufficient.\\
        Another way to factorize is:
        \begin{align*}
            \frac{1}{(2\pi)^{\frac{n}{2}}} \exp\left(-\frac{1}{2} \sum_{i=1}^{n} (x_{i} - \mu)^{2}\right) &= \frac{1}{(2\pi)^{\frac{n}{2}}} \exp\left[-\frac{1}{2}\left(\sum_{i=1}^{n} (x_{i} - \overline{x})^{2} + n(\overline{x} - \mu)^{2}\right)\right]\\
            &= \frac{1}{(2\pi)^{\frac{n}{2}}} \exp\left(-\frac{n}{2}(\overline{x} - \mu)^{2}\right) \times \exp\left(-\frac{1}{2} \sum_{i=1}^{n} (x_{i} - \overline{x})^{2}\right)\\
            &= g(\overline{x} | \mu) \times h(x_{1}, \cdots, x_{n}).
        \end{align*}
        Therefore, we find that $S_{2} = \overline{X}$ is sufficient.
    \end{eg}
    \newpage
    
    Based on the above example, we may notice that $\overline{X}$ and $\sum_{i=1}^{n} X_{i}$ are functions of each other. Is any transformation of $S$ also sufficient? Yes, if it is one-to-one!
    \begin{lem}\named{One-to-one sufficiency}
        \label{Chapter 3 (Lemma) One-to-one sufficiency}
        Let $\{X_{1}, \cdots, X_{n}\}$ be a random sample of size $n$. If a set of statistics $\{S_{1}, \cdots, S_{r}\}$, where $r \geq k$ and $S_{i} = S_{i}(X_{1}, \cdots, X_{n})$ for $i = 1, \cdots, r$, is jointly sufficient, then any set of one-to-one functions $\{h_{1}, \cdots, h_{m}\}$ for some $m$, where $m \geq r$ and $h_{i} = h_{i}(S_{1}, \cdots, S_{r})$ for $i = 1, \cdots, m$, is also jointly sufficient.
    \end{lem}
    \begin{eg}
        Let $\{X_{1}, \cdots, X_{n}\}$ be a random sample of size $n$ from a PDF $f(x | \theta)$ or a PMF $p(x | \theta)$, where $\theta \in \Theta \subset \mathbb{R}^{k}$. Assume that $\sum_{i=1}^{n} X_{i}$ and $\sum_{i=1}^{n} X_{i}^{2}$ are jointly sufficient. We may find that:
        \begin{align*}
            \overline{X} &= \frac{1}{n} \sum_{i=1}^{n} X_{i}, & \sum_{i=1}^{n} (X_{i} - \overline{X})^{2} &= \sum_{i=1}^{n} X_{i}^{2} - \frac{1}{n}\left(\sum_{i=1}^{n} X_{i}\right)^{2}.
        \end{align*}
        Both are one-to-one functions of $\sum_{i=1}^{n} X_{i}$ and $\sum_{i=1}^{n} X_{i}^{2}$.\\
        Therefore, by Lemma \ref{Chapter 3 (Lemma) One-to-one sufficiency}, $\overline{X}$ and $\sum_{i=1}^{n} (X_{i} - \overline{X})^{2}$ are jointly sufficient.\\
        However:
        \begin{equation*}
            (\overline{X})^{2} = \frac{1}{n^{2}}\left(\sum_{i=1}^{n} X_{i}\right)^{2}.
        \end{equation*}
        It is not a one-to-one function. Therefore, $(\overline{X})^{2}$ and $\sum_{i=1}^{n} (X_{i} - \overline{X})^{2}$ may not be jointly sufficient.
    \end{eg}
    From previous examples, the number of sufficient statistics can sometimes be more than the number of unknown parameters. How much should data be condensed most without losing any information about the unknown parameter $\theta$?
    \begin{defn}
        A set of jointly sufficient statistics $\{S_{1}, \cdots, S_{n}\}$ is \textbf{minimal jointly sufficient} if and only if for any other set of jointly sufficient statistics $\{T_{1}, \cdots, T_{m}\}$, there exists a set of functions $\{f_{1}, \cdots, f_{n}\}$ such that for $i = 1, \cdots, n$:
        \begin{equation*}
            S_{i} = f_{i}(T_{1}, \cdots, T_{m}).
        \end{equation*}
    \end{defn}
    or in the one-statistic case:
    \begin{defn}
        A sufficient statistic $S$ is \textbf{minimal sufficient} if and only if for any other sufficient statistic $T$, there exists a function $f$ such that:
        \begin{equation*}
            S = f(T).
        \end{equation*}
    \end{defn}
    \begin{rem}
        Minimal jointly sufficient statistics may not be unique. We can say minimal joint sufficiency is closed under any one-to-one transformation.
    \end{rem}
    In general, it is not easy to find the minimal jointly sufficient statistics except for some special distributions. One of those special distributions is called the exponential family, which we will discuss later.
    \newpage

\section{Relationship of Sufficiency with UMVUE}
    Recall that we actually want to find the UMVUE. Does sufficiency help us find the UMVUE? By the Rao-Blackwell Theorem, it helps us find an improved unbiased estimator!
    \begin{thm}\named{Rao-Blackwell Theorem}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample from a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$, and let $\{S_{1},\cdots,S_{r}\}$ be a set of jointly sufficient statistics, where $r\geq k$ and $S_{i}=S_{i}(\mathbf{X})$ for $i=1,\cdots,r$. Suppose that $T=T(\mathbf{X})$ is an unbiased estimator for $g(\theta)$ for a function $g$. Define $T'$ by $\E(T|S_{1},\cdots,S_{r})$. Then:
        \begin{enumerate}
            \item $T'$ is a statistic, and it is a function of the jointly sufficient statistics.
            \item $T'$ is unbiased for $g(\theta)$.
            \item $\Var(T')\leq\Var(T)$.
        \end{enumerate}
    \end{thm}
    \begin{proofing}
        \begin{enumerate}
            \item 
            \begin{equation*}
                T'=\E(T|S_{1},\cdots,S_{r})=\int_{-\infty}^\infty tf_{T|S_{1},\cdots,S_{r}}(t|S_{1},\cdots,S_{r})\,dt
            \end{equation*}
            By definition, $T'$ is a statistic, and it is a function of the jointly sufficient statistics $\{S_{1},\cdots,S_{r}\}$.
            \item 
            \begin{equation*}
                \E(T')=\E(\E(T|S_{1},\cdots,S_{r}))=\E(T)=g(\theta).
            \end{equation*}
            Therefore, $T'$ is unbiased for $g(\theta)$.
            \item 
            \begin{align*}
                \Var(T')&=\Var(\E(T|S_{1},\cdots,S_{r}))\\
                \tag{$\Var(Y)=\Var(\E(Y|X))+\E[\Var(Y|X)]$}
                &=\Var(T)-\E[\Var(T|S_{1},\cdots,S_{r})]\leq\Var(T).
            \end{align*}
        \end{enumerate}
    \end{proofing}
    \begin{eg}
        Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Bern(\theta)$, where $\theta$ is unknown.
        \begin{enumerate}
            \item Since $\E(X_{1})=\theta$, $X_{1}$ is an unbiased estimator of $\theta$.
            \item From Example \ref{Chapter 1 (Example) Sufficient statistics of Bern(theta)}, we have found that $\sum_{i=1}^{n}X_{i}$ is a sufficient statistic. It is also evident that it is minimal.
            \item By the Rao-Blackwell Theorem, $T'=\E(X_{1}|\sum_{i=1}^{n}X_{i})$ is an unbiased estimator for $\theta$ with $\Var(T')\leq\Var(X_{1})$.
        \end{enumerate}
        We want to find $T'$. Assume that we are given $\sum_{i=1}^{n}X_{i}=s$. We have:
        \begin{align*}
            \prob\left(X_{1}=0\left|\sum_{i=1}^{n}X_{i}=s\right.\right)&=\frac{\prob\left(X_{1}=0,\sum_{i=1}^{n}X_{i}=s\right)}{\prob\left(\sum_{i=1}^{n}X_{i}=s\right)}\\
            &=\frac{\prob(X_{1}=0)\prob\left(\sum_{i=2}^{n}X_{i}=s\right)}{\prob\left(\sum_{i=1}^{n}X_{i}=s\right)}\\
            &=\frac{(1-\theta)\binom{n-1}{s}\theta^{s}(1-\theta)^{n-1-s}}{\binom{n}{s}\theta^{s}(1-\theta)^{n-s}}\\
            &=\frac{n-s}{n}.
        \end{align*}
        Therefore, we have:
        \begin{equation*}
            \E\left(X_{1}\left|\sum_{i=1}^{n}X_{i}=s\right.\right)=\prob\left(X_{1}=1\left|\sum_{i=1}^{n}X_{i}=s\right.\right)=1-\prob\left(X_{1}=0\left|\sum_{i=1}^{n}X_{i}=s\right.\right)=\frac{s}{n}.
        \end{equation*}
        We have found that $T'=\E\left(X_{1}|\sum_{i=1}^{n}X_{i}\right)=\frac{1}{n}\sum_{i=1}^{n}X_{i}$. We may find that:
        \begin{equation*}
            \Var(T')=\frac{1}{n}\theta(1-\theta)\leq\theta(1-\theta)=\Var(X_{1}).
        \end{equation*}
    \end{eg}
    \newpage
    
    \begin{rem}
        If $T$ is already a function of jointly sufficient statistics, then $T'$ would be identical to $T$.
    \end{rem}
    \begin{eg}
        Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Bern(\theta)$, and let $\overline{X}$ be the sample mean. We know that:
        \begin{equation*}
            \E(\overline{X})=\theta.
        \end{equation*}
        Therefore, $\overline{X}$ is an unbiased estimator of $\theta$. We want to find $T'$. We have:
        \begin{equation*}
            T'=\E\left(\overline{X}\left|\sum_{i=1}^{n}X_{i}\right.\right)=\overline{X}=T.
        \end{equation*}
    \end{eg}
    \begin{rem}
        Although the Rao-Blackwell Theorem provides us with a constructive way to improve a given unbiased estimator, it does not guarantee that the one constructed must be a UMVUE.
    \end{rem}
    \begin{eg}
        Consider a random sample $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ of size $n$ from $\N(\theta,1)$, where $\theta$ is unknown. Let $g(\theta)=\theta$. Consider $T=T(\mathbf{X})=X_{1}$, and let the random sample be a set of jointly sufficient statistics $\{S_{1},\cdots,S_{n}\}$. We may find that:
        \begin{equation*}
            \tag{The expectation of $X_{1}$ given $X_{1}$ is, of course, $X_{1}$}
            \E(X_{1}|X_{1},\cdots,X_{n})=X_{1}.
        \end{equation*}
        However, from Example \ref{Chapter 3 (Example) Sufficient statistic for normal distribution with unit variance}, we have found a better statistic $S^{*}=\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}$ since:
        \begin{equation*}
            \Var(\overline{X})=\frac{1}{n}\leq\Var(X_{1}).
        \end{equation*}
        Therefore, $T'=X_{1}$ is not a UMVUE.
    \end{eg}

\section{Complete Statistics}
    In addition to sufficiency, we need completeness in order to find the UMVUE.
    \begin{defn}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$ for an integer $k>1$. A set of statistics $\{T_{1},\cdots,T_{r}\}$, where $r\geq k$ and $T_{i}=T_{i}(\mathbf{X})$ for $i=1,\cdots,r$, is said to be \textbf{jointly complete} if and only if for any function $g$:
        \begin{equation*}
            \E[g(T_{1},\cdots,T_{r})]=0\text{ for all }\theta\in\Theta\implies\prob(g(T_{1},\cdots,T_{r})=0)=1\text{ for all }\theta\in\Theta.
        \end{equation*}
    \end{defn}
    In the one-parameter case:
    \begin{defn}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}$. A statistic $T=T(\mathbf{X})$ is said to be \textbf{complete} if and only if for any function $g$:
        \begin{equation*}
            \E[g(T)]=0\text{ for all }\theta\in\Theta\implies\prob(g(T)=0)=1\text{ for all }\theta\in\Theta.
        \end{equation*} 
    \end{defn}
    \begin{rem}
        Function $g(T)$ or $g(T_{1},\cdots,T_{r})$ is not an unbiased estimator for $\theta$.
    \end{rem}
    \begin{rem}
        If there exists a function $g^{*}$ such that $\E[g^{*}(T)]=0$ but $\prob(g^{*}(T)\neq 0)>0$, then $T$ is not complete. This is the same for joint completeness. 
    \end{rem}
    \newpage
    
    \begin{eg}
        Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Bern(\theta)$, where $\theta\in(0,1)$ is unknown. Let $T_{1}=X_{1}-X_{2}$. We can easily find that for all $\theta\in(0,1)$:
        \begin{equation*}
            \E(X_{1}-X_{2})=0\text{ but }\prob(X_{1}-X_{2}\neq 0)>0.
        \end{equation*}
        Therefore, $T_{1}=X_{1}-X_{2}$ is not a complete statistic.\\
        Let $T_{2}=\sum_{i=1}^{n}X_{i}$. For any function $g$,
        \begin{equation*}
            \E[g(T_{2})]=\sum_{i=0}^{n}g(t)\binom{n}{t}\theta^{t}(1-\theta)^{n-t}=(1-\theta)^{n}\sum_{i=1}^{n}g(t)\binom{n}{t}\left(\frac{\theta}{1-\theta}\right)^{t}.
        \end{equation*}
        Thus, $\E[g(T_{2})]=0$ for all $\theta\in(0,1)$ implies that the equation $\sum_{i=1}^{n}g(t)\binom{n}{t}\left(\frac{\theta}{1-\theta}\right)^{t}=0$ holds for all $\theta\in(0,1)$.\\
        If not all coefficients $g(t)\binom{n}{t}$ are equal to zero, then there are at most $n$ solutions to the equation for all $\theta\in(0,1)$. This means only $n$ values of $\theta\in\Theta$ satisfy the equation, but not all $\theta\in(0,1)$.\\
        Therefore, $g(t)\binom{n}{t}=0$, and thus $g(t)=0$ for $t=0,\cdots,n$ and for all $\theta\in(0,1)$.\\
        Since the only possible values of $T_{2}=\sum_{i=1}^{n}X_{i}$ are in $\{0,\cdots,n\}$, we find that:
        \begin{equation*}
            \prob(g(T_{2})=0)=1.
        \end{equation*}
        We conclude that $T_{2}=\sum_{i=1}^{n}X_{i}$ is complete.
    \end{eg}
    \begin{eg}
        \label{Chapter 3 (Example) complete and sufficient statistic of U[0,theta]}
        Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\U[0,\theta]$, where $\theta>0$ is unknown. We check if the sufficient statistic $X_{(n)}$ is complete. Note that for any function $g$,
        \begin{equation*}
            \E[g(X_{(n)})]=\int_{-\infty}^\infty g(y)f_{X_{(n)}}(y)\,dy=\frac{n}{\theta^{n}}\int_{0}^{\theta}g(y)y^{n-1}\,dy.
        \end{equation*}
        Therefore, if $\E[g(X_{(n)})]=0$ for all $\theta>0$, then:
        \begin{equation*}
            \int_{0}^{\theta}g(y)y^{n-1}\,dy=0.
        \end{equation*}
        Differentiating both sides with respect to $\theta$ gives $g(\theta)\theta^{n-1}=0$, and hence $g(\theta)=0$ for $\theta>0$. Replacing the parameter $\theta$ with a number $y$, we get $g(y)=0$ for $y\in(0,\theta]$ for all $\theta>0$.\\
        Since $0\leq X_{(n)}\leq\theta$, we find that for all $\theta\in\Theta$:
        \begin{equation*}
            \prob(g(X_{(n)})=0)=1.
        \end{equation*}
        Therefore, $X_{(n)}$ is complete.
    \end{eg}

\section{Exponential Family}
    Most of the time, it is quite difficult to check the completeness and minimal sufficiency of a statistic by definition, especially for joint completeness. However, there is one special distribution for which we can check these properties easily. It is called the exponential family.
    \begin{defn}
        Suppose that a random variable $X$ has a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$ for an integer $k>1$. If we find that:
        \begin{enumerate}
            \item $\supp(X)$ does not depend on $\theta$.
            \item The PDF or PMF of $X$ can be written in the form:
            \begin{equation*}
                \exp\left(a(\theta)+b(x)+\sum_{j=1}^{k}c_{j}(\theta)d_{j}(x)\right),
            \end{equation*}
            where $a(\theta)$, $b(x)$, $c_{j}(\theta)$, and $d_{j}(x)$ for $j=1,\cdots,k$ are real-valued functions.
        \end{enumerate}
        then the distribution of $X$ is said to be a member of the \textbf{$k$-parameter exponential family}.
    \end{defn}
    \newpage
    
    or in the one-parameter case,
    \begin{defn}
        Suppose that a random variable $X$ has a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}$. If we find that:
        \begin{enumerate}
            \item $\supp(X)$ does not depend on $\theta$.
            \item The PDF or PMF of $X$ can be written in the form:
            \begin{equation*}
                \exp[a(\theta)+b(x)+c(\theta)d(x)],
            \end{equation*}
            where $a(\theta)$, $b(x)$, $c(\theta)$, and $d(x)$ are real-valued functions.
        \end{enumerate}
        then the distribution of $X$ is said to be a member of the \textbf{one-parameter exponential family}.
    \end{defn}
    \begin{rem}
        A distribution whose support depends on $\theta$ does not belong to the exponential family, e.g., $\U[0,\theta]$.
    \end{rem}
    \begin{rem}
        Most of the parametric distributions we discussed are members of an exponential family, e.g., the normal distribution, gamma distribution, Poisson distribution, binomial distribution, and so on.
    \end{rem}
    The following results show that we can find complete and minimal sufficient statistics from the exponential family.
    \begin{thm}
        \label{Chapter 3 (Theorem) Exponential family to complete and minimal sufficient statistic}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a distribution in the one-parameter exponential family with a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}$, that can be written in the form:
        \begin{equation*}
            \exp[a(\theta)+b(x)+c(\theta)d(x)].
        \end{equation*}
        Then, $\sum_{i=1}^{n}d(X_{i})$ is a complete and minimal sufficient statistic.
    \end{thm}
    \begin{thm}
        \label{Chapter 3 (Theorem) Exponential family to jointly complete and minimal sufficient statistics}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a distribution in the $k$-parameter exponential family with a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$ for an integer $k>1$, that can be written in the form:
        \begin{equation*}
            \exp\left(a(\theta)+b(x)+\sum_{j=1}^{k}c_{j}(\theta)d_{j}(x)\right).
        \end{equation*}
        Then the set $\{\sum_{i=1}^{n}d_{1}(X_{i}),\cdots,\sum_{i=1}^{n}d_{k}(X_{i})\}$ is a set of jointly complete and minimal sufficient statistics.
    \end{thm}
    \begin{eg}
        \label{Chapter 3 (Example) Complete and minimal sufficient statistic of Poisson distribution}
        Consider a random sample from $\Poisson(\lambda)$, where $\lambda\in(0,\infty)$ is unknown. We have:
        \begin{equation*}
            p(x|\lambda)=\frac{\lambda^{x}e^{-\lambda}}{x!}=\exp[-\lambda-\ln(x!)+x\ln{\lambda}].
        \end{equation*}
        Since the support $\{0,1,\cdots\}$ does not depend on $\lambda$, by Theorem \ref{Chapter 3 (Theorem) Exponential family to complete and minimal sufficient statistic}, $\sum_{i=1}^{n}X_{i}$ is a complete and minimal sufficient statistic.
    \end{eg}
    \begin{eg}
        Consider a random sample from $\Bern(\theta)$, where $\theta\in(0,1)$ is unknown. We have:
        \begin{equation*}
            p(x|\theta)=\theta^{x}(1-\theta)^{1-x}=\exp\left[\ln(1-\theta)+x\ln\left(\frac{\theta}{1-\theta}\right)\right].
        \end{equation*}
        Since the support $\{0,1\}$ does not depend on $\theta$, by Theorem \ref{Chapter 3 (Theorem) Exponential family to complete and minimal sufficient statistic}, $\sum_{i=1}^{n}X_{i}$ is a complete and minimal sufficient statistic.
    \end{eg}
    \begin{eg}
        Consider a random sample from $\N(\mu,\sigma^{2})$, where $\mu$ and $\sigma>0$ are unknown. We have:
        \begin{equation*}
            f(x|\mu,\sigma^{2})=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right)=\exp\left(-\frac{\mu^{2}}{2\sigma^{2}}-\frac{1}{2}\ln(2\pi\sigma^{2})+\frac{\mu}{\sigma^{2}}x-\frac{1}{2\sigma^{2}}x^{2}\right).
        \end{equation*}
        Since the support does not depend on $\mu$ and $\sigma^{2}$, by Theorem \ref{Chapter 3 (Theorem) Exponential family to jointly complete and minimal sufficient statistics}, $\sum_{i=1}^{n}X_{i}$ and $\sum_{i=1}^{n}X_{i}^{2}$ are jointly complete and minimal sufficient statistics.
    \end{eg}
    \newpage
    
\section{Relationship of completeness and sufficiency with UMVUE}
    We still haven't explained why a complete and minimal sufficient statistic can lead to the UMVUE. This is due to the following theorem.
    \begin{thm}\named{Lehmann-Scheff\'e Theorem} 
        Let $CS$ be a complete and (minimal) sufficient statistic. If there exists a function $h(CS)$ that is unbiased for $g(\theta)$, then $h(CS)$ is the unique UMVUE of $g(\theta)$.
    \end{thm}
    \begin{thm}
        Let $CS$ be a complete and (minimal) sufficient statistic. If $\E[f(\mathbf{X})]=g(\theta)$ for all $\theta$, then $h(CS)=\E[f(\mathbf{X})|CS]$ is the UMVUE for $g(\theta)$.
    \end{thm}
    \begin{rem}
        From this theorem, we can formulate some strategies to find the UMVUE, which is a function of $CS$:
        \begin{enumerate}
            \item Guess the correct form of the function of $CS$.
            \item Solve for $h(CS)$ using $\E[h(CS)]=g(\theta)$.
            \item Use the Rao-Blackwell Theorem to construct $h(CS)$ by guessing or finding any unbiased estimator $T$ for $g(\theta)$ and then evaluating $h(CS)=\E(T|CS)$.
        \end{enumerate}
    \end{rem}
    \begin{eg}
        \label{Chapter 3 (Example) UMVUE of Exp(theta)}
        Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $X\sim\Exp(\theta)$, where $\theta\in(0,\infty)$ is unknown. Find the UMVUE of $g(\theta)=\theta$. We try Strategy 1.\\
        We have $\theta=\frac{1}{\E(X)}$. Since the exponential distribution of $X$ belongs to an exponential family, we can find that $CS=\sum_{i=1}^{n}X_{i}$. We suspect that the UMVUE is related to $\frac{n}{\sum_{i=1}^{n}X_{i}}$. For $n>1$, since $\Exp(\theta)=\Gam(1,\theta)$,
        \begin{equation*}
            \E\left(\frac{1}{\sum_{i=1}^{n}X_{i}}\right)=\int_{0}^{\infty}\frac{\theta^{n}}{x\Gamma(n)}x^{n-1}e^{-\theta x}\,dx=\frac{\theta}{\Gamma(n)}\int_{0}^{\infty}\theta(\theta x)^{n-2}e^{-\theta x}\,dx=\frac{\theta\Gamma(n-1)}{\Gamma(n)}=\frac{\theta}{n-1}.
        \end{equation*}
        Therefore, we have found that $\frac{n-1}{\sum_{i=1}^{n}X_{i}}$ is the UMVUE.
    \end{eg}
    \begin{eg}
        We continue the example above. This time we try Strategy 2, which involves solving for $h(CS)$ using $\E[h(CS)]=g(\theta)=\theta$.
        \begin{align*}
            \int_{0}^{\infty}h(x)\frac{\theta^{n}}{\Gamma(n)}x^{n-1}e^{-\theta x}\,dx&=\theta,\\
            \int_{0}^{\infty}h(x)\frac{\theta^{n-1}}{\Gamma(n)}x^{n-1}e^{-\theta x}\,dx&=1,\\
            \int_{0}^{\infty}\left(h(x)\frac{x}{n-1}\right)\frac{\theta^{n-1}}{\Gamma(n-1)}x^{(n-1)-1}e^{-\theta x}\,dx&=1.
        \end{align*} 
        This is only true if $h(x)\frac{x}{n-1}=1$ for all $x>0$. Thus, $h(x)=\frac{n-1}{x}$, and therefore:
        \begin{equation*}
            h\left(\sum_{i=1}^{n}X_{i}\right)=\frac{n-1}{\sum_{i=1}^{n}X_{i}}.
        \end{equation*}
        Since it is unbiased for $\theta$ and is a function of $CS$, by the Lehmann-Scheff\'e Theorem, it is the UMVUE of $\theta$.
    \end{eg}
    \newpage
    
    \begin{eg}
        Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Poisson(\lambda)$, where $\lambda\in(0,\infty)$ is unknown. Find the UMVUE of $g(\lambda)=e^{-\lambda}$. We try Strategy 3.\\
        From Example \ref{Chapter 3 (Example) Complete and minimal sufficient statistic of Poisson distribution}, $\sum_{i=1}^{n}X_{i}$ is a complete and minimal sufficient statistic. Note that $g(\lambda)=e^{-\lambda}=\prob(X_{1}=0)=\mathbf{1}_{X_{1}=0}$, and thus it is a trivial unbiased estimator of $g(\lambda)$. By the Rao-Blackwell Theorem, $\E(\mathbf{1}_{X_{1}=0}|\sum_{i=1}^{n}X_{i})$ is unbiased for $g(\lambda)$. By the Lehmann-Scheff\'e Theorem, it is the unique UMVUE of $g(\lambda)$. We compute the UMVUE.\\
        For $n=1$,
        \begin{equation*}
            \E\left(\mathbf{1}_{X_{1}=0}\left|\sum_{i=1}^{n}X_{i}\right.\right)=\E(\mathbf{1}_{X_{1}=0}|X_{1})=\prob(X_{1}=0|X_{1})=\mathbf{1}_{X_{1}=0}.
        \end{equation*}
        For $n>1$,
        \begin{align*}
            \E\left(\mathbf{1}_{X_{1}=0}\left|\sum_{i=1}^{n}X_{i}=s\right.\right)=\prob\left(X_{1}=0\left|\sum_{i=1}^{n}X_{i}=s\right.\right)&=\frac{\prob(X_{1}=0,\sum_{i=1}^{n}X_{i}=s)}{\prob(\sum_{i=1}^{n}X_{i}=s)}\\
            &=\frac{\prob(X_{1}=0)\prob(\sum_{i=2}^{n}X_{i}=s)}{\prob(\sum_{i=1}^{n}X_{i}=s)}\\
            &=\frac{e^{-\lambda}e^{-(n-1)\lambda}[(n-1)\lambda]^{s}s!}{e^{-n\lambda}(n\lambda)^{s}s!}\\
            &=\left(\frac{n-1}{n}\right)^{s}.
        \end{align*}
        Therefore, the UMVUE for $g(\lambda)=e^{-\lambda}$ is:
        \begin{equation*}
            \E\left(\mathbf{1}_{X_{1}=0}\left|\sum_{i=1}^{n}X_{i}\right.\right)=\begin{cases}
                \mathbf{1}_{X_{1}=0}, &n=1,\\
                \left(\frac{n-1}{n}\right)^{\sum_{i=1}^{n}X_{i}}, &n>1.
            \end{cases}
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\U[0,\theta]$, where $\theta>0$ is unknown. Since the uniform distribution is not in an exponential family, we cannot use Theorem \ref{Chapter 3 (Theorem) Exponential family to complete and minimal sufficient statistic} to find a complete and minimal sufficient statistic.\\
        From Example \ref{Chapter 3 (Example) complete and sufficient statistic of U[0,theta]}, we have found that $X_{(n)}$ is a complete and sufficient statistic. By checking for unbiasedness,
        \begin{equation*}
            \E(X_{(n)})=\frac{n}{n+1}\theta.
        \end{equation*}
        Therefore, by the Lehmann-Scheff\'e Theorem, the UMVUE of $\theta$ is $\frac{n+1}{n}X_{(n)}$.
    \end{eg}

\section{Cram\'er-Rao Inequality}
    Recall Theorem \ref{Chapter 2 (Thoerem) Sequence of MME is asympt. normal}, where we claim that a sequence of MLE is asymptotically efficient, which means that:
    \begin{equation*}
        \mathcal{I}_{X}^{-1}(\theta)
    \end{equation*}
    is the lowest possible bound for any unbiased estimator. This is due to the Cram\'er-Rao Inequality (C-R Inequality).
    \begin{thm}\named{Cram\'er-Rao Inequality}
        Under the regularity conditions, the variance of an unbiased estimator $T(\mathbf{X})=T(X_{1},\cdots,X_{n})$ for $\theta$, based on a set of random variables $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ from their joint PDF $f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)$, satisfies the following inequality:
        \begin{equation*}
            \Var(T(\mathbf{X}))\geq\frac{1}{\mathcal{I}_{X_{1},\cdots,X_{n}}(\theta)}=\frac{1}{\E\left[\odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}\right]^{2}}.
        \end{equation*}
        The lower bound is called the \textbf{Cram\'er-Rao lower bound} (CRLB).
    \end{thm}
    \begin{rem}
        The Cram\'er-Rao Inequality can also be written as:
        \begin{equation*}
            \Var(T(\mathbf{X}))\geq\frac{1}{-\E\left[\odv*[order={2}]{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}\right]}.
        \end{equation*}
    \end{rem}
    \newpage
    
    \begin{rem}
        If $\mathbf{X}$ is a random sample of size $n$, then we have:
        \begin{equation*}
            \Var(T(\mathbf{X}))\geq\frac{1}{n\mathcal{I}_{X_{1}}(\theta)}=\frac{1}{n\E\left[\odv*{\ln{f_{X_{1}}(X_{1}|\theta)}}{\theta}\right]^{2}}=\frac{1}{-n\E\left[\odv*[order={2}]{\ln{f_{X_{1}}(X_{1}|\theta)}}{\theta}\right]}.
        \end{equation*}
    \end{rem}
    \begin{eg}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\N(\theta,\sigma^{2})$, where $\sigma^{2}$ is known and $\theta$ is unknown. The CRLB for $\theta$ is:
        \begin{equation*}
            \frac{1}{n\mathcal{I}_{X_{1}}(\theta)}=\frac{\sigma^{2}}{n}.
        \end{equation*}
    \end{eg}
    \begin{eg}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Bern(p)$, where $p$ is unknown. The CRLB for $p$ is:
        \begin{equation*}
            \frac{1}{n\mathcal{I}_{X_{1}}(p)}=\frac{p(1-p)}{n}.
        \end{equation*}
    \end{eg}
    Often, we want to estimate a function of $\theta$, $g(\theta)$, instead of $\theta$.
    \begin{thm}
        Under the regularity conditions, if $T(\mathbf{X})=T(X_{1},\cdots,X_{n})$ is an unbiased estimator for $g(\theta)$, then the Cram\'er-Rao Inequality for $g(\theta)$ is:
        \begin{equation*}
            \Var(T(\mathbf{X}))\geq\frac{\left[\odv*{g(\theta)}{\theta}\right]^{2}}{\mathcal{I}_{X_{1},\cdots,X_{n}}(\theta)}=\frac{\left[\odv*{g(\theta)}{\theta}\right]^{2}}{\E\left[\odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}\right]^{2}}.
        \end{equation*}
    \end{thm}
    \begin{proofing}
        Let $U=\odv*{\ln{f_{\mathbf{X}}(\mathbf{X}|\theta)}}{\theta}$ and $V=T(\mathbf{X})$. We have:
        \begin{equation*}
            -1\leq\frac{\cov(U,V)}{\sqrt{\Var(U)\Var(V)}}\leq 1\implies\frac{(\cov(U,V))^{2}}{\Var(U)}\leq\Var(V).
        \end{equation*}
        We may find that $\Var(U)=\Var\left(\odv*{\ln{f_{\mathbf{X}}(\mathbf{X}|\theta)}}{\theta}\right)=\mathcal{I}_{\mathbf{X}}(\theta)$. In addition,
        \begin{align*}
            \cov\left(\odv*{\ln{f_{\mathbf{X}}(\mathbf{X}|\theta)}}{\theta},T(\mathbf{X})\right)&=\E\left[T(\mathbf{X})\odv*{\ln{f_{\mathbf{X}}(\mathbf{X}|\theta)}}{\theta}\right]-\E\left[\odv*{\ln{f_{\mathbf{X}}(\mathbf{X}|\theta)}}{\theta}\right]\E[T(\mathbf{X})]\\
            &=\E\left[T(\mathbf{X})\odv*{\ln{f_{\mathbf{X}}(\mathbf{X}|\theta)}}{\theta}\right]\\
            &=\int_{-\infty}^\infty\cdots\int_{-\infty}^\infty T(\mathbf{x})\left(\odv*{\ln{f_{\mathbf{X}}(\mathbf{x}|\theta)}}{\theta}\right)f_{\mathbf{X}}(\mathbf{x}|\theta)\,d\mathbf{x}\\
            &=\int_{-\infty}^\infty\cdots\int_{-\infty}^\infty T(\mathbf{x})\odv*{f_{\mathbf{X}}(\mathbf{x}|\theta)}{\theta}\,d\mathbf{x}\\
            &=\odv*{\E[T(\mathbf{X})]}{\theta}\\
            &=\odv*{g(\theta)}{\theta}.
        \end{align*}
        Therefore, we have:
        \begin{equation*}
            \Var(T(\mathbf{X}))\geq\frac{\left[\odv*{g(\theta)}{\theta}\right]^{2}}{\mathcal{I}_{\mathbf{X}}(\theta)}.
        \end{equation*}
    \end{proofing}
    \begin{rem}
        Since the CRLB is the lowest bound of variance for any unbiased estimator, any unbiased estimator whose variance achieves the CRLB for $g(\theta)$ is the UMVUE for $g(\theta)$.
    \end{rem}
    \begin{rem}
        It is not necessary for a UMVUE to have a variance equal to the CRLB.
    \end{rem}
    \begin{eg}
        Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Exp(\theta)$, where $\theta\in(0,\infty)$ is unknown.\\
        The CRLB of $\theta$ is $\frac{\theta^{2}}{n}$. From Example \ref{Chapter 3 (Example) UMVUE of Exp(theta)}, we have found that $\frac{n-1}{\sum_{i=1}^{n}X_{i}}$ is the UMVUE of $\theta$ when $n>1$.\\
        After some tedious calculations, for $n>2$, we find that $\Var\left(\frac{n-1}{\sum_{i=1}^{n}X_{i}}\right)=\frac{\theta^{2}}{n-2}\geq\frac{\theta^{2}}{n}$.
    \end{eg}
    \newpage

    When does the equality for the C-R inequality hold?
    \begin{thm}
        \label{Chapter 3 (Theorem) C-R equality}
        Under the regularity conditions, the C-R equality holds if and only if:
        \begin{equation*}
            \odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}=A(\theta,n)[T(X_{1},\cdots,X_{n})-g(\theta)],
        \end{equation*}
        where $A(\theta,n)$ is a non-zero function. The statistic $T(X_{1},\cdots,X_{n})$ is a UMVUE of $g(\theta)$. 
    \end{thm}
    This theorem has an interesting result: if we can write $\odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}$ as $A(\theta,n)[T(X_{1},\cdots,X_{n})-g(\theta)]$, then the statistic must be a UMVUE.
    \begin{lem}
        If $T(X_{1},\cdots,X_{n})$ is a UMVUE of $g(\theta)$ such that the C-R equality holds, then $aT(X_{1},\cdots,X_{n})+b$ is a UMVUE of $ag(\theta)+b$, where $a\neq 0$.
    \end{lem}
    \begin{proofing}
        \begin{equation*}
            \odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}=A(\theta,n)[T(X_{1},\cdots,X_{n})-g(\theta)]=\frac{A(\theta,n)}{a}[(aT(X_{1},\cdots,X_{n})+b)-(ag(\theta)+b)].
        \end{equation*}
        By setting $A^{*}(\theta,n)=\frac{A(\theta,n)}{a}$, we find that $aT(X_{1},\cdots,X_{n})+b$ is a UMVUE of $ag(\theta)+b$.
    \end{proofing}
    \begin{eg}
        Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Poisson(\lambda)$, where $\lambda$ is unknown.
        \begin{align*}
            \odv*{\ln{p_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\lambda)}}{\lambda}&=\odv*{\ln\left(\prod_{i=1}^{n}\frac{\lambda^{X_{i}}e^{-\lambda}}{X_{i}!}\right)}{\lambda}\\
            &=\odv*{\sum_{i=1}^{n}[X_{i}\ln{\lambda}-\lambda-\ln(X_{i}!)]}{\lambda}\\
            &=\sum_{i=1}^{n}\left(\frac{X_{i}}{\lambda}-1\right)\\
            &=\frac{n}{\lambda}(\overline{X}-\lambda).
        \end{align*}
        Therefore, by Theorem \ref{Chapter 3 (Theorem) C-R equality}, $\overline{X}$ is a UMVUE of $\lambda$, and:
        \begin{equation*}
            \Var(\overline{X})=\frac{1}{n\mathcal{I}_{X_{1}}(\lambda)}=\frac{\lambda}{n}.
        \end{equation*}
    \end{eg}
    \begin{rem}
        For any particular function of $\theta$ other than a Euclidean transformation, Theorem \ref{Chapter 3 (Theorem) C-R equality} is not useful.
    \end{rem}
    \begin{eg}
        Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Exp(\theta)$, where $\theta$ is unknown.
        \begin{align*}
            \odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}&=\odv*{\ln\left(\prod_{i=1}^{n}\theta e^{-\theta X_{i}}\right)}{\theta}\\
            &=\odv*{\sum_{i=1}^{n}(\ln{\theta}-\theta X_{i})}{\theta}\\
            &=\sum_{i=1}^{n}\left(\frac{1}{\theta}-X_{i}\right)\\
            &=-n\left(\overline{X}-\frac{1}{\theta}\right).
        \end{align*}
        Therefore, by Theorem \ref{Chapter 3 (Theorem) C-R equality}, $\overline{X}$ is a UMVUE of $\frac{1}{\theta}$, and:
        \begin{equation*}
            \Var(\overline{X})=\frac{1}{n\mathcal{I}_{X_{1}}(\theta)}=\frac{\theta^{2}}{n}.
        \end{equation*}
        However, since $\theta$ cannot be written as a Euclidean transformation of $\frac{1}{\theta}$, we cannot use the theorem to find the UMVUE of $\theta$.
    \end{eg}
    \begin{rem}
        Theorem \ref{Chapter 3 (Theorem) C-R equality} can only be used under regularity conditions. For instance, for $\U[0,\theta]$, we cannot use this theorem.
    \end{rem}

\chapter{Hypothesis Testing}
    This chapter will primarily focus on comparing different unbiased point estimators. 

    In engineering and science fields, people usually hypothesize something about a system. Before proving the conjecture using experimental data, they need to define a hypothesis.
    \begin{defn}
        A \textbf{statistical hypothesis} is an assertion or conjecture about the random variable of interest. If a parametric distribution is considered, then a statistical hypothesis can be a conjecture about the true value of the unknown parameters of the parametric distribution.
    \end{defn}
    \begin{eg}
        \label{Chapter 4 (Example) Engineer example}
        An engineer decides, based on sample data, whether the true average lifetime of a certain kind of tire is at least $22,000$ miles. The engineer has to test the hypothesis that $\theta$ in $\Exp(\theta)$ is at least $22,000$.
    \end{eg}
    \begin{eg}
        An agronomist wants to decide, based on experiments, whether one kind of fertilizer produces a higher yield of soybeans than another. The agronomist has to test the hypothesis that $\mu_{1}>\mu_{2}$ from two distributions $\N(\mu_{1},\sigma_{1}^{2})$ and $\N(\mu_{2},\sigma_{2}^{2})$.
    \end{eg}
    \begin{eg}
        \label{Chapter 4 (Example) Manufacturer example}
        A manufacturer of pharmaceutical products decides, based on samples, whether $90\%$ of all patients given a new medication will recover from a certain disease. The manufacturer has to test the hypothesis that $\theta$ in $\Bin(n,\theta)$ equals $0.90$.
    \end{eg}

\section{Null and Alternative Hypotheses}
    The hypothesis of interest is related to a particular class of $\theta$, say $\Theta_{0}$, and its complement $\Theta_{1}$. These two classes are subsets of the parameter space $\Theta$ of $\theta$. We have $\Theta_{0}\cup\Theta_{1}\subseteq\Theta$ and $\Theta_{0}\cap\Theta_{1}=\emptyset$.
    \begin{defn}
        The hypothesis with $\theta\in\Theta_{0}$ is the \textbf{null hypothesis} $H_{0}$, and the hypothesis with $\theta\in\Theta_{1}$ is the \textbf{alternative hypothesis}.
    \end{defn}
    \begin{rem}
        We usually use signs with implied equality in $H_{0}$.
    \end{rem}
    \begin{defn}
        The hypothesis is \textbf{simple} if the parametric distribution would be fully specified under the hypothesis. Otherwise, the hypothesis is \textbf{composite}.
    \end{defn}
    \begin{eg}
        Using Example \ref{Chapter 4 (Example) Engineer example}, we have:
        \begin{equation*}
            \begin{cases}
                H_{0}: & \theta\geq 22,000,\\
                H_{1}: & \theta<22,000.
            \end{cases}
        \end{equation*}
        Both $H_{0}$ and $H_{1}$ are composite because they do not specify the parameter.
    \end{eg}
    \begin{eg}
        Using Example \ref{Chapter 4 (Example) Manufacturer example}, we have:
        \begin{equation*}
            \begin{cases}
                H_{0}: &\theta=0.9,\\
                H_{1}: &\theta\neq 0.9.
            \end{cases}
        \end{equation*}
        $H_{0}$ is simple, while $H_{1}$ is composite.
    \end{eg}
    \newpage
    
    In hypothesis testing, we want to see whether or not we can find evidence to say that $H_{0}$ is false.
    \begin{rem}
        Hypothesis testing usually follows these three steps:
        \begin{enumerate}
            \item Determine $H_{0}$ and $H_{1}$.
            \item Under $H_{0}$, define a rare event, an event that happens with a very small probability in one experiment with $n$ data points.
            \item Collect data.
            \begin{enumerate}
                \item If the data causes the rare event to happen, it contradicts $H_{0}$. This means we can say that $H_{0}$ is false and reject $H_{0}$.
                \item If the data does not cause the rare event to happen, it does not contradict $H_{0}$. This means we cannot say that $H_{0}$ is false, and we do not reject $H_{0}$.
            \end{enumerate}
        \end{enumerate}
    \end{rem}
    \begin{rem}
        Not rejecting $H_{0}$ does not mean we accept $H_{0}$. It just means there is no sufficient evidence to reject $H_{0}$. The whole idea is to try gathering enough evidence to have great confidence that $H_{0}$ is false and $H_{1}$ is true.
    \end{rem}
    \begin{eg}
        We want to know whether or not a coin is fair. Consider a random experiment of flipping the coin $10$ times. We may determine that:
        \begin{equation*}
            \begin{cases}
                H_{0}: &\prob(\{H\})=\prob(\{T\})=0.5,\\
                H_{1}: &\prob(\{H\})\neq\prob(\{T\}).
            \end{cases}
        \end{equation*}
        Under $H_{0}$, we define the event of getting $10$ tails as the rare event under $H_{0}$ since the probability of getting $10$ tails in one experiment is $0.5^{10}\approx 0.00098$.
        
        We can then perform the experiment to collect data by flipping the coin $10$ times. If we get $10$ tails, then the collected data tells us that getting $10$ tails is not a rare event, which contradicts $H_{0}$. Therefore, we have evidence to suspect the reliability of $H_{0}$ and thus reject $H_{0}$ and accept $H_{1}$.
    \end{eg}

\section{Test Errors and Error Probabilities}
    After we decide the null and alternative hypotheses, we need to determine a test statistic, i.e., the point estimator, to construct a test for rejecting or not rejecting the null hypothesis.
    \begin{defn}
        The \textbf{non-rejection region} $C_{0}$ is a subset of $\Theta$ such that we do not reject $H_{0}$, i.e.,
        \begin{equation*}
            C_{0}=\{\mathbf{x}:\text{Not reject }H_{0}\}.
        \end{equation*}
        The \textbf{rejection region} $C_{1}$ is a subset of $\Theta$ such that we reject $H_{0}$, i.e.,
        \begin{equation*}
            C_{1}=\{\mathbf{x}:\text{Reject }H_{0}\}.
        \end{equation*}
    \end{defn}
    However, there is no perfect test statement due to the randomness of the sample data. Each test would lead to the following two kinds of errors.
    \begin{defn}
        A \textbf{Type I Error} is the error of rejecting $H_{0}$ when it is true. A \textbf{Type II Error} is the error of not rejecting $H_{0}$ when it is false.
    \end{defn}
    \begin{figure}[h]
        \centering
        \begin{tabular}{|c||c|c|}
            \hline
            & Not reject $H_{0}$ & Reject $H_{0}$\\
            \hline
            If $H_{0}$ is true & No error & Type I Error\\
            If $H_{0}$ is false & Type II Error & No error\\
            \hline
        \end{tabular}
    \end{figure}
    \newpage
    
    We may define their corresponding probabilities.
    \begin{defn}
        The \textbf{Type I error probability}, denoted by $\gamma(\theta)$, is the probability of rejecting $H_{0}$ for $\theta\in\Theta_{0}$.
        \begin{equation*}
            \gamma(\theta)=\prob(\text{Reject }H_{0}|\theta)=\prob(\mathbf{X}\in C_{1}|\theta).
        \end{equation*}
        The \textbf{Type II error probability}, denoted by $\beta(\theta)$, is the probability of not rejecting $H_{0}$ for $\theta\in\Theta_{1}$.
        \begin{equation*}
            \beta(\theta)=\prob(\text{Not reject }H_{0}|\theta)=\prob(\mathbf{X}\in C_{0}|\theta).
        \end{equation*}
    \end{defn}
    \begin{rem}
        Since we cannot control $\gamma(\theta)$ and $\beta(\theta)$ at the same time, conventionally, we assign an upper bound to $\gamma(\theta)$ over $\Theta_{0}$ and find a test with $\beta(\theta)$ as small as possible. If we are dealing with the continuous case,
        \begin{equation*}
            \sup_{\theta\in\Theta_{0}}\gamma(\theta)=\alpha.
        \end{equation*}
        If we are dealing with the discrete case,
        \begin{equation*}
            \sup_{\theta\in\Theta_{0}}\gamma(\theta)\leq\alpha.
        \end{equation*}
        We usually call $\alpha$ the \textbf{significance threshold} or \textbf{significance level}. $\sup$ can be replaced with $\max$ if it exists.
    \end{rem}
    \begin{rem}
        We use a point estimator $T(\mathbf{X})$ to formulate our test with:
        \begin{enumerate}
            \item \textbf{One-sided right tests}: $C_{1}=\{\mathbf{x}:T(\mathbf{x})>k\}$ or $H_{1}:\theta>\theta_{0}$.
            \item \textbf{One-sided left tests}: $C_{1}=\{\mathbf{x}:T(\mathbf{x})<k\}$ or $H_{1}:\theta<\theta_{0}$.
            \item \textbf{Two-sided tests}: $C_{1}=\{\mathbf{x}:T(\mathbf{x})<k_{1}\text{ or }T(\mathbf{x})>k_{2}\}$ or $H_{1}:\theta\neq\theta_{0}$.
        \end{enumerate}
        For one-sided tests, $k$ can be obtained by solving:
        \begin{equation*}
            \sup_{\theta\in\Theta_{0}}\prob(\mathbf{X}\in C_{1}|\theta)\begin{cases}
                =\alpha, &\text{Continuous case},\\
                \leq\alpha, &\text{Discrete case}.
            \end{cases}
        \end{equation*}
        For two-sided tests, $k_{1}$ and $k_{2}$ can be obtained by solving:
        \begin{align*}
            \prob(T(\mathbf{X})<k_{1}|\theta_{0})&=\frac{\alpha}{2}, & \prob(T(\mathbf{X})>k_{2}|\theta_{0})&=\frac{\alpha}{2}.
        \end{align*}
        If they cannot be found exactly (such as when we cannot determine the exact distribution of $T(\mathbf{X})$), then we can approximate them by using the limiting distribution of $T(\mathbf{X})$ or simplified terms.
    \end{rem}
    \begin{eg}
        Assume that we have a random sample $X\sim\N(\mu,\sigma^{2})$, where $\sigma$ is known. We want to see if $\mu\geq 3423$ with a $2\%$ significance level. Let $\mathbf{X}$ be a random sample of size $n$ from $X$. We define the test as follows:
        \begin{align*}
            T(\mathbf{X})&=\overline{X}, & &\begin{cases}
                H_{0}: &\mu\geq3423,\\
                H_{1}: &\mu<3423.
            \end{cases} & C_{1}&=\{\mathbf{x}:\overline{x}<k\}.
        \end{align*}
        We find $k$ by solving:
        \begin{equation*}
            0.02=\max_{\mu\geq3423}\prob(\overline{X}<k|\mu).
        \end{equation*}
        It is obvious to see that:
        \begin{align*}
            0.02&=\max_{\mu\geq3423}\prob\left(Z<\frac{\sqrt{n}(k-\mu)}{\sigma}\right)=\prob\left(Z<\frac{\sqrt{n}(k-3423)}{\sigma}\right), & 0.98&=\prob\left(Z\geq\frac{\sqrt{n}(k-3423)}{\sigma}\right).
        \end{align*}
        Therefore, we can find that:
        \begin{equation*}
            k=3423-z_{0.02}\frac{\sigma}{\sqrt{n}}=3423+z_{0.98}\frac{\sigma}{\sqrt{n}}.
        \end{equation*}
        We would reject $H_{0}$ at $\alpha=0.02$ if $\overline{x}<3423-z_{0.02}\frac{\sigma}{\sqrt{n}}$.
    \end{eg}
    However, in many cases, $\theta$ may not be $\mu$ or $\sigma^{2}$. What do we do if the distribution of $T(\mathbf{X})$ cannot be easily determined?
    \newpage
    
\section{Likelihood Test}
    Similar to finding the MLE in Chapter 2, we also have a general method called the likelihood ratio test.
    \begin{defn}
        The \textbf{Likelihood Ratio Test} (LRT) for testing $H_{0}:\theta\in\Theta_{0}$ against $H_{1}:\theta\in\Theta_{1}$ at a significance level of $\alpha$ is a test with a rejection region:
        \begin{equation*}
            C_{1}=\{\mathbf{x}:\lambda(\mathbf{x})\leq k\},
        \end{equation*}
        where $k\in(0,1)$ satisfies $\max_{\theta\in\Theta_{0}}\prob(\lambda(\mathbf{X})\leq k|\theta)=\alpha$, and the LRT statistic $\lambda$ is given by:
        \begin{equation*}
            \lambda(\mathbf{x})=\frac{L(\hat{\theta}_{0})}{L(\hat{\theta})},
        \end{equation*}
        with MLE $\hat{\theta}_{0}$ of $\theta$ over $\Theta_{0}$ and MLE $\hat{\theta}$ over $\Theta^{*}=\Theta_{0}\cup\Theta_{1}\subseteq\Theta$.
    \end{defn}
    \begin{rem}
        Since $\Theta_{0}\subset\Theta_{0}\cup\Theta_{1}$, we can see that $L(\hat{\theta}_{0})\leq L(\hat{\theta})$. Therefore, $0<\lambda(\mathbf{x})\leq 1$.
    \end{rem}
    \begin{rem}
        If $\lambda(\mathbf{x})$ is close to $0$, then it suggests that the data is not compatible with $H_{0}$. Therefore, $H_{0}$ should be rejected.
    \end{rem}
    \begin{rem}
        If the hypothesis is simple, then there is no point in finding the MLE. We use the hypothesized value of $\theta$ instead of the MLE.
    \end{rem}
    \begin{eg}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Exp(\theta)$, where $\theta$ is unknown. We can construct an LRT at a significance level of $\alpha$:
        \begin{equation*}
            \begin{cases}
                H_{0}: &\theta=\theta_{0},\\
                H_{1}: &\theta>\theta_{0},
            \end{cases}
        \end{equation*}
        where $\theta_{0}$ is known and positive. Note that $\Theta_{0}=\{\theta_{0}\}$ and $\Theta_{1}=(\theta_{0},\infty)$. The parameter space $\Theta^{*}$ is restricted to be at least $\theta_{0}$. We may find that:
        \begin{equation*}
            \odv*{l(\theta)}{\theta}=\frac{n}{\theta}-n\overline{x}.
        \end{equation*}
        Therefore, the MLE of $\theta$ over $\Theta^{*}$ is:
        \begin{equation*}
            \hat{\theta}=\max\biggl\{\theta_{0},\frac{1}{\overline{x}}\biggr\}.
        \end{equation*}
        The likelihood ratio test statistic can be found by:
        \begin{align*}
            L(\hat{\theta}_{0})&=\theta_{0}^{n}e^{-n\theta_{0}\overline{x}}, & L(\hat{\theta})&=\begin{cases}
                \left(\frac{1}{\overline{x}}\right)^{n}e^{-n}, &\text{if }\frac{1}{\overline{x}}>\theta_{0},\\
                \theta_{0}^{n}e^{-n\theta_{0}\overline{x}}, &\text{if }\frac{1}{\overline{x}}\leq\theta_{0}.
            \end{cases}
        \end{align*}
        \begin{equation*}
            \lambda(\mathbf{x})=\begin{cases}
                \frac{\theta_{0}^{n}e^{-n\theta_{0}\overline{x}}}{(\overline{x})^{-n}e^{-n}}, &\text{if }\frac{1}{\overline{x}}>\theta_{0},\\
                1, &\text{if }\frac{1}{\overline{x}}\leq\theta_{0}.
            \end{cases}=\begin{cases}
                (\theta_{0}\overline{x})^{n}e^{-n(\theta_{0}\overline{x}-1)}, &\text{if }\frac{1}{\overline{x}}>\theta_{0},\\
                1, &\text{if }\frac{1}{\overline{x}}\leq\theta_{0}.
            \end{cases}
        \end{equation*}
        Therefore, we reject $H_{0}$ if $\frac{1}{\overline{x}}>\theta_{0}$ and $(\theta_{0}\overline{x})^{n}e^{-n(\theta_{0}\overline{x}-1)}\leq k$. But how do we determine $k$?
        
        If the term $(\theta_{0}\overline{x})^{n}e^{-n(\theta_{0}\overline{x}-1)}$ is a function of some quantity $y$, where the distribution of $Y$ can be easily determined, then the test based on $y$ will be equivalent to the original test.
        
        Let $y=\theta_{0}\overline{x}$. The function $y^{n}e^{-n(y-1)}$ attains its maximum at $y=1$. Therefore,
        \begin{equation*}
            C_{1}=\bigl\{\mathbf{x}:y<1\text{ and }y^{n}e^{-n(y-1)}\leq k\bigr\}=\{\mathbf{x}:y\leq K\in(0,1)\}=\biggl\{\mathbf{x}:\sum_{i=1}^{n}x_{i}\leq\frac{nK}{\theta_{0}}=K'\biggr\}.
        \end{equation*}
        Therefore, we can reject $H_{0}$ when $\sum_{i=1}^{n}x_{i}\leq K'$, where $\sum_{i=1}^{n}X_{i}\sim\Gam(n,\theta)$. $K'$ can be determined by:
        \begin{equation*}
            \prob\left(\left.\sum_{i=1}^{n}X_{i}\leq K'\right|\theta_{0}\right)=\alpha.
        \end{equation*}
    \end{eg}
    \newpage
    
    \begin{eg}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\N(\theta,\sigma^{2})$, where both $\theta$ and $\sigma$ are unknown. We want to test, at a significance level of $\alpha$, the following hypotheses:
        \begin{equation*}
            \begin{cases}
                H_{0}: &\theta=\theta_{0},\\
                H_{1}: &\theta\neq\theta_{0}.
            \end{cases}
        \end{equation*}
        Since $H_{0}$ is simple, we have:
        \begin{align*}
            \hat{\theta}_{0}&=\theta_{0}, & \hat{\sigma}_{0}^{2}&=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\theta_{0})^{2}.
        \end{align*}
        To find the denominator, we determine the MLE of $\theta$ and $\sigma^{2}$:
        \begin{align*}
            \hat{\theta}&=\overline{x}, & \hat{\sigma}^{2}&=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}.
        \end{align*}
        Therefore,
        \begin{align*}
            L(\hat{\theta}_{0},\hat{\sigma}_{0}^{2})&=\left(\frac{2\pi}{n}\sum_{i=1}^{n}(X_{i}-\theta_{0})^{2}\right)^{-\frac{n}{2}}\exp\left(-\frac{\sum_{i=1}^{n}(X_{i}-\theta_{0})^{2}}{\frac{2}{n}\sum_{i=1}^{n}(X_{i}-\theta_{0})^{2}}\right)\\
            &=\left(\frac{2\pi}{n}\sum_{i=1}^{n}(X_{i}-\theta_{0})^{2}\right)^{-\frac{n}{2}}\exp\left(-\frac{n}{2}\right),\\
            L(\hat{\theta},\hat{\sigma}^{2})&=\left(\frac{2\pi}{n}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}\right)^{-\frac{n}{2}}\exp\left(-\frac{\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}}{\frac{2}{n}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}}\right)\\
            &=\left(\frac{2\pi}{n}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}\right)^{-\frac{n}{2}}\exp\left(-\frac{n}{2}\right),\\
            \lambda(\mathbf{X})&=\frac{L(\hat{\theta}_{0},\hat{\sigma}_{0}^{2})}{L(\hat{\theta},\hat{\sigma}^{2})}=\left(\frac{\sum_{i=1}^{n}(X_{i}-\theta_{0})^{2}}{\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}}\right)^{-\frac{n}{2}}.
        \end{align*}
        By the CLT, we have that if $\theta=\theta_{0}$,
        \begin{align*}
            \frac{\sqrt{n}(\overline{X}-\theta_{0})}{\sigma}&\sim\N(0,1), & \frac{n(\overline{X}-\theta_{0})^{2}}{\sigma^{2}}&\sim\chi^{2}(1).
        \end{align*}
        By Theorem \ref{Chapter 1 (Theorem) Normal and chi-squared distribution related to sample mean and variance}, we find that:
        \begin{equation*}
            \frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}\sim\chi^{2}(n-1).
        \end{equation*}
        Therefore, using the definition of the F-distribution, define $F$ by:
        \begin{equation*}
            F=(n-1)\frac{n(\overline{X}-\theta_{0})^{2}}{\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}}=\frac{\frac{n(\overline{X}-\theta_{0})^{2}}{\sigma^{2}}}{\frac{\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}}{\sigma^{2}(n-1)}}\sim F(1,n-1).
        \end{equation*}
        We may find that:
        \begin{equation*}
            \lambda(\mathbf{X})=\left(\frac{\sum_{i=1}^{n}(X_{i}-\theta_{0})^{2}}{\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}}\right)^{-\frac{n}{2}}=\left(1+\frac{\sum_{i=1}^{n}(\overline{X}-\theta_{0})^{2}}{\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}}\right)^{-\frac{n}{2}}=\left(1+\frac{F}{n-1}\right)^{-\frac{n}{2}}.
        \end{equation*}
        We can now modify the problem to:
        \begin{equation*}
            \lambda(\mathbf{X})\leq k\implies F\geq(n-1)\left(k^{-\frac{2}{n}}-1\right)=K'.
        \end{equation*}
        Finally, we find that:
        \begin{equation*}
            \alpha=\prob(\lambda(\mathbf{X})\leq k)=\prob(F\geq K').
        \end{equation*}
        We can find that $K'=f_{\alpha,(1,n-1)}$. Therefore, we reject $H_{0}$ if $F\geq f_{\alpha,(1,n-1)}$.
    \end{eg}
    \newpage

    For tests with large $n$, if the large-$n$ results such as the CLT can be used for the point estimator, then we can easily draw the conclusion.
    \begin{eg}
        Consider the MLE $\hat{\theta}_{n}(\mathbf{X})$. Similar to Remark \ref{Chapter 2 (Remark) Cases when Fisher Information cannot be determined easily}, we can use:
        \begin{equation*}
            T_{1}=\sqrt{-l''(\hat{\theta}_{n})}(\hat{\theta}_{n}(\mathbf{X})-\theta_{0})\to\N(0,1).
        \end{equation*}
        For $H_{1}:\theta>\theta_{0}$, we reject $H_{0}$ at a significance level of $\alpha$ if the actual value of $T_{1}>z_{\alpha}$.
        
        For $H_{1}:\theta<\theta_{0}$, we reject $H_{0}$ at a significance level of $\alpha$ if the actual value of $T_{1}<z_{\alpha}$.
    \end{eg}
    However, for the likelihood ratio test, it is a bit more complicated. We only consider the cases with one unknown parameter for two-sided tests. Since the parameter space is restricted ($\theta\geq\theta_{0}$ or $\theta\leq\theta_{0}$), the result for the large-sample likelihood ratio test has to be further adjusted. We omit it due to its complexity.
    \begin{defn}
        The \textbf{Large-Sample Likelihood Ratio Test Statistic} is defined by:
        \begin{equation*}
            X_{L}=-2\ln{\lambda(\mathbf{X})}=2\left[l\left(\hat{\theta}_{n}(\mathbf{X})\right)-l(\theta_{0})\right].
        \end{equation*}
    \end{defn}
    \begin{thm}
        Under $H_{0}$, the large-sample likelihood ratio test statistic follows an asymptotic $\chi^{2}(1)$. Thus, we reject $H_{0}$ at a significance level of $\alpha$ when:
        \begin{equation*}
            \mathbf{x}_{L}>\chi_{\alpha,1}^{2},
        \end{equation*}
        which is the $(1-\alpha)$-th quantile of the chi-square distribution with $1$ degree of freedom.
    \end{thm}
    \begin{rem}
        $X_{L}$ and $T_{1}^{2}$ are asymptotically equivalent for two-sided tests.
    \end{rem}
    \begin{eg}
        Suppose the following data is from $\Exp(\lambda)$:
        \begin{equation*}
            1,3,5,8,10,15,18,19,22,25.
        \end{equation*}
        Perform a likelihood ratio test with $H_{0}:\lambda=0.06$ against $H_{1}:\lambda\neq 0.06$ and draw a conclusion at a significance level of $0.05$. Use the fact that $\chi_{0.95,1}^{2}=3.841459$.
        
        We have:
        \begin{equation*}
            l(\lambda)=n\ln{\lambda}-\lambda\sum_{i=1}^{n}x_{i}.
        \end{equation*}
        Over $\Theta=(0,\infty)$, the MLE for $\lambda$ is $\hat{\lambda}_{n}=\frac{1}{\overline{x}}$. Therefore,
        \begin{equation*}
            l(\hat{\lambda}_{n})=-n\ln{\overline{x}}-n=-35.33697.
        \end{equation*}
        Since $H_{0}$ is simple, we have:
        \begin{equation*}
            l(\hat{\lambda}_{0})=n\ln(0.06)-0.06(126)=-35.69411.
        \end{equation*}
        Therefore, the actual value of $X_{L}$ is $2(-35.33697+35.69411)=0.7143\leq\chi_{0.95,1}^{2}$. We do not reject $H_{0}$ at $\alpha=0.05$.
    \end{eg}
    \newpage
    
\section{Power Function and Power of a Test Statement}
    In parameter estimation, we have many point estimators to estimate unknown parameters. In hypothesis testing, we can also use different test statistics to construct tests for testing $H_{0}$ against $H_{1}$. Which one of them is the best? We need a quantity for comparison.
    \begin{defn}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$. For a test, the \textbf{power function} $Q:\Theta\to[0,1]$ is defined for $\theta\in\Theta$ as:
        \begin{equation*}
            Q(\theta)=\begin{cases}
                \sum_{\mathbf{x}\in C_{1}}p_{\mathbf{X}}(\mathbf{x}|\theta), &\text{Discrete case,}\\
                \int_{C_{1}}f_{\mathbf{X}}(\mathbf{x}|\theta)\,d\mathbf{x}, &\text{Continuous case.}
            \end{cases}
        \end{equation*}
    \end{defn}
    \begin{rem}
        The power function of a test is the probability of rejecting $H_{0}$. In particular, for $\theta\in\Theta_{1}$, $Q(\theta)=1-\beta(\theta)$ is called the \textbf{power of the test at $\theta$}, which is the probability of rejecting $H_{0}$ at $\theta\in\Theta_{1}$.
    \end{rem}
    \begin{rem}
        In terms of the power function, our goal is to find a test for which the value of the power at $\theta\in\Theta_{1}$ is as large as possible, subject to the condition that:
        \begin{equation*}
            \max_{\theta\in\Theta_{0}}Q(\theta)=\alpha.
        \end{equation*}
    \end{rem}
    Given two tests, we would first require them to have the same significance level $\alpha$. How do we compare them?
    \begin{defn}
        A test is said to be \textbf{more powerful} at a value $\theta^{*}\in\Theta_{1}$ if it has a higher power at $\theta^{*}$.
        
        A test is said to be the \textbf{most powerful} at $\theta^{*}$ if it is more powerful than any other test at $\theta^{*}$.
        
        A test is said to be \textbf{uniformly most powerful} (UMP) if it is the most powerful for all $\theta\in\Theta_{1}$. More precisely, the UMP test at a significance level $\alpha$ is the test with a power function $Q(\theta)$ satisfying:
        \begin{enumerate}
            \item $\max_{\theta\in\Theta_{0}}Q(\theta)=\alpha,$
            \item $Q(\theta)\geq Q^{*}(\theta)$ for all $\theta\in\Theta_{1}$ for any test with $Q^{*}(\theta).$
        \end{enumerate}
    \end{defn}
    \begin{rem}
        The UMP test in hypothesis testing has a similar role to the best estimator under the criterion of MSE in parameter estimation.
    \end{rem}
    \begin{rem}
        In general, a UMP test does not exist for two-sided tests. To address this, we usually consider a smaller class, which is the class of unbiased estimators.
    \end{rem}
    We first consider how to find the UMP test for simple tests.

    For testing $H_{0}:\theta=\theta_{0}$ against $H_{1}:\theta=\theta_{1}$ at a significance level $\alpha=\gamma(\theta_{0})=Q(\theta_{0})$ and its power $Q(\theta_{1})=1-\beta(\theta_{1})$, we have the Neyman-Pearson Lemma.
    \begin{lem}\named{Neyman-Pearson Lemma}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a PDF $f(\mathbf{x}|\theta)$ or a PMF $p(\mathbf{x}|\theta)$, where $\theta\in\Theta=\{\theta_{0},\theta_{1}\}$ and $\mathbf{x}$ is its realization. Then, at a significance level $\alpha$, a test with a rejection region:
        \begin{equation*}
            C_{1}=\biggl\{\mathbf{x}:\frac{L(\theta_{0})}{L(\theta_{1})}\leq k\biggr\}
        \end{equation*}
        is the UMP test for testing $H_{0}:\theta=\theta_{0}$ against $H_{1}:\theta=\theta_{1}$, at a significance level $\alpha$, where $k>0$.
    \end{lem}
    \begin{thm}
        The likelihood ratio test for a simple test is a UMP test.
    \end{thm}
    \begin{proofing}
        For a simple test, the LRT at a significance level of $\alpha$ has a rejection region:
        \begin{equation*}
            C_{1}=\{\mathbf{x}:\lambda(\mathbf{x})\leq k<1\},
        \end{equation*}
        where $\prob(\lambda(\mathbf{X})\leq k|\theta_{0})=\alpha$ and:
        \begin{equation*}
            \lambda(\mathbf{x})=\frac{L(\theta_{0})}{\max\{L(\theta_{0}),L(\theta_{1})\}}=\begin{cases}
                1, &\text{if }L(\theta_{0})\geq L(\theta_{1}),\\
                \frac{L(\theta_{0})}{L(\theta_{1})}, &\text{if }L(\theta_{0})<L(\theta_{1}).
            \end{cases}
        \end{equation*}
        Therefore, we have:
        \begin{equation*}
            C_{1}=\{\mathbf{x}:\lambda(\mathbf{x})\leq k<1\}=\biggl\{\mathbf{x}:\frac{L(\theta_{0})}{L(\theta_{1})}\leq k<1\biggr\}.
        \end{equation*}
        By the Neyman-Pearson Lemma, the LRT is a UMP test for a simple test.
    \end{proofing}
    \begin{eg}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\N(\theta,\sigma_{0}^{2})$, where $\theta$ is unknown but $\sigma_{0}^{2}$ is known. We want to construct a UMP test for testing $H_{0}:\theta=\theta_{0}$ against $H_{1}:\theta=\theta_{1}$ at a significance level of $\alpha$, where $\theta_{0}<\theta_{1}$. Note that:
        \begin{equation*}
            \frac{L(\theta_{0})}{L(\theta_{1})}=\exp\left(\frac{n[\theta_{1}^{2}-\theta_{0}^{2}-2\overline{x}(\theta_{1}-\theta_{0})]}{2\sigma_{0}^{2}}\right).
        \end{equation*}
        Thus, we can modify the rejection region into:
        \begin{equation*}
            C_{1}=\biggl\{\mathbf{x}:\frac{L(\theta_{0})}{L(\theta_{1})}\leq k\biggr\}=\biggl\{\mathbf{x}:\overline{x}\geq \frac{1}{2}(\theta_{0}+\theta_{1})-\frac{\sigma_{0}^{2}\ln{k}}{n(\theta_{1}-\theta_{0})}=K\biggr\}.
        \end{equation*}
        Since $\overline{X}\sim\N(\theta,\frac{\sigma_{0}^{2}}{n})$, we can determine $K$ by:
        \begin{equation*}
            \alpha=\prob(\overline{X}\geq K|\theta_{0})=\prob\left(Z\geq\frac{\sqrt{n}(K-\theta_{0})}{\sigma_{0}}\right).
        \end{equation*}
        We have that $K=\theta+z_{\alpha}\frac{\sigma_{0}}{\sqrt{n}}$. Therefore, by the Neyman-Pearson Lemma, the UMP test for testing $H_{0}:\theta=\theta_{0}$ against $H_{1}:\theta=\theta_{1}$ at a significance level of $\alpha$ is the test with a rejection region:
        \begin{equation*}
            C_{1}=\biggl\{\mathbf{x}:\overline{x}\geq\theta_{0}+z_{\alpha}\frac{\sigma_{0}}{\sqrt{n}}\biggr\},
        \end{equation*}
        where $\theta_{0}<\theta_{1}$.
    \end{eg}
    The Neyman-Pearson Lemma only provides us with a way of constructing a UMP test for a simple test. For UMP one-sided tests, we need another result. Without loss of generality, we only discuss how to find the UMP test for a one-sided right test.
    \begin{defn}
        A distribution has the property of \textbf{monotone likelihood ratio} (MLR) in $T$ if the likelihood ratio $\frac{L(\theta')}{L(\theta'')}$ is non-decreasing in $T$ for $\theta'>\theta''$, where at least one of $L(\theta')$ and $L(\theta'')$ is positive.
    \end{defn}
    \begin{eg}
        Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Bern(\theta)$, where $\theta\in(0,1)$ is unknown. For $\theta'>\theta''$, the likelihood ratio:
        \begin{equation*}
            \frac{L(\theta')}{L(\theta'')}=\left(\frac{1-\theta'}{1-\theta''}\right)^{n}\left(\frac{\theta'(1-\theta'')}{\theta''(1-\theta')}\right)^{\sum_{i=1}^{n}x_{i}}
        \end{equation*}
        is non-decreasing in $T=\sum_{i=1}^{n}x_{i}$ because $\frac{\theta'(1-\theta'')}{\theta''(1-\theta')}>1$. Therefore, MLR holds for $\Bern(\theta)$ in $T=\sum_{i=1}^{n}x_{i}$.
    \end{eg}
    \begin{eg}
        \label{Chapter 4 (Example) MLR for N(mu,sigma^2)}
        Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\N(\theta,\sigma_{0}^{2})$, where $\theta$ is unknown but $\sigma_{0}^{2}$ is known. For $\theta'>\theta''$, the likelihood ratio:
        \begin{equation*}
            \frac{L(\theta')}{L(\theta'')}=\exp\left(\frac{n[(\theta'')^{2}-(\theta')^{2}-2\overline{x}(\theta''-\theta')]}{2\sigma_{0}^{2}}\right)
        \end{equation*}
        is non-decreasing in $T=\overline{x}$. Therefore, MLR holds for $\N(\theta,\sigma_{0}^{2})$ in $T=\overline{x}$.
    \end{eg}
    We now have some theorems that we can use to obtain the UMP one-sided right test.
    \begin{thm}\named{Karlin-Rubin Theorem}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a distribution with a parameter $\theta$ having MLR in $T(\mathbf{x})$, where $\mathbf{x}$ is the realization of $\mathbf{X}$. At a significance level of $\alpha$, a test with a rejection region:
        \begin{equation*}
            C_{1}=\{\mathbf{x}:T(\mathbf{x})\geq K\}
        \end{equation*}
        is a UMP one-sided right test for $H_{0}:\theta\leq\theta_{0}$ against $H_{1}:\theta>\theta_{0}$ at a significance level $\alpha$ for some $K$. The test is also a UMP one-sided right test for $H_{0}:\theta=\theta_{0}$ against $H_{1}:\theta>\theta_{0}$.
    \end{thm}
    \newpage
    
    \begin{thm}\named{Karlin-Rubin Theorem with sufficient statistic}
        \label{Chapter 4 (Theorem) Karlin-Rubin Theorem for sufficient statistic}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample from a distribution with a parameter $\theta$, and let $S(\mathbf{X})$ be a sufficient statistic for $\theta$. If the distribution of $S(\mathbf{X})$ has MLR in itself, then at a significance level $\alpha$, a test with a rejection region:
        \begin{equation*}
            C_{1}=\{\mathbf{x}:S(\mathbf{x})\geq K\}
        \end{equation*}
        is a UMP one-sided right test for $H_{0}:\theta\leq\theta_{0}$ against $H_{1}:\theta>\theta_{0}$ at a significance level $\alpha$ for some $K$. The test is also a UMP one-sided right test for $H_{0}:\theta=\theta_{0}$ against $H_{1}:\theta>\theta_{0}$.
    \end{thm}
    \begin{eg}
        Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\N(\theta,\sigma_{0}^{2})$, where $\theta$ is unknown but $\sigma_{0}^{2}$ is known. We aim to construct a UMP test for testing at a significance level $\alpha$:
        \begin{equation*}
            \begin{cases}
                H_{0}: &\theta\leq\theta_{0},\\
                H_{1}: &\theta>\theta_{0}.
            \end{cases}
        \end{equation*}
        The MLE of $\theta$ over $\Theta_{0}$ is $\min\{\overline{x},\theta_{0}\}$. Therefore,
        \begin{align*}
            \lambda(\mathbf{x})&=\begin{cases}
                1, &\text{if }\overline{x}\leq\theta_{0},\\
                \exp\left(-\frac{1}{2\sigma_{0}^{2}}\left[\sum_{i=1}^{n}(x_{i}-\theta_{0})^{2}-\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}\right]\right), &\text{if }\overline{x}>\theta_{0}.
            \end{cases}\\
            &=\begin{cases}
                1, &\text{if }\overline{x}\leq\theta_{0},\\
                \exp\left(-\frac{n(\overline{x}-\theta_{0})^{2}}{2\sigma_{0}^{2}}\right), &\text{if }\overline{x}>\theta_{0}.
            \end{cases}
        \end{align*}
        We find that the rejection region is:
        \begin{equation*}
            C_{1}=\biggl\{\mathbf{x}:\overline{x}>\theta_{0}\text{ and }\exp\left(-\frac{n(\overline{x}-\theta_{0})^{2}}{2\sigma_{0}^{2}}\right)\leq k<1\biggr\}=\biggl\{\mathbf{x}:\overline{x}\geq\theta_{0}+\sqrt{-\frac{2\sigma_{0}^{2}}{n}\ln{k}}=K'\biggr\}.
        \end{equation*}
        Since $\overline{X}\sim\N(\theta,\frac{\sigma_{0}^{2}}{n})$, we can easily find that $K'=\theta_{0}+z_{\alpha}\frac{\sigma_{0}}{\sqrt{n}}$.
        
        From Example \ref{Chapter 4 (Example) MLR for N(mu,sigma^2)}, we have found that MLR holds for $\N(\theta,\sigma_{0}^{2})$ in $T=\overline{x}$. By the Karlin-Rubin Theorem, a test with a rejection region:
        \begin{equation*}
            C_{1}=\{\mathbf{x}:\overline{x}\geq K'\}
        \end{equation*}
        is a UMP one-sided right test for testing $H_{0}:\theta\leq\theta_{0}$ against $H_{1}:\theta>\theta_{0}$ at a significance level of $\alpha$. Therefore, we have found the UMP one-sided test.
    \end{eg}
    Recall the exponential family; it is also very useful in hypothesis testing.
    \begin{cor}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a distribution belonging to a one-parameter exponential family in the form:
        \begin{equation*}
            \exp[a(\theta)+b(x)+c(\theta)d(x)].
        \end{equation*}
        For the test at a significance level $\alpha$ of:
        \begin{equation*}
            \begin{cases}
                H_{0}: &\theta\leq\theta_{0},\\
                H_{1}: &\theta>\theta_{0},
            \end{cases}\text{ or }\begin{cases}
                H_{0}: &\theta=\theta_{0},\\
                H_{1}: &\theta>\theta_{0},
            \end{cases}
        \end{equation*}
        the test with a rejection region:
        \begin{enumerate}
            \item for an increasing function $c(\theta)$,
            \begin{equation*}
                C_{1}=\biggl\{\mathbf{x}:\sum_{i=1}^{n}d(x_{i})\geq K\biggr\},
            \end{equation*}
            \item for a decreasing function $c(\theta)$,
            \begin{equation*}
                C_{1}=\biggl\{\mathbf{x}:\sum_{i=1}^{n}d(x_{i})\leq K\biggr\},
            \end{equation*}
        \end{enumerate}
        is the UMP test at a significance level $\alpha$ for some $K$.
    \end{cor}
    \newpage
    
    \begin{proofing}
        For $\theta'>\theta''$, the likelihood ratio is:
        \begin{equation*}
            \frac{L(\theta')}{L(\theta'')}=\exp\left[n(a(\theta')-a(\theta''))+(c(\theta')-c(\theta''))\sum_{i=1}^{n}d(x_{i})\right].
        \end{equation*}
        If $c(\theta)$ is increasing, then $c(\theta')-c(\theta'')>0$. We find that $\frac{L(\theta')}{L(\theta'')}$ is non-decreasing in $\sum_{i=1}^{n}d(x_{i})$. Therefore, by the Karlin-Rubin Theorem, a test with a rejection region:
        \begin{equation*}
            C_{1}=\biggl\{\mathbf{x}:\sum_{i=1}^{n}d(x_{i})\geq K\biggr\}
        \end{equation*}
        is a UMP test at a significance level $\alpha$ for some $K$.
        
        If $c(\theta)$ is decreasing, then $c(\theta'')-c(\theta')<0$. We find that $\frac{L(\theta')}{L(\theta'')}$ is non-decreasing in $-\sum_{i=1}^{n}d(x_{i})$. Therefore, by the Karlin-Rubin Theorem, a test with a rejection region:
        \begin{equation*}
            C_{1}=\biggl\{\mathbf{x}:-\sum_{i=1}^{n}d(x_{i})\geq K'\biggr\}=\biggl\{\mathbf{x}:\sum_{i=1}^{n}d(x_{i})\leq -K'=K\biggr\}
        \end{equation*}
        is a UMP test at a significance level $\alpha$ for some $K$.
    \end{proofing}
    \begin{eg}
        Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\U[0,\theta]$, where $\theta>0$ is unknown. We want to construct a UMP test for testing:
        \begin{equation*}
            \begin{cases}
                H_{0}: &\theta\leq\theta_{0},\\
                H_{1}: &\theta>\theta_{0},
            \end{cases}
        \end{equation*}
        at a significance level $\alpha$, where $\theta_{0}>0$. From Example \ref{Chapter 3 (Example) Sufficient statistic for U[0,theta]}, we have found that $X_{(n)}$ is sufficient for $\theta$ with PDF:
        \begin{equation*}
            f_{X_{(n)}}(y|\theta)=\frac{ny^{n-1}}{\theta^{n}}\mathbf{1}_{y\leq\theta}.
        \end{equation*}
        For $\theta'>\theta''$, since it only has one term, we find the likelihood ratio of itself:
        \begin{equation*}
            \frac{L(\theta')}{L(\theta'')}=\frac{f_{X_{(n)}}(y|\theta')}{f_{X_{(n)}}(y|\theta'')}=\begin{cases}
                \left(\frac{\theta''}{\theta'}\right)^{n}<1, &\text{if } y\leq\theta'',\\
                \infty, &\text{if } \theta''<y\leq\theta'.
            \end{cases}
        \end{equation*}
        This is non-decreasing in itself. Note that we only consider $y\leq\theta'$ since both $L(\theta')$ and $L(\theta'')$ would be zero if $y>\theta'$. Therefore, MLR holds in $X_{(n)}$ itself. By Theorem \ref{Chapter 4 (Theorem) Karlin-Rubin Theorem for sufficient statistic}, a test with a rejection region:
        \begin{equation*}
            C_{1}=\{\mathbf{x}:x_{(n)}\geq K\}
        \end{equation*}
        is a UMP test for testing $H_{0}:\theta\leq\theta_{0}$ against $H_{1}:\theta>\theta_{0}$ at a significance level $\alpha$. To find $K$, we consider:
        \begin{align*}
            \alpha&=\max_{\theta\in\Theta_{0}}\prob(X_{(n)}\geq K)\\
            &=\max_{\theta\in\Theta_{0}}\int_{K}^{\theta}\frac{ny^{n-1}}{\theta^{n}}\,dy\\
            &=\max_{\theta\in\Theta_{0}}\left[1-\left(\frac{K}{\theta}\right)^{n}\right]\\
            &=1-\left(\frac{K}{\theta_{0}}\right)^{n}.
        \end{align*}
        Therefore, we find that $K=\theta_{0}\left(1-\alpha\right)^{\frac{1}{n}}$.
    \end{eg}

\appendix
\renewcommand{\thechapter}{\Alph{chapter}}
\chapter{Over-simplified Summary}
    \begin{thm}\named{Weak Law of Large Numbers (WLLN)}
        Let $\{X_{n}\}$ be a sequence of i.i.d. random variables. Let $\E(X_{i})=\mu$ for all $i=1,2,\cdots$. As $n\to\infty$, we have:
        \begin{equation*}
            \overline{X}\xrightarrow{D}\mu
        \end{equation*}
    \end{thm}
    \begin{thm}\named{Central Limit Theorem (CLT)}
        Let $\{X_{n}\}$ be a sequence of i.i.d. random variables whose MGFs exist on a neighborhood of $0$. Let $\E(X_{i})=\mu$ and $\Var(X_{i})=\sigma^{2}>0$ for all $i=1,2,\cdots$. As $n\to\infty$, we have:
        \begin{equation*}
            \frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}=\frac{\sum_{i=1}^{n}X_{i}-n\mu}{\sqrt{n}\sigma}\xrightarrow{D}\N(0,1)
        \end{equation*}
    \end{thm}
    \begin{thm}\named{L\'evy-Linderberg Central Limit Theorem} 
        Let $\{X_{n}\}$ be a sequence of i.i.d. random variables with common population mean $\mu$ and population variance $\sigma^{2}$. Assume that $0<\sigma^{2}<\infty$. As $n\to\infty$,
        \begin{equation*}
            \frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}=\frac{\sum_{i=1}^{n}X_{i}-n\mu}{\sqrt{n}\sigma}\xrightarrow{D}\N(0,1)
        \end{equation*}
    \end{thm}
    \begin{thm}\named{Slutsky's Theorem}
        If $X_{n}\xrightarrow{D}X$ and $Y_{n}\xrightarrow{\prob}c$ for some constant $c$, then:
        \begin{enumerate}
            \item $X_{n}+Y_{n}\xrightarrow{D}X+c$
            \item $X_{n}Y_{n}\xrightarrow{D}cX$
            \item $\frac{X_{n}}{Y_{n}}\xrightarrow{D}\frac{X}{c}$ if $c\neq 0$
        \end{enumerate}
    \end{thm}
    \begin{thm}\named{Continuous Mapping Theorem}
        Let $\{X_{n}\}$ be a sequence of random variables and $X$ be a random variable. Suppose there is a function $g$ with a set of discontinuity points $D_{g}$ such that $\prob(X\in D_{g})=0$. We have:
        \begin{enumerate}
            \item If $X_{n}\xrightarrow{D}X$, then $g(X_{n})\xrightarrow{D}g(X)$.
            \item If $X_{n}\xrightarrow{\prob}X$, then $g(X_{n})\xrightarrow{\prob}g(X)$.
        \end{enumerate}
    \end{thm}
    \begin{thm}\named{Delta Method}
        Let $\{X_{n}\}$ be a sequence of random variables such that for constants $a$ and $b>0$, as $n\to\infty$,
        \begin{equation*}
            \sqrt{n}(X_{n}-a)\xrightarrow{D}\N(0,b^{2})
        \end{equation*}
        Then for a given function $g$, suppose that $g'(a)$ exists and is not $0$. As $n\to\infty$:
        \begin{equation*}
            \sqrt{n}(g(X_{n})-g(a))\xrightarrow{D}\N(0,[g'(a)b]^{2})
        \end{equation*}
        In particular, if $\{X_{n}\}$ is a random sample of size $n$ from a distribution with a finite mean $\mu$ and variance $\sigma^{2}>0$, such that $g'(\mu)$ exists and is not $0$, then as $n\to\infty$,
        \begin{equation*}
            \sqrt{n}(g(\overline{X})-g(\mu))\xrightarrow{D}\N(0,[g'(\mu)\sigma]^{2})
        \end{equation*}
    \end{thm}
    \begin{thm}
        A sequence of MMEs $\{\tilde{\theta}_{n}\in\mathbb{R}^{k}\}$ is consistent, asymptotically unbiased for $\theta$, and asymptotically normally distributed. More precisely, under certain assumptions like $\E\abs{X}^{2k}<\infty$, as $n\to\infty$,
        \begin{equation*}
            \sqrt{n}(\tilde{\theta}_{n}-\theta)\xrightarrow{D}\N_{k}(\mathbf{0},\mathbf{GHG}^{T})
        \end{equation*}
        where $\mathbf{G}$ is a $k\times k$ matrix with $\pdv{g_{i}}{\mu_{j}'}$ as its $(i,j)$-th entry, and $\mathbf{H}$ is a $k\times k$ matrix with $\mu_{i+j}'-\mu_{i}'\mu_{j}'$ as its $(i,j)$-th entry, for $i=1,\cdots,k$ and $j=1,\cdots,k$.
    \end{thm}
    \begin{thm}
        A sequence of MLEs $\{\hat{\theta}_{n}\in\mathbb{R}^{k}\}$ is consistent, asymptotically unbiased for $\theta$, asymptotically efficient, and asymptotically normally distributed. More precisely, under regularity assumptions, as $n\to\infty$,
        \begin{equation*}
            \sqrt{n}(\hat{\theta}_{n}-\theta)\xrightarrow{D}\N_{k}(\mathbf{0},\mathcal{I}_{X}^{-1}(\theta))
        \end{equation*}
        where $\mathcal{I}_{X}(\theta)$ is the $k\times k$ Fisher Information matrix with the $(i,j)$-th entry defined as:
        \begin{equation*}
            \begin{cases}
                \E\left[\left(\pdv*{\ln{f_{X}(X|\theta)}}{\theta_{i}}\right)\left(\pdv*{\ln{f_{X}(X|\theta)}}{\theta_{j}}\right)\right], &\text{Continuous case}\\
                \E\left[\left(\pdv*{\ln{p_{X}(X|\theta)}}{\theta_{i}}\right)\left(\pdv*{\ln{p_{X}(X|\theta)}}{\theta_{j}}\right)\right], &\text{Discrete case}
            \end{cases}
        \end{equation*}
        for $i=1,\cdots,k$ and $j=1,\cdots,k$.
    \end{thm}
    \begin{thm}\named{Fisher-Neyman Factorization Theorem}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$. A set of statistics $\{S_{1},\cdots,S_{r}\}$, where $r\geq k$ and $S_{i}=S_{i}(\mathbf{X})$ for $i=1,\cdots,r$, is jointly sufficient if and only if:
        \begin{equation*}
            \begin{cases}
                p_{\mathbf{X}}(\mathbf{x}|\theta)=g(S_{1}(\mathbf{x}),\cdots,S_{r}(\mathbf{x})|\theta)h(\mathbf{x}), &\text{Discrete case}\\
                f_{\mathbf{X}}(\mathbf{x}|\theta)=g(S_{1}(\mathbf{x}),\cdots,S_{r}(\mathbf{x})|\theta)h(\mathbf{x}), &\text{Continuous case}
            \end{cases}
        \end{equation*}
        where $g$ is a non-negative function of $x_{1},\cdots,x_{n}$ only through the statistics $S_{1},\cdots,S_{r}$ and depends on $\theta$, and $h$ is a non-negative function of $x_{1},\cdots,x_{n}$ not depending on $\theta$.
    \end{thm}
    \begin{thm}\named{Rao-Blackwell Theorem}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$, and let $\{S_{1},\cdots,S_{r}\}$ be a set of jointly sufficient statistics, where $r\geq k$ and $S_{i}=S_{i}(\mathbf{X})$ for $i=1,\cdots,r$. Suppose that $T=T(\mathbf{X})$ is an unbiased estimator for $g(\theta)$ for some function $g$. Then:
        \begin{enumerate}
            \item $\E(T|S_{1},\cdots,S_{r})$ is a statistic and is a function of the jointly sufficient statistics.
            \item $\E(T|S_{1},\cdots,S_{r})$ is unbiased for $g(\theta)$.
            \item $\Var(\E(T|S_{1},\cdots,S_{r}))\leq\Var(T)$.
        \end{enumerate}
    \end{thm}
    \begin{thm}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a distribution in a one-parameter exponential family with a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}$, in the form:
        \begin{equation*}
            \exp[a(\theta)+b(x)+c(\theta)d(x)].
        \end{equation*}
        Then, $\sum_{i=1}^{n}d(X_{i})$ is a complete and minimal sufficient statistic.
    \end{thm}
    \begin{thm}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a distribution in a one-parameter exponential family with a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$, in the form:
        \begin{equation*}
            \exp\left(a(\theta)+b(x)+\sum_{j=1}^{k}c_{j}(\theta)d_{j}(x)\right).
        \end{equation*}
        Then, the set $\{\sum_{i=1}^{n}d_{1}(X_{i}),\cdots,\sum_{i=1}^{n}d_{k}(X_{i})\}$ is a set of jointly complete and minimal sufficient statistics.
    \end{thm}
    \begin{thm}\named{Lehmann-Scheff\'e Theorem}
        Let $CS$ be a complete and (minimal) sufficient statistic. If there exists a function $h(CS)$ that is unbiased for $g(\theta)$, then $h(CS)$ is the unique UMVUE of $g(\theta)$. In particular, if $\E[f(\mathbf{X})]=g(\theta)$, then $h(CS)=\E[f(\mathbf{X})|CS]$ is the UMVUE for $g(\theta)$.
    \end{thm}
    \begin{thm}\named{Cram\'er-Rao Inequality}
        Under the regularity conditions, the variance of an unbiased estimator $T(\mathbf{X})$ for $\theta$, based on a set of random variables $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ from their joint PDF $f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)$, satisfies the following inequality:
        \begin{equation*}
            \Var(T(\mathbf{X}))\geq\frac{1}{\mathcal{I}_{X_{1},\cdots,X_{n}}(\theta)}=\frac{1}{\E\left[\odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}\right]^{2}},
        \end{equation*}
        with the lower bound being the Cram\'er-Rao lower bound.
        
        If $T(\mathbf{X})$ is an unbiased estimator for $g(\theta)$, then it becomes:
        \begin{equation*}
            \Var(T(\mathbf{X}))\geq\frac{\left[\odv*{g(\theta)}{\theta}\right]^{2}}{\mathcal{I}_{X_{1},\cdots,X_{n}}(\theta)}=\frac{\left[\odv*{g(\theta)}{\theta}\right]^{2}}{\E\left[\odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}\right]^{2}}.
        \end{equation*}
        
        Equality holds if and only if:
        \begin{equation*}
            \odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}=A(\theta,n)[T(X_{1},\cdots,X_{n})-g(\theta)],
        \end{equation*}
        where $A(\theta,n)$ is a non-zero function. Thus, $T(X_{1},\cdots,X_{n})$ would be the UMVUE of $g(\theta)$.
    \end{thm}
    \begin{thm}
        Under $H_{0}$, the large-sample likelihood ratio test statistic follows an asymptotic $\chi^{2}(1)$. We reject $H_{0}$ at a significance level $\alpha$ when:
        \begin{equation*}
            \mathbf{x}_{L}>\chi^{2}_{\alpha,1}.
        \end{equation*}
    \end{thm}
    \begin{lem}\named{Neyman-Pearson Lemma}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a PDF $f(\mathbf{x}|\theta)$ or a PMF $p(\mathbf{x}|\theta)$, where $\theta\in\Theta=\{\theta_{0},\theta_{1}\}$ and $\mathbf{x}$ is its realization. Then, at a significance level $\alpha$, a test with a rejection region:
        \begin{equation*}
            C_{1}=\biggl\{\mathbf{x}:\frac{L(\theta_{0})}{L(\theta_{1})}\leq k\biggr\},
        \end{equation*}
        is the UMP test for testing $H_{0}:\theta=\theta_{0}$ against $H_{1}:\theta=\theta_{1}$, at a significance level $\alpha$, where $k>0$.
    \end{lem}
    \begin{thm}\named{Karlin-Rubin Theorem}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a distribution with a parameter $\theta$ having MLR in $T(\mathbf{x})$. At a significance level of $\alpha$, a test with a rejection region:
        \begin{equation*}
            C_{1}=\{\mathbf{x}:T(\mathbf{x})\geq K\},
        \end{equation*}
        is a UMP one-sided right test for:
        \begin{equation*}
            \begin{cases}
                H_{0}: &\theta\leq\theta_{0},\\
                H_{1}: &\theta>\theta_{0},
            \end{cases}\text{ or }\begin{cases}
                H_{0}: &\theta=\theta_{0},\\
                H_{1}: &\theta>\theta_{0},
            \end{cases}
        \end{equation*}
        at a significance level $\alpha$ for some $K$.
    \end{thm}
    \begin{thm}\named{Karlin-Rubin Theorem with sufficient statistic}
        Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample from a distribution with a parameter $\theta$, and let $S(\mathbf{X})$ be a sufficient statistic for $\theta$. If the distribution of $S(\mathbf{X})$ has MLR in itself, then at a significance level $\alpha$, a test with a rejection region:
        \begin{equation*}
            C_{1}=\{\mathbf{x}:S(\mathbf{x})\geq K\},
        \end{equation*}
        is a UMP one-sided right test for:
        \begin{equation*}
            \begin{cases}
                H_{0}: &\theta\leq\theta_{0},\\
                H_{1}: &\theta>\theta_{0},
            \end{cases}\text{ or }\begin{cases}
                H_{0}: &\theta=\theta_{0},\\
                H_{1}: &\theta>\theta_{0},
            \end{cases}
        \end{equation*}
        at a significance level $\alpha$ for some $K$.
    \end{thm}
\end{document}