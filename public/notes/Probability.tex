\documentclass{huhtakm-template-book}
\usepackage{enumitem}
\newcommand{\independent}{\perp\!\!\!\perp}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\expect}{\mathbb{E}}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\NBin}{NBin}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\Poisson}{Poisson}
\DeclareMathOperator{\Hypergeometric}{Hypergeometric}
\DeclareMathOperator{\U}{U}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\Cauchy}{Cauchy}
\DeclareMathOperator{\Gam}{Gamma}
\DeclareMathOperator{\Beta}{Beta}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\setlength{\parindent}{0pt}
\title{
	\Huge Probability\\
	\small Version 2.0
}
\author{
	HU-HTAKM\\
	\small Website: \url{https://htakm.github.io/htakm_test/}
}
\date{
	Last major change: January 6, 2025\\
	Last small update: January 10, 2025
}
\begin{document}
\maketitle
This is a rewritten version of my lecture notes named "MATH 2431: Honor Probability". I realised that the ordering of topics is a bit of a mess and there are some missing topics that will probably be necessary for future courses.\\
My plan is to make this notes a mixture of MATH 2421 (Probability) and MATH 2431 (Honor Probability). Again, if you can find any typos, you are already pretty good at the topics or you have good eyes. ;)\\
Please note that all proofs that may be combinatorial proofs are omitted.
\begin{figure}[h]
	\begin{subfigure}{0.45\textwidth}
		\centering
		\begin{tabular}{cc}
			Notations & Meaning\\
			\hline
			$\mathbb{N}_{+}$ & Set of positive integers\\
			$\mathbb{N}$ & Set of natural numbers\\
			$\mathbb{Z}$ & Set of integers\\
			$\mathbb{Q}$ & Set of rational numbers\\
			$\mathbb{R}$ & Set of real numbers\\
			$\emptyset$ & Empty set\\
			$\Omega$ & Sample space / Entire set\\
			$\omega$ & Outcome\\
			$\mathcal{F},\mathcal{G},\mathcal{H}$ & $\sigma$-field / $\sigma$-algebra\\
			$A,B,C,\cdots$ & Events\\
			$A^{\complement}$ & Complement of events\\
			$\prob$ & Probability measure\\
			$X$ & Random variable\\
			$\mathcal{B}(\mathbb{R})$ & Borel $\sigma$-field of $\mathbb{R}$ \\
			$f_{X}$ & PMF/PDF of $X$\\
			$F_{X}$ & CDF of $X$\\
			$\mathbf{1}_{A}$ & Indicator function\\
			$\mathbb{E}$ & Expectation\\
			$\psi$ & Conditional expectation\\
			$\mathbf{u},\mathbf{v},\mathbf{w},\cdots$ & Vector\\
			$\mathbf{A},\mathbf{B},\mathbf{C},\cdots$ & Matrix\\
			$\mathbf{X}$ & Random vector\\
			$\overline{X}$ & Sample mean of $X$\\
			$S_{n-1}^{2}$ & Sample variance of $X$\\
			$G_{X}$ & Probability generating function of $X$\\
			$M_{X}$ & Moment generating function of $X$\\
			$\phi$ & CF / PDF of $X\sim\N(0,1)$\\
			$\Phi$ & CDF of $X\sim\N(0,1)$
		\end{tabular}
		\caption{Notations}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\begin{tabular}{cc}
			Abbreviations & Meaning\\
			\hline
			CDF & Cumulative distribution function\\
			JCDF & Joint cumulative distribution function\\
			PMF & Probability mass function\\
			JPMF & Joint probability mass function\\
			PDF & Probability density function\\
			JPDF & Joint probability density function\\
			PGF & Probability generating function\\
			MGF & Moment generating function\\
			JMGF & Joint moment generating function\\
			CF & Characteristic function\\
			JCF & Joint characteristic function\\
			i.i.d. & independent and identically distributed\\
			WLLN & Weak Law of Large Numbers\\
			SLLN & Strong Law of Large Numbers\\
			CLT & Central Limit Theorem\\
			BCI & Borel-Cantelli Lemma I\\
			BCII & Borel-Cantelli Lemma II\\
			i.o. & infinitely often\\
			f.o. & finitely often\\
			a.s. & almost surely
		\end{tabular}
		\caption{Abbreviations}
	\end{subfigure}
\end{figure}
\begin{defn}
	This is definition.
\end{defn}
\begin{rem}
	This is remark.
\end{rem}
\begin{lem}
	This is lemma.
\end{lem}
\begin{prop}
	This is proposition.
\end{prop}
\begin{thm}
	This is theorem.
\end{thm}
\begin{cla}
	This is claim.
\end{cla}
\begin{cor}
	This is corollary.
\end{cor}
\begin{eg}
	This is example.
\end{eg}
\tableofcontents
\chapter{Combinatorial Analysis}
\section{Probabilities}
In our life, we mostly believe that the future is largely unpredictable. We express this belief in chance behaviour and assign quantitative and qualitative meanings to its usages. Therefore, we create the concept of "probability", which tries to create a numerical descriptions of how likely an event would occur.
\begin{defn}
	\textbf{Probability} is a numerical measurement of how likely an event would occur.
\end{defn}
If we want to determine the probability, we can use random experiments.
\begin{defn}
	\textbf{Experiment} is a process that has a random outcome.
\end{defn}
\begin{eg}
	Example of an experiment:
	\begin{enumerate}
		\item Randomly picking a number from $1$ to $10$
		\item Randomly toss a coin
	\end{enumerate}
\end{eg}
The most basic way to find a probability is by counting.
\begin{thm}(\textbf{Fundamental Principle of Counting})
	Suppose that $m_{i}$ represents the number of outcomes of the $i$-th event. The total number of outcomes of $n$ independent events is the product of the number of each individual event:
	\begin{equation*}
		\prod_{i=1}^{n}m_{i}
	\end{equation*} 
\end{thm}
\begin{eg}
	Assume that we choose a president and a vice-president from $30$ people. By the Fundamental Principal of Counting, the total number of possible outcomes is:
	\begin{equation*}
		30\times 29=870
	\end{equation*}
\end{eg}
Sometimes, we want to focus on how many ways we can arrange a set of objects.
\begin{defn}
	Given a set with $n$ distinct elements.
	\begin{enumerate}
		\item \textbf{Permutation} of the set is an ordered arrangement of all elements of the set.
		\item If $k\leq n$, \textbf{$k$-permutation} of the set is an ordered arrangement of $k$ elements of the set.
	\end{enumerate}
\end{defn}
\begin{rem}
	The $n$-permutation is just the regular permutation.
\end{rem}
\begin{rem}
	If we want to find the total number of permutations, we can see it as finding the number of outcomes for putting one object into the 1st position, 2nd position, and so on.
\end{rem}
\begin{eg}
	Given a set $\{1,2,3\}$.
	\begin{enumerate}
		\item The ordered arrangement $(3,1,2)$ is a permutation of the set.
		\item The ordered arrangement $(3,2)$ is a $2$-permutation of the set.
	\end{enumerate}
\end{eg}

\newpage
We have a formula to find the number of permutations.
\begin{thm}
	Let $n$ and $k$ be integers with $0\leq k\leq n$. The number of $k$-permutations of a set with $n$ distinct elements, denoted by $P_{k}^{n}$, can be obtained by:
	\begin{equation*}
		P_{k}^{n}=\frac{n!}{(n-k)!}
	\end{equation*}
\end{thm}
We also have cases when two ends of the ordered arrangement are connected together. When we want to find the number of arrangements for putting people into seats of circular table, rotating clockwise or anti-clockwise should be considered as same arrangement.
\begin{thm}
	Given a set with $n$ distinct elements. The number of arrangements of elements into the circle is:
	\begin{equation*}
		(n-1)!
	\end{equation*}
\end{thm}

\section{Combinations}
Now, assume that we don't care about the ordering of chosen objects. We only want to choose $k$ objects from $n$ objects.

Then we have the following definition.
\begin{defn}
	If $k\leq n$, \textbf{$k$-combination} of a set with $n$ distinct elements is an unordered arrangement of $k$ elements of the set.
\end{defn}
\begin{thm}
	Let $n$ and $k$ be integers with $0\leq k\leq n$. The number of $k$-combination of a set with $n$ distinct elements, denoted by $C_{k}^{n}$ or $\binom{n}{k}$, can be obtained by:
	\begin{equation*}
		C_{k}^{n}=\binom{n}{k}=\frac{n!}{k!(n-k)!}
	\end{equation*}
\end{thm}
\begin{proofing}
	We know that the number of permutations of $k$ objects is $k!$ and the number of ordered arrangement of choosing $k$ objects from $n$ objects is:
	\begin{equation*}
		P_{k}^{n}=\frac{n!}{(n-k)!}
	\end{equation*}
	We don't care about the order. The number of unordered arrangement of choosing $k$ objects from $n$ objects would be:
	\begin{equation*}
		\binom{n}{k}=\frac{P_{k}^{n}}{k!}=\frac{n!}{k!(n-k)!}
	\end{equation*}
\end{proofing}
\begin{rem}
	By convention, when $n$ is non-negative integers and $k<0$ or $k>n$, we define:
	\begin{equation*}
		\binom{n}{r}=0
	\end{equation*}
\end{rem}
We can immediately derive the following corollary.
\begin{cor}
	Let $n$ be integers. For all integers $k$ with $0\leq k\leq n$, we have:
	\begin{equation*}
		\binom{n}{k}=\binom{n}{n-k}
	\end{equation*}
\end{cor}
\begin{proofing}
	\begin{equation*}
		\binom{n}{n-k}=\frac{n!}{(n-k)!(n-n+k)!}=\frac{n!}{k!(n-k)!}=\binom{n}{k}
	\end{equation*}
\end{proofing}

\newpage
From here, we will provide some important combinatorial identities that could be very useful.
\begin{thm}(\textbf{Pascal's Identity})
	Let $n$ and $k$ be integers with $0<k<n$. Then:
	\begin{equation*}
		\binom{n}{k}=\binom{n-1}{k-1}+\binom{n-1}{k}
	\end{equation*}
\end{thm}
\begin{proofing}
	From the right-hand side, we have:
	\begin{equation*}
		\binom{n-1}{k-1}+\binom{n-1}{k}=\frac{(n-1)!}{(k-1)!(n-k)!}+\frac{(n-1)!}{k!(n-k-1)!}=\frac{(k+n-k)(n-1)!}{k!(n-k)!}=\frac{n!}{k!(n-k)!}=\binom{n}{k}
	\end{equation*}
\end{proofing}
We have the famous Binomial Theorem.
\begin{thm}(\textbf{Binomial Theorem})
	Let $n$ be a non-negative integer. We have:
	\begin{equation*}
		(x+y)^{n}=\sum_{k=0}^{n}\binom{n}{k}x^{k}y^{n-k}
	\end{equation*} 
	where for all $k$, $\binom{n}{k}$ is called the \textbf{binomial coefficient}.
\end{thm}
\begin{cor}
	Let $n$ be a non-negative integer. We have:
	\begin{equation*}
		\sum_{k=0}^{n}\binom{n}{k}=2^{n}
	\end{equation*}
\end{cor}
\begin{proofing}
	Using the Binomial Theorem and substitute $x=1$ and $y=1$, we have:
	\begin{equation*}
		2^{n}=(1+1)^{n}=\sum_{k=0}^{n}\binom{n}{k}(1)^{k}(1)^{n-k}=\sum_{k=0}^{n}\binom{n}{k}
	\end{equation*}
\end{proofing}
\begin{cor}
	Let $n$ be a positive integer. We have:
	\begin{equation*}
		\sum_{k=0}^{n}(-1)^{k}\binom{n}{k}=0
	\end{equation*}
\end{cor}
\begin{proofing}
	Using the Binomial Theorem and substitute $x=-1$ and $y=1$, we have:
	\begin{equation*}
		0=(-1+1)^{n}=\sum_{k=0}^{n}\binom{n}{k}(-1)^{k}(1)^{n-k}=\sum_{k=0}^{n}(-1)^{k}\binom{n}{k}
	\end{equation*}
\end{proofing}
\begin{cor}
	Let $n$ be a positive integer. We have:
	\begin{equation*}
		\sum_{k=0}^{n}2^{k}\binom{n}{k}=3^{n}
	\end{equation*}
\end{cor}
\begin{proofing}
	Using the Binomial Theorem and substitute $x=2$ and $y=1$, we have:
	\begin{equation*}
		3^{n}=(2+1)^{n}=\sum_{k=0}^{n}\binom{n}{k}(2)^{k}(1)^{n-k}=\sum_{k=0}^{n}2^{k}\binom{n}{k}
	\end{equation*}
\end{proofing}
\newpage

Using the Binomial Theorem, we can prove the following identity.
\begin{thm}(\textbf{Vandermonde's Identity})
	Let $m,n,r\in\mathbb{Z}$ with $0\leq r\leq m$ and $0\leq r\leq n$. We have:
	\begin{equation*}
		\binom{m+n}{r}=\sum_{k=0}^{r}\binom{m}{r-k}\binom{n}{k}
	\end{equation*}
\end{thm}
\begin{proofing}
	Assume that we want to find $(x+y)^{m+n}$. Using the Binomial Theorem, we have:
	\begin{align*}
		\sum_{r=0}^{m+n}\binom{m+n}{r}x^{r}y^{m+n-r}=(x+y)^{m+n}&=\left(\sum_{i=0}^{m}\binom{m}{i}x^{i}y^{m-i}\right)\left(\sum_{j=0}^{n}\binom{n}{j}x^{j}y^{n-j}\right)\\
		&=\sum_{i=0}^{m}\sum_{j=0}^{n}\binom{m}{i}\binom{n}{j}x^{i+j}y^{m+n-i-j}\\
		\tag{Setting $r=i+j$ and $k=j$}
		&=\sum_{r=0}^{m+n}\sum_{k=0}^{r}\binom{m}{r-k}\binom{n}{k}x^{r}y^{m+n-r}
	\end{align*}
	If we look at each binomial coefficient, we can find that:
	\begin{equation*}
		\binom{m+n}{r}=\sum_{k=0}^{r}\binom{m}{r-k}\binom{n}{k}
	\end{equation*}
\end{proofing}
\begin{cor}
	Let $n$ be a non-negative integer. We have:
	\begin{equation*}
		\binom{2n}{n}=\sum_{k=0}^{n}\binom{n}{k}^{2}
	\end{equation*}
\end{cor}
\begin{proofing}
	Using the Vandermonde's Identity and substitute $m=n$ and $r=n$, we have:
	\begin{equation*}
		\binom{2n}{n}=\sum_{k=0}^{n}\binom{n}{n-k}\binom{n}{k}=\sum_{k=0}^{n}\binom{n}{k}^{2}
	\end{equation*}
\end{proofing}
\begin{thm}
	Let $n$ and $r$ be integers such that $0\leq r\leq n$. We have:
	\begin{equation*}
		\binom{n+1}{r+1}=\sum_{i=r}^{n}\binom{i}{r}
	\end{equation*}
\end{thm}
\begin{proofing}
	We can use the Pascal's Identity on the right-hand side.
	\begin{equation*}
		\sum_{i=r}^{n}\binom{i}{r}=\sum_{i=r+1}^{n}\binom{i}{r}+1=\sum_{i=r+1}^{n}\binom{i}{r}+\binom{r+1}{r+1}=\binom{n}{r}+\binom{n}{r+1}=\binom{n+1}{r+1}
	\end{equation*}
\end{proofing}

\newpage
\section{Multinomial}
What about when some objects are in the same type?
\begin{thm}
	Given a set with $n$ elements. If we choose $n_{i}$ objects to $i$-th group for all $i$ and $n_{1}+n_{2}+\cdots+n_{k}=n$, then the number of different combinations, denoted by $\binom{n}{n_{1},n_{2},\cdots,n_{k}}$, is:
	\begin{equation*}
		\binom{n}{n_{1},n_{2},\cdots,n_{k}}=\frac{n!}{n_{1}!n_{2}!\cdots n_{k}!}
	\end{equation*}
\end{thm}
\begin{rem}
	When you choose $k$ objects from $n$ objects, you may consider it as classifying $k$ objects as chosen and remaining $n-k$ objects as not chosen. 
\end{rem}
We have a more generalized version of Binomial Theorem.
\begin{thm}(\textbf{Multinomial Theorem})
	Let $n$ be a non-negative integers. We have:
	\begin{equation*}
		(x_{1}+x_{2}+\cdots+x_{k})^{n}=\sum_{(n_{1},n_{2},\cdots,n_{k}):n_{1}+n_{2}+\cdots+n_{k}=n}\binom{n}{n_{1},n_{2},\cdots,n_{k}}x_{1}^{n_{1}}x_{2}^{n_{2}}\cdots x_{k}^{n_{k}}
	\end{equation*}
	where $(n_{1},n_{2},\cdots,n_{k})$ are all non-negative integer-valued vectors.
\end{thm}
The formula is way too complicated! Luckily, we have a formula to know how many terms the above equation has.
\begin{thm}
	There are $\binom{n+r-1}{r-1}$ distinct non-negative integer-valued vectors $(x_{1},x_{2},\cdots,x_{r})$ that satisfies:
	\begin{equation*}
		x_{1}+x_{2}+\cdots+x_{r}=n
	\end{equation*}
	where $x_{i}\geq 0$ for all $i$.
\end{thm}
We can also use the above to find number of ways to put $n$ objects into $r$ boxes. If in addition every box must have an object, we have the following.
\begin{thm}
	There are $\binom{n-1}{r-1}$ distinct positive integer-valued vectors $(x_{1},x_{2},\cdots,x_{r})$ that satisfies:
	\begin{equation*}
		x_{1}+x_{2}+\cdots+x_{r}=n
	\end{equation*}
	where $x_{i}\geq 0$ for all $i$.
\end{thm}
\begin{eg}
	Suppose that a cookie shop has $4$ different kinds of cookies. How many different ways can $6$ cookies be chosen?\\
	Number of ways to choose $6$ cookies is the number of $6$-combinations with repetition from set with $4$ elements, which is:
	\begin{equation*}
		\binom{4+6-1}{6}=\binom{9}{6}=84
	\end{equation*}
\end{eg}


\chapter{Events and their probabilities}
We have now understood how we can find the number of possibilities. However, we still haven't started rigorously formalize probability. Therefore, we need to define some basic terminologies around probability.

\section{Fundamentals}
We start with some basic terminology. Many statements in probability take the form of "the probability of event $A$ is $p$", which the events usually include some of the elements of sample space.
\begin{defn}
	These are the basic object of probabilities.
	\begin{enumerate}
		\item \textbf{Experiment} is an activity that produces distinct and well-defined possibilities called \textbf{outcomes}, denoted by $\omega$.
		\item \textbf{Sample space} is the set of all outcomes of an experiment, denoted by $\Omega$.
		\item \textbf{Event} is a subset of the sample space and is usually represented by $A,B,C,\cdots$.
		\item Outcomes are called \textbf{elementary events}.
	\end{enumerate}	
\end{defn}
\begin{eg}
	Examples of sample space:
	\begin{enumerate}
		\item Die rolling: $\Omega=\{1,2,3,4,5,6\}$
		\item Life time of bulb: $\Omega=[0,\infty)$
		\item Two coins flipping: $\Omega=\{(H,H),(H,T),(T,H),(T,T)\}$
	\end{enumerate} 
\end{eg}
\begin{rem}
	It is not necessary for all subset of $\Omega$ to be an event. However, we do not discuss this issue for the moment.
\end{rem}
\begin{eg}
	Events for dice rolling: Odd ($A=\{1,3,5\}$), Even ($A=\{2,4,6\}$), $\cdots$
\end{eg}
\begin{rem}
	If only the outcome $\omega=2$ is given, then there exists many events that can obtain this outcome. E.g. $\{2\}$, $\{2,4\}$, $\cdots$
\end{rem}

\section{Event Operations}
We can perform operations to events, similar to sets.
\begin{defn}
	Given two events $A$ and $B$.
	\begin{enumerate}
		\item \textbf{Union} of $A$ and $B$ is an event $A\cup B=\{\omega\in\Omega:\omega\in A\text{ or }\omega\in B\}$.
		\item \textbf{Intersection} of $A$ and $B$ is an event $A\cap B=\{\omega\in\Omega:\omega\in A\text{ and }\omega\in B\}$.
		\item \textbf{Complement} of $A$ is an event containing all elements in sample space $\Omega$ that is not in $A$. It is denoted by $A^{\complement}$.
		\item \textbf{Complement} of $B$ in $A$ is an event $A\setminus B=\{\omega\in\Omega:\omega\in A\text{ and }\omega\not\in B\}$.
		\item \textbf{Symmetric difference} of $A$ and $B$ is an event $A\Delta B=\{\omega\in\Omega:\omega\in A\cup B\text{ and }\omega\not\in A\cap B\}$.
	\end{enumerate}
\end{defn}

\newpage
We also need to define inclusion of all outcomes of events in another events.
\begin{defn}
	For any two events $A$ and $B$, if all of the outcomes in $A$ are also in $B$, then we say $A$ is \textbf{contained} in $B$, written as $A\subset B$ or $B\supset A$.
\end{defn}
\begin{rem}
	If $A\subset B$, the occurrence of $A$ necessarily implies the occurrence of $B$.
\end{rem}
We can describe the events in a sample space.
\begin{defn}
	Given a sequence of events $A_{1},A_{2},\cdots,A_{k}$.
	\begin{enumerate}
		\item For any $i$ and $j$, if $A_{i}\cap A_{j}=\emptyset$, then $A_{i}$ and $A_{j}$ are called \textbf{disjoint}.
		\item If $A_{i}\cap A_{j}=\emptyset$ for all $i$ and $j$, the sequence of events is called \textbf{mutually exclusive}.
		\item If $A_{1}\cup A_{2}\cup\cdots\cup A_{k}=\Omega$, the sequence of events is called \textbf{exhaustive}.
		\item If the sequence is both mutually exclusive and exhaustive, it is called a \textbf{partition}.
	\end{enumerate}
\end{defn}
We have some fundamental laws for event operations.
\begin{thm}
	Let $A,B,C$ be any three events and $A_{1},A_{2},\cdots,A_{k}$ be a sequence of events.
	\begin{enumerate}
		\item Commutative Law: $A\cup B=B\cup A\qquad A\cap B=B\cap A$
		\item Associative Law: $A\cup(B\cup C)=(A\cup B)\cup C\qquad A\cap(B\cap C)=(A\cap B)\cap C$
		\item Distributive Law: $A\cup(B\cap C)=(A\cup B)\cap(A\cup C)\qquad A\cap(B\cup C)=(A\cap B)\cup(A\cap C)$
		\item De Morgan's Law: $(\cup_{i=1}^{k}A_{i})^{\complement}=\cap_{i=1}^{k}A_{i}^{\complement}\qquad (\cap_{i=1}^{k}A_{i})^{\complement}=\cup_{i=1}^{k}A_{i}^{\complement}$
	\end{enumerate}
\end{thm}
We can also split any event into an union of two events.
\begin{lem}
	For any events $A$ and $B$, we have:
	\begin{equation*}
		A=(A\cap B)\cup(A\cap B^{\complement})
	\end{equation*}
\end{lem}
\begin{proofing}
	By distributive law,
	\begin{equation*}
		(A\cap B)\cup(A\cap B^{\complement})=A\cap(B\cup B^{\complement})=A\cap\Omega=A
	\end{equation*}
\end{proofing}
We may start defining probability. Let's start with defining a collection of subsets of the sample space.
\begin{defn}
	\textbf{Field} $\mathcal{F}$ is any collection of subsets of $\Omega$ which satisfies the following conditions:
	\begin{enumerate}
		\item If $A\in\mathcal{F}$, then $A^{\complement}\in\mathcal{F}$.
		\item If $A,B\in\mathcal{F}$, then $A\cup B\in\mathcal{F}$ and $A\cap B=(A^{\complement}\cup B^{\complement})^{\complement}\in\mathcal{F}$. (Closed under \textit{finite} unions or intersections)
		\item $\emptyset\in\mathcal{F}$ and $\Omega=A\cup A^{\complement}\in\mathcal{F}$.
	\end{enumerate}
\end{defn}
We are more interested on $\sigma$-field that is closed under countably infinite unions.
\begin{defn} % Sigma-field
	\textbf{$\sigma$-field} (or \textbf{$\sigma$-algebra}) $\mathcal{F}$ is any collection of subsets of $\Omega$ which satisfies the following conditions:
	\begin{enumerate}
		\item If $A\in\mathcal{F}$, then $A^{\complement}\in\mathcal{F}$.
		\item If $A_{1},A_{2},\cdots\in\mathcal{F}$, then $\bigcup_{i=1}^{\infty}A_{i}\in\mathcal{F}$. (Closed under \textit{countably infinite} unions)
		\item $\emptyset\in\mathcal{F}$ and $\Omega=A\cup A^{\complement}\cup\cdots\in\mathcal{F}$.
	\end{enumerate}
\end{defn}
\begin{rem}
	All $\sigma$-fields are fields. The converse are not necessarily true.
\end{rem}
\begin{rem}
	From this point onwards, $\mathcal{F}$ represents the $\sigma$-field.
\end{rem}
\begin{eg}
	Smallest $\sigma$-field: $\mathcal{F}=\{\emptyset,\Omega\}$
\end{eg}
\begin{eg}
	If $A$ is any subset of $\Omega$, then $\mathcal{F}=\{\emptyset,A,A^{\complement},\Omega\}$ is a $\sigma$-field.
\end{eg}
\begin{eg}
	Largest $\sigma$-field: Power set of $\Omega$: $2^{\Omega}=\{0,1\}^{\Omega}:=\{\text{All subsets of }\Omega\}$\\
	When $\Omega$ is infinite, the power set is too large a collection for probabilities to be assigned reasonably.
\end{eg}
\begin{rem}
	These two formulae will be very useful.
	\begin{align*}
		(a,b)&=\bigcup_{n=1}^{\infty}\left[a+\frac{1}{n},b-\frac{1}{n}\right] &            [a,b]&=\bigcap_{n=1}^{\infty}\left[a-\frac{1}{n},b+\frac{1}{n}\right]
	\end{align*}
\end{rem}

\section{Probability measure and Kolmogorov axioms}
Now that we define some fundamental terminologies, we can finally define probability.
\begin{defn}
	\textbf{Measurable space} $(\Omega,\mathcal{F})$ is a pair comprising a sample space $\Omega$ and a $\sigma$-field $\mathcal{F}$.\\
	\textbf{Measure} $\mu$ on a measurable space $(\Omega,\mathcal{F})$ is a function $\mu:\mathcal{F}\to [0,\infty]$ satisfying:
	\begin{enumerate}
		\item $\mu(\emptyset)=0$.
		\item If $A_{i}\in\mathcal{F}$ for all $i$ and they are disjoint, then $\mu(\bigcup_{i=1}^{\infty}A_{i})=\sum_{i=1}^{\infty}\mu(A_{i})$. (Countable additivity)
	\end{enumerate}
	\textbf{Probability measure} $\prob$ is a measure with $\prob(\Omega)=1$.
\end{defn}
You may ask, "Isn't it just probability?" The probability that we know is indeed a probability measure, which we will define soon. However, there are in fact other measures that satisfy the definition of probability measure. E.g. Risk-neutral measure.\\
The following measures are not probability measures.
\begin{eg}
	Lebesgue measure: $\mu((a,b))=b-a$, $\Omega=\mathbb{R}$
\end{eg}
\begin{eg}
	Counting measure: $\mu(A)=\#\{A\}$, $\Omega=\mathbb{R}$
\end{eg}
We can combine measurable space and measure into a measure space.
\begin{defn}
	\textbf{Measure space} is the triple $(\Omega,\mathcal{F},\mu)$, comprising:
	\begin{enumerate}
		\item A sample space $\Omega$
		\item A $\sigma$-field $\mathcal{F}$ of certain subsets of $\Omega$
		\item A measure $\mu$ on $(\Omega,\mathcal{F})$
	\end{enumerate}
	\textbf{Probability space} $(\Omega,\mathcal{F},\prob)$ is a measure space with probability measure $\prob$ as the measure.
\end{defn}
Kolmogorov axioms of probability uses axioms to formalize probability.
\begin{defn}(\textbf{Kolmogorov axioms of probability})
	Let $(\Omega,\mathcal{F},\prob)$ be a probability space, with sample space $\Omega$, $\sigma$-field $\mathcal{F}$, and probability measure $\prob$.
	\begin{enumerate}
		\item The probability of an event is a non-negative real number. For all $E\in\mathcal{F}$,
		\begin{align*}
			\prob(E)&\in\mathbb{R} & \prob(E)&\geq 0
		\end{align*}
		\item The probability that at least one of the elementary events in the entire sample space will occur is $1$.
		\begin{equation*}
			\prob(\Omega)=1
		\end{equation*}
		\item Any countable sequence of disjoint events $E_{1},E_{2},\cdots$ satisfies:
		\begin{equation*}
			\prob\left(\bigcup_{i=1}^{\infty}E_{i}\right)=\sum_{i=1}^{\infty}\prob(E_{i})
		\end{equation*}
	\end{enumerate}
	By this definition, we call $\prob(A)$ the \textbf{probability} of the event $A$.
\end{defn}

\newpage
\begin{eg}
	We consider a coin flip. We can find that sample space $\Omega=\{H,T\}$ and $\sigma$-field $\mathcal{F}=\{\emptyset,H,T,\Omega\}$. Let $\prob(H)=p$ where $p\in[0,1]$. We define $A=\{\omega\in\Omega:\omega = H\}$. Then we can get:
	\begin{equation*}
		\prob(A)=\begin{cases}
			0, &A=\emptyset\\
			p, &A=\{H\}\\
			1-p, &A=\{T\}\\
			1, &A=\Omega
		\end{cases}
	\end{equation*}
	If $p=\frac{1}{2}$, then the coin is fair.   
\end{eg}
\begin{eg}
	We consider a die roll. We can find that sample space $\Omega=\{1,2,3,4,5,6\}$ and $\sigma$-field $\mathcal{F}=\{0,1\}^{\Omega}$. Let $p_{i}=\prob(\{i\})$ where $i\in\Omega$. For all $A\in\mathcal{F}$,
	\begin{equation*}
		\prob(A)=\sum_{i\in A}p_{i}
	\end{equation*}
	If $p_{i}=\frac{1}{6}$ for all $i$, then the die is fair. $\prob(A)=\frac{|A|}{6}$.
\end{eg}
The following properties are important and build a foundation of probability.
\begin{lem}
	Basic properties of $\prob$:
	\begin{enumerate}
		\item $\prob(A^{\complement})=1-\prob(A)$.
		\item If $A\subseteq B$, then $\prob(B)=\prob(A)+\prob(B\setminus A)\geq\prob(A)$.
		\item $\prob(A\cup B)=\prob(A)+\prob(B)-\prob(A\cap B)$. If $A$ and $B$ are disjoint, then $\prob(A\cup B)=\prob(A)+\prob(B)$.
		\item (\textbf{Inclusion-exclusion formula}) For any set of events $\{A_{1},\cdots,A_{n}\}$,
		\begin{equation*}
			\prob\left(\bigcup_{i=1}^{n}A_{i}\right)=\sum_{i}\prob(A_{i})-\sum_{i<j}\prob(A_{i}\cap A_{j})+\cdots+(-1)^{n+1}\prob(A_{1}\cap A_{2}\cap\cdots\cap A_{n})
		\end{equation*}
	\end{enumerate}
\end{lem}
\begin{proofing}
	\begin{enumerate}
		\item $A\cup A^{\complement}=\Omega$ and 
		$A\cap A^{\complement}=\emptyset \Longrightarrow \prob(A\cup A^{\complement})=\prob(A)+\prob(A^{\complement})=1$
		\item $A\subseteq B\Longrightarrow B=A\cup (B\setminus A)\Longrightarrow\prob(B)=\prob(A)+\prob(B\setminus A)$
		\item $A\cup B=A\cup (B\setminus A)\Longrightarrow\prob(A\cup B)=\prob(A)+\prob(B\setminus A)=\prob(A)+\prob(B\setminus(A\cap B))=\prob(A)+\prob(B)-\prob(A\cap B)$
		\item By induction. When $n=1$, it is obviously true. Assume it is true for some positive integers $m$. When $n=m+1$,
		\begin{align*}
			\tag{Item 3}
			\prob\left(\bigcup_{i=1}^{m+1}A_{i}\right)&=\prob\left(\bigcup_{i=1}^{m}A_{i}\right)+\prob(A_{m+1})-\prob\left(\bigcup_{i=1}^{m}A_{i}\cap A_{m+1}\right)\\
			&=\sum_{i=1}^{m+1}\prob(A_{i})-\sum_{1\leq i<j\leq m}\prob(A_{i}\cap A_{j})+\cdots(-1)^{m+1}\prob\left(\bigcap_{i=1}^{m}A_{i}\right)-\prob\left(\bigcup_{i=1}^{m}A_{i}\cap A_{m+1}\right)\\
			&=\sum_{i=1}^{m+1}\prob(A_{i})-\sum_{1\leq i<j\leq m+1}\prob(A_{i}\cap A_{j})+\cdots+(-1)^{m+2}\prob\left(\bigcap_{i=1}^{m+1}A_{i}\right)
		\end{align*}
		Therefore, by induction, the Inclusion-exclusion formula is true for any set of events $\{A_{1},\cdots,A_{n}\}$ for any $n\in\mathbb{N}_{+}$.
	\end{enumerate}
\end{proofing}
We recall the continuity of function $f:\mathbb{R}\to\mathbb{R}$. $f$ is continuous at some point $x$ if for all $x_{n}$, $x_{n}\to x$ when $n\to\infty$. We have:
\begin{equation*}
	\lim_{n\to\infty}f(x_{n})=f\left(\lim_{n\to\infty}x_{n}\right)=f(x)
\end{equation*}
Similarly, we say a set function $\mu$  is continuous if for all $A_{n}$ with $A=\lim_{n\to\infty}A_{n}$, we have:
\begin{equation*}
	\lim_{n\to\infty}\mu(A_{n})=\mu\left(\lim_{n\to\infty}A_{n}\right)=\mu(A)
\end{equation*}
\begin{rem} 
	Given a sequence of sets $A_{n}$. We have two types of set limit:
	\begin{align*}
		\limsup_{n\to\infty}A_{n}&=\lim_{n\to\infty}\sup_{m\geq n}A_{m}=\bigcap_{n=1}^{\infty}\bigcup_{m=n}^{\infty}A_{m}=\{\omega\in\Omega:\omega\in A_{n}\text{ for infinitely many }n\}\\
		\liminf_{n\to\infty}A_{n}&=\lim_{n\to\infty}\inf_{m\geq n}A_{m}=\bigcup_{n=1}^{\infty}\bigcap_{m=n}^{\infty}A_{m}=\{\omega\in\Omega: \omega\in A_{n}\text{ for all but finitely many }n\}
	\end{align*}
	Apparently, $\liminf_{n\to\infty}A_{n}\subseteq\limsup_{n\to\infty}A_{n}$
\end{rem}
\begin{defn}
	We say a sequence of events $A_{n}$ \textbf{converges} and $\lim_{n\to\infty}A_{n}$ exists if:
	\begin{equation*}
		\limsup_{n\to\infty}A_{n}=\liminf_{n\to\infty}A_{n}
	\end{equation*}
	Given a probability space $(\Omega,\mathcal{F},\prob)$. If $A_{1},A_{2},\cdots\in\mathcal{F}$ such that $A=\lim_{n\to\infty}A_{n}$ exists, then:
	\begin{equation*}
		\lim_{n\to\infty}\prob(A_{n})=\prob\left(\lim_{n\to\infty}A_{n}\right)
	\end{equation*}
\end{defn}
From the definition, we can get the following important lemma.
\begin{lem}
	\label{Chapter 2 (Lemma) Continuous probability}
	If $A_{1},A_{2},\cdots$ are an increasing sequence of events ($A_{1}\subseteq A_{2}\subseteq\cdots$), then:
	\begin{equation*}
		\prob(A)=\prob\left(\bigcup_{n=1}^{\infty}A_{n}\right)=\lim_{n\to\infty}\prob(A_{n})
	\end{equation*}
	Similarly, if $A_{1},A_{2},\cdots$ are a decreasing sequence of events ($A_{1}\supseteq A_{2}\supseteq\cdots)$, then:
	\begin{equation*}
		\prob(A)=\prob\left(\bigcap_{n=1}^{\infty}A_{n}\right)=\lim_{n\to\infty}\prob(A_{n})
	\end{equation*}
\end{lem}
\begin{proofing}
	For $A_{1}\subseteq A_{2}\subseteq\cdots$, let $B_{n}=A_{n}\setminus A_{n-1}$
	\begin{equation*}
		\prob\left(\bigcup_{n=1}^{\infty}A_{n}\right)=\prob\left(\bigcup_{n=1}^{\infty}B_{n}\right)=\sum_{m=1}^{\infty}\prob(B_{m})=\lim_{n\to\infty}\sum_{m=1}^{n}\prob(B_{m})=\lim_{n\to\infty}\prob\left(\bigcup_{m=1}^{n}B_{m}\right)=\lim_{n\to\infty}\prob(A_{n})
	\end{equation*}
	For $A_{1}\supseteq A_{2}\supseteq\cdots$, we get $A^{\complement}=\bigcup_{i=1}^{\infty}A_{i}^{\complement}$ and $A_{1}^{\complement}\subseteq A_{2}^{\complement}\subseteq\cdots$. Therefore,
	\begin{equation*}
		\prob\left(\bigcap_{n=1}^{\infty}A_{n}\right)=1-\prob\left(\bigcup_{n=1}^{\infty}A_{n}^{\complement}\right)=1-\lim_{n\to\infty}\prob(A_{n}^{\complement})=\lim_{n\to\infty}\prob(A_{n})
	\end{equation*}
\end{proofing}
We can give some terminology to some special probabilities.
\begin{defn}
	Event $A$ is \textbf{null} if $\prob(A)=0$.
\end{defn}
\begin{rem}
	Null events need not to be impossible. For example, the probability of choosing a point in a plane is $0$.
\end{rem}
\begin{defn}
	Event $A$ occurs \textbf{almost surely} if $\prob(A)=1$.
\end{defn}

\newpage
\section{Conditional probability}
Sometimes, we are interested in the probability of a certain event given that another event has occurred.
\begin{defn}
	If $\prob(B)>0$, then the \textbf{conditional probability} that $A$ occurs given that $B$ occurs is:
	\begin{equation*}
		\prob(A|B)=\frac{\prob(A\cap B)}{\prob(B)}
	\end{equation*}
\end{defn}
\begin{rem}
	For any event $A$, $\prob(A)$ can be regarded as $\prob(A|\Omega)$.
\end{rem}
\begin{rem}
	When $\prob(E)=\prob(E|F)$, $E$ and $F$ are said to be \textbf{independent}.
\end{rem}
\begin{eg}
	Two fair dice are thrown. Given that the first shows $3$, what is the probability that the sum of number shown exceeds $6$?
	\begin{equation*}
		\prob(\text{Sum}>3|\text{First die shows }3)=\frac{\frac{3}{36}}{\frac{1}{6}}=\frac{1}{6}
	\end{equation*}
\end{eg}
\begin{lem}
	For any $B\in\mathcal{F}$, if $\prob(B)>0$, $\prob(\cdot|B)$ is a probability measure on $\mathcal{F}$.
\end{lem}
\begin{proofing}
	We prove from definition of probability measure.
	\begin{enumerate}
		\item We prove $\prob(\emptyset|B)=0$. Since $\prob(B)>0$,
		\begin{equation*}
			\prob(\emptyset|B)=\frac{\prob(\emptyset\cap B)}{\prob(B)}=\frac{\prob(\emptyset)}{\prob(B)}=0
		\end{equation*}
		\item We prove $\prob(\Omega|B)=0$. Since $\prob(B)>0$,
		\begin{equation*}
			\prob(\Omega|B)=\frac{\prob(\Omega\cap B)}{\prob(B)}=\frac{\prob(B)}{\prob(B)}=1
		\end{equation*}
		\item We prove the countable additivity. Since $\prob(B)>0$, for any disjoint sequence of events $A_{i}\in\mathcal{F}$ for all $i$,
		\begin{equation*}
			\prob\left(\left.\bigcup_{i=1}^{\infty}A_{i}\right|B\right)=\frac{1}{\prob(B)}\prob\left(\bigcup_{i=1}^{\infty}A_{i}\cap B\right)=\frac{1}{\prob(B)}\prob\left(\bigcup_{i=1}^{\infty}(A_{i}\cap B)\right)=\frac{1}{\prob(B)}\sum_{i=1}^{\infty}\prob(A_{i}\cap B)=\sum_{i=1}^{\infty}\prob(A_{i}|B)
		\end{equation*}
	\end{enumerate}
	Therefore, for any $B\in\mathcal{F}$, if $\prob(B)>0$, then $\prob(\cdot|B)$ is a probability measure.
\end{proofing}
We may create a series of probability based on previous events. This is useful if you are dealing with a sequence of events in time.
\begin{lem}(\textbf{General Multiplication Rule})
	Let $A_{1},A_{2},\cdots,A_{n}$ be a sequence of events. We have:
	\begin{equation*}
		\prob\left(\bigcap_{i=1}^{n}A_{i}\right)=\prob(A_{1})\prob(A_{2}|A_{1})\prob(A_{3}|A_{1}\cap A_{2})\cdots\prob(A_{n}|A_{1}\cap A_{2}\cap\cdots\cap A_{n-1})
	\end{equation*}
\end{lem}
\begin{proofing}
	\begin{align*}
		\prob(A_{1})\prob(A_{2}|A_{1})\prob(A_{3}|A_{1}\cap A_{2})\cdots\prob(A_{n}|A_{1}\cap A_{2}\cap\cdots\cap A_{n-1})&=\prob(A_{1}\cap A_{2})\prob(A_{3}|A_{1}\cap A_{2})\cdots\prob(A_{n}|A_{1}\cap A_{2}\cap\cdots\cap A_{n-1})\\
		&=\prob(A_{1}\cap A_{2}\cap A_{3})\cdots\prob(A_{n}|A_{1}\cap A_{2}\cap\cdots\cap A_{n-1})\\
		&=\prob\left(\bigcap_{i=1}^{n}A_{i}\right)
	\end{align*}
\end{proofing}
It is obvious that a certain event occurs when another event either occurs or not occurs.
\begin{lem}
	For any events $A$ and $B$ such that $0<\prob(B)<1$,
	\begin{equation*}
		\prob(A)=\prob(A|B)\prob(B)+\prob(A|B^{\complement})\prob(B^{\complement})
	\end{equation*}
\end{lem}
\begin{proofing}
	\begin{equation*}
		A=(A\cap B)\cup(A\cap B^{\complement})\Longrightarrow\prob(A)=\prob(A\cap B)+\prob(A\cap B^{\complement})=\prob(A|B)\prob(B)+\prob(A|B^{\complement})\prob(B^{\complement})
	\end{equation*}
\end{proofing}

\newpage
\begin{eg}
	In medical cases, we usually identify the efficiency and effectiveness of a type of medical test. We have a name for each types of results:
	\begin{enumerate}
		\item True positive (TP): Sick people correctly identified as sick (Found positive and correct)
		\item False positive (FP): Healthy people incorrectly identified as sick (Found positive but incorrect)
		\item True negative (TN): Healthy people correctly identified as healthy (Found negative and correct)
		\item False negative (FN): Sick people incorrectly identified as healthy (Found negative but incorrect)
	\end{enumerate}
\end{eg}
There are some cases when multiple events allow certain event to occur.
\begin{lem}(\textbf{Law of total probability})
	Let $\{B_{1},B_{2},\cdots,B_{n}\}$ be a partition of $\Omega$. Suppose that $\prob(B_{i})>0$ for all $i$. Then:
	\begin{equation*}
		\prob(A)=\sum_{i=1}^{n}\prob(A|B_{i})\prob(B_{i})
	\end{equation*}
\end{lem}
\begin{proofing}
	\begin{equation*}
		\prob(A)=\prob(A\cap\Omega)=\prob\left(A\cap\left(\bigcup_{i=1}^{n}B_{i}\right)\right)=\prob\left(\bigcup_{i=1}^{n}(A\cap B_{i})\right)=\sum_{i=1}^{n}\prob(A\cap B_{i})=\sum_{i=1}^{n}\prob(A|B_{i})\prob(B_{i})
	\end{equation*}
\end{proofing}
At this point, we can finally prove a theorem that is used in a lot of field outside of mathematics. Imagine that you know the probability of getting each type of disease and the probability of having a specific symptom if you have the disease. If a patient have the symptom, what is the chance that he gets the type of disease you are considering?
\begin{thm}(\textbf{Bayes' Theorem})
	Suppose that a sequence of events $A_{1},A_{2},\cdots,A_{n}$ is a partition of sample space. Assume further that $\prob(A_{i})>0$ for all $i$. Let $B$ be any event, then for any $i$:
	\begin{equation*}
		\prob(A_{i}|B)=\frac{\prob(B|A_{i})\prob(A_{i})}{\sum_{k=1}^{n}\prob(B|A_{k})\prob(A_{k})}
	\end{equation*}
\end{thm}
\begin{proofing}
	\begin{equation*}
		\prob(A_{i}|B)=\frac{\prob(A_{i}\cap B)}{\prob(B)}=\frac{\prob(B|A_{i})\prob(A_{i})}{\prob(B)}=\frac{\prob(B||A_{i})\prob(A_{i})}{\sum_{k=1}^{n}\prob(B|A_{k})\prob(A_{k})}
	\end{equation*}
\end{proofing}

\section{Independence}
In general, probability of a certain event is affected by the occurrence of other events. There are some exception.
\begin{defn}
	Two events $A$ and $B$ are \textbf{independent} if $\prob(A\cap B)=\prob(A)\prob(B)$. It is denoted by $A\independent B$.
\end{defn}
\begin{rem}
	If events $A$ and $B$ are independent and $A\cap B=\emptyset$, then either $\prob(A)=0$ or $\prob(B)=0$.
\end{rem}
It is relatively simple to prove the following.
\begin{lem}
	For any two events $A$ and $B$, if $A\independent B$, then:
	\begin{equation*}
		\prob(A|B)=\prob(A)
	\end{equation*}
\end{lem}
\begin{proofing}
	\begin{equation*}
		\prob(A|B)=\frac{\prob(A\cap B)}{\prob(B)}=\frac{\prob(A)\prob(B)}{\prob(B)}=\prob(A)
	\end{equation*}
\end{proofing}
\begin{prop}
	If events $A$ and $B$ are independent, then so are $A\independent B^{\complement}$ and $A^{\complement}\independent B^{\complement}$.
\end{prop}
\begin{proofing}
	\begin{equation*}
		\prob(A\cap B^{\complement})=\prob(A)-\prob(A\cap B)=\prob(A)-\prob(A)\prob(B)=\prob(A)(1-\prob(B))=\prob(A)\prob(B^{\complement})
	\end{equation*}
	Therefore, $A\independent B^{\complement}$ and also $A^{\complement}\independent B^{\complement}$.
\end{proofing}
\begin{prop}
	If events $A,B,C$ are independent, then:
	\begin{enumerate}
		\item $A\independent(B\cup C)$
		\item $A\independent(B\cap C)$
	\end{enumerate}
\end{prop}
\begin{proofing}
	\begin{enumerate}
		\item Using the properties of probability,
		\begin{align*}
			\prob(A\cap(B\cup C))&=\prob((A\cap B)\cup(A\cap C))\\
			&=\prob(A\cap B)+\prob(A\cap C)-\prob(A\cap B\cap C)\\
			&=\prob(A)\prob(B)+\prob(A)\prob(C)-\prob(A)\prob(B)\prob(C)\\
			&=\prob(A)\prob(B\cup C)
		\end{align*}
		\item
		\begin{equation*}
			\prob(A\cap(B\cap C))=\prob(A)\prob(B)\prob(C)=\prob(A)\prob(B\cap C)
		\end{equation*}
	\end{enumerate}
\end{proofing}
Sometimes, we may deal with more than $2$ events. We have a more specific way to describe their relationship.
\begin{defn}
	Given a family of events $\{A_{i}:i\in I\}$ for some $I\subset\mathbb{N}_{+}$.
	\begin{enumerate}
		\item If $\prob(A_{i}\cap A_{j})=\prob(A_{i})\prob(A_{j})$ for any $i\neq j$, it is \textbf{pairwise independent}.
		\item If additionally that for all subsets $J$ of $I$:
		\begin{equation*}
			\prob\left(\bigcap_{i\in J}A_{i}\right)=\prod_{i\in J}\prob(A_{i})
		\end{equation*}
		then it is \textbf{mutually independent}.
	\end{enumerate}
\end{defn}
\begin{rem}
	Usually, when we say multiple events are independent, we are saying they are mutually independent.
\end{rem}
\begin{eg}
	Roll for dice twice: $\Omega=\{1,2,\cdots,6\}\times\{1,2,\cdots,6\}$ and $\mathcal{F}=2^{\Omega}$\\
	Let $A$ be event that the sum is $7$. Event $A=\{(1,6),(2,5),(3,4),(4,3),(5,2),(6,1)\}$.\\
	Let $B$ be event that the first roll is $4$. Event $B=\{(4,1),(4,2),(4,3),(4,4),(4,5),(4,6)\}$\\
	Let $C$ be event that the second roll is $3$. Event $C=\{(1,3),(2,3),(3,3),(4,3),(5,3),(6,3)\}$
	\begin{align*}
		\prob(A\cap B)&=\prob((4,3))=\frac{1}{36}=\frac{1}{6}\left(\frac{1}{6}\right)=\prob(A)\prob(B)\\
		\prob(B\cap C)&=\prob((4,3))=\frac{1}{36}=\frac{1}{6}\left(\frac{1}{6}\right)=\prob(B)\prob(C)\\
		\prob(A\cap C)&=\prob((4,3))=\frac{1}{36}=\frac{1}{6}\left(\frac{1}{6}\right)=\prob(A)\prob(C)\\
		\prob(A\cap B\cap C)&=\prob((4,3))=\frac{1}{36}\neq\prob(A)\prob(B)\prob(C)
	\end{align*}
	Therefore, events $A$, $B$ and $C$ are pairwise independent, but not mutually independent.
\end{eg}

\newpage
\section{Product space}
There are many $\sigma$-fields you can generate using a collection of subset of $\Omega$. However, many of those may be too big to be useful. Therefore, we have the following definition. 
\begin{defn}
	Let $A$ be a collection of subsets of $\Omega$. The \textbf{$\sigma$-field generated by $A$} is:
	\begin{equation*}
		\sigma(A)=\bigcap_{A\subseteq\mathcal{G}}\mathcal{G}
	\end{equation*}
	where $\mathcal{G}$ is also a $\sigma$-field. 
\end{defn}
\begin{rem}
	$\sigma(A)$ is the smallest $\sigma$-field containing $A$.
\end{rem}
\begin{eg}
	Let $\Omega=\{1,2,\cdots,6\}$ and $A=\{\{1\}\}\subseteq 2^{\Omega}$. $\sigma(A)=\{\emptyset,\{1\},\{2,3,\cdots,6\},\Omega\}$
\end{eg}
\begin{cor}
	Suppose $(\mathcal{F}_{i})_{i\in I}$ is a system of $\sigma$-fields in $\Omega$. Then:
	\begin{equation*}
		\bigcap_{i\in I}\mathcal{F}_{i}=\{A\in\Omega: A\in\mathcal{F}_{i}\text{ for all }i\in I\}
	\end{equation*}
\end{cor}
Now that we know which $\sigma$-field we should generate, we can finally combine two probability spaces together to form a new probability space.
\begin{defn}
	\textbf{Product space} of two probability spaces $(\Omega_{1},\mathcal{F}_{1},\prob_{1})$ and $(\Omega_{2},\mathcal{F}_{2},\prob_{2})$ is the probability space $(\Omega_{1}\times\Omega_{2},\mathcal{G},\prob_{12})$ comprising:
	\begin{enumerate}
		\item a collection of ordered pairs $\Omega_{1}\times\Omega_{2}=\{(\omega_{1},\omega_{2}):\omega_{1}\in\Omega_{1},\omega_{2}\in\Omega_{2}\}$
		\item a $\sigma$-algebra $\mathcal{G}=\sigma(\mathcal{F}_{1}\times\mathcal{F}_{2})$ where $\mathcal{F}_{1}\times\mathcal{F}_{2}=\{A_{1}\times A_{2}:A_{1}\in\mathcal{F}_{1},A_{2}\in\mathcal{F}_{2}\}$ 
		\item a probability measure $\prob_{12}:\mathcal{F}_{1}\times\mathcal{F}_{2}\to [0,1]$ given by:
		\begin{equation*}
			\prob_{12}(A_{1}\times A_{2})=\prob_{1}(A_{1})\prob_{2}(A_{2})
		\end{equation*}
		for $A_{1}\in\mathcal{F}_{1},A_{2}\in\mathcal{F}_{2}$.
	\end{enumerate}
\end{defn}
\begin{eg}
	Assume that we want to consider the probabilities of getting a head in coin flipping and getting a $5$ in die tossing at the same time. We already know that:
	\begin{align*}
		\tag{Coin flipping}
		\Omega_{1}&=\{H,T\} & \mathcal{F}_{1}&=\{\emptyset, \{H\}, \{T\}, \Omega_{1}\} & \prob_{1}&=\prob(\cdot|\Omega_{1})\\
		\tag{Die tossing}
		\Omega_{2}&=\{1,2,3,4,5,6\} & \mathcal{F}_{2}&=2^{\Omega_{2}} & \prob_{2}&=\prob(\cdot|\Omega_{2})		
	\end{align*}
	The probability space we are considering is the produce space $(\Omega_{1}\times\Omega_{2},\mathcal{G},\prob_{12})$, where:
	\begin{align*}
		\Omega_{1}\times\Omega_{2}&=\{(H,1),(H,2),(H,3),(H,4),(H,5),(H,6),(T,1),(T,2),(T,3),(T,4),(T,5),(T,6)\}\\ 
		\mathcal{G}&=2^{\Omega}\\
		\prob_{12}&=\prob(\cdot|\Omega_{1}\times\Omega_{2})=\prob(\cdot|\Omega_{1})\prob(\cdot|\Omega_{2})
	\end{align*}
\end{eg}

\chapter{Random variables and their distribution}
\section{Introduction of random variables}
Sometimes, we are not interested in an experiment itself, but rather the consequence of its random outcome. We can consider this consequence as a function which maps a sample space into a real number field. We call these functions "random variable".
\begin{defn}
	\textbf{Random variable} (r.v.) is a function $X:\Omega\to\mathbb{R}$ with the property that for any $x\in\mathbb{R}$,
	\begin{equation*}
		X^{-1}((-\infty,x])=\{\omega\in\Omega:X(\omega)\leq x\}\in\mathcal{F}
	\end{equation*}
\end{defn}
\begin{rem}
	More generally, random variable is a function $X$ with the property that for all intervals $A\subseteq\mathbb{R}$, 
	\begin{equation*}
		X^{-1}(A)=\{\omega\in\Omega: X(\omega)\in A\}\in\mathcal{F}
	\end{equation*}
	We say the function is \textbf{$\mathcal{F}$-measurable}. Any function that is $\mathcal{F}$-measurable is a random variable.
\end{rem}
\begin{rem}
	All intervals can be replaced by any of following classes:
	\begin{enumerate}
		\item $(a,b)$ for all $a<b$
		\item $(a,b]$ for all $a<b$
		\item $[a,b)$ for all $a<b$
		\item $[a,b]$ for all $a<b$
		\item $(-\infty,x]$ for all $x\in\mathbb{R}$
	\end{enumerate}
	It is due to following reasons:
	\begin{enumerate}
		\item $X^{-1}$ can be interchanged with any set functions.
		\item $\mathcal{F}$ is a $\sigma$-field.
	\end{enumerate}
\end{rem}
\begin{cla}
	Suppose $X^{-1}(B)\in\mathcal{F}$ for all open sets $B$. Then $X^{-1}(B')\in\mathcal{F}$ for all closed sets $B'$.
\end{cla}
\begin{proofing}
	For any $a,b\in\mathbb{R}$,
	\begin{equation*}
		X^{-1}([a,b])=X^{-1}\left(\bigcap_{n=1}^{\infty}\left(a-\frac{1}{n},b+\frac{1}{n}\right)\right)=\bigcap_{n=1}^{\infty}X^{-1}\left(\left(a-\frac{1}{n},b+\frac{1}{n}\right)\right)\in\mathcal{F}
	\end{equation*}
\end{proofing}

\newpage
\begin{eg}
	\label{Chapter 3 (Example) Toss coin twice}
	A fair coin is tossed twice. $\Omega=\{HH,HT,TH,TT\}$. For all $\omega\in\Omega$, let $X(\omega)$ be the number of heads.
	\begin{align*}
		X(\omega)&=\begin{cases}
			0, &\omega\in\{TT\}\\
			1, &\omega\in\{HT,TH\}\\
			2, &\omega\in\{HH\}
		\end{cases} & X^{-1}((-\infty,x])&=\begin{cases}
			\emptyset, & x<0\\
			\{TT\}, & x\in[0,1)\\
			\{HT,TH,TT\}, & x\in[1,2)\\
			\Omega, &x\in[2,\infty)
		\end{cases}
	\end{align*}
	If we choose $\mathcal{F}=\{\emptyset,\Omega\}$, then $X$ is not a random variable.
\end{eg}
We can create new random variables from $X$.
\begin{lem}
	Given a random variable $X$ and $c,d\in\mathbb{R}$.
	\begin{enumerate}
		\item If $Y=cX+d$, then $Y$ is a random variable.
		\item If $Z=X^{2}$, then $Z$ is a random variable.
	\end{enumerate}
\end{lem} 
\begin{proofing}
	Let $y\in\mathbb{R}$ and $z\in\mathbb{R}_{\geq 0}$.
	\begin{enumerate}
		\item If $c>0$, then:
		\begin{equation*}
			Y^{-1}((-\infty,y])=\{\omega\in\Omega:Y(\omega)\leq y\}=\bigg\{\omega\in\Omega:X(\omega)\leq\frac{y-d}{c}\bigg\}\in\mathcal{F}
		\end{equation*}
		If $c=0$, then:
		\begin{equation*}
			Y^{-1}((-\infty,y])=\{\omega\in\Omega:d\leq y\}=\begin{cases}
				\emptyset\in\mathcal{F}, &y<d\\
				\Omega\in\mathcal{F}, &y\geq d
			\end{cases}
		\end{equation*}
		If $c<0$, then:
		\begin{equation*}
			Y^{-1}((-\infty,y])=\{\omega\in\Omega:Y(\omega)\leq y\}=\bigg\{\omega\in\Omega:X(\omega)\geq\frac{y-d}{c}\bigg\}\in\mathcal{F}
		\end{equation*}
		Therefore, for any $c,d\in\mathbb{R}$, $Y=cX+d$ is a random variable.
		\item We have:
		\begin{equation*}
			Z^{-1}([0,z])=\{\omega\in\Omega:0\leq Z(\omega)\leq z\}=\{\omega\in\Omega:0\leq X(\omega)\leq\sqrt{z}\}\in\mathcal{F}
		\end{equation*}
		Therefore, $Z=X^{2}$ is a random variable.
	\end{enumerate}
\end{proofing}
Before we continue, it is best if we know about Borel set first.
\begin{defn}
	\textbf{Borel set} is a set which can be obtained by taking countable union, intersection or complement repeatedly. (Countably many steps)
\end{defn}
\begin{defn}
	\textbf{Borel $\sigma$-field} of $\mathbb{R}$ is a $\sigma$-field $\mathcal{B}(\mathbb{R})$ that is generated by all open sets. It is a collection of Borel sets.
\end{defn}
\begin{eg}
	$\{(a,b),[a,b],\{a\},\mathbb{Q},\mathbb{R}\setminus\mathbb{Q}\}\subset\mathcal{B}(\mathbb{R})$. Note that closed sets can be generated by open sets.
\end{eg}
\begin{rem}
	In modern way of understanding, $(\Omega,\mathcal{F},\prob)\xrightarrow{X}(\mathbb{R},\mathcal{B},\prob\circ X^{-1})$
\end{rem}
\begin{cla}
	$\prob\circ X^{-1}$ is a probability measure on $(\mathbb{R},\mathcal{B})$.
\end{cla}
\begin{proofing}
	\begin{enumerate}
		\item For all $B\in\mathcal{B}$, $\prob\circ X^{-1}(B)=\prob(\{\omega:X(\omega)\in B\})\in [0,1]$
		\begin{align*}
			\prob\circ X^{-1}(\emptyset)&=\prob(\{\omega:X(\omega)\in\emptyset\})=\prob(\emptyset)=0\\
			\prob\circ X^{-1}(\mathbb{R})&=\prob(\{\omega:X(\omega)\in\mathbb{R}\})=\prob(\Omega)=1
		\end{align*}
		\item For any disjoint $B_{1},B_{2},\cdots\in\mathcal{B}$,
		\begin{equation*}
			\prob\circ X^{-1}\left(\bigcup_{i=1}^{\infty}B_{i}\right)=\prob\left(\bigcup_{i=1}^{\infty}X^{-1}(B_{i})\right)=\sum_{i=1}^{\infty}\prob(X^{-1}(B_{i}))=\sum_{i=1}^{\infty}\prob\circ X^{-1}(B_{i})
		\end{equation*}
	\end{enumerate}
\end{proofing}
\begin{rem}
	We can derive the probability of all $A\in\mathcal{B}$.
	\begin{align*}
		\prob([a,b])&=\prob((-\infty,b])-\prob((-\infty,a))\\
		&=\prob((-\infty,b])-\prob\left(\bigcup_{n=1}^{\infty}\left(-\infty,a-\frac{1}{n}\right]\right)\\
		&=\prob((-\infty,b])-\lim_{n\to\infty}\prob\left(\left(-\infty,a-\frac{1}{n}\right]\right)
	\end{align*}
\end{rem}

\section{CDF of random variables}
Every random variable has its own distribution function.
\begin{defn}
	\textbf{(Cumulative) distribution function} (CDF) of a random variable $X$ is a function $F_{X}:\mathbb{R}\to [0,1]$ given by:
	\begin{equation*}
		F_{X}(x)=\prob(X\leq x):=\prob\circ X^{-1}((-\infty,x])
	\end{equation*}
\end{defn}
\begin{eg}
	From Example \ref{Chapter 3 (Example) Toss coin twice},
	\begin{align*}
		\prob(\omega)&=\frac{1}{4} & F_{X}(x)&=\prob(X\leq x)=\begin{cases}
			0, &x<0\\
			\frac{1}{4}, &0\leq x<1\\
			\frac{3}{4}, &1\leq x<2\\
			1, &x\geq 2
		\end{cases}
	\end{align*}
\end{eg}
\begin{lem}
	CDF $F_{X}$ of a random variable $X$ has the following properties:
	\begin{enumerate}
		\item $\lim_{x\to -\infty}F_{X}(x)=0$ and $\lim_{x\to\infty}F_{X}(x)=1$.
		\item If $x<y$, then $F_{X}(x)\leq F_{X}(y)$.
		\item $F_{X}$ is right-continuous ($F_{X}(x+h)\to F_{X}(x)$ as $h\to 0$)
	\end{enumerate}
\end{lem}
\begin{proofing}
	\begin{enumerate}
		\item Let $B_{n}=\{\omega\in\Omega:X(\omega)\leq -n\}=\{X\leq -n\}$. Since $B_{1}\supseteq B_{2}\supseteq\cdots$, by Lemma \ref{Chapter 2 (Lemma) Continuous probability},
		\begin{equation*}
			\lim_{x\to -\infty}F_{X}(x)=\prob\left(\lim_{n\to\infty}B_{n}\right)=\prob(\emptyset)=0
		\end{equation*}
		Alternative proof:
		\begin{equation*}
			\lim_{x\to-\infty}F_{X}(x)=\lim_{x\to-\infty}\prob\circ X^{-1}((-\infty,x])=\lim_{n\to\infty}\prob\circ X^{-1}((-\infty,-n])=\prob\circ X^{-1}(\emptyset)=0
		\end{equation*}
		Let $C_{n}=\{\omega\in\Omega:X(\omega)\leq n\}=\{X\leq n\}$. Since $C_{1}\subseteq C_{2}\subseteq\cdots$, by Lemma \ref{Chapter 2 (Lemma) Continuous probability},
		\begin{equation*}
			\lim_{x\to\infty}F_{X}(x)=\prob\left(\lim_{n\to\infty}C_{n}\right)=\prob(\Omega)=1
		\end{equation*}
		Alternative Proof:
		\begin{equation*}
			\lim_{x\to\infty}F_{X}(x)=\lim_{x\to\infty}\prob\circ X^{-1}((-\infty,x])=\prob\circ X^{-1}(\mathbb{R})=1
		\end{equation*}
		\item Let $A(x)=\{X\leq x\}, A(x,y)=\{x<X\leq y\}$. Then $A(y)=A(x)\cup A(x,y)$ is a disjoint union.
		\begin{equation*}
			F_{X}(y)=\prob(A(y))=\prob(A(x))+\prob(A(x,y))=F_{X}(x)+\prob(x<X\leq y)\geq F_{X}(x)
		\end{equation*}
		\item Let $B_{n}=\big\{\omega\in\Omega:X(\omega)\leq x+\frac{1}{n}\big\}$. Since $B_{1}\supseteq B_{2}\supseteq\cdots$, by Lemma \ref{Chapter 2 (Lemma) Continuous probability},
		\begin{equation*}
			\lim_{h\to 0^{+}}F_{X}(x+h)=\prob\left(\bigcap_{n=1}^{\infty}B_{n}\right)=\prob\left(\lim_{n\to\infty}B_{n}\right)=\prob(\{\omega\in\Omega:X(\omega)\leq x\})=F_{X}(x)
		\end{equation*}
		Alternative Proof:
		\begin{equation*}
			\lim_{h\to 0^{+}}F_{X}(x+h)=\lim_{h\to 0^{+}}\prob\circ X^{-1}((-\infty,x+h])=\lim_{n\to\infty}\prob\circ X^{-1}\left(\left(-\infty,x+\frac{1}{n}\right]\right)=\prob\circ X^{-1}((-\infty,x])=F_{X}(x)
		\end{equation*}
	\end{enumerate}
\end{proofing}
\begin{rem}
	$F$ is not left-continuous because:
	\begin{equation*}
		\lim_{h\to 0^{+}}F_{X}(x-h)=\lim_{n\to\infty}\prob\circ X^{-1}\left(\left(-\infty,x-\frac{1}{n}\right)\right)=\prob\circ X^{-1}((-\infty,x))=F_{X}(x)-\prob\circ X^{-1}(\{x\})
	\end{equation*}
\end{rem}
\begin{lem}
	Let $F_{X}$ be the CDF of a random variable $X$. Then
	\begin{enumerate}
		\item $\prob(X>x)=1-F_{X}(x)$.
		\item $\prob(x<X\leq y)=F_{X}(y)-F_{X}(x)$.
	\end{enumerate}
\end{lem}
\begin{proofing}
	\begin{enumerate}
		\item $\prob(X>x)=\prob(\Omega\setminus\{X\leq x\})=\prob(\Omega)-\prob(X\leq x)=1-F_{X}(x)$.
		\item $\prob(x<X\leq y)=\prob(\{X\leq y\}\setminus\{X\leq x\})=\prob(X\leq y)-\prob(X\leq x)=F_{X}(y)-F_{X}(x)$.
	\end{enumerate}
\end{proofing}
In some cases, we want to find a number where a specific percentage of outcomes go below it. This is very useful if you know that the random variable follows a certain distribution.
\begin{defn}
	The \textbf{$q$-th quantile} of a random variable $X$ is defined as a number $z_{q}$ such that:
	\begin{equation*}
		\prob(X\leq z_{q})=q
	\end{equation*}
\end{defn}

\section{PMF / PDF of random variables}
We can classify some random variables into either discrete or continuous. This two will be further discussed in the next two chapters.
\begin{defn}
	Random variable $X$ is \textbf{discrete} if it takes value in some countable subsets $\{x_{1},x_{2},\cdots\}$ only of $\mathbb{R}$.\\
	Discrete random variable $X$ has \textbf{probability mass function} (PMF) $f_{X}:\mathbb{R}\to [0,1]$ given by: 
	\begin{equation*}
		f_{X}(x)=\prob(X=x)=\prob\circ X^{-1}(\{x\})
	\end{equation*}
\end{defn}
\begin{rem}
	Some textbooks will use $p_{X}(x)$ to mean PMF in order to prevent confusion with PDF.
\end{rem}
\begin{lem}
	\label{Chapter 3 (Lemma) Relationship between pmf and cdf}
	Relationship between PMF $f_{X}$ and CDF $F_{X}$ of a random variable $X$:
	\begin{enumerate}
		\item $F_{X}(x)=\sum_{y\leq x}f_{X}(y)$
		\item $f_{X}(x)=F_{X}(x)-\lim_{y\to x^{-}}F_{X}(y)$
	\end{enumerate}
\end{lem}
\begin{proofing}
	\begin{enumerate}
		\item 
		\begin{equation*}
			F_{X}(x)=\prob(X\leq x)=\sum_{i:x_{i}\leq x}\prob(X=x_{i})=\sum_{y\leq x}f_{X}(y)
		\end{equation*}
		\item Let $B_{n}=\big\{x-\frac{1}{n}<X\leq x\big\}$. Since $B_{1}\supseteq B_{2}\supseteq\cdots$, by Lemma \ref{Chapter 2 (Lemma) Continuous probability},
		\begin{equation*}
			F_{X}(x)-\lim_{y\to x^{-}}F_{X}(y)=\prob\left(\bigcap_{n=1}^{\infty}B_{n}\right)=\prob\left(\lim_{n\to\infty}B_{n}\right)=\prob\left(\left\{\lim_{n\to\infty}\left(x-\frac{1}{n}\right)<X\leq x\right\}\right)=\prob(X=x)
		\end{equation*}
	\end{enumerate}
\end{proofing}

\newpage
This is problematic when random variable $X$ is continuous because using PMF will get the result of $f_{X}(x)=0$ for all $x$. Therefore, we would need another definition for continuous random variable.
\begin{defn}
	Random variable $X$ is called \textbf{continuous} if its distribution function can be expressed as:
	\begin{align*}
		F_{X}(x)&=\intlu{-\infty}{x}f(u)\,du & x&\in\mathbb{R}
	\end{align*}
	for some integrable \textbf{probability density function} (PDF) $f_{X}:\mathbb{R}\to [0,\infty)$ of $X$. 
\end{defn}
\begin{rem}
	For small $\delta>0$:
	\begin{equation*}
		\prob(x<X\leq x+\delta)=F_{X}(x+\delta)-F_{X}(x)=\intlu{x}{x+\delta}f_{X}(u)\,du\approx f_{X}(x)\delta
	\end{equation*}
\end{rem}
\begin{rem}
	On discrete random variable, the distribution is \textbf{atomic} because the distribution function has jump discontinuities at values $x_{1},x_{2},\cdots$ and is constant in between.
\end{rem}
\begin{rem}
	On continuous random variable, the CDF of a continuous variable is \textbf{absolutely continuous}.\\
	Not every continuous function can be written as $\intlu{-\infty}{x}f_{X}(u)\,du$. E.g. Canton function
\end{rem}
\begin{rem}
	It is possible that a random variable is neither continuous nor discrete.
\end{rem}
\section{JCDF of random variables}
How do we deal with cases when there are more than one random variables?
\begin{defn}
	Let $X_{1},X_{2}:\Omega\to\mathbb{R}$ be random variables. We define \textbf{random vector} $\mathbf{X}=(X_{1},X_{2}):\Omega^{2}\to\mathbb{R}^{2}$ with properties
	\begin{equation*}
		\mathbf{X}^{-1}(D)=\{\omega\in\Omega:\mathbf{X}(\omega)=(X_{1}(\omega),X_{2}(\omega))\in D\}\in\mathcal{F}
	\end{equation*}
	for all $D\in\mathcal{B}(\mathbb{R}^{2})$.\\
	We can also say $\mathbf{X}=(X_{1},X_{2})$ is a random vector if both $X_{1},X_{2}:\Omega\to\mathbb{R}$ are random variables. That means:
	\begin{equation*}
		X_{a}^{-1}(B)\in\mathcal{F}
	\end{equation*}
	for all $B\in\mathcal{B}(\mathbb{R}),a=1,2$.
\end{defn}
\begin{cla}
	Both definitions of random vectors are equivalent.
\end{cla}
\begin{proofing}
	By first definition, $\mathbf{X}^{-1}(A_{1}\times A_{2})\in\mathcal{F}$. If we choose $A_{2}=\mathbb{R}$,
	\begin{align*}
		\mathbf{X}^{-1}(A_{1}\times \mathbb{R})&=\{\omega\in\Omega:(X_{1}(\omega),X_{2}(\omega))\in A_{1}\times\mathbb{R}\}\\
		&=\{\omega\in\Omega:X_{1}(\omega)\in A_{1}\}\cap\{\omega\in\Omega:X_{2}(\omega)\in\mathbb{R}\}\\
		&=X_{1}^{-1}(A_{1})
	\end{align*}
	This means $X_{1}$ is a random variable. Using similar method, we can also find that $X_{2}$ is a random variable.\\
	Therefore, we can obtain the second definition from the first definition.\\
	By second definition, $X_{1}$ and $X_{2}$ are random variables. Therefore,
	\begin{align*}
		\mathbf{X}^{-1}(A_{1}\times A_{2})&=\{\omega\in\Omega:(X_{1}(\omega),X_{2}(\omega))\in A_{1}\times A_{2}\}\\
		&=\{\omega\in\Omega:X_{1}(\omega)\in A_{1}\}\cap\{\omega\in\Omega:X_{2}(\omega)\in A_{2}\}\\
		&=X_{1}^{-1}(A_{1})\cap X_{2}^{-1}(A_{2})\in\mathcal{F}
	\end{align*}
	Therefore, we can obtain the first definition from the second definition.\\
	Therefore, two definitions are equivalent.
\end{proofing}
\begin{rem}
	We can write $\prob\circ\mathbf{X}^{-1}(D)=\prob(\mathbf{X}\in D)=\prob(\{\omega\in\Omega:\mathbf{X}(\omega)=(X_{1}(\omega),X_{2}(\omega))\in D\})$.
\end{rem}

\newpage
Of course, there is a distribution function corresponding to the random vector.
\begin{defn}
	\textbf{Joint distribution function} (JCDF) $F_{\mathbf{X}}:\mathbb{R}^{2}\to [0,1]$ is defined as
	\begin{equation*}
		F_{\mathbf{X}}(x_{1},x_{2})=F_{X_{1},X_{2}}(x_{1},x_{2})=\prob\circ\mathbf{X}^{-1}((-\infty,x_{1}]\times(-\infty,x_{2}])=\prob(X_{1}\leq x_{1},X_{2}\leq x_{2})
	\end{equation*}
\end{defn}
\begin{rem}
	We can replace all Borel sets by the form $[a_{1},b_{1}]\times[a_{2},b_{2}]\times\cdots\times[a_{n},b_{n}]$.
\end{rem}
Joint distribution function has quite similar properties with normal distribution function.
\begin{lem}
	JCDF $F_{X,Y}$ of random vector $(X,Y)$ has the following properties:
	\begin{enumerate}
		\item $\lim_{(x,y)\to (-\infty,-\infty)}F_{X,Y}(x,y)=0$ and $\lim_{(x,y)\to (\infty,\infty)}F_{X,Y}(x,y)=1$.
		\item If $x_{1}\leq y_{1}$ and $x_{2}\leq y_{2}$, then $F_{X,Y}(x_{1},y_{1})\leq F_{X,Y}(x_{2},y_{2})$.
		\item $F_{X,Y}$ is continuous from above, in that $F_{X,Y}(x+u,y+v)\to F_{X,Y}(x,y)$ as $u\to 0^{+}$ and $v\to 0^{+}$.
	\end{enumerate}
\end{lem}
We can find the probability distribution of one random variable by disregarding another variable. We get the following distribution.
\begin{defn}
	Let $X,Y$ be random variables. We can get a \textbf{marginal distribution} (marginal CDF) by having:
	\begin{equation*}
		F_{X}(x)=\prob\circ X^{-1}((-\infty,x])=\prob\left(X^{-1}((-\infty,x])\cap Y^{-1}((-\infty,\infty))\right)=\lim_{y\to\infty}\prob\left(X^{-1}((-\infty,x])\cap Y^{-1}((-\infty,y])\right)=\lim_{y\to\infty}F_{X,Y}(x,y)
	\end{equation*}
\end{defn}
Joint distribution function also has its probability mass function and probability density function too.
\begin{defn}
	Two random variables $X$ and $Y$ on $(\Omega,\mathcal{F},\prob)$ are \textbf{jointly discrete} if the vector $(X,Y)$ takes values in some countable subset of $\mathbb{R}^{2}$ only. The corresponding \textbf{joint (probability) mass function} (JPMF) $f:\mathbb{R}^{2}\to [0,1]$ is given by
	\begin{align*}
		f_{X,Y}(x,y)&=\prob((X,Y)=(x,y))=\prob\circ(X,Y)^{-1}(\{x,y\}) & F_{X,Y}(x,y)&=\sum_{u\leq x}\sum_{v\leq y}f(u,v) & x,y&\in\mathbb{R}
	\end{align*}
\end{defn}
\begin{rem}
	\begin{equation*}
		f_{X,Y}(x,y)=F_{X,Y}(x,y)-F_{X,Y}(x^{-},y)-F_{X,Y}(x,y^{-})+F_{X,Y}(x^{-},y^{-})
	\end{equation*}
\end{rem}
\begin{rem}
	More generally, for all $B\in\mathcal{B}(\mathbb{R}^{2})$,
	\begin{equation*}
		\prob\circ(X,Y)^{-1}(B)=\sum_{(u,v)\in B}f_{X,Y}(u,v)
	\end{equation*}
\end{rem}
\begin{defn}
	Two random variables $X$ and $Y$ on $(\Omega,\mathcal{F},\prob)$ are \textbf{jointly continuous} if the \textbf{joint probability density function} (JPDF) $f:\mathbb{R}^{2}\to [0,\infty)$ of $(X,Y)$ can be expressed as:
	\begin{align*}
		f_{X,Y}(x,y)&=\pdv*{F_{X,Y}(x,y)}{x,y} & F_{X,Y}(x,y)&=\intlu{-\infty}{x}\intlu{-\infty}{y}f_{X,Y}(u,v)\,du\,dv & x,y&\in\mathbb{R}
	\end{align*}
\end{defn}
\begin{rem}
	More generally, for all $B\in\mathcal{B}(\mathbb{R}^{2})$,
	\begin{equation*}
		\prob\circ(X,Y)^{-1}(B)=\prob((X,Y)\in B)=\iint_{B}f_{X,Y}(u,v)\,du\,dv
	\end{equation*}
\end{rem}
\begin{eg}
	Assume that a special three-sided coin is provided. Each toss results in head (H), tail (T) or edge (E) with equal probabilities. What is the probability of having $h$ heads, $t$ tails and $e$ edges after $n$ tosses?\\
	Let $H_{n},T_{n},E_{n}$ be the numbers of such outcomes in $n$ tosses of the coin. The vector $(H_{n},T_{n},E_{n})$ satisfy $H_{n}+T_{n}+E_{n}=n$.
	\begin{equation*}
		\prob((H_{n},T_{n},E_{n})=(h,t,e))=\frac{n!}{h!t!e!}\left(\frac{1}{3}\right)^{n}
	\end{equation*}
\end{eg}
\begin{rem}
	It is not generally true for two continuous random variables $X$ and $Y$ to be jointly continuous.
\end{rem}
\begin{eg}
	Let $X$ be uniformly distributed on $[0,1]$ ($f_{X}(x)=\mathbf{1}_{[0,1]}$). This means $f_{X}(x)=1$ when $x\in[0,1]$ and $0$ otherwise.\\
	Let $Y=X$ ($Y(\omega)=X(\omega)$ for all $\omega\in\Omega$). That means $(X,Y)=(X,X)$. Let $B=\{(x,y):x=y\text{ and }x\in[0,1]\}\in\mathcal{B}(\mathbb{R}^{2})$.\\
	Since $y=x$ is just a line,
	\begin{align*}
		\prob\circ(X,Y)^{-1}(B)&=1\\
		\iint_{B}f_{X,Y}(u,v)\,du\,dv&=0\neq\prob\circ(X,Y)^{-1}(B)
	\end{align*}
	Therefore, $X$ and $Y$ are not jointly continuous.
\end{eg}

\chapter{Discrete random variables}
\section{Introduction of discrete random variables}
Let's recall some of the definitions on discrete random variable in previous chapter.
\begin{defn}
	Random variable $X$ is \textbf{discrete} if it takes value in some countable subsets $\{x_{1},x_{2},\cdots\}$ only of $\mathbb{R}$.\\
	\textbf{(Cumulative) distribution function} (CDF) of discrete random variable $X$ is the function $F_{X}:\mathbb{R}\to [0,1]$ given by:
	\begin{equation*}
		F_{X}(x)=\prob(X\leq x)
	\end{equation*}
	\textbf{Probability mass function} (PMF) of discrete random variable $X$ is the function $f_{X}:\mathbb{R}\to [0,1]$ given by:
	\begin{equation*}
		f_{X}(x)=\prob(X=x)
	\end{equation*}
	CDF and PMF are related by
	\begin{align*}
		F_{X}(x)&=\sum_{i:x_{i}\leq x}f_{X}(x_{i}) & f_{X}(x)&=F_{X}(x)-\lim_{y\to x^{-}}F_{X}(y)
	\end{align*}
\end{defn}
\begin{lem}
	PMF $f_{X}:\mathbb{R}\to [0,1]$ of a discrete random variable $X$ satisfies:
	\begin{enumerate}
		\item The set of $x$ such that $f_{X}(x)\neq 0$ is countable.
		\item $\sum_{i}f_{X}(x_{i})=1$, where $x_{1},x_{2},\cdots$ are values of $x$ such that $f_{X}(x)\neq 0$.
	\end{enumerate}
\end{lem}
We also recall the definition of joint distribution function and joint mass function.
\begin{defn}
	For jointly discrete random variables $X$ and $Y$, \textbf{joint probability mass function} (JPMF) $f_{X,Y}:\mathbb{R}^{2}\to [0,1]$ is given by
	\begin{align*}
		f_{X,Y}(x,y)&=\prob((X,Y)=(x,y))=\prob\circ(X,Y)^{-1}(\{x,y\}) & F_{X,Y}(x,y)&=\sum_{u\leq x}\sum_{v\leq y}f(u,v) & x,y&\in\mathbb{R}
	\end{align*}
\end{defn}
Recall that events $A$ and $B$ are independent if the occurrence of $A$ does not change the probability of $B$ occurring.
\begin{defn}
	Discrete random variables $X$ and $Y$ are \textbf{independent} if the events $\{X=x\}$ and $\{Y=y\}$ are independent for all $x,y$. Equivalently, $X$ and $Y$ are independent if
	\begin{enumerate}
		\item $\prob((X,Y)\in A\times B)=\prob(X\in A)\prob(Y\in B)$ for all $A,B\in\mathcal{B}(\mathbb{R})$.
		\item $F_{X,Y}(x,y)=F_{X}(x)F_{Y}(y)$ for all $x,y\in\mathbb{R}$.
		\item $f_{X,Y}(x,y)=f_{X}(x)f_{Y}(y)$ for all $x,y\in\mathbb{R}$.
	\end{enumerate}
\end{defn}

\newpage
\begin{cla}
	Three definitions are equivalent.
\end{cla}
\begin{proofing}
	We can get definition 2 from definition 1.
	\begin{align*}
		F_{X,Y}(x,y)&=\prob(X\leq x,Y\leq y)=\prob(X\leq x)\prob(Y\leq y)=F_{X}(x)F_{Y}(y)
	\end{align*}
	We can get definition 3 from definition 2.
	\begin{align*}
		f_{X,Y}(x,y)&=F_{X,Y}(x,y)-F_{X,Y}(x^{-},y)-F_{X,Y}(x,y^{-})+F_{X,Y}(x^{-},y^{-})\\
		&=F_{X}(x)F_{Y}(y)-F_{X}(x^{-})F_{Y}(y)-F_{X}(x)F_{Y}(y^{-})+F_{X}(x^{-})F_{Y}(y^{-})\\
		&=(F_{X}(x)-F_{X}(x^{-}))(F_{Y}(y)-F_{Y}(y^{-}))=f_{X}(x)f_{Y}(y)
	\end{align*}
	We can get definition 1 from definition 3.
	\begin{equation*}
		\prob\circ(X,Y)^{-1}(E\times F)=\sum_{(x,y)\in E\times F}f_{X,Y}(x,y)=\sum_{x\in E}\sum_{y\in F}f_{X}(x)f_{Y}(y)=(\prob\circ X^{-1}(E))(\prob\circ Y^{-1}(F))
	\end{equation*}
	Therefore, three definitions are equivalent.
\end{proofing}
\begin{rem}
	More generally, let $X_{1},X_{2},\cdots,X_{n}:\Omega\to\mathbb{R}$ be discrete random variables. They are \textbf{independent} if
	\begin{enumerate}
		\item For all $A_{i}\in\mathcal{B}(\mathbb{R})$,
		\begin{equation*}
			\prob\circ(X_{1},X_{2},\cdots,X_{n})^{-1}(A_{1}\times A_{2}\times\cdots\times A_{n})=\prod_{i=1}^{n}\prob\circ X_{i}^{-1}(A_{i})
		\end{equation*}
		\item For all $x_{i}\in\mathbb{R}$,
		\begin{equation*}
			F_{X_{1},X_{2},\cdots,X_{n}}(x_{1},x_{2},\cdots,x_{n})=\prod_{i=1}^{n}F_{X_{i}}(x_{i})
		\end{equation*}
		\item For all $x_{i}\in\mathbb{R}$,
		\begin{equation*}
			f_{X_{1},X_{2},\cdots,X_{n}}(x_{1},x_{2},\cdots,x_{n})=\prod_{i=1}^{n}f_{X_{i}}(x_{i})
		\end{equation*}
	\end{enumerate}
\end{rem}
Recall that we say $A_{1},A_{2},\cdots,A_{n}$ are independent if for any $I\subseteq\{1,2,\cdots,n\}$:
\begin{equation*}
	\prob\left(\bigcap_{i\in I}A_{i}\right)=\prod_{i\in I}\prob(A_{i})
\end{equation*}
\begin{rem}
	From the definition, we can see that $X\independent Y$ means that $X^{-1}(E)\independent Y^{-1}(F)$ for all $E,F\in\mathcal{B}(\mathbb{R})$.
\end{rem}
\begin{rem}
	We can generate $\sigma$-field using random variables by defining $\sigma$-field generated by random variable $X$
	\begin{equation*}
		\sigma(X)=\{X^{-1}(E):E\in\mathcal{B}(\mathbb{R})\}\subseteq\mathcal{F}
	\end{equation*}
\end{rem}
From the remarks, we can extend the definition of independence from random variables to $\sigma$-fields.
\begin{defn}
	Let $\mathcal{G},\mathcal{H}\subseteq\mathcal{F}$ be two $\sigma$-fields. We say $\mathcal{G}$ and $\mathcal{H}$ are \textbf{independent} if $A\independent B$ for all $A\in\mathcal{G},B\in\mathcal{H}$.
\end{defn}
\begin{rem}
	$\sigma(X)\independent\sigma(Y)\iff X\independent Y$
\end{rem}
\begin{thm}
	Given two random variables $X$ and $Y$. If $X\independent Y$ and we have two functions $g,h:\mathbb{R}\to\mathbb{R}$ such that $g(X)$ and $h(Y)$ are still random variables, then $g(X)\independent h(Y)$.
\end{thm}
\begin{proofing}
	For all $A,B\in\mathcal{B}$,
	\begin{align*}
		\prob((g(X),h(Y))\in A\times B)&=\prob(g(X)\in A,h(Y)\in B)\\
		&=\prob(X\in\{x:g(x)\in A\},Y\in\{y:h(y)\in B\})\\
		&=\prob(X\in\{x:g(x)\in A\})\prob(Y\in\{y:h(y)\in B\})\\
		&=\prob(g(X)\in A)\prob(h(Y)\in B)
	\end{align*}
	Therefore, $g(X)\independent h(Y)$.
\end{proofing}
\begin{rem}
	We assume a product space $(\Omega,\mathcal{F},\prob)$ of two probability space $(\Omega_{1},\mathcal{F}_{1},\prob_{1})$ and $(\Omega_{2},\mathcal{F}_{2},\prob_{2})$.\\
	Any pair of events of the form $E_{1}\times\Omega_{2}$ and $\Omega_{1}\times E_{2}$ are independent.
	\begin{equation*}
		\prob((E_{1}\times\Omega_{2})\cap(\Omega_{1}\times E_{2}))=\prob(E_{1}\times E_{2})=\prob_{1}(E_{1})\prob_{2}(E_{2})=\prob(E_{1}\times\Omega_{2})\prob(\Omega_{1}\times E_{2})
	\end{equation*}
\end{rem}

\section{Conditional distribution of discrete random variables}
In the first chapter, we have discussed the conditional probability $\prob(B|A)$. We can use this to define a distribution function.
\begin{defn}
	Suppose $X,Y:\Omega\to\mathbb{R}$ are two discrete random variables. \textbf{Conditional distribution} of $Y$ given $X=x$ for any $x$ such that $\prob(X=x)>0$ is defined by
	\begin{equation*}
		\prob(Y\in \cdot|X=x)
	\end{equation*}
	\textbf{Conditional distribution function} (Conditional CDF) of $Y$ given $X=x$ for any $x$ such that $\prob(X=x)>0$ is defined by
	\begin{equation*}
		F_{Y|X}(y|x)=\prob(Y\leq y|X=x)
	\end{equation*}
	\textbf{Conditional mass function} (Conditional PMF) of $Y$ given $X=x$ or any $x$ such that $\prob(X=x)>0$ is defined by
	\begin{equation*}
		f_{Y|X}(y|x)=\prob(Y=y|X=x)
	\end{equation*}
\end{defn}
\begin{rem}
	By definition, 
	\begin{equation*}
		f_{Y|X}(y|x)=\frac{\prob(Y=y,X=x)}{\prob(X=x)}=\frac{\prob(Y=y,X=x)}{\sum_{v}\prob((X,Y)=(x,v))}
	\end{equation*}.
\end{rem}
\begin{rem}
	For any $x\in\mathbb{R}$, the conditional PMF $f_{Y|X}(y|x)$ is a probability mass function in $y$.
\end{rem}
\begin{rem}
	If $X$ and $Y$ are independent, then $f_{Y|X}(y|x)=f_{Y}(y)$.
\end{rem}
Conditional distributions still have properties of original distribution.
\begin{lem}
	Given two discrete random variables $X$ and $Y$. Conditional distributions have following properties:
	\begin{enumerate}
		\item $F_{Y|X}(y|x)=\sum_{v\leq y}f_{Y|X}(v|x)$
		\item $f_{Y|X}(y|x)=F_{Y|X}(y|x)-F_{Y|X}(y^{-}|x)$
	\end{enumerate}
\end{lem}
\begin{proofing}
	\begin{enumerate}
		\item 
		\begin{equation*}
			\sum_{v\leq y}f_{Y|X}(v|x)=\sum_{v\leq y}\prob(Y=v|X=x)=\prob(Y\leq y|X=x)=F_{Y|X}(y|x)
		\end{equation*}
		\item 
		This is just Lemma \ref{Chapter 3 (Lemma) Relationship between pmf and cdf}.
	\end{enumerate}
\end{proofing}

\section{Convolution of discrete random variables}
Finally, a lot of times, we consider the sum of the two variables. For example, the number of heads in $n$ tosses of a coin. However, there are situations that are more complicated, especially when the summands are dependent. We tries to find a formula for describing the mass function of the sum $Z=X+Y$.
\begin{thm}
	Given two jointly discrete random variables $X$ and $Y$. The probability of sum of two random variables is given by:
	\begin{equation*}
		\prob(X+Y=z)=\sum_{x}f_{X,Y}(x,z-x)=\sum_{y}f_{X,Y}(z-y,y)
	\end{equation*}
\end{thm}
\begin{proofing}
	We have the disjoint union:
	\begin{equation*}
		\{X+Y=z\}=\bigcup_{x}(\{X=x\}\cap\{Y=z-x\})
	\end{equation*}
	At most countably many of its distributions have non-zero probability. Therefore,
	\begin{equation*}
		\prob(X+Y=z)=\sum_{x}\prob(X=x,Y=z-x)=\sum_{x}f_{X,Y}(x,z-x)
	\end{equation*}
\end{proofing}
\begin{defn}
	\textbf{Convolution} $f_{X+Y}$ ($f_{X}*f_{Y}$) of PMFs of two independent discrete random variables $X$ and $Y$ is the PMF of $X+Y$:
	\begin{equation*}
		f_{X+Y}(z)=\prob(X+Y=z)=\sum_{x}f_{X}(x)f_{Y}(z-x)=\sum_{y}f_{X}(z-y)f_{Y}(y)
	\end{equation*}
\end{defn}

\section{Examples of discrete random variables}
We have some important examples of random variables that have wide number of applications.
\begin{defn}
	\textbf{Parametric distribution} of a discrete random variable is a distribution where the PMF depends on one or more parameters.
\end{defn}
The following examples are some of the most useful distributions.
\begin{eg}(\textbf{Constant variables})
	Let $X:\Omega\to\mathbb{R}$ be defined by $X(\omega)=c$ for all $\omega\in\Omega$. For all $B\in\mathcal{B}$,
	\begin{equation*}
		F_{X}(x)=\prob\circ X^{-1}(B)=\begin{cases}
			0, &B\cap\{c\}=\emptyset\\
			1, &B\cap\{c\}=\{c\}
		\end{cases}
	\end{equation*}
	$X$ is constant almost surely if there exists $c\in\mathbb{R}$ such that $\prob(X=c)=1$.
\end{eg}
\begin{eg}(\textbf{Bernoulli distribution}) $X\sim \Bern(p)$\\
	Let $A\in\mathcal{F}$ be a specific event. A Bernoulli trial is considered a success if $A$ occurs. Let $X:\Omega\to\mathbb{R}$ be such that
	\begin{align*}
		X(\omega)&=\mathbf{1}_{A}(\omega)=\begin{cases}
			1, &\omega\in A\\
			0, &\omega\in A^{\complement}
		\end{cases} & \prob(A)&=\prob(X=1)=p & \prob(A^{\complement})&=\prob(X=0)=1-p
	\end{align*}
\end{eg}
\begin{eg}
	Let $A$ be an event in $\mathcal{F}$ and \textbf{indicator functions} $\mathbf{1}_{A}:\Omega\to\mathbb{R}$ such that for all $B\in\mathcal{B}(\mathbb{R})$:
	\begin{align*}
		\mathbf{1}_{A}(\omega)&=\begin{cases}
			1, &\omega\in A\\
			0, &\omega\in A^{\complement}
		\end{cases} & \mathbf{1}_{A}^{-1}(B)&=\begin{cases}
			\emptyset, &B\cap\{0,1\}=\emptyset\\
			A^{\complement}, & B\cap\{0,1\}=\{0\}\\
			A, &B\cap\{0,1\}=\{1\}\\
			\Omega, &B\cap\{0,1\}=\{0,1\}
		\end{cases} & \prob\circ \mathbf{1}_{A}^{-1}(B)&=\begin{cases}
			0, &B\cap\{0,1\}=\emptyset\\
			\prob(A^{\complement}), & B\cap\{0,1\}=\{0\}\\
			\prob(A), &B\cap\{0,1\}=\{1\}\\
			1, &B\cap\{0,1\}=\{0,1\}
		\end{cases}
	\end{align*}
	Then $\mathbf{1}_{A}$ is a Bernoulli random variable taking values $1$ and $0$ with probabilities $\prob(A)$ and $\prob(A^{\complement})$ respectively.
\end{eg}
\begin{eg}(\textbf{Binomial distribution}) $Y\sim\Bin(n,p)$\\
	Suppose we perform $n$ independent Bernoulli trials $X_{1},X_{2},\cdots,X_{n}$. Let $Y=X_{1}+X_{2}+\cdots+X_{n}$ be total number of successes.
	\begin{equation*}
		f_{Y}(k)=\prob(Y=k)=\prob\left(\sum_{i=1}^{k}X_{i}=k\right)=\prob(\{\#\{i:X_{i}=1\}=k\})
	\end{equation*}
	We denote $A=\{\#\{i:X_{i}=1\}=k\}=\bigcup_{\sigma}A_{\sigma}$ where $\sigma=(\sigma_{1},\sigma_{2},\cdots,\sigma_{n})$ can be any sequence satisfying $\#\{i:\sigma_{i}=1\}=k$ and $A_{\sigma}:=$ events that $(X_{1},X_{2},\cdots,X_{n})=(\sigma_{1},\sigma_{2},\cdots,\sigma_{n})$. Events $A_{\sigma}$ are mutually exclusive. Hence $\prob(A)=\sum_{\sigma}\prob(A_{\sigma})$.\\
	There are totally $\binom{n}{k}$ different $\sigma$'s in the sum. By independence, we have
	\begin{equation*}
		\prob(A_{\sigma})=\prob(X_{1}=\sigma_{1},X_{2}=\sigma_{2},\cdots,X_{n}=\sigma_{n})=\prob(X_{1}=\sigma_{1})\prob(X_{2}=\sigma_{2})\cdots\prob(X_{n}=\sigma_{n})=p^{k}(1-p)^{n-k}
	\end{equation*}
	Hence, $f_{Y}(k)=\prob(A)=\binom{n}{k}p^{k}(1-p)^{n-k}$.
\end{eg}

\newpage
\begin{eg}(\textbf{Trinomial distribution})
	Suppose we perform $n$ trials, each of which result in three outcomes $A$, $B$ and $C$, where $A$ occurs with probability $p$, $B$ with probability $q$, and $C$ with probability $1-p-q$. Probability of $r$ $A$'s, $w$ $B$'s, and $n-r-w$ $C$'s is
	\begin{equation*}
		\prob(\#A=r, \#B=w, \#C=n-r-w)=\binom{n}{r,w,n-r-w}p^{r}q^{w}(1-p-q)^{n-r-w}
	\end{equation*}
\end{eg}
\begin{eg}(\textbf{Geometric distribution}) $W\sim\Geom(p)$\\
	Suppose we keep performing independent Bernoulli trials until the first success shows up. Let $p$ be the probability of success and $W$ be the \textbf{waiting time} which elapses before first success.
	\begin{align*}
		\prob(W>k)&=(1-p)^{k} & \prob(W=k)&=\prob(W>k-1)-\prob(W>k)=p(1-p)^{k-1}
	\end{align*}
\end{eg}
\begin{eg}(Alternative Geometric distribution)
	Suppose we keep performing independent Bernoulli trials until the first success shows up. Let $p$ be the probability of success and $W'$ be the number of failures before the first success.
	\begin{align*}
		\prob(W'=k)&=p(1-p)^{k} & \expect{W'}&=\frac{1-p}{p} & \Var(W')&=\frac{1-p}{p^{2}}
	\end{align*}
\end{eg}
\begin{rem}
	Conventionally, when we consider the geometric distribution, we usually refer to the one related to waiting time instead of number of failures.
\end{rem}
\begin{eg}(\textbf{Negative binomial distribution}) $W_{r}\sim\NBin(r,p)$\\
	Similar with examples of geometric distribution, let $W_{r}$ be the waiting time for the $r$-th success. For $k\geq r$,
	\begin{equation*}
		f_{W_{r}}(k)=\prob(W_{r}=k)=\binom{k-1}{r-1}p^{r}(1-p)^{k-r}
	\end{equation*}
\end{eg}
\begin{rem}
	$W_{r}$ is the sum of $r$ independent geometric variables. 
\end{rem}
\begin{eg}(\textbf{Poisson distribution}) $X\sim\Poisson(\lambda)$\\
	\textbf{Poisson variable} is a discrete random variable with Poisson PMF:
	\begin{align*}
		f_{X}(k)&=\frac{\lambda^{k}}{k!}e^{-\lambda} & k&=0,1,2,\cdots
	\end{align*}
	for some parameter $\lambda>0$.\\
	This is used for approximation of binomial random variable $\Bin(n,p)$ when $n$ is large, $p$ is small and $np$ is moderate.\\
	Let $X\sim\Bin(n,p)$ and $\lambda=np$.
	\begin{align*}
		\prob(X=k)&=\binom{n}{k}p^{k}(1-p)^{n-k}=\frac{n!}{(n-k)!k!}\left(\frac{\lambda}{n}\right)^{k}\left(1-\frac{\lambda}{n}\right)^{n-k}=\frac{\lambda^{k}}{k!}\left(\frac{n!}{n^{k}(n-k)!}\right)\frac{\left(1-\frac{\lambda}{n}\right)^{n}}{\left(1-\frac{\lambda}{n}\right)^{k}}\approx\frac{\lambda^{k}}{k!}(1)\left(\frac{e^{-\lambda}}{1}\right)=\frac{\lambda^{k}}{k!}e^{-\lambda}
	\end{align*}
\end{eg}

\newpage
We have an interesting example concerning independence with Poisson distribution involved.
\begin{eg}(\textbf{Poisson flips)}
	A coin is tossed once and head turns up with probability $p$.\\
	Let random variables $X$ and $Y$ be the numbers of heads and tails respectively. $X$ and $Y$ are not independent since
	\begin{align*}
		\prob(X=1,Y=1)&=0 & \prob(X=1)\prob(Y=1)&=p(1-p)\neq 0
	\end{align*}
	Suppose now that the coin is tosses $N$ times, where $N$ has the Poisson distribution with parameter $\lambda$.\\
	In this case, random variables $X$ and $Y$ are independent since
	\begin{align*}
		\prob(X=x,Y=y)&=\prob(X=x,Y=y|N=x+y)\prob(N=x+y)=\binom{x+y}{x}p^{x}(1-p)^{y}\frac{\lambda^{x+y}}{(x+y)!}e^{-\lambda}=\frac{(\lambda p)^{x}(\lambda(1-p))^{y}}{x!y!}e^{-\lambda}\\
		\prob(X=x)\prob(Y=y)&=\sum_{i\geq x}\prob(X=x|N=i)\prob(N=i)\sum_{j\geq y}\prob(Y=y|N=j)\prob(N=j)\\
		&=\sum_{i\geq x}\binom{i}{x}p^{x}(1-p)^{i-x}\frac{\lambda^{i}}{i!}e^{-\lambda}\sum_{j\geq y}\binom{j}{y}p^{j-y}(1-p)^{y}\frac{\lambda^{j}}{j!}e^{-\lambda}\\
		&=\frac{(\lambda p)^{x}}{x!}e^{-\lambda}\left(\sum_{i\geq x}\frac{(\lambda(1-p))^{i-x}}{(i-x)!}\right)\frac{(\lambda(1-p))^{y}}{y!}e^{-\lambda}\left(\sum_{j\geq y}\frac{(\lambda p)^{j-y}}{(j-y)!}\right)\\
		&=\frac{(\lambda p)^{x}}{x!}e^{-\lambda+\lambda(1-p)}\frac{(\lambda(1-p))^{y}}{y!}e^{-\lambda+\lambda p}\\
		&=\frac{(\lambda p)^{x}(\lambda(1-p))^{y}}{x!y!}e^{-\lambda}=\prob(X=x,Y=y)
	\end{align*}
\end{eg}
\begin{thm}
	If the number of occurrence of an event in unit time or space followings Poisson distribution with rate $\lambda$, then the number of occurrence in $t$ units times or spaces follows $\Poisson(\lambda t)$.
\end{thm}
\begin{proofing}
	Number of occurrence of an event in $t$ unit time or space is equivalent to the sum of $t$ numbers of occurrence of an event in unit time or space. Let $X_{1}\sim\Poisson(\mu)$, $X_{2}\sim\Poisson(\lambda)$ are independent. For any $k\geq 2$,
	\begin{align*}
		\prob(X_{1}+X_{2}=k)&=\sum_{i=0}^{k}\left(\frac{\mu^{i}}{i!}e^{-\mu}\right)\left(\frac{\lambda^{k-i}}{(k-i)!}e^{-\lambda}\right)\\
		&=\frac{1}{k!}e^{-(\mu+\lambda)}\sum_{i=0}^{k}\frac{k!}{i!(k-i)!}\mu^{i}\lambda^{k-i}\\
		&=\frac{1}{k!}e^{-(\mu+\lambda)}\sum_{i=0}^{k}\binom{k}{i}\mu^{i}\lambda^{k-i}\\
		&=\frac{(\mu-\lambda)^{k}}{k!}e^{-(\mu+\lambda)}
	\end{align*}
	Therefore, $(X_{1}+X_{2})\sim\Poisson(\lambda+\mu)$.\\
	When $t=2$, the number of occurrence in $2$ units times or spaces follows $\Poisson(2\lambda)$.\\
	By induction, the number of occurrence in $t$ units times or spaces follows $\Poisson(\lambda t)$.
\end{proofing}
\begin{eg}(\textbf{Hypergeometric distribution}) $X\sim\Hypergeometric(N,m,n)$\\
	Suppose that we have a set of $N$ balls. There are $m$ red balls and $N-m$ blue balls. We choose $n$ of these balls, without replacement, and define $X$ to be the number of red balls in our sample. Then:
	\begin{equation*}
		\prob(X=k)=\frac{\binom{m}{k}\binom{N-m}{n-k}}{\binom{N}{n}}
	\end{equation*}
	for $x=0,1,\cdots,\min(m,n)$.
\end{eg}

There is an important example that has a wide range of applications in real life. However, we will not discuss this here. You can find the example in Appendix \ref{Appendix A Simple random walk}.

\chapter{Continuous random variables}
\section{Introduction of continuous random variables}
We recall some definitions of continuous random variables.
\begin{defn}
	Random variable $X$ is \textbf{continuous} if its \text{distribution function} (CDF) $F_{X}(x)$ can be written as:
	\begin{equation*}
		F_{X}(x)=\prob(X\leq x)=\intlu{-\infty}{x}f(u)\,du
	\end{equation*}
	for some integrable probability density function (PDF) $f_{X}:\mathbb{R}\to[0,\infty)$.
\end{defn}
\begin{rem}
	PDF $f_{X}$ is not prescribed uniquely since two integrable function which take identical values except at some specific point have the same integral. However, if $F_{X}$ is \textbf{differentiable} at $u$, we set $f_{X}(u)=F_{X}'(u)$.
\end{rem}
Note that we have used the same letter $f$ for mass functions and density functions since both are performing similar task.
\begin{rem}
	Numerical value $f_{X}(x)$ is not a probability. However, we can consider $f_{X}(x)\,dx=\prob(x<X\leq x+dx)$ as element of probability.
\end{rem}
\begin{lem}
	\label{Chapter 5 (Lemma) Properties of PDF}
	If continuous random variable $X$ has a density function $f_{X}$, then
	\begin{enumerate}
		\item $\intinfty f_{X}(x)\,dx=1$
		\item $\prob(X=x)=0$ for all $x\in\mathbb{R}$
		\item $\prob(a\leq X\leq b)=\intlu{a}{b}f_{X}(x)\,dx$
	\end{enumerate}
\end{lem}
\begin{proofing}
	\begin{enumerate}
		\item 
		\begin{equation*}
			\intinfty f_{X}(x)\,dx=\lim_{x\to\infty}F_{X}(x)=1
		\end{equation*}
		\item
		\begin{equation*}
			\prob(X=x)=\lim_{h\to 0^{+}}\intlu{x-h}{x}f_{X}(x)\,dx=F_{X}(x)-\lim_{h\to\infty}F(x-h)=F_{X}(x)-F_{X}(x)=0
		\end{equation*}
		\item
		\begin{equation*}
			\prob(a\leq X\leq b)=F(b)-F(a)=\intlu{-\infty}{b}f_{X}(x)\,dx-\intlu{-\infty}{a}f_{X}(x)\,dx=\intlu{a}{b}f_{X}(x)\,dx
		\end{equation*}
	\end{enumerate}
\end{proofing}
\begin{rem}
	More generally, for an interval $B$, we have
	\begin{equation*}
		\prob(X\in B)=\int_{B}f_{X}(x)\,dx
	\end{equation*}
\end{rem}

\newpage
We also recall the definition of independence. This definition also works for continuous random variables.
\begin{defn}
	Two continuous random variables $X$ and $Y$ are called \textbf{independent} if for all $x,y\in\mathbb{R}$,
	\begin{equation*}
		F_{X,Y}(x,y)=F_{X}(x)F_{Y}(y)
	\end{equation*}
\end{defn}
\begin{thm}
	Let two continuous random variables $X$ and $Y$ be independent. Suppose $g(X)$ and $h(Y)$ are still continuous random variables, then $g(X)$ and $h(Y)$ are independent.
\end{thm}
Similar to discrete case, there is a joint distribution function for two random variables.
\begin{defn}
	\textbf{Joint distribution function} (JCDF) of two continuous  random variables $X$ and $Y$ is the function $F:\mathbb{R}^{2}\to[0,1]$ such that:
	\begin{equation*}
		F_{X,Y}(x,y)=\prob(X\leq x,Y\leq y)
	\end{equation*}
	Two continuous random variables $X$ and $Y$ are \textbf{jointly continuous} if the have a \textbf{joint density function} (JPDF) $f:\mathbb{R}^{2}\to[0,\infty)$ such that:
	\begin{align*}
		F_{X,Y}(x,y)&=\intlu{-\infty}{y}\intlu{-\infty}{x}f_{X,Y}(u,v)\,du\,dv & f_{X,Y}(x,y)&=\pdv*{F_{X,Y}(x,y)}{x,y} & \prob((X,Y)\in D)&=\iint_{D}f_{X,Y}(x,y)\,dx\,dy
	\end{align*}
\end{defn}
We also recall the definition of marginal distribution function.
\begin{defn}
	Given two continuous random variables $X$ and $Y$. Marginal distribution function (Marginal PDF) of $X$ given $Y$ is
	\begin{equation*}
		f_{X}(x)=\intinfty f_{X,Y}(x,u)\,dv
	\end{equation*}
\end{defn}

\section{Conditional distribution of continuous random variables}
Recall the definition of conditional distribution function of discrete random variable $Y$ given $X=x$.
\begin{equation*}
	F_{Y|X}(y|x)=\prob(Y\leq y|X=x)=\frac{\prob(Y\leq y,X=x)}{\prob(X=x)}
\end{equation*}
However, for the continuous random variables, $\prob(X=x)=0$ for all $x$. We take a limiting point of view.\\
Suppose the probability distribution function $f_{X}(x)>0$,
\begin{align*}
	F_{Y|X}(y|x)=\prob(Y\leq y|x\leq X\leq x+dx)&=\frac{\prob(Y\leq y,x\leq X\leq x+dx)}{\prob(x\leq X\leq x+dx)}\\
	&=\frac{\intlu{-\infty}{y}\intlu{x}{x+dx}f_{X,Y}(u,v)\,du\,dv}{\intlu{x}{x+dx}f_{X}(u)\,du}\\
	&\approx\frac{\intlu{-\infty}{y}f_{X,Y}(x,v)\,dx\,dv}{f_{X}(x)\,dx}\\
	&=\intlu{-\infty}{y}\frac{f_{X,Y}(x,v)}{f_{X}(x)}\,dv
\end{align*}
\begin{defn}
	Suppose $X,Y:\Omega\to\mathbb{R}$ are two continuous random variables with PDF $f_{X}(x)>0$ for some 
	$x\in\mathbb{R}$. \textbf{Conditional distribution function} (Conditional CDF) of $Y$ given $X=x$ is defined by
	\begin{equation*}
		F_{Y|X}(y|x)=\prob(Y\leq y|X=x)=\intlu{-\infty}{y}\frac{f_{X,Y}(x,v)}{f_{X}(x)}\,dv
	\end{equation*}
	\textbf{Conditional density function} (Conditional PDF) of $Y$ given $X=x$ is defined by
	\begin{equation*}
		f_{Y|X}(y|x)=\pdv*{F_{Y|X}(y|x)}{y}=\frac{f_{X,Y}(x,y)}{f_{X}(x)}
	\end{equation*}
\end{defn}

\newpage
\begin{rem}
	Since $f_{X}(x)$ can also be computed from $f(x,y)$, we can simply compute
	\begin{equation*}
		f_{Y|X}(y|x)=\frac{f_{X,Y}(x,y)}{\intinfty f_{X,Y}(x,y)\,dy}
	\end{equation*}
\end{rem}
\begin{rem}
	More generally, for two continuous random variables $X$ and $Y$ with PDF  $f_{X}(x)>0$ for some $x\in\mathbb{R}$,
	\begin{equation*}
		\prob(Y\in A|X=x)=\int_{A}\frac{f_{X,Y}(x,v)}{f_{X}(x)}\,dv=\int_{A}f_{Y|X}(y|x)\,dy
	\end{equation*}
\end{rem}
\begin{eg}
	\label{Chapter 5 (Example) JPDF to conditional PDF example}
	Assume that two jointly continuous random variables $X$ and $Y$ have a JPDF:
	\begin{equation*}
		f_{X,Y}(x,y)=\begin{cases}
			\frac{1}{x}, &0\leq y\leq x\leq 1\\
			0, &\text{Otherwise}
		\end{cases}=\frac{1}{x}\mathbf{1}_{0\leq y\leq x\leq 1}
	\end{equation*}
	We want to compute $f_{X}(x)$ and $f_{Y|X}(y|x)$.
	For $x\in [0,1]$,
	\begin{equation*}
		f_{X}(x)=\intinfty f_{X,Y}(x,y)\,dy=\intinfty\frac{1}{x}\mathbf{1}_{0\leq y\leq x\leq 1}\,dy=\intlu{0}{x}\frac{1}{x}\,dy=1
	\end{equation*}
	Therefore, $X\sim\U[0,1]$.\\
	For $0\leq y\leq x$ and $0\leq x\leq 1$,
	\begin{equation*}
		f_{Y|X}(y|x)=\frac{f_{X,Y}(x,y)}{f_{X}(x)}=\frac{1}{x}
	\end{equation*}
	Therefore, $(Y|X=x)\sim\U[0,x]$.
\end{eg}
\begin{eg}
	We want to find $\prob(X^{2}+Y^{2}\leq 1)$ with two jointly continuous random variables $X$ and $Y$ having JPDF in Example \ref{Chapter 5 (Example) JPDF to conditional PDF example}. Let $Y\in A_{x}=\{y:|y|\leq\sqrt{1-x^{2}}\}$.
	\begin{align*}
		\prob(X^{2}+Y^{2}\leq 1|X=x)=\prob(|Y|\leq\sqrt{1-x^{2}}|X=x)&=\int_{A_{x}}f_{Y|X}(y|x)\,dy\\
		&=\int_{A_{x}\cap[0,1]}\frac{1}{x}\,dy\\
		&=\intlu{0}{\min\{x,\sqrt{1-x^{2}}\}}\frac{1}{x}\,dy\\
		&=\min\left\{1,\sqrt{\frac{1}{x^{2}}-1}\right\}
	\end{align*}
	\begin{align*}
		\prob(X^{2}+Y^{2}\leq 1)=\iint_{x^{2}+y^{2}\leq 1}f_{X,Y}(x,y)\,dy\,dx&=\iint_{x^{2}+y^{2}\leq 1}f_{Y|X}(y|x)f_{X}(x)\,dy\,dx\\
		&=\intlu{0}{1}\min\left\{1,\sqrt{\frac{1}{x^{2}}-1}\right\}\,dx\\
		&=\intlu{0}{\frac{1}{\sqrt{2}}}\,dx+\intlu{\frac{1}{\sqrt{2}}}{1}\sqrt{\frac{1}{x^{2}}-1}\,dx\\
		\tag{$x=\sin\theta$}
		&=\frac{1}{\sqrt{2}}+\intlu{\frac{\pi}{4}}{\frac{\pi}{2}}\left(\frac{1}{\sin\theta}-\sin\theta\right)\,d\theta\\
		&=\left.\ln\left(\tan\frac{\theta}{2}\right)\right|_{\frac{\pi}{4}}^{\frac{\pi}{2}}=\ln(1)-\ln(\sqrt{2}-1)=\ln(1+\sqrt{2})
	\end{align*}
\end{eg}

\newpage
Similar to discrete random variables, we can find the distribution of $X+Y$ when $X$ and $Y$ are jointly continuous.
\begin{thm}
	If two jointly continuous random variables $X$ and $Y$ have JPDF $f_{X,Y}$, then $X+Y$ has a PDF
	\begin{equation*}
		f_{X+Y}(z)=\intinfty f_{X,Y}(x,z-x)\,dx=\intinfty f_{X,Y}(z-y,y)\,dy
	\end{equation*}
\end{thm}
\begin{proofing}
	\begin{align*}
		F_{X+Y}(z)=\prob(X+Y\leq z)&=\iint_{x+y\leq z}f_{X,Y}(x,y)\,dx\,dy\\
		&=\intinfty\intlu{-\infty}{z-y}f_{X,Y}(x,y)\,dx\,dy\\
		\tag{$v=x+y$}
		&=\intinfty\intlu{-\infty}{z}f_{X,Y}(v-y,y)\,dv\,dy\\
		&=\intlu{-\infty}{z}\intinfty f_{X,Y}(v-y,y)\,dy\,dv\\
		f_{X+Y}(z)=F_{X+Y}'(z)&=\intinfty f_{X,Y}(z-y,y)\,dy=\intinfty f_{X,Y}(x,z-x)\,dx
	\end{align*}
\end{proofing}
\begin{defn}
	Given two independent continuous random variables $X$ and $Y$. \textbf{Convolution} $f_{X+Y}$ ($f_{X}*f_{Y}$) of PDFs of $X$ and $Y$ is the PDF of $X+Y$:
	\begin{equation*}
		f_{X+Y}(z)=\intinfty f_{X}(z-y)f_{Y}(y)\,dy=\intinfty f_{X}(x)f_{Y}(z-x)\,dx
	\end{equation*}
\end{defn}

\section{Examples of continuous random variables}
Similar with discrete random variables, we have some useful parametric distributions.
\begin{defn}
	\textbf{Parametric distribution} of a continuous random variable is a distribution where the PDF depends on one or more parameters.
\end{defn}
\begin{eg}(\textbf{Uniform distribution}) $X\sim\U[a,b]$\\
	Random variable $X$ is \textbf{uniform} on $[a,b]$ if CDF and PDF of $X$ is
	\begin{align*}
		F_{X}(x)&=\begin{cases}
			0, &x\leq a\\
			\frac{x-a}{b-a}, &a<x\leq b\\
			1, &x>b
		\end{cases} & f_{X}(x)&=\begin{cases}
			\frac{1}{b-a}, &a<x\leq b\\
			0, &\text{Otherwise}
		\end{cases}=\frac{1}{b-a}\mathbf{1}_{a<x\leq b}
	\end{align*}
\end{eg}
\begin{eg}
	If $X\sim\U[0,1]$ and $Y\sim\U[0,1]$. In case of $X\independent Y$,
	\begin{align*}
		f_{X}(t)&=f_{Y}(t)=\begin{cases}
			1, &0\leq t\leq 1\\
			0, &\text{Otherwise}
		\end{cases}\\
		f_{X+Y}(z)=\intinfty f_{X}(z-y)f_{Y}(y)\,dy&=\intlu{0}{1}f_{X}(z-y)\,dy\\
		&=\intlu{0}{1}\mathbf{1}_{0\leq z-y\leq 1}\,dy\\
		\tag{$z-1\leq y\leq z$}
		&=\intlu{\max\{0,z-1\}}{\min\{1,z\}}\,dy\\
		&=\min\{1,z\}-\max\{0,z-1\}=\begin{cases}
			z, &0\leq z\leq 1\\
			2-z, &1\leq z\leq 2\\
			0, &\text{Otherwise}
		\end{cases}
	\end{align*}
\end{eg}
\begin{eg}
	Assume that a plane is ruled by horizontal lines separated by $D$ and a needle of length $L\leq D$ is cast randomly on the plane. What is the probability that the needle intersects some lines?\\
	Let $X$ be the distance from center of the needle to the nearest line and $\Theta$ be the acute angle between the needle and vertical line.\\
	We have $\prob(\text{Intersection})=\prob\left(\frac{L}{2}\cos\Theta\geq X\right)$.\\
	Assume that $X\independent\Theta$. We have $X\sim\U\left[0,\frac{D}{2}\right]$ and $\Theta\sim\U\left[0,\frac{\pi}{2}\right]$.
	\begin{align*}
		f_{X,\Theta}(x,\theta)&=\begin{cases}
			\frac{4}{D\pi}, &0\leq x\leq\frac{D}{2},0\leq\theta\leq\frac{\pi}{2}\\
			0, &\text{Otherwise}
		\end{cases}\\
		\prob\left(\frac{L}{2}\cos\Theta\geq X\right)&=\iint_{\frac{L}{2}\cos\theta\geq x}\frac{4}{D\pi}\mathbf{1}_{0\leq x\leq\frac{D}{2}}\mathbf{1}_{0\leq\theta\leq\frac{\pi}{2}}\,dx\,d\theta=\intlu{0}{\frac{\pi}{2}}\intlu{0}{\frac{L}{2}\cos\theta}\frac{4}{D\pi}\,dx\,d\theta=\frac{2L}{D\pi}
	\end{align*}
	Suppose that we throw the needle for $n$ times.
	\begin{equation*}
		\frac{\#\{\text{Intersection}\}}{n}\approx\prob(\text{Intersection})=\frac{2L}{D\pi}
	\end{equation*}
\end{eg}
\begin{eg} (\textbf{Inverse transform sampling})
	If we have an invertible CDF $G(x)$. How can we generate a random variable $Y$ with the given distribution function?\\
	We only need to generate an uniform random variable $U\sim\U[0,1]$. We claim that $Y=G^{-1}(U)$ has the distribution function $G(x)$.
	\begin{equation*}
		F_{Y}(x)=\prob(Y\leq x)=\prob(G^{-1}(U)\leq x)=\prob(U\leq G(x))=F_{U}(G(x))=G(x)
	\end{equation*}
\end{eg}
\begin{eg}(\textbf{Exponential distribution}) $X\sim\Exp(\lambda)$\\
	Random variable $X$ is \textbf{exponential} with parameter $\lambda>0$ if CDF and PDF of $X$ is
	\begin{align*}
		F_{X}(x)&=\begin{cases}
			1-e^{-\lambda x}, &x\geq 0\\
			0, &x<0
		\end{cases} & f_{X}(x)&=\begin{cases}
			\lambda e^{-\lambda x}, &x\geq 0\\
			0, &x<0
		\end{cases}=\lambda e^{-\lambda x}\mathbf{1}_{x\geq 0}
	\end{align*}
\end{eg}
\begin{thm}
	Exponential distribution has memoryless property. It means that for all $s>0$ and $t>0$,
	\begin{equation*}
		\prob(X>s+t|X>s)=\prob(X>t)
	\end{equation*}
\end{thm}
\begin{proofing}
	Assume that $X\sim\Exp(\lambda)$.
	\begin{equation*}
		\prob(X>s+t|X>s)=\frac{\prob(\{X>s+t\}\cap\{X>s\})}{\prob(X>s)}=\frac{\prob(X>s+t)}{\prob(X>s)}=\frac{e^{-\lambda(s+t)}}{e^{-\lambda s}}=e^{-\lambda t}=\prob(X>t)
	\end{equation*}
\end{proofing}
\begin{eg}(\textbf{Normal distribution} / \textbf{Gaussian distribution}) $X\sim\N(\mu,\sigma^{2})$\\
	Random variable $X$ is \textbf{normal} if it has two parameters $\mu$ and $\sigma^{2}$, and its PDF and CDF is
	\begin{align*}
		f_{X}(x)&=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right) & F_{X}(x)&=\intlu{-\infty}{x}f_{X}(u)\,du
	\end{align*}
	This distribution is the most important distribution.\\
	The random variable $X$ is \textbf{standard normal} if $\mu=0$ and $\sigma^{2}=1$. ($X\sim\N(0,1)$)
	\begin{align*}
		f_{X}(x)&=\phi(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^{2}}{2}} & F_{X}(x)&=\Phi(x)=\intlu{-\infty}{x}\phi(u)\,du
	\end{align*}
\end{eg}
\begin{cla}
	$\phi(x)$ is a probability distribution function.
\end{cla}
\begin{proofing}
	Let $I=\intinfty\phi(x)\,dx$.
	\begin{equation*}
		I^{2}=\intinfty\phi(x)\,dx\intinfty\phi(y)\,dy=\frac{1}{2\pi}\intinfty\intinfty e^{-\frac{x^{2}+y^{2}}{2}}\,dx\,dy
	\end{equation*}
	Let $x=r\cos\theta$ and $y=r\sin\theta$ where $r\in[0,\infty)$ and $\theta\in[0,2\pi]$
	\begin{equation*}
		I^{2}=\frac{1}{2\pi}\intlu{0}{2\pi}\intlu{0}{\infty}e^{-\frac{r^{2}}{2}}r\,dr\,d\theta=\frac{1}{2\pi}\intlu{0}{2\pi}\intlu{0}{\infty}e^{-\frac{r^{2}}{2}}\,d\left(\frac{r^{2}}{2}\right)\,d\theta=\frac{1}{2\pi}\intlu{0}{2\pi}\,d\theta=1
	\end{equation*}
	Since $\phi(x)>0$, $I=1$. Therefore, $\phi(x)$ is a probability distribution function.
\end{proofing}
These are some properties that are used frequently.
\begin{lem}
	\label{Chapter 5 (Lemma) Properties of Normal distribution}
	The normal distribution has the following properties:
	\begin{enumerate}
		\item Let $X\sim\N(0,1)$. If a random variable $Y=bX+a$ for some $a,b\in\mathbb{R}$ and $b\neq 0$, then $Y\sim\N(a,b^{2})$.
		\item Let $X\sim\N(a,b^{2})$ for some $a,b\in\mathbb{R}$ and $b\neq 0$. If a random variable $Y=\frac{X-a}{b}$, then $Y\sim\N(0,1)$.
	\end{enumerate}
\end{lem}
\begin{proofing}
	\begin{enumerate}
		\item Let $z=bx+a$.
		\begin{equation*}
			F_{Y}(y)=\prob(Y\leq y)=\prob\left(X\leq\frac{y-a}{b}\right)=\frac{1}{\sqrt{2\pi}}\intlu{-\infty}{\frac{y-a}{b}}e^{-\frac{x^{2}}{2}}\,dx=\frac{1}{\sqrt{2\pi b^{2}}}\intlu{-\infty}{y}e^{-\frac{(z-a)^{2}}{2b^{2}}}\,dz
		\end{equation*}
		Therefore, $Y\sim\N(a,b^{2})$.
		\item Let $x=bz+a$.
		\begin{equation*}
			F_{Y}(y)=\prob(Y\leq y)=\prob(X\leq by+a)=\frac{1}{\sqrt{2\pi b^{2}}}\intlu{-\infty}{by+a}e^{-\frac{(x-a)^{2}}{2b^{2}}}\,dx=\frac{1}{\sqrt{2\pi}}\intlu{-\infty}{y}e^{-\frac{z^{2}}{2}}\,dz
		\end{equation*}
		Therefore, $Y\sim\N(0,1)$.
	\end{enumerate}
\end{proofing}
\begin{lem}
	If $X\sim\N(\mu,\sigma^{2})$, then for all $s\leq t$:
	\begin{equation*}
		\prob(s\leq X\leq t)=\prob\left(\frac{s-\mu}{\sigma}\leq\frac{X-\mu}{\sigma}\leq\frac{t-\mu}{\sigma}\right)=\Phi\left(\frac{t-\mu}{\sigma}\right)-\Phi\left(\frac{s-\mu}{\sigma}\right)
	\end{equation*}
\end{lem}
\begin{proofing}
	Just apply Lemma \ref{Chapter 5 (Lemma) Properties of PDF} and you would get the equation.
\end{proofing}
This is a very important theorem, as it claims that the sum of normal distribution is still normal.
\begin{thm}
	\label{Chapter 5 (Theorem) Additivity of Normal Distribution}
	If $X_{i}\sim\N(\mu_{i},\sigma_{i}^{2})$ for $i=1,2,\cdots,n$ and they are independent, then $\sum_{i=1}^{n}X_{i}\sim\N\left(\sum_{i=1}^{n}\mu_{i},\sum_{i=1}^{n}\sigma_{i}^{2}\right)$.
\end{thm}
\begin{proofing}
	We first consider a special case when $X\sim\N(0,\sigma^{2})$, $Y\sim\N(0,1)$ and $X\independent Y$.
	\begin{align*}
		f_{X+Y}(z)&=\intinfty f_{X}(z-y)f_{Y}(y)\,dy\\
		&=\intinfty\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(z-y)^{2}}{2\sigma^{2}}\right)\left(\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{y^{2}}{2}\right)\right)\,dy\\
		&=\frac{1}{2\pi\sigma}\exp\left(-\frac{z^{2}}{2\sigma^{2}}\right)\intinfty\exp\left(-\frac{1}{2\sigma^{2}}(-2yz+y^{2}(1+\sigma^{2}))\right)\,dy\\
		&=\frac{1}{2\pi\sigma}\exp\left(-\frac{z^{2}}{2\sigma^{2}}+\frac{z^{2}}{2\sigma^{2}(1+\sigma^{2})}\right)\intinfty\exp\left(-\frac{1+\sigma^{2}}{2\sigma^{2}}\left(\frac{z^{2}}{(1+\sigma^{2})^{2}}-\frac{2yz}{1+\sigma^{2}}+y^{2}\right)\right)\,dy\\
		&=\frac{1}{\sqrt{2\pi}\sqrt{1+\sigma^{2}}}\exp\left(-\frac{z^{2}}{2\sigma^{2}}+\frac{z^{2}}{2\sigma^{2}(1+\sigma^{2})}\right)\intinfty\left(\frac{1}{\sqrt{2\pi}\frac{\sigma}{\sqrt{1+\sigma^{2}}}}\right)\exp\left(-\frac{\left(y-\frac{z}{1+\sigma^{2}}\right)^{2}}{2\left(\frac{\sigma}{\sqrt{1+\sigma^{2}}}\right)^{2}}\right)\,dy\\
		&=\frac{1}{\sqrt{2\pi}\sqrt{1+\sigma^{2}}}\exp\left(-\frac{z^{2}}{2(1+\sigma^{2})}\right)
	\end{align*}
	Therefore, $X+Y\sim\N(0,1+\sigma^{2})$. In general case when $X_{1}\sim\N(\mu_{1},\sigma_{1}^{2})$, $X_{2}\sim\N(\mu_{2},\sigma_{2}^{2})$ and $X_{1}\independent X_{2}$,
	\begin{equation*}
		X_{1}+X_{2}=\sigma_{2}\left(\frac{X_{1}-\mu_{1}}{\sigma_{2}}+\frac{X_{2}-\mu_{2}}{\sigma_{2}}\right)+\mu_{1}+\mu_{2}
	\end{equation*}
	We get $\frac{X_{1}-\mu_{1}}{\sigma_{2}}\sim\N\left(0,\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\right)$. Now we can apply this to special case and we get $\frac{X_{1}-\mu_{1}}{\sigma_{2}}+\frac{X_{2}-\mu_{2}}{\sigma_{2}}\sim\N\left(0,1+\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\right)$.\\
	Therefore, $X_{1}+X_{2}\sim\N(\mu_{1}+\mu_{2},\sigma_{1}^{2}+\sigma_{2}^{2})$. By induction, if $X_{i}\sim\N(\mu_{i},\sigma_{i}^{2})$ for $i=1,2,\cdots,n$ and they are independent, then
	\begin{equation*}
		\sum_{i=1}^{n}X_{i}\sim\N\left(\sum_{i=1}^{n}\mu_{i},\sum_{i=1}^{n}\sigma_{i}^{2}\right)
	\end{equation*}
\end{proofing}

\newpage
Combining two normal distributions into a joint distribution can be really useful.
\begin{eg}(\textbf{Standard bivariate normal distribution}) 
	Two continuous random variables $X$ and $Y$ are \textbf{standard bivariate normal} if they have JPDF:
	\begin{equation*}
		f_{X,Y}(x,y)=\frac{1}{2\pi\sqrt{1-\rho^{2}}}\exp\left(-\frac{x^{2}-2\rho xy+y^{2}}{2(1-\rho^{2})}\right)
	\end{equation*}
	where $\rho$ is a constant satisfying $-1<\rho<1$.
\end{eg}
\begin{rem}
	If $X\sim\N(0,1)$ and $Y\sim\N(0,1)$,
	\begin{align*}
		f_{Y}(y)&=\intinfty f_{X,Y}(x,y)\,dx=\frac{1}{2\pi\sqrt{1-\rho^{2}}}\intinfty\exp\left(-\frac{(x-\rho y)^{2}+(1-\rho^{2})y}{2(1-\rho^{2})}\right)\,dx\\
		&=\frac{1}{\sqrt{2\pi}}e^{-\frac{y^{2}}{2}}\intinfty\frac{1}{\sqrt{2\pi(1-\rho^{2})}}e^{-\frac{(x-\rho y)^{2}}{2(1-\rho^{2})}}\,dx=\frac{1}{\sqrt{2\pi}}e^{-\frac{y^{2}}{2}}
	\end{align*}
\end{rem}
\begin{rem}
	$\rho$ is called the \textbf{population correlation coefficient} between $X$ and $Y$. It will be discussed in Chapter 6.
\end{rem}
In most of the cases, normal random variables $X$ and $Y$ do not have a mean of $0$ and a variance $1$. If we also include mean and variance into the distribution, we would have the following distribution.
\begin{eg}(\textbf{Bivariate normal distribution})
	Two continuous random variables $X$ and $Y$ are \textbf{bivariate normal} with means $\mu_{X}$ and $\mu_{Y}$, variance $\sigma_{X}^{2}$ and $\sigma_{Y}^{2}$, and correlation coefficient $\rho$ if JPDF is given by
	\begin{equation*}
		f_{X,Y}(x,y)=\frac{1}{2\pi\sigma_{X}\sigma_{Y}\sqrt{1-\rho^{2}}}\exp\left(-\frac{1}{2(1-\rho^{2})}\left(\left(\frac{x-\mu_{X}}{\sigma_{X}}\right)^{2}-2\rho\left(\frac{x-\mu_{X}}{\sigma_{X}}\right)\left(\frac{y-\mu_{Y}}{\sigma_{Y}}\right)+\left(\frac{y-\mu_{Y}}{\sigma_{Y}}\right)^{2}\right)\right)
	\end{equation*}
\end{eg}
\begin{eg}
	Assume that random variables $X\sim\N(0,1)$ and $Y\sim\N(0,1)$ are standard bivariate normal. For $-1<\rho<1$,
	\begin{equation*}
		f_{X,Y}(x,y)=\frac{1}{2\pi\sqrt{1-\rho^{2}}}\exp\left(-\frac{x^{2}-2\rho xy+y^{2}}{2(1-\rho^{2})}\right)
	\end{equation*}
	We want to find $f_{X|Y}(x|y)$.
	\begin{align*}
		f_{X|Y}(x|y)=\frac{f_{X,Y}(x,y)}{f_{Y}(y)}=\sqrt{2\pi}e^{\frac{1}{2}y^{2}}f_{X,Y}(x,y)&=\frac{1}{\sqrt{2\pi}\sqrt{1-\rho^{2}}}e^{\frac{1}{2}y^{2}-\frac{y^{2}}{2(1-\rho^{2})}}\exp\left(-\frac{x^{2}-2\rho xy}{2(1-\rho^{2})}\right)\\
		&=\frac{1}{\sqrt{2\pi}{\sqrt{1-\rho^{2}}}}e^{\left(\frac{1}{2}-\frac{1}{2(1-\rho^{2})}-\frac{\rho^{2}}{2(1-\rho^{2})}\right)y^{2}}\exp\left(-\frac{(x-\rho y)^{2}}{2(1-\rho^{2})}\right)\\
		&=\frac{1}{\sqrt{2\pi}\sqrt{1-\rho^{2}}}\exp\left(-\frac{(x-\rho y)^{2}}{2(1-\rho^{2})}\right)
	\end{align*}
	Therefore, we have $(X|Y=y)\sim\N(\rho y,1-\rho^{2})$. As $\rho\to 1$, we have $X\to Y$. As $\rho\to -1$, we have $X\to -Y$.\\
	In general, there exists a random variable $Z\sim\N(0,1)$ such that
	\begin{align*}
		X&=\rho Y+\sqrt{1-\rho^{2}}Z & (X|Y=y)&=\rho y+\sqrt{1-\rho^{2}}Z & \begin{pmatrix}
			X\\
			Y
		\end{pmatrix}&=\begin{pmatrix}
			\rho & \sqrt{1-\rho^{2}}\\
			1 & 0
		\end{pmatrix}\begin{pmatrix}
			Y\\
			Z
		\end{pmatrix}
	\end{align*}
	We can see that bivariate normal distribution is a linear transform of two independent normal distribution.\\
	More generally, for any orthogonal matrix $\mathbf{A}$, we have two random variables $W$ and $U$ such that if they can be obtained by:
	\begin{equation*}
		\begin{pmatrix}
			W\\
			U
		\end{pmatrix}=\begin{pmatrix}
			\rho & \sqrt{1-\rho^{2}}\\
			1 & 0
		\end{pmatrix}\mathbf{A}\begin{pmatrix}
			Y\\
			Z
		\end{pmatrix}
	\end{equation*}
	then $W$ and $U$ will also be bivariate normal with $\rho$.
\end{eg}
There are some remarks that may be important to know about.
\begin{rem}
	$X$ and $Y$ are bivariate normal and uncorrelated if and only if $X$ and $Y$ are independent normal. We will talk about what uncorrelatedness means.
\end{rem}
\begin{rem}
	$X$ and $Y$ are jointly continuous and they are both normal does not mean they are bivariate normal.
\end{rem}
\begin{eg}
	Consider a JPDF of random variables $X$ and $Y$
	\begin{equation*}
		f_{X,Y}(x,y)=\begin{cases}
			\frac{1}{\pi}e^{-\frac{1}{2}(x^{2}+y^{2})}, &xy>0\\
			0, &xy\leq 0
		\end{cases}
	\end{equation*}
	As you can see, this is not a bivariate normal distribution.\\
	However, if you look at their marginal PDF,
	\begin{align*}
		f_{X}(x)&=\intlu{0}{\infty}\frac{1}{\pi}e^{-\frac{1}{2}(x^{2}+y^{2})}\,dy=\frac{1}{2\pi}\intinfty e^{-\frac{1}{2}(x^{2}+y^{2})}\,dy=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^{2}} & x&>0\\
		f_{X}(x)&=\intlu{-\infty}{0}\frac{1}{\pi}e^{-\frac{1}{2}(x^{2}+y^{2})}\,dy=\frac{1}{2\pi}\intinfty e^{-\frac{1}{2}(x^{2}+y^{2})}\,dy=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^{2}} & x&<0
	\end{align*}
	This is the same to $f_{Y}(x)$.\\
	Therefore, $X$ and $Y$ are jointly continuous and they are both normal does not mean they are bivariate normal.
\end{eg}
\begin{rem}
	Two random variables $X$ and $Y$ are jointly continuous and uncorrelated Gaussian does not mean they are independent Gaussian.
\end{rem}
More generally, we could create a multivariate normal distribution from more than two random variables.
\begin{eg}(\textbf{Multivariate normal distrubution}) $\mathbf{X}\sim\N_{p}(\boldsymbol{\mu},\mathbf{\Sigma})$\\
	Random vector $\mathbf{X}$ with dimension $p$ is $p$-dimensional normal with $p\times 1$ mean vector $\boldsymbol{\mu}$ and $p\times p$ variance-covariance matrix $\mathbf{\Sigma}$ if we have:
	\begin{equation*}
		f(\mathbf{x})=(2\pi)^{-\frac{p}{2}}\abs{\mathbf{\Sigma}}^{-\frac{1}{2}}e^{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T}\mathbf{\Sigma}(\mathbf{x}-\boldsymbol{\mu})}
	\end{equation*}
\end{eg}
\begin{rem}
	The elements $a_{ij}$ in $i$-th row and $j$-th column of variance-covariance matrix $\mathbf{\Sigma}$ is obtained by:
	\begin{equation*}
		a_{ij}=\cov(X_{i},X_{j})
	\end{equation*} 
	We will talk about how to calculate the covariance in the next chapter.
\end{rem}
\begin{eg}(\textbf{Cauchy distribution}) $X\sim\Cauchy(\theta)$\\
	Random variable $X$ has a Cauchy distribution if it has a PDF:
	\begin{equation*}
		f_{X}(x)=\frac{1}{\pi(1+(x-\theta)^{2})}
	\end{equation*}
\end{eg}
\begin{rem}
	If $X\sim\N(0,1)$ and $Y\sim\N(0,1)$, then $\frac{X}{Y}\sim\Cauchy(0)$.
\end{rem}
\begin{eg}(\textbf{Gamma distribution)} $X\sim\Gam(\alpha,\lambda)$\\
	Random variable $X$ has a gamma distribution with parameters $\alpha$ and $\lambda$ if it has a PDF:
	\begin{equation*}
		f_{X}(x)=\begin{cases}
			\frac{1}{\Gamma(\alpha)}\lambda e^{-\lambda x}(\lambda x)^{\alpha-1}, &x\geq 0\\
			0, &x<0
		\end{cases}=\frac{1}{\Gamma(\alpha)}\lambda e^{-\lambda x}(\lambda x)^{\alpha-1}\mathbf{1}_{x\geq 0}
	\end{equation*}
	where $\Gamma(\alpha)$ is called the \textbf{gamma function} defined by:
	\begin{equation*}
		\Gamma(\alpha)=\intlu{0}{\infty}e^{-y}y^{\alpha-1}\,dy
	\end{equation*}
	Note that $\Gamma(\alpha)=(\alpha-1)\Gamma(\alpha-1)$. If $\alpha$ is a positive integer, $\Gamma(\alpha)=(\alpha-1)!$.
\end{eg}
\begin{lem}
	When $\alpha=1$, it is an exponential distribution.
\end{lem}
\begin{proofing}
	Let $X\sim\Gamma(1,\lambda)$. The PDF is:
	\begin{equation*}
		f_{X}(x)=\begin{cases}
			\frac{1}{\Gamma(1)}\lambda e^{-\lambda x}(\lambda x)^{1-1}, &x\geq 0\\
			0, &x<0
		\end{cases}=\begin{cases}
			\lambda e^{-\lambda x}, &x\geq 0\\
			0, &x<0
		\end{cases}
	\end{equation*}
\end{proofing}
\begin{eg}(\textbf{Chi-squared distribution}) $Y\sim\chi^{2}(n)$\\
	Assume that $X_{1},X_{2},\cdots,X_{n}$ are independent standard normal random variables. Let $Y=\sum_{i=1}^{n}X_{i}^{2}$. We say $Y$ has a $\chi^{2}$-distribution with parameter $n$ if it has a PDF:
	\begin{equation*}
		f_{Y}(x)=\begin{cases}
			\frac{1}{\Gamma(\frac{n}{2})}2^{-\frac{n}{2}}x^{\frac{n}{2}-1}e^{-\frac{x}{2}}, &x\geq 0\\
			0, &x<0
		\end{cases}=\frac{1}{\Gamma(\frac{n}{2})}2^{-\frac{n}{2}}x^{\frac{n}{2}-1}e^{-\frac{x}{2}}\mathbf{1}_{x\geq 0}
	\end{equation*}
\end{eg}
\begin{rem}
	$\Gamma(\frac{1}{2})=\sqrt{\pi}$.
\end{rem}
\begin{lem}
	Random variable $\chi^{2}(n)$ is equivalent to $\Gam(\frac{n}{2},\frac{1}{2})$.
\end{lem}
\begin{proofing}
	Let $X\sim\Gam(\frac{n}{2},\frac{1}{2})$. Substituting $\alpha=\frac{n}{2}$ and $\lambda=\frac{1}{2}$ into PDF of $X$, we have:
	\begin{equation*}
		f_{X}(x)=\begin{cases}
			\frac{1}{2\Gamma(\frac{n}{2})}e^{-\frac{x}{2}}\left(\frac{x}{2}\right)^{\frac{n}{2}-1}, &x\geq 0\\
			0, &x<0
		\end{cases}=\begin{cases}
		\frac{1}{\Gamma(\frac{n}{2})}2^{\frac{n}{2}}x^{\frac{n}{2}-1}e^{-\frac{x}{2}}, &x\geq 0\\
		0, &x<0
		\end{cases}
	\end{equation*}
	Therefore, $X\sim\chi^{2}(n)$.
\end{proofing}
\begin{lem}
	\label{Chapter 5 (Lemma) Additivity of chi-squared distribution}
	Given that $V\sim\chi^{2}(n_{1})$ and $W\sim\chi^{2}(n_{2})$. If $V$ and $W$ are independent, then $V+W\sim\chi^{2}(n_{1}+n_{2})$.
\end{lem}
\begin{proofing}
	Let $V=X_{1}^{2}+X_{2}^{2}+\cdots+X_{n_{1}}^{2}$ and $W=Z_{1}^{2}+Z_{2}^{2}+\cdots+Z_{n_{2}}^{2}$, where $X_{i},Z_{j}\sim\N(0,1)$ for all $i,j$.
	\begin{equation*}
		V+W=X_{1}^{2}+X_{2}^{2}+\cdots+X_{n_{1}}^{2}+Z_{1}^{2}+Z_{2}^{2}+\cdots+Z_{n_{2}}^{2}\sim\chi^{2}(n_{1}+n_{2})
	\end{equation*}
\end{proofing}
We may derive further from the chi-squared distribution.
\begin{eg}(\textbf{Student's t-distribution}) $W\sim t(n)$\\
	Given $Y\sim\chi^{2}(n)$ and $Z\sim\N(0,1)$. If $Y$ and $Z$ are independent, let
	\begin{equation*}
		W=\frac{Z}{\sqrt{\frac{Y}{n}}}
	\end{equation*}
	The random variable $W$ follows the $t$-distribution with $n$ degree of freedom and PDF:
	\begin{equation*}
		f(w)=\frac{\Gamma\left(\frac{n+1}{2}\right)}{\sqrt{n\pi}\Gamma\left(\frac{n}{2}\right)}\left(1+\frac{w^{2}}{n}\right)^{-\frac{n+1}{2}}
	\end{equation*}
\end{eg}
\begin{rem}
	If $W\sim t(1)$, then from the PDF:
	\begin{equation*}
		f(w)=\frac{\Gamma(1)}{\sqrt{\pi}\Gamma\left(\frac{1}{2}\right)}(1+w^{2})^{-1}=\frac{1}{\pi(1+w^{2})}
	\end{equation*}
	This means that $W\sim\Cauchy(0)$.
\end{rem}
\begin{rem}
	Fixing $Y=y$ for some constant $y\neq 0$, we can easily find that $W\sim\N(0,\frac{n}{y})$.
\end{rem}

\newpage
\begin{eg}(\textbf{Beta distribution}) $X\sim\Beta(a,b)$\\
	Random variable $X$ has a beta distribution with parameters $a$ and $b$ if it has a PDF:
	\begin{equation*}
		f_{X}(x)=\begin{cases}
			\frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1}, &0<x<1\\
			0, &\text{Otherwise}
		\end{cases}=\frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1}\mathbf{1}_{0<x<1}
	\end{equation*}
	where $B(a,b)$ is called the \textbf{beta function} defined as:
	\begin{equation*}
		B(a,b)=\intlu{0}{1}x^{a-1}(1-x)^{b-1}\,dx=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}
	\end{equation*}
\end{eg}
\begin{eg}(\textbf{F distribution}) $F\sim F(r_{1},r_{2})$\\
	Assume that $X$ and $Y$ are independent random variables with $X\sim\chi^{2}(r_{1})$ and $Y\sim\chi^{2}(r_{2})$. Let:
	\begin{equation*}
		F=\frac{\frac{X}{r_{1}}}{\frac{Y}{r_{2}}}
	\end{equation*}
	Then $F$ has a F-distribution with $r_{1}$ and $r_{2}$ degrees of freedom with:
	\begin{equation*}
		f_{F}(w)=\frac{\Gamma\left(\frac{r_{1}+r_{2}}{2}\right)}{\Gamma\left(\frac{r_{1}}{2}\right)\Gamma\left(\frac{r_{2}}{2}\right)}\left(\frac{r_{1}}{r_{2}}\right)^{\frac{r_{1}}{2}}w^{\frac{r_{1}}{2}-1}\left(1+\frac{r_{1}w}{r_{2}}\right)^{-\frac{r_{1}+r_{2}}{2}}
	\end{equation*}
	where $0<w<\infty$.
\end{eg}
\begin{lem}
	If $U\sim F(r_{1},r_{2})$, then $\frac{1}{U}\sim F(r_{2},r_{1})$
\end{lem}
\begin{proofing}
	\begin{enumerate}
		\item By definition,
		\begin{equation*}
			U=\frac{\frac{X}{r_{1}}}{\frac{Y}{r_{2}}}
		\end{equation*}
		where $X\sim\chi^{2}(r_{1})$ and $Y\sim\chi^{2}(r_{2})$. Therefore,
		\begin{equation*}
			\frac{1}{U}=\frac{\frac{Y}{r_{2}}}{\frac{X}{r_{1}}}\sim F(r_{2},r_{1})
		\end{equation*}
	\end{enumerate}
\end{proofing}
\section{Functions of continuous random variables}
Given a continuous random variable $X$ and a function $g$ such that $g(X)$ is still a random variable, we have $\expect g(X)=\intinfty g(x)f_{X}(x)\,dx$. Therefore, we only need $f_{x}(x)$ to compute $\expect g(X)$. However, very often, we want to know the distribution of $g(X)$.
\begin{eg}
	Assume that $X$ is continuous random variable with PDF $f_{X}(x)$. Let $Y=g(X)$ be a continuous random variable. How do we find the PDF $f_{Y}(y)$? We work with $F_{Y}(y)$ first. Let $g^{-1}(A)=\{x\in\mathbb{R}:g(x)\in A\}$.
	\begin{align*}
		F_{Y}(y)&=\prob(Y\leq y)=\prob(g(X)\in(-\infty,y])=\prob(X\in g^{-1}((-\infty,y]))=\int_{g^{-1}((-\infty,y])}f_{X}(x)\,dx\\
		f_{Y}(y)&=\pdv*{\int_{g^{-1}((-\infty,y])}f_{X}(x)\,dx}{y}
	\end{align*}
\end{eg}
\begin{eg}
	Let $X\sim\N(0,1)$. Let $Y=g(X)=X^{2}$. We want to find the PDF $f_{Y}(y)$.
	\begin{align*}
		F_{Y}(y)&=\prob(Y\leq y)=\prob(-\sqrt{y}\leq X\leq \sqrt{y})=\Phi(\sqrt{y})-\Phi(-\sqrt{y})=2\Phi(\sqrt{y})-1\\
		f_{Y}(y)&=F'(y)=2\phi(\sqrt{y})\left(\frac{1}{2\sqrt{y}}\right)=\frac{1}{\sqrt{y}}\phi(\sqrt{y})=\begin{cases}
			\frac{1}{\sqrt{2\pi y}}\exp\left(\frac{-y}{2}\right), &y>0\\
			0, &y<0
		\end{cases}
	\end{align*}
	We have $X^{2}\sim\chi^{2}(1)$. (This is a distribution)
\end{eg}
\begin{thm}
	In case that $g(x)$ is strictly monotonic (strictly increasing or strictly decreasing) and differentiable, let $Y=g(X)$. We have
	\begin{equation*}
		f_{Y}(y)=\begin{cases}
			f_{X}(g^{-1}(y))\abs{\pdv*{g^{-1}(y)}{y}}, &\text{if }y=g(x)\text{ for some }x\\
			0, &\text{Otherwise}
		\end{cases}
	\end{equation*}
\end{thm}
\begin{proofing}
	If $g(x)$ is a strictly increasing function,
	\begin{align*}
		F_{Y}(y)&=\prob(g(X)\leq y)=\prob(X\leq g^{-1}(y))=F_{X}(g^{-1}(y))\\
		f_{Y}(y)&=F_{Y}'(y)=f_{X}(g^{-1}(y))\pdv*{g^{-1}(y)}{y}=f_{X}(g^{-1}(y))\abs{\pdv*{g^{-1}(y)}{y}}
	\end{align*}
	If $g(x)$ is a strictly decreasing function,
	\begin{align*}
		F_{Y}(y)&=\prob(g(X)\leq y)=\prob(X\geq g^{-1}(y))=1-F_{X}(g^{-1}(y))\\
		f_{Y}(y)&=F_{Y}'(y)=-f_{X}(y^{-1}(y))\pdv*{g^{-1}(y)}{y}=f_{X}(g^{-1}(y))\abs{\pdv*{g^{-1}(y)}{y}}
	\end{align*}
\end{proofing}
We can consider the multivariable case.
\begin{eg}
	Suppose two random variables $X$ and $Y$ are jointly continuous with JPDF $f_{X,Y}$. Given that $U=g(X,Y)$ and $V=h(X,Y)$. What is $f_{U,V}(u,v)$? For simplifying the process, we need to first make some following assumptions.
	\begin{enumerate}
		\item $X,Y$ can be uniquely solved from $U,V$. (There exists only 1 pair of functions $a,b$ such that $X=a(U,V)$ and $Y=b(U,V)$)
		\item The function $g$ and $h$ are differentiable and the Jacobian determinant
		\begin{equation*}
			J(x,y)=\begin{vmatrix}
				\pdv{g}{x} & \pdv{g}{y}\\
				\pdv{h}{x} & \pdv{h}{y}
			\end{vmatrix}\neq 0
		\end{equation*}
	\end{enumerate}
	Then
	\begin{equation*}
		f_{U,V}(u,v)=\frac{1}{\abs{J(x,y)}}f_{X,Y}(x,y)=\begin{cases}
			\frac{1}{\abs{J(a(u,v),b(u,v))}}f_{X,Y}(a(u,v),b(u,v)), &(u,v)=(g(x,y),h(x,y))\text{ for some }x,y\\
			0, &\text{Otherwise}
		\end{cases}
	\end{equation*}
\end{eg}
\begin{eg}
	Given two jointly continuous random variables $X_{1},X_{2}$ and their JPDF $f_{X_{1},X_{2}}$.\\
	Let $Y_{1}=X_{1}+X_{2}$ and $Y_{2}=X_{1}-X_{2}$.
	\begin{align*}
		X_{1}&=\frac{Y_{1}+Y_{2}}{2}=a(Y_{1},Y_{2}) & X_{2}&=\frac{Y_{1}-Y_{2}}{2}=b(Y_{1},Y_{2}) & J(x_{1},x_{2})&=\begin{vmatrix}
			1 & 1\\
			1 & -1
		\end{vmatrix}=-2
	\end{align*}
	\begin{equation*}
		f_{Y_{1},Y_{2}}(y_{1},y_{2})=\frac{1}{\abs{J(x_{1},x_{2})}}f_{X_{1},X_{2}}(x_{1},x_{2})=\frac{1}{2}f_{X_{1},X_{2}}\left(\frac{y_{1}+y_{2}}{2},\frac{y_{1}-y_{2}}{2}\right)
	\end{equation*}
	More specifically, if $X_{1}\sim\N(0,1)$, $X_{2}\sim\N(0,1)$ and $X_{1}\independent X_{2}$,
	\begin{align*}
		f_{X_{1},X_{2}}(x_{1},x_{2})&=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x_{1}^{2}+x_{2}^{2})}\\
		f_{Y_{1},Y_{2}}(y_{1},y_{2})&=\frac{1}{2}f_{X_{1},X_{2}}\left(\frac{y_{1}+y_{2}}{2},\frac{y_{1}-y_{2}}{2}\right)\\
		&=\frac{1}{4\pi}e^{-\frac{1}{2}\left(\left(\frac{1}{2}(y_{1}+y_{2})\right)^{2}+\left(\frac{1}{2}(y_{1}-y_{2})\right)^{2}\right)}\\
		&=\frac{1}{4\pi}e^{-\frac{1}{4}(y_{1}^{2}+y_{2}^{2})}
	\end{align*}
	Therefore, $Y_{1}\independent Y_{2}$ and we have $Y_{1}\sim\N(0,2)$ and $Y_{2}\sim\N(0,2)$.
\end{eg}

\newpage
\begin{eg}
	We do the same thing as the previous example but instead we have two independent random variables $X_{1}\sim\U[0,1]$ and $X_{2}\sim\U[0,1]$. For all $x_{1},x_{2}\in\mathbb{R}$,
	\begin{align*}
		f_{X_{1},X_{2}}(x_{1},x_{2})&=\begin{cases}
			1, &x_{1},x_{2}\in[0,1]\\
			0, &\text{Otherwise}
		\end{cases}=\mathbf{1}_{0\leq x_{1}\leq 1,0\leq x_{2}\leq 1}\\
		f_{Y_{1},Y_{2}}(y_{1},y_{2})&=\frac{1}{2}f_{X_{1},X_{2}}\left(\frac{y_{1}+y_{2}}{2},\frac{y_{1}-y_{2}}{2}\right)\\
		&=\frac{1}{2}\mathbf{1}_{0\leq y_{1}+y_{2}\leq 2,0\leq y_{1}-y_{2}\leq 2}
	\end{align*}
\end{eg}

\chapter{Expectation}
\textit{In this chapter, if we are discussing more than one random variables, we assume that either all are discrete or all are continuous. In practical sense, it is possible to have some to be discrete and others continuous, but we will not tackle this situation. It is relatively easy to proof all the theorems and lemmas in this case once you know how to prove when all are discrete or all are continuous.}
\section{Introduction to expectation}
In real life, we also want to know about the expected final result given the probabilities we calculated. The result is usually a theoretical approximation of empirical average. Assume we have random variables $X_{1},X_{2},\cdots,X_{N}$ which take values in $\{x_{1},x_{2},\cdots,x_{n}\}$ with probability mass function $f_{X}(x)$. We get an empirical average:
\begin{equation*}
	\mu=\frac{1}{N}\sum_{i=1}^{N}X_{i}\approx\frac{1}{N}\sum_{i=1}^{N}x_{i}Nf(x_{i})=\sum_{i=1}^{N}x_{i}f(x_{i})
\end{equation*}
We get the formula of expectation of discrete random variable. However, for continuous random variables, the probability in every single point is $0$. In order to make sense, we use the probability density function to obtain the expectation:
\begin{equation*}
	\mu=\int_{-\infty}^{\infty}xf(x)\,dx
\end{equation*}
\begin{defn}
	\textbf{Mean value}, \textbf{expectation} or \textbf{expected value} of a discrete random variable $X$ with PMF $f_{X}$ is defined to be:
	\begin{equation*}
		\expect{X}=\expect(X):=\sum_{x:f_{X}(x)>0}xf_{X}(x)
	\end{equation*}
	whenever this sum is absolutely convergent.\\
	\textbf{Expectation} of a continuous random variable $X$ with PDF $f_{X}$ is defined to be:
	\begin{equation*}
		\expect{X}=\int_{-\infty}^{\infty}xf_{X}(x)\,dx
	\end{equation*}
	whenever this integral exists.
\end{defn}
\begin{rem}
	We usually can define $\expect{X}$ only if $\expect\abs{X}$ exists.
\end{rem}
\begin{eg}
	Suppose a product is sold seasonally. Let $b$ be net profit for each sold unit, $\ell$ be net loss for each left unit, and $X$ be number of products ordered by customer. If $y$ units are stocked, what is the expected profit $Q(y)$?
	\begin{equation*}
		Q(y)=\begin{cases}
			bX-(y-X)\ell, &X\leq y\\
			yb, &X>y
		\end{cases}
	\end{equation*}
\end{eg}

\newpage
\begin{thm}
	Given two random variables $X$ and $Y$. Expectation operator $\expect$ has the following properties:
	\begin{enumerate}
		\item For any $a\leq b$, if $a\leq X\leq b$, then $a\leq\expect{X}\leq b$.
		\item If $X\geq 0$, then $\expect X\geq 0$.
		\item If $a,b\in\mathbb{R}$, then $\expect(aX+bY)=a\expect X+b\expect Y$.
	\end{enumerate}
\end{thm}
\begin{proofing}
	\begin{enumerate}
		\item If $X$ is discrete, then:
		\begin{align*}
			\expect{X}&=\sum_{x}xf_{X}(x)\geq\sum_{x}af_{X}(x)=a & \expect{X}&=\sum_{x}xf_{X}(x)\leq\sum_{x}bf_{X}(x)=b
		\end{align*}
		If $X$ is continuous, then:
		\begin{align*}
			\expect{X}&=\intinfty xf_{X}(x)\geq\intinfty af_{X}(x)=a & \expect{X}&=\intinfty xf_{X}(x)\leq\intinfty bf_{X}(x)=b
		\end{align*}
		Therefore, we have $a\leq\expect{X}\leq b$.
		\item If $X$ is discrete, since $f_{X}(x)\geq 0$ for all $x\geq 0$, $\expect{X}=\sum_{x}xf_{X}(x)\geq 0$ if $X\geq 0$.\\
		If $X$ is continuous, since $f_{X}(x)\geq 0$ for all $x\geq 0$, $\expect{X}=\int_{0}^{\infty}xf_{X}(x)\,dx\geq 0$.
		\item When $X$ and $Y$ are discrete,
		\begin{align*}
			\expect(aX+bY)=\sum_{x,y}(ax+by)f_{X,Y}(x,y)&=a\sum_{x}x\left(\sum_{y}f_{X,Y}(x,y)\right)+b\sum_{y}y\left(\sum_{x}f_{X,Y}(x,y)\right)\\
			&=a\sum_{x}xf_{X}(x)+b\sum_{y}yf_{Y}(y)=a\expect{X}+b\expect{Y}
		\end{align*}
		When $X$ and $Y$ are continuous,
		\begin{align*}
			\expect(aX+bY)=\intinfty\intinfty(ax+by)f_{X,Y}(x,y)\,dy\,dx&=a\intinfty x\intinfty f_{X,Y}(x,y)\,dy\,dx+b\intinfty\intinfty yf_{X,Y}(x,y)\,dy\,dx\\
			&=\intinfty xf_{X}(x)\,dx+b\intinfty yf_{Y}(y)\,dy=a\expect{X}+b\expect{Y}
		\end{align*}
	\end{enumerate}
\end{proofing}
By applying the part 3 of the above Theorem, we obtain the following result. 
\begin{lem}(\textbf{Linearity of expectation}) 
	More generally, for any sequence of random variables $\{X_{1},X_{2},\cdots,X_{n}\}$, we have
	\begin{equation*}
		\expect\left(\sum_{i=1}^{n}a_{i}X_{i}\right)=\sum_{i=1}^{n}a_{i}\expect X_{i}
	\end{equation*}
\end{lem}
\begin{thm}(\textbf{Tail Sum Formula})
	\label{Chapter 6 (Theorem) Tail sum formula}
	If discrete random variable $X$ has a PMF $f_{X}$ satisfies $f_{X}(x)=0$ when $x<0$, then:
	\begin{equation*}
		\expect{X}=\sum_{k=0}^{\infty}\prob(X>k)
	\end{equation*}
	If continuous random variable $X$ has a PDF $f_{X}$ satisfies $f_{X}(x)=0$ when $x<0$, then:
	\begin{equation*}
		\expect{X}=\int_{0}^{\infty}\prob(X>x)\,dx
	\end{equation*}
\end{thm}
\begin{proofing}
	For discrete random variable $X$ with $f_{X}(x)$ for any $x<0$,
	\begin{equation*}
		\sum_{k=0}^{\infty}\prob(X>k)=\sum_{k=1}^{\infty}\prob(X\geq k)=\sum_{k=1}^{\infty}\sum_{i=k}^{\infty}\prob(X=i)=\sum_{k=1}^{\infty}k\prob(X=k)=\expect{X}
	\end{equation*}
	For continuous random variable $X$ with $f_{X}(x)$ for any $x<0$,
	\begin{equation*}
		\int_{0}^{\infty}\prob(X>x)\,dx=\int_{0}^{\infty}\int_{x}^{\infty}f_{X}(y)\,dy\,dx=\int_{0}^{\infty}\int_{0}^{y}f_{X}(y)\,dx\,dy=\int_{0}^{\infty}yf_{X}(y)\,dy=\expect{X}
	\end{equation*}
\end{proofing}
The following lemma is a formula I developed just for proving the next theorem.
\begin{lem}
	\label{Chapter 6 (Lemma) Expectation as integral of CDF}
	If continuous random variable $X$ has a PDF $f_{X}$ with $f_{X}(x)=0$ when $x>0$, and a CDF $F_{X}$, then
	\begin{equation*}
		\expect X=\intlu{-\infty}{0}-F_{X}(x)\,dx
	\end{equation*}
\end{lem}
\begin{proofing}
	\begin{equation*}
		\intlu{-\infty}{0}-F_{X}(x)\,dx=\intlu{-\infty}{0}\intlu{-\infty}{x}-f_{X}(y)\,dy\,dx=\intlu{-\infty}{0}\intlu{y}{0}-f_{X}(y)\,dx\,dy=\intlu{-\infty}{0}yf_{X}(y)\,dy=\expect X
	\end{equation*}
\end{proofing}
\begin{thm}
	\label{Chapter 6 (Theorem) Expectation of function of random variable}
	Given a function $g:\mathbb{R}\to\mathbb{R}$ and a random variable $X$.
	\begin{enumerate}
		\item If  $X$ is discrete with a PMF $f_{X}(x)$, and $g(X)$ is still a discrete random variable, then:
		\begin{equation*}
			\expect{g(X)}=\sum_{x}g(x)f_{X}(x)
		\end{equation*}
		whenever this sum is absolutely convergent.
		\item If $X$ is continuous with a PDF $f_{X}(x)$, and $g(X)$ is still a continuous random variable, then:
		\begin{equation*}
			\expect{g(X)}=\intinfty g(x)f_{X}(x)\,dx
		\end{equation*}
		whenever this integral exists.
	\end{enumerate}
\end{thm}
\begin{proofing}
	\begin{enumerate}
		\item We first tackle the first part. Let $Y=g(X)$. We have:
		\begin{align*}
			\sum_{x}g(x)f_{X}(x)=\sum_{y}\sum_{x:g(x)=y}g(x)f_{X}(x)&=\sum_{y}y\left(\sum_{x:g(x)=y}\prob(\{\omega\in\Omega:X(\omega)=x\})\right)\\
			&=\sum_{y}y\prob(\{\omega\in\Omega:g(X(\omega))=g(y)\})\\
			&=\sum_{y}yf_{Y}(y)=\expect Y=\expect g(X)
		\end{align*}
		\item We first consider that $g(x)\geq 0$ for all $x$. Let $B=\{x:g(x)>y\}$. By Lemma \ref{Chapter 6 (Theorem) Tail sum formula},
		\begin{equation*}
			\expect{g(X)}=\intlu{0}{\infty}\prob(g(X)>y)\,dy=\intlu{0}{\infty}\int_{B}f_{X}(x)\,dx\,dy=\intlu{0}{\infty}\intlu{0}{g(x)}f_{X}(x)\,dy\,dx=\intlu{0}{\infty}g(x)f_{X}(x)\,dx
		\end{equation*}
		We then consider that $g(x)\leq 0$ for all $x$. Let $C=\{x:g(x)<z\}$. By Lemma \ref{Chapter 6 (Lemma) Expectation as integral of CDF},
		\begin{equation*}
			\expect{g(X)}=\intlu{-\infty}{0}-F_{g(X)}(z)\,dz=\intlu{-\infty}{0}\int_{C}-f_{X}(x)\,dx\,dz=\intlu{-\infty}{0}\intlu{g(x)}{0}-f_{X}(x)\,dz\,dx=\intlu{-\infty}{0}g(x)f_{X}(x)\,dx
		\end{equation*}
		Now we combined both formulas into one. If $g(X)$ is a random variable, then:
		\begin{equation*}
			\expect{g(X)}=\int_{0}^{\infty}g(x)f_{X}(x)\,dx+\intlu{-\infty}{0}g(x)f_{X}(x)\,dx=\intinfty g(x)f_{X}(x)\,dx
		\end{equation*}
	\end{enumerate}
\end{proofing}

\newpage
\begin{thm}
	Given a function $g:\mathbb{R}^{2}\to\mathbb{R}$ and two random variables $X$ and $Y$.
	\begin{enumerate}
		\item If $X$ and $Y$ are jointly discrete with JPMF $f_{X,Y}(x,y)$, and $g(X,Y)$ is a discrete random variable, then:
		\begin{equation*}
			\expect{g(X,Y)}=\sum_{y}\sum_{x}g(x,y)f_{X,Y}(x,y)
		\end{equation*}
		\item If $X$ and $Y$ are jointly continuous with JPDF $f_{X,Y}(x,y)$, and $g(X,Y)$ is a continuous random variable, then:
		\begin{equation*}
			\expect{g(X,Y)}=\intinfty\intinfty g(x,y)f_{X,Y}(x,y)\,dx\,dy
		\end{equation*}
	\end{enumerate} 
\end{thm}
\begin{proofing}
	\begin{enumerate}
		\item Let $Z=g(X,Y)$. We have.
		\begin{align*}
			\sum_{x,y}g(x,y)f_{X,Y}(x,y)=\sum_{z}\sum_{x,y:g(x,y)=z}g(x,y)f_{X,Y}(x,y)&=\sum_{z}z\left(\sum_{x,y:g(x,y)=z}\prob((X,Y)=(x,y))\right)\\
			&=\sum_{z}z\prob(\{\omega\in\Omega:g(X,Y)(\omega)=z\})\\
			&=\sum_{z}zf_{Z}(z)=\expect Z=\expect g(X,Y)
		\end{align*}
		\item \textit{We will not prove it. Just note that it works similar to previous theorem.}
	\end{enumerate}
\end{proofing}
\begin{rem}
	We may generalize it into a random vector. 
\end{rem}
We have some special terms for some special expectations.
\begin{defn}
	Let $k\in\mathbb{N}_{+}$. We have a special term for each of the following expectations:
	\begin{enumerate}
		\item The \textbf{$k$-th moment} $m_{k}$ of $X$ is defined by:
		\begin{equation*}
			m_{k}=\expect(X^{k})
		\end{equation*}
		\item The \textbf{$k$-th central moment} $\alpha_{k}$ is defined by:
		\begin{equation*}
			\alpha_{k}=\expect(X-\expect X)^{k}=\expect(X-m_{1})^{k}
		\end{equation*}
	\end{enumerate}
\end{defn}
\begin{rem}
	Not all random variables have $k$-th moments for all $k\in\mathbb{N}_{+}$.
\end{rem}
\begin{rem}
	We cannot use finite number of moments to uniquely determine a distribution with $k$-th moments for all $k\in\mathbb{N}$.
\end{rem}
\begin{defn}
	Given a random variable $X$.
	\begin{enumerate}
		\item \textbf{Mean} of $X$ is the $1$st moment, denoted by $\mu$, defined by:
		\begin{equation*}
			\mu=\expect{X}
		\end{equation*}
		\item \textbf{Variance} of $X$ is the $2$nd central moment, denoted by $\Var(X)$, defined by:
		\begin{equation*}
			\Var(X)=\expect(X-\mu)^{2}=\expect(X^{2})-\mu^{2}
		\end{equation*}
		\item \textbf{Standard deviation} of $X$, denoted by $\sigma$, is defined by:
		\begin{equation*}
			\sigma=\sqrt{\Var(X)}
		\end{equation*}
	\end{enumerate}
\end{defn}
\begin{lem}
	If two random variables $X$ and $Y$ are independent, then $\expect(XY)=\expect{X}\expect{Y}$.
\end{lem}
\begin{proofing}
	If $X$ and $Y$ are both discrete,
	\begin{equation*}
		\expect(XY)=\sum_{x,y}xyf_{X,Y}(x,y)=\sum_{x,y}xyf_{X}(x)f_{Y}(y)=\sum_{x}xf_{X}(x)\sum_{y}yf_{Y}(y)=\expect X\expect Y
	\end{equation*}
	If $X$ and $Y$ are both continuous,
	\begin{equation*}
		\expect(XY)=\intinfty\intinfty xyf_{X,Y}(x,y)\,dy\,dx=\intinfty\intinfty xyf_{X}(x)f_{Y}(y)\,dy\,dx=\intinfty xf_{X}(x)\,dx\intinfty yf_{Y}(y)\,dy=\expect{X}\expect{Y}
	\end{equation*}
\end{proofing}
\begin{rem}
	The converse is not generally true.
\end{rem}
We may generalize it into a function of $X$ and $Y$. It is very important, as it implies that two resultant random variables of function $g(X)$ and $h(Y)$ are "uncorrelated" as long as two random variables from the domain $X$ and $Y$ are independent.
\begin{thm}
	Given two random variables $X$ and $Y$ and two functions $g,h:\mathbb{R}\to\mathbb{R}$ such that $g(X)$ and $h(Y)$ are still random variables. Let $X$ and $Y$ be independent. If $\expect(g(X)h(Y)),\expect{g(X)}$ and $\expect{h(Y)}$ exist, then $\expect(g(X)h(Y))=\expect g(X)\expect h(Y)$.
\end{thm}
\begin{proofing}
	If $X$ and $Y$ are both discrete,
	\begin{equation*}
		\expect(g(X)h(Y))=\sum_{x,y}g(x)h(y)f_{X,Y}(x,y)=\sum_{x,y}g(x)h(y)f_{X}(x)f_{Y}(y)=\sum_{x}g(x)f_{X}(x)\sum_{y}h(y)f_{Y}(y)=\expect g(X)\expect h(Y)
	\end{equation*}
	If $X$ and $Y$ are both continuous,
	\begin{align*}
		\expect(g(X)h(Y))=\intinfty\intinfty g(x)h(y)f_{X,Y}(x,y)\,dy\,dx&=\intinfty\intinfty g(x)h(y)f_{X}(x)f_{Y}(y)\,dy\,dx\\
		&=\intinfty g(x)f_{X}(x)\,dx\intinfty h(y)f_{Y}(y)\,dy=\expect{g(X)}\expect{h(Y)}
	\end{align*}
\end{proofing}
We can use the properties of expectations to deduce the properties of variance.
\begin{thm}
	For random variables $X$ and $Y$,
	\begin{enumerate}
		\item $\Var(aX+b)=a^{2}\Var(X)$ for all $a,b\in\mathbb{R}$.
		\item $\Var(X+Y)=\Var(X)+\Var(Y)$ if $X$ and $Y$ are uncorrelated.
	\end{enumerate}
\end{thm}
\begin{proofing}
	\begin{enumerate}
		\item Using linearity of $\expect$,
		\begin{equation*}
			\Var(aX+b)=\expect((aX+b-\expect(aX+b))^{2})=\expect(a^{2}(X-\expect X)^{2})=a^{2}\expect((X-\expect X)^{2})=a^{2}\Var(X)
		\end{equation*}
		\item When $X$ and $Y$ are uncorrelated,
		\begin{align*}
			\Var(X+Y)=\expect((X+Y-\expect(X+Y))^{2})&=\expect((X-\expect X)^{2}+2(XY-\expect X\expect Y)+(Y-\expect Y)^{2})\\
			&=\Var(X)+2(\expect(XY)-\expect(X)\expect(Y))+\Var(Y)\\
			&=\Var(X)+\Var(Y)
		\end{align*}
	\end{enumerate}
\end{proofing}

\newpage
\section{Conditional expectation}
Sometimes, it is not practical to find $\expect{X}$ itself. What if we want to find the expectation of $X$ given that another result happened? Similar with conditional probability, we may also have conditional expectation too.
\begin{defn}
	Given two random variables $X$ and $Y$.
	\begin{enumerate}
		\item If $X$ and $Y$ is discrete, then the \textbf{conditional expectation} $\psi$ of $Y$ given $X=x$ for any $x$ is defined by:
		\begin{equation*}
			\psi(x)=\expect(Y|X=x)=\sum_{y}yf_{Y|X}(y|x)
		\end{equation*}
		\item If $X$ and $Y$ is continuous, then the \textbf{conditional expectation} $\psi$ of $Y$ given $X=x$ for any $x$ is defined by:
		\begin{equation*}
			\psi(x)=\expect(Y|X=x)=\intinfty yf_{Y|X}(y|x)\,dy
		\end{equation*}
	\end{enumerate}
	\textbf{Conditional expectation} $\psi$ of $Y$ given $X$ is defined by:
	\begin{equation*}
		\psi(X)=\expect(Y|X)
	\end{equation*}
\end{defn}
\begin{eg}
	Assume we roll a fair dice.
	\begin{align*}
		\Omega&=\{1,2,\cdots,6\} & Y(\omega)&=\omega & X(\omega)&=\begin{cases}
			1, &\omega\in\{2,4,6\}\\
			0, &\omega\in\{1,3,5\}
		\end{cases}
	\end{align*}
	We try to guess $Y$. If we do not have any information about $X$, 
	\begin{equation*}
		\expect Y=\argmin_{e}(\expect((Y-e)^{2}))=3.5
	\end{equation*}
	If we know that $X=x$, in which we have two cases: $X=1$ and $X=0$
	\begin{align*}
		f_{Y|X}(y|1)&=\frac{\prob(X=1,Y=y)}{\prob(X=1)}=\begin{cases}
			\frac{1}{3}, &y=2,4,6\\
			0, &y=1,3,5
		\end{cases} & f_{Y|X}(y|0)&=\frac{\prob(X=0,Y=y)}{\prob(X=0)}=\begin{cases}
			0, &y=2,4,6\\
			\frac{1}{3}, &y=1,3,5
		\end{cases}\\
		\expect(Y|X=1)&=\sum_{y}yf_{Y|X}(y|1)=\frac{2+4+6}{3}=4 & \expect(Y|X=0)&=\frac{1+3+5}{3}=3
	\end{align*}
	Finally, if we want to guess $Y$ based on the future information of $X$,
	\begin{equation*}
		\psi(X)=\expect(Y|X)=4(\mathbf{1}_{X=1})+3(\mathbf{1}_{X=0})
	\end{equation*}
\end{eg}
\begin{eg}
	If $Y=X$, then $\psi(X)=\expect(Y|X)=\expect(X|X)=X$.
\end{eg}
\begin{lem}
	Given two random variables $X$ and $Y$. We have the following properties:
	\begin{enumerate}
		\item $\expect(aY+bZ|X)=a\expect(Y|X)+b\expect(Z|X)$
		\item If $Y\geq 0$, then $\expect(Y|X)\geq 0$.
		\item If $X$ and $Y$ are independent, then $\expect(Y|X)=\expect(Y)$.
	\end{enumerate}
\end{lem}
\begin{proofing}
	\begin{enumerate}
		\item If $X$, $Y$ and $Z$ are discrete, then for all $x$,
		\begin{align*}
			\expect(aY+bZ|X=x)=\sum_{y,z}(ay+bz)\prob(Y=y,Z=z|X=x)&=a\sum_{y,z}y\prob(Y=y,Z=z|X=x)+b\sum_{y,z}\prob(Y=y,Z=z|X=x)\\
			&=a\sum_{y}yf_{Y|X}(y|x)+b\sum_{z}zf_{Z|X}(z|x)\\
			&=a\expect(Y|X=x)+b\expect(Z|X=x)
		\end{align*}
		If $X$, $Y$ and $Z$ are continuous, then for all $x$,
		\begin{align*}
			\expect(aY+bZ|X=x)&=\intinfty\intinfty(ay+bz)\frac{f_{Y,Z,X}(y,z,x)}{f_{X}(x)}\,dy\,dz\\
			&=a\intinfty y\intinfty\frac{f_{Y,Z,X}(y,z,x)}{f_{X}(x)}\,dz\,dy+b\intinfty z\intinfty\frac{f_{Y,Z,X}(y,z,x)}{f_{X}(x)}\,dy\,dz\\
			&=a\intinfty y\frac{f_{Y,X}(y,x)}{f_{X}(x)}\,dy+b\intinfty z\frac{f_{Z,X}(z,x)}{f_{X}(x)}\,dz=a\expect(Y|X=x)+b\expect(Z|X=x)
		\end{align*}
		Therefore, $\expect(aY+bZ|X)=a\expect(Y|X)+b\expect(Z|X)$.
		\item If $X$ and $Y$ are discrete, then for all $x$,
		\begin{equation*}
			\expect(Y|X=x)=\sum_{y}yf_{Y|X}(y|x)\geq 0
		\end{equation*}
		If $X$ and $Y$ are continuous, then for all $x$,
		\begin{equation*}
			\expect(Y|X=x)=\intinfty yf_{Y|X}(y|x)\,dy\geq 0
		\end{equation*}
		Therefore, $\expect(Y|X)\geq 0$ if $Y\geq 0$.
		\item If $X$ and $Y$ are discrete, then for all $x$,
		\begin{equation*}
			\expect(Y|X=x)=\sum_{y}f_{Y|X}(y|x)=\sum_{y}f_{Y}(y)=\expect{Y}
		\end{equation*}
		If $X$ and $Y$ are continuous, then for all $x$,
		\begin{equation*}
			\expect(Y|X=x)=\intinfty yf_{Y|X}(y|x)\,dy=\intinfty yf_{Y}(y)\,dy=\expect{Y}
		\end{equation*}
		Therefore, if $X$ and $Y$ are independent, then $\expect(Y|X)=\expect{Y}$.
	\end{enumerate}
\end{proofing}
In fact, we can extend the definition of conditional expectation into $\sigma$-field.
\begin{defn}
	Given a random variable $Y$ and a $\sigma$-field $\mathcal{H}\subseteq\mathcal{F}$. $\expect(Y|\mathcal{H})$ is any random variable $Z$ satisfying the following properties:
	\begin{enumerate}
		\item $Z$ is $\mathcal{H}$-measurable. ($Z^{-1}(B)\in\mathcal{H}$ for all $B\in\mathcal{B}(\mathbb{R})$)
		\item $\expect(Y\mathbf{1}_{A})=\expect(Z\mathbf{1}_{A})$ for all $A\in\mathcal{H}$.
	\end{enumerate}
\end{defn}
\begin{rem}
	Under this definition,
	\begin{equation*}
		\expect(Y|X)=\expect(Y|\sigma(X))
	\end{equation*}
\end{rem}
\begin{thm}(\textbf{Law of Total Expectation})
	\label{Chapter 6 (Theorem) Law of total expectation}
	Given two random variables $X$ and $Y$. Conditional expectation $\psi(X)=\expect(Y|X)$ satisfies:
	\begin{equation*}
		\expect{\psi(X)}=\expect{Y}
	\end{equation*}
\end{thm}
\begin{proofing}
	We can apply Theorem \ref{Chapter 6 (Theorem) Expectation of function of random variable}. If $X$ and $Y$ are discrete, 
	\begin{equation*}
		\expect{\psi(X)}=\sum_{x}\psi(x)f_{X}(x)=\sum_{x,y}yf_{Y|X}(y|x)f_{X}(x)=\sum_{x,y}yf_{X,Y}(x,y)=\sum_{y}yf_{Y}(y)=\expect{Y}
	\end{equation*}
	If $X$ and $Y$ are continuous,
	\begin{equation*}
		\expect{\psi(X)}=\intinfty\psi(x)f_{X}(x)\,dx=\intinfty\intinfty yf_{Y|X}(y|x)f_{X}(x)\,dy\,dx=\intinfty\intinfty yf_{X,Y}(x,y)\,dy\,dx=\intinfty y\intinfty f_{X,Y}(x,y)\,dx\,dy=\intinfty yf_{Y}(y)\,dy=\expect{Y}
	\end{equation*}
	The proof is similar if one of them is discrete and another is continuous.
\end{proofing}
\begin{eg}
	A miner is trapped in a mine with doors, each will lead to a tunnel.\\
	Tunnel 1 will help the miner reach safety after 3 hours respectively.\\
	However, tunnel 2 and 3 will send the miner back after 5 and 7 hours respectively.\\
	What is the expected amount of time the miner need to reach safety? (Assume that the miner is memoryless)\\
	Let $X$ be the amount of time to reach safety, $Y$ be the door number he chooses for the first time.
	\begin{align*}
		\expect X&=\expect(\expect(X|Y))=\sum_{k=1}^{3}\expect(X|Y=k)\prob(Y=k)=3\left(\frac{1}{3}\right)+(\expect X+5)\left(\frac{1}{3}\right)+(\expect X+7)\left(\frac{1}{3}\right)\\
		\expect X&=15
	\end{align*}
	What is the expected amount of time the miner needed to reach safety after he chose the second door and sent back?\\
	Let $\widetilde{X}$ be the time for the miner to reach safety after the first round. 
	\begin{align*}
		\expect(X|Y=2)&=\sum_{x}xf_{X|Y}(x|2)=\sum_{x}x\frac{\prob(X=x, Y=2)}{\prob(Y=2)}=\sum_{x}x\frac{\prob(\widetilde{X}=x-5,Y=2)}{\prob(Y=2)}=\sum_{\widetilde{x}}(\widetilde{x}+5)\prob(\widetilde{X}=\widetilde{x})=\expect X+5
	\end{align*}
\end{eg}
\begin{eg}
	We consider a sum of random number of random variables.\\
	Let $N$ be the number of customers and $X_{i}$ be the amount of money spent by the $i$-th customers.\\
	Assume that $N$ and $X_{i}$'s are all independent and $\expect X_{i}=\expect X$, what is the expected total amount of money spent by all $N$ customers?
	\begin{align*}
		\expect\left(\sum_{i=1}^{N}X_{i}\right)&=\expect\left(\expect\left(\left.\sum_{i=1}^{N}X_{i}\right| N\right)\right)\\
		&=\sum_{n=0}^{\infty}\expect\left(\left.\sum_{i=1}^{N}X_{i}\right|N=n\right)\prob(N=n)\\
		&=\sum_{n=0}^{\infty}\sum_{y}y\left(\frac{\prob\left(\sum_{i=1}^{N}X_{i}=y,N=n\right)}{\prob(N=n)}\right)\prob(N=n)\\
		&=\sum_{n=0}^{\infty}\sum_{y}y\prob\left(\sum_{i=1}^{n}X_{i}=y\right)\prob(N=n)\\
		&=\sum_{n=0}^{\infty}\expect\left(\sum_{i=1}^{n}X_{i}\right)\prob(N=n)\\
		&=\sum_{n=0}^{\infty}n\expect X\prob(N=n)=\expect N\expect X
	\end{align*}
\end{eg}
The following theorem is the generalization of Law of total expectation.
\begin{thm}
	\label{Chapter 6 (Theorem) Generalization of Law of Total Expectation}
	Given two random variables $X$ and $Y$. Conditional expectation $\psi(X)=\expect(Y|X)$ satisfies:
	\begin{equation*}
		\expect(\psi(X)g(X))=\expect(Yg(X))
	\end{equation*}
	for any function $g$ for which both expectations exist.
\end{thm}
\begin{proofing}
	We can apply Theorem \ref{Chapter 6 (Theorem) Expectation of function of random variable}. If $X$ and $Y$ are discrete,
	\begin{equation*}
		\expect(\psi(X)g(X))=\sum_{x}\psi(x)g(x)f_{X}(x)=\sum_{x,y}yf_{Y|X}(y|x)g(x)f_{X}(x)=\sum_{x,y}yf_{X,Y}(x,y)g(x)=\expect(Yg(X))
	\end{equation*}
	If $X$ and $Y$ are continuous,
	\begin{align*}
		\expect(\psi(X)g(X))&=\intinfty\psi(x)g(x)f_{X}(x)\,dx=\intinfty\intinfty yf_{Y|X}(y|x)f_{X}(x)g(x)\,dy\,dx=\intinfty\intinfty yf_{X,Y}(x,y)g(x)\,dy\,dx=\expect(Yg(X))
	\end{align*}
\end{proofing}

\newpage
If there is conditional expectation, there will also be conditional variance.
\begin{defn}
	Given two random variables $X$ and $Y$. \textbf{Conditional variance} is defined by:
	\begin{equation*}
		\Var(Y|X)=\expect((Y-\expect(Y|X))^{2}|X)
	\end{equation*}
\end{defn}
We can obtain the variance of a random variables based on conditional variance.
\begin{thm}(\textbf{Law of Total Variance}) 
	Given two random variables $X$ and $Y$. We have:
	\begin{equation*}
		\Var(Y)=\expect(\Var(Y|X))+\Var(\expect(Y|X))
	\end{equation*}
\end{thm}
\begin{proofing}
	Using Theorem \ref{Chapter 6 (Theorem) Law of total expectation} and Theorem \ref{Chapter 6 (Theorem) Generalization of Law of Total Expectation},
	\begin{align*}
		\expect(\Var(Y|X))+\Var(\expect(Y|X))&=\expect(\expect((Y-\expect(Y|X))^{2}|X))+\expect(\expect(Y|X))^{2}-(\expect(\expect(Y|X)))^{2}\\
		&=\expect(Y-\expect(Y|X))^{2}+\expect(\expect(Y|X))^{2}-(\expect{Y})^{2}\\
		&=\expect(Y^{2})-2\expect(Y\expect(Y|X))+\expect(\expect(Y|X))^{2}+\expect(\expect(Y|X))^{2}-(\expect{Y})^{2}\\
		&=\expect(Y^{2})-2\expect(\expect(Y|X))^{2}+2\expect(\expect(Y|X))^{2}-(\expect{Y})^{2}\\
		&=\expect(Y^{2})-(\expect{Y})^{2}=\Var(Y)
	\end{align*}
\end{proofing}
Sometimes, we want to find the tendency in the linear relationship between two random variables. We say it is called the covariance of two random variables.
\begin{defn}
	\textbf{Covariance} of two random variables $X$ and $Y$ is:
	\begin{equation*}
		\cov(X,Y)=\expect((X-\expect X)(Y-\expect Y))=\expect(XY)-\expect X\expect Y
	\end{equation*}
\end{defn}
\begin{rem}
	Magnitude of covariance is the geometric mean of the variances of two random variables. The sign represents the linear relationship between two random variables. If the sign is positive, then two random variables show similar behaviour. If the sign is negative, then two random variables show opposite behaviour.
\end{rem}
\begin{lem}
	For any random variables $X$, $Y$ and $Z$, we have:
	\begin{enumerate}
		\item $\Var(X)=\cov(X,X)$.
		\item $\cov(X,Y)=\cov(Y,X)$
		\item $\cov(X,Y+Z)=\cov(X,Y)+\cov(X,Z)$
		\item If $X$ and $Y$ are uncorrelated, then $\cov(X,Y)=0$.
	\end{enumerate}
\end{lem}
\begin{proofing}
	\begin{enumerate}
		\item \begin{equation*}
			\cov(X,X)=\expect((X-\expect{X})(X-\expect{X}))=\expect(X-\expect{X})^{2}=\Var(X)
		\end{equation*}
		\item \begin{equation*}
			\cov(X,Y)=\expect(XY)-\expect{X}\expect{Y}=\expect{YX}-\expect{Y}\expect{X}=\cov(Y,X)
		\end{equation*}
		\item \begin{equation*}
			\cov(X,Y+Z)=\expect(X(Y+Z))-\expect{X}\expect(Y+Z)=\expect(XY)-\expect{X}\expect{Y}+\expect(XZ)-\expect{X}\expect{Z}=\cov(X,Y)+\cov(X,Z)
		\end{equation*}
		\item If $X$ and $Y$ are independent, then
		\begin{equation*}
			\cov(X,Y)=\expect(XY)-\expect{X}\expect{Y}=\expect{X}\expect{Y}-\expect{X}\expect{Y}=0
		\end{equation*}
	\end{enumerate}
\end{proofing}

\newpage
\begin{rem}
	In general, for any random variables $X_{1},X_{2},\cdots,X_{n}$,
	\begin{equation*}
		\Var(X_{1}+X_{2}+\cdots+X_{n})=\sum_{i=1}^{n}\Var(X_{i})+2\sum_{i<j}(\expect(X_{i}X_{j})-\expect X_{i}\expect X_{j})=\sum_{i=1}^{n}\Var(X_{i})+2\sum_{i<j}\cov(X_{i},X_{j})
	\end{equation*}
\end{rem}
\begin{eg}
	If $X_{i}$ are independent and $\Var(X_{i})=1$ for all $i$, then:
	\begin{equation*}
		\Var\left(\sum_{i=1}^{n}X_{i}\right)=\sum_{i=1}^{n}\Var(X_{i})=n
	\end{equation*}
	If $X_{i}=X$ for all $i$ and $\Var(X)=1$, then:
	\begin{equation*}
		\Var\left(\sum_{i=1}^{n}X_{i}\right)=\Var(nX)=n^{2}
	\end{equation*}
\end{eg}
We usually only care about the normalized covariance, which is called correlation coefficient.
\begin{defn}
	\textbf{Population correlation coefficient} between two random variables $X$ and $Y$, denoted by $\rho$, is given by:
	\begin{equation*}
		\rho=\frac{\cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}
	\end{equation*}
	We can find the relationship between $X$ and $Y$ based on their correlation coefficient.
	\begin{enumerate}
		\item If $\rho>0$, then $X$ and $Y$ are \textbf{positively correlated}.
		\item If $\rho<0$, then $X$ and $Y$ are \textbf{negatively correlated}.
		\item If $\rho=0$, then $X$ and $Y$ are \textbf{uncorrelated}.
	\end{enumerate}
\end{defn}
\begin{rem}
	Population correlation coefficient $\rho$ of random variables $X$ and $Y$ satisfies $-1<\rho<1$.
\end{rem}
\begin{rem}
	If $\rho$ is near $1$ or near $-1$, then it shows a strong linear relationship between $X$ and $Y$
\end{rem}
\begin{rem}
	The constant $\rho$ used in bivariate normal distribution is the population correlation coefficient.
\end{rem}
\begin{eg}
	If $X\sim\N(0,1)$ and $Y\sim\N(0,1)$,
	\begin{align*}
		\cov(X,Y)&=\expect(XY)-\expect X\expect Y=\expect(XY)\\
		&=\intinfty\intinfty\frac{y}{\sqrt{2\pi}}e^{-\frac{y^{2}}{2}}\frac{x}{\sqrt{2\pi(1-\rho^{2})}}e^{-\frac{(x-\rho y)^{2}}{2(1-\rho^{2})}}\,dx\,dy\\
		&=\intinfty\frac{y}{\sqrt{2\pi}}e^{-\frac{y^{2}}{2}}\rho y\,dy=\rho\intinfty y^{2}\phi(y)\,dy=\rho
	\end{align*}
\end{eg}
\begin{lem}
	Two random variables are uncorrelated if $\expect(XY)=\expect{X}\expect{Y}$.
\end{lem}
\begin{proofing}
	Based on the definition of the correlation coefficient, $\rho=0$ if and only if $\cov(X,Y)=0$. Therefore,
	\begin{equation*}
		\cov(X,Y)=\expect(XY)-\expect{X}\expect{Y}=0\iff \expect(XY)=\expect{X}\expect{Y}
	\end{equation*}
\end{proofing}

\newpage
\begin{rem}
	If $X$ and $Y$ are independent, then they are uncorrelated. The converse is generally not true.
\end{rem}
\begin{eg}
	Let $X$ be such that $f_{X}(0)=f_{X}(1)=f_{X}(-1)=\frac{1}{3}$ and $Y$ be such that $Y=0$ if $X\neq0$ and $Y=1$ if $X=0$.
	\begin{align*}
		\expect(XY)&=0 &  \expect X&=0=\expect(XY)
	\end{align*}
	However,
	\begin{align*}
		\prob(X=0,Y=0)&=0 & \prob(X=0)&\neq 0 & \prob(Y=0)&\neq 0 & \prob(X=0)\prob(Y=0)&\neq 0
	\end{align*}
	Therefore, $X$ and $Y$ are uncorrelated, but they are not independent.
\end{eg}
When would the converse be true? It turns out it is true when $X$ and $Y$ are uncorrelated and bivariate normal. 
\begin{thm}
	\label{Chapter 6 (Theorem) Bivariate normal and uncorrelated}
	Random variables $X\sim\N(\mu_{X},\sigma_{X}^{2})$ and $Y\sim\N(\mu_{Y},\sigma_{Y}^{2})$ are bivariate normal and uncorrelated if and only if $X$ and $Y$ are independent normal.
\end{thm}
\begin{proofing}
	Since $X$ and $Y$ are uncorrelated, $\cov(X,Y)=0$ and thus population correlation coefficient $\rho=0$.\\
	Therefore, we have:
	\begin{align*}
		f_{X,Y}(x,y)&=\frac{1}{2\pi\sigma_{X}\sigma_{Y}}\exp\left(-\frac{1}{2}\left(\left(\frac{x-\mu_{X}}{\sigma_{X}}\right)^{2}+\left(\frac{y-\mu_{Y}}{\sigma_{Y}}\right)^{2}\right)\right)\\
		&=\left(\frac{1}{\sqrt{2\pi\sigma_{X}^{2}}}\exp\left(-\frac{(x-\mu_{X})^{2}}{2\sigma_{X}^{2}}\right)\right)\left(\frac{1}{\sqrt{2\pi\sigma_{Y}^{2}}}\exp\left(-\frac{(y-\mu_{Y})^{2}}{2\sigma_{Y}^{2}}\right)\right)\\
		&=f_{X}(x)f_{Y}(y)
	\end{align*}
	Therefore, $X$ and $Y$ are independent if $X$ and $Y$ are uncorrelated bivariate normal.\\
	If $X$ and $Y$ are independent normal, then we have:
	\begin{equation*}
		f_{X,Y}(x,y)=f_{X}(x)f_{Y}(y)
	\end{equation*}
	Therefore, $X$ and $Y$ are both uncorrelated and bivariate normal with $\rho=0$.
\end{proofing}
\section{Expectation and Variance of distributions}
In this section, we will primarily focus on finding the expectation and variance of distributions we have taught.
\begin{thm}
	Given a discrete variable $X$.
	\begin{enumerate}
		\item If $X\sim\Bern(p)$, then $\expect{X}=p$ and $\Var(X)=p(1-p)$.
		\item If $X\sim\Bin(n,p)$, then $\expect{X}=np$ and $\Var(X)=np(1-p)$.
	\end{enumerate}
\end{thm}
\begin{proofing}
	\item If $X\sim\Bern(p)$,
	\begin{align*}
		\expect{X}&=0(1-p)+1(p)=p & \Var(X)&=\expect(X^{2})-(\expect{X})^{2}=p-p^{2}=p(1-p)
	\end{align*}
	\item If $X\sim\Bin(n,p)$, then by definition, $X=Y_{1}+\cdots+Y_{n}$ where $Y_{i}\sim\Bern(p)$. Therefore, by 
\end{proofing}
\begin{thm}
	If $X\sim\Geom(p)$, then $\expect{X}=\frac{1}{p}$ and $\Var(X)=\frac{1-p}{p^{2}}$.
\end{thm}
\begin{proofing}
	\begin{align*}
		\expect{X}&=\sum_{k=1}^{\infty}kp(1-p)^{k-1}=p\sum_{k=1}^{\infty}k(1-p)^{k-1}=\frac{p}{p^{2}}=\frac{1}{p}\\
		\Var(X)&=\expect(X(x-1))-\expect{X}+(\expect{X})^{2}=\frac{1}{p}-\frac{1}{p^{2}}+\sum_{k=2}^{\infty}k(k-1)p(1-p)^{k-1}\\
		&=\frac{1}{p}-\frac{1}{p^{2}}+p(1-p)\sum_{k=2}^{\infty}k(k-1)(1-p)^{k-2}\\
		&=\frac{2p(1-p)}{p^{3}}=\frac{2(1-p)}{p^{2}}
	\end{align*}
\end{proofing}
\begin{eg}
	Assume we have $N$ different types of card and each time one gets a card to be any one of the $N$ types. Each types is equally likely to be gotten. What is the expected number of types of card we can get if we get $n$ cards?\\
	Let $X=X_{1}+X_{2}+\cdots+X_{N}$ where $X_{i}=1$ if at least one type $i$ card is among the $n$ cards and otherwise $0$.
	\begin{align*}
		\expect X_{i}&=\prob(X_{i}=1)=1-\left(\frac{N-1}{N}\right)^{n} & \expect X&=\sum_{i=1}^{N}\expect X_{i}=N\left(1-\left(\frac{N-1}{N}\right)^{n}\right)
	\end{align*}
	What is the expected number of cards one needs to collect in order to get all $N$ types?\\
	Let $Y=Y_{0}+Y_{1}+\cdots+Y_{N-1}$ where $Y_{i}$ is the number of additional cards we need to get in order to get a new type after having $i$ distinct types.
	\begin{align*}
		\tag{$Y_{i}\sim\Geom\left(\frac{N-i}{N}\right)$}
		\prob(Y_{i}=k)&=\left(\frac{i}{N}\right)^{k-1}\frac{N-i}{N}\\
		\expect Y&=\sum_{i=0}^{N-1}\expect Y_{i}=\sum_{i=0}^{N-1}\frac{N}{N-i}=N\left(\frac{1}{N}+\frac{1}{N-1}+\cdots+1\right)
	\end{align*}
\end{eg}
\begin{thm}
	If $X\sim\NBin(r,p)$, then $\expect{X}=\frac{r}{p}$ and $\Var(X)=\frac{r(1-p)}{p^{2}}$.
\end{thm}
\begin{proofing}
	Assume that $X_{i}\sim\Geom(p)$ for all $i$. Since $X$ is the sum of $r$ independent geometric random variable, we get:
	\begin{align*}
		\expect{X}&=\sum_{k=1}^{r}\expect{X_{k}}=\frac{r}{p} & \Var(X)&=\sum_{k=1}^{r}\Var(X_{k})=\frac{r(1-p)}{p^{2}}
	\end{align*}
\end{proofing}
\begin{thm}
	If $X\sim\Poisson(\lambda)$, then $\expect{X}=\lambda$ and $\Var(X)=\lambda$.
\end{thm}
\begin{proofing}
	\begin{align*}
		\expect{X}&=\sum_{k=1}^{\infty}\frac{\lambda^{k}}{(k-1)!}e^{-\lambda}=\lambda\sum_{k=1}^{\infty}\frac{\lambda^{k-1}}{(k-1)!}e^{-\lambda}=\lambda\\
		\Var(X)=\expect(X(X-1))+\expect{X}-(\expect{X})^{2}&=\lambda-\lambda^{2}+\sum_{k=2}^{\infty}\frac{\lambda^{k}}{(k-2)!}e^{-\lambda}\\
		&=\lambda-\lambda^{2}+\lambda^{2}\sum_{k=2}^{\infty}\frac{\lambda^{k-2}}{(k-2)!}e^{-\lambda}\\
		&=\lambda-\lambda^{2}+\lambda^{2}=\lambda
	\end{align*}
\end{proofing}
\begin{thm}
	If $X\sim\Hypergeometric(N,m,n)$, then the expectation and variance are:
	\begin{align*}
		\expect{X}&=\frac{mn}{N} & \Var(X)&=\frac{mn}{N}\left(\frac{(m-1)(n-1)}{N-1}+1-\frac{mn}{N}\right)
	\end{align*}
\end{thm}

\newpage
We are not going to prove the variance one. In order to prove the expectation, we imagine the following scenario.
\begin{eg}
	Suppose that we have a set of $N$ balls, which $m$ of them are red and the rest are blue. We choose $n$ of these balls, without replacement. What is the expected number of red balls in our sample? We let $X\sim\Hypergeometric(N,m,n)$.
	
	How do we find $\expect{X}$? Let $X=X_{1}+X_{2}+\cdots+X_{m}$, where for all $i$,
	\begin{equation*}
		X_{i}=\begin{cases}
			1, &\text{if the $i$-th red ball is selected}\\
			0, &\text{Otherwise}
		\end{cases}
	\end{equation*}
	From this, for all $i$, we have:
	\begin{equation*}
		\expect{X_{i}}=\frac{\binom{N-1}{n-1}}{\binom{N}{n}}=\frac{\frac{(N-1)!}{(n-1)!(N-n)!}}{\frac{N!}{n!(N-n)!}}=\frac{n}{N}
	\end{equation*}
	Therefore, $\expect{X}=\frac{mn}{N}$.
\end{eg}
\begin{thm}
	If $X\sim\U[a,b]$, then $\expect{X}=\frac{1}{2}(a+b)$ and $\Var(X)=\frac{1}{12}(b-a)^{2}$.
\end{thm}
\begin{proofing}
	\begin{align*}
		\expect{X}&=\int_{a}^{b}\frac{x}{b-a}\,dx=\frac{1}{2}(a+b)\\
		\Var(X)&=-(\expect{X})^{2}+\expect(X^{2})\\
		&=-\frac{1}{4}(a+b)^{2}+\int_{a}^{b}\frac{x^{2}}{b-a}\,dx\\
		&=-\frac{1}{4}(a^{2}+2ab+b^{2})+\frac{1}{3}(a^{2}+ab+b^{2})\\
		&=\frac{1}{12}(a^{2}-2ab+b^{2})=\frac{1}{12}(b-a)^{2}
	\end{align*}
\end{proofing}
\begin{thm}
	If $X\sim\Exp(\lambda)$, then $\expect{X}=\frac{1}{\lambda}$ and $\Var(X)=\frac{1}{\lambda^{2}}$.
\end{thm}
\begin{proofing}
	\begin{align*}
		\expect{X}&=\int_{0}^{\infty}x\lambda e^{-\lambda x}\,dx=\left.-xe^{-\lambda x}\right|_{0}^{\infty}+\int_{0}^{\infty}e^{-\lambda x}\,dx=\left.-\frac{1}{\lambda}e^{-\lambda x}\right|_{0}^{\infty}=\frac{1}{\lambda}\\
		\Var(X)=-(\expect{X})^{2}+\expect(X^{2})&=-\frac{1}{\lambda^{2}}+\int_{0}^{\infty}x^{2}\lambda e^{-\lambda x}\,dx\\
		&=-\frac{1}{\lambda^{2}}-\left.x^{2}e^{-\lambda x}\right|_{0}^{\infty}+\int_{0}^{\infty}2xe^{-\lambda x}\,dx\\
		&=-\frac{1}{\lambda^{2}}+\frac{2}{\lambda}\expect{X}\\
		&=-\frac{1}{\lambda^{2}}+\frac{2}{\lambda^{2}}=\frac{1}{\lambda^{2}}
	\end{align*}
\end{proofing}
\begin{thm}
	\label{Chapter 6 (Theorem) Expectation and Variance of normal distribution}
	If $X\sim\N(\mu,\sigma^{2})$, then $\expect{X}=\mu$ and $\Var(X)=\sigma^{2}$.
\end{thm}
\begin{proofing}
	\item Let $x=\sigma z+\mu$.
	\begin{equation*}
		\expect{X}=\frac{1}{\sqrt{2\pi\sigma^{2}}}\intinfty ye^{-\frac{(y-\mu)^{2}}{2\sigma^{2}}}\,dy=\frac{1}{\sqrt{2\pi}}\left(\intinfty\sigma ze^{-\frac{z^{2}}{2}}\,dz+\intinfty\mu e^{-\frac{z^{2}}{2}}\,dz\right)=\frac{\mu}{\sqrt{2\pi}}\intinfty e^{-\frac{z^{2}}{2}}\,dz=\mu
	\end{equation*}
	\begin{equation*}
		\Var(X)=\frac{1}{\sqrt{2\pi\sigma^{2}}}\intinfty(y-\mu)^{2}e^{-\frac{(y-\mu)^{2}}{2\sigma^{2}}}\,dy=\frac{\sigma^{2}}{\sqrt{2\pi}}\intinfty z^{2}e^{-\frac{z^{2}}{2}}\,dz=\frac{-\sigma^{2}}{\sqrt{2\pi}}\intinfty zd\left(e^{-\frac{z^{2}}{2}}\right)=\frac{\sigma^{2}}{\sqrt{2\pi}}\intinfty e^{-\frac{z^{2}}{2}}\,dz=\sigma^{2}
	\end{equation*}
\end{proofing}
\begin{thm}
	All Cauchy random variables $X\sim\Cauchy(\theta)$ do not have defined expectation and variance.
\end{thm}
\begin{proofing}
	We check if $\expect\abs{X}$ is infinite.
	\begin{equation*}
		\expect\abs{X}=\intinfty\frac{\abs{x}}{\pi(1+(x-\theta)^{2})}\,dx=2\int_{0}^{\infty}\frac{x}{\pi(1+(x-\theta)^{2})}\,dx=\infty
	\end{equation*}
	Therefore, expectation and thus variance do not exist.
\end{proofing}
\begin{thm}
	If $X\sim\chi^{2}(n)$, then $\expect{X}=n$ and $\Var(X)=2n$.
\end{thm}

Solving the following expectations are out of our scope.
\begin{thm}
	Given a continuous random variable $X$.
	\begin{enumerate}
		\item If $X\sim\Gam(a,\lambda)$, then $\expect{X}=\frac{\alpha}{\lambda}$ and $\Var(X)=\frac{\alpha}{\lambda^{2}}$.
		\item If $X\sim t(n)$, then:
		\begin{align*}
			\expect{X}&=\begin{cases}
				0, &n>1\\
				\text{undefined}, &\text{Otherwise}
			\end{cases} & \Var(X)&=\begin{cases}
				\frac{n}{n-2}, &n>2\\
				\infty, & 2<n\leq 4\\
				\text{undefined}, &\text{Otherwise}
			\end{cases}
		\end{align*}
		\item If $X\sim\Beta(a,b)$, then $\expect{X}=\frac{a}{a+b}$ and $\Var(X)=\frac{ab}{(a+b)^{2}(a+b+1)}$.
	\end{enumerate}
\end{thm}
\section{Combining expectation from discrete and continuous random variables}
Recall that the expectations are given respectively by
\begin{equation*}
	\expect X=\begin{cases}
		\sum xf_{X}(x), &X\text{ is discrete}\\
		\int xf_{X}(x)\,dx, &X\text{ is continuous}
	\end{cases}
\end{equation*}
We want a notation which incorporates both these cases. Suppose that $X$ has a CDF $F_{X}$. We can rewrite the equations as
\begin{equation*}
	\expect X=\begin{cases}
		\sum x\,dF_{X}(x), &dF_{X}(x)=F_{X}(x)-\lim_{y\to x^{-}}F_{X}(y)=f_{X}(x)\\
		\int x\,dF_{X}(x), &dF_{X}(x)=\pdv{F_{X}}{x}\,dx=f_{X}(x)\,dx
	\end{cases}
\end{equation*}
Instead of using the regular Riemann integral, which cannot deal with discrete case, we can use the Riemann-Stieltjes integral, which is a generalization of the Riemann integral.
\begin{align*}
	\intlu{a}{b}g(x)\,dx&=\lim_{\max_{i}\abs{x_{i+1}-x_{i}}}\sum_{i}g(x_{i}^{*})(x_{i+1}-x_{i})\\
	\intlu{a}{b}g(x)\,dF(x)&=\lim_{\max_{i}\abs{x_{i+1}-x_{i}}}\sum_{i}g(x_{i}^{*})(F(x_{i+1})-F(x_{i}))
\end{align*}
if the limit does not depend on the choice of $x_{i}^{*}\in[x_{i},x_{i+1})$.
\begin{defn}
	\textbf{Expectation} of a random variable $X$ is given by:
	\begin{equation*}
		\expect X=\int x\,dF_{X}
	\end{equation*}
\end{defn}
\begin{lem}
	If $g:\mathbb{R}\to\mathbb{R}$ such that $g(X)$ is also a random variable, then
	\begin{equation*}
		\expect(g(X))=\int g(x)\,dF_{X}
	\end{equation*}
\end{lem}
\begin{rem}
	The notation of $\int g(x)\,dF_{X}(x)$ does not mean Riemann-Stieltjes integral.
\end{rem}
\begin{eg}
	If $g$ is regular (differentiable at every point and every values in the domain maps to a value in range), then
	\begin{equation*}
		\sum_{i}g(x_{i}^{*})(F(x_{i+1}-F(x_{i}))\approx\sum_{i}g(x_{i}^{*})f(x_{i}^{*})(x_{i+1}-x_{i})\approx\int g(x)f(x)\,dx
	\end{equation*}
\end{eg}
\begin{eg}
	In irregular case, assume that the function $g$ is the Dirichlet function. That is
	\begin{align*}
		\mathbf{1}_{\mathbb{Q}}(x)&=\begin{cases}
			1, &x\in\mathbb{Q}\\
			0, &x\not\in\mathbb{Q}
		\end{cases} & \sum_{i}g(x_{i}^{*})(F(x_{i+1})-F(x_{i}))=\sum_{i}g(x_{i}^{*})(x_{i+1}-x_{i})
	\end{align*}
	Since the limit depends on the choice of $x_{i}^{*}$, Riemann-Stieltjes integral of $\mathbf{1}_{\mathbb{Q}}(x)$ with respect to $F(x)=x$ is not well defined.\\
	Therefore, $\expect\mathbf{1}_{\mathbb{Q}}(X)$ cannot be defined as a Riemann-Stieltjes integral.\\
	However, on the other hand,
	\begin{equation*}
		\expect\mathbf{1}_{\mathbb{Q}}(X)=\prob(\mathbf{1}_{\mathbb{Q}}(x)=1)=\prob\circ X^{-1}(\mathbb{Q}\cap[0,1])=0
	\end{equation*}
\end{eg}

\addcontentsline{toc}{chapter}{Summary of Chapter 1-6}
\chapter*{Summary}
\section*{Definition}
\begin{sdefn}
	Given a set with $n$ distinct elements.
	\begin{enumerate}
		\item \textbf{Permutation} of the set is an ordered arrangement of all elements of the set.
		\item If $k\leq n$, \textbf{$k$-permutation} of the set is an ordered arrangement of $k$ elements of the set.
	\end{enumerate}
\end{sdefn}
\begin{sdefn}
	If $k\leq n$, \textbf{$k$-combination} of a set with $n$ distinct elements is an unordered arrangement of $k$ elements of the set.
\end{sdefn}
\begin{sdefn}
	These are the basic object of probabilities.
	\begin{enumerate}
		\item \textbf{Experiment} is an activity that produces distinct and well-defined possibilities called \textbf{outcomes}, denoted by $\omega$.
		\item \textbf{Sample space} is the set of all outcomes of an experiment, denoted by $\Omega$.
		\item \textbf{Event} is a subset of the sample space and is usually represented by $A,B,C,\cdots$.
		\item Outcomes are called \textbf{elementary events}.
	\end{enumerate}	
\end{sdefn}
\begin{sdefn}
	Given two events $A$ and $B$.
	\begin{enumerate}
		\item \textbf{Union} of $A$ and $B$ is an event $A\cup B=\{\omega\in\Omega:\omega\in A\text{ or }\omega\in B\}$.
		\item \textbf{Intersection} of $A$ and $B$ is an event $A\cap B=\{\omega\in\Omega:\omega\in A\text{ and }\omega\in B\}$.
		\item \textbf{Complement} of $A$ is an event containing all elements in sample space $\Omega$ that is not in $A$. It is denoted by $A^{\complement}$.
		\item \textbf{Complement} of $B$ in $A$ is an event $A\setminus B=\{\omega\in\Omega:\omega\in A\text{ and }\omega\not\in B\}$.
		\item \textbf{Symmetric difference} of $A$ and $B$ is an event $A\Delta B=\{\omega\in\Omega:\omega\in A\cup B\text{ and }\omega\not\in A\cap B\}$.
	\end{enumerate}
\end{sdefn}
\begin{sdefn}
	For any two events $A$ and $B$, if all of the outcomes in $A$ are also in $B$, then we say $A$ is \textbf{contained} in $B$, written as $A\subset B$ or $B\supset A$.
\end{sdefn}
\begin{sdefn}
	Given a sequence of events $A_{1},A_{2},\cdots,A_{k}$.
	\begin{enumerate}
		\item For any $i$ and $j$, if $A_{i}\cap A_{j}=\emptyset$, then $A_{i}$ and $A_{j}$ are called \textbf{disjoint}.
		\item If $A_{i}\cap A_{j}=\emptyset$ for all $i$ and $j$, the sequence of events is called \textbf{mutually exclusive}.
		\item If $A_{1}\cup A_{2}\cup\cdots\cup A_{k}=\Omega$, the sequence of events is called \textbf{exhaustive}.
		\item If the sequence is both mutually exclusive and exhaustive, it is called a \textbf{partition}.
	\end{enumerate}
\end{sdefn}

\newpage
\begin{sdefn}(\textbf{Kolmogorov axioms of probability})
	Let $(\Omega,\mathcal{F},\prob)$ be a probability space, with sample space $\Omega$, $\sigma$-field $\mathcal{F}$, and probability measure $\prob$.
	\begin{enumerate}
		\item The probability of an event is a non-negative real number. For all $E\in\mathcal{F}$,
		\begin{align*}
			\prob(E)&\in\mathbb{R} & \prob(E)&\geq 0
		\end{align*}
		\item The probability that at least one of the elementary events in the entire sample space will occur is $1$.
		\begin{equation*}
			\prob(\Omega)=1
		\end{equation*}
		\item Any countable sequence of disjoint events $E_{1},E_{2},\cdots$ satisfies:
		\begin{equation*}
			\prob\left(\bigcup_{i=1}^{\infty}E_{i}\right)=\sum_{i=1}^{\infty}\prob(E_{i})
		\end{equation*}
	\end{enumerate}
	By this definition, we call $\prob(A)$ the \textbf{probability} of the event $A$.
\end{sdefn}
\begin{sdefn}
	\textbf{$\sigma$-field} (\textbf{$\sigma$-algebra}) $\mathcal{F}$ is any collection of subsets of $\Omega$ which satisfied the following conditions:
	\begin{enumerate}
		\item If $A\in\mathcal{F}$, then $A^{\complement}\in\mathcal{F}$.
		\item If $A_{i}\in\mathcal{F}$ for all $i$, then $\bigcup_{i=1}^{\infty}A_{i}\in\mathcal{F}$.
		\item $\emptyset\in\mathcal{F}$.
	\end{enumerate}
\end{sdefn}
\begin{sdefn}
	\textbf{Measurable space} $(\Omega,\mathcal{F})$ is a pair comprising a sample space $\Omega$ and a $\sigma$-field $\mathcal{F}$.
\end{sdefn}
\begin{sdefn}
	\textbf{Probability measure} $\prob:\mathcal{F}\to[0,1]$ is a measure on a measurable space $(\Omega,\mathcal{F})$ satisfying:
	\begin{enumerate}
		\item $\prob(\emptyset)=0$
		\item $\prob(\Omega)=1$
		\item If $A_{i}\in\mathcal{F}$ for all $i$ and they are disjoint, then $\prob(\bigcup_{i=1}^{\infty}A_{i})=\sum_{i=1}^{\infty}\prob(A_{i})$.
	\end{enumerate}
\end{sdefn}
\begin{sdefn}
	\textbf{Probability space} $(\Omega,\mathcal{F},\prob)$ is a triple comprising 
	\begin{enumerate}
		\item a sample space $\Omega$
		\item a $\sigma$-field $\mathcal{F}$ of certain subsets of $\Omega$
		\item a probability measure $\prob$ on $(\Omega,\mathcal{F})$
	\end{enumerate}
\end{sdefn}
\begin{sdefn}
	We say a sequence of events $A_{n}$ \textbf{converges} and $\lim_{n\to\infty}A_{n}$ exists if
	\begin{equation*}
		\limsup_{n\to\infty}A_{n}=\liminf_{n\to\infty}A_{n}
	\end{equation*}
	Given a probability space $(\Omega,\mathcal{F},\prob)$. Let $A_{i}\in\mathcal{F}$ for all $i$ such that $A=\lim_{n\to\infty}A_{n}$ exists. Then
	\begin{equation*}
		\lim_{n\to\infty}\prob(A_{n})=\prob\left(\lim_{n\to\infty}A_{n}\right)
	\end{equation*}
\end{sdefn}
\begin{sdefn}
	Event $A$ is \textbf{null} if $\prob(A)=0$.
\end{sdefn}
\begin{sdefn}
	Event $A$ is \textbf{almost surely} if $\prob(A)=1$.
\end{sdefn}
\begin{sdefn}
	Given $\prob(B)>0$. \textbf{Conditional probability} that $A$ occurs given that $B$ occurs is:
	\begin{equation*}
		\prob(A|B)=\frac{\prob(A\cap B)}{\prob(B)}
	\end{equation*}
\end{sdefn}

\newpage
\begin{sdefn}
	Events $A$ and $B$ are independent ($A\independent B$) if $\prob(A\cap B)=\prob(A)\prob(B)$.\\
	Given $A_{k}$ for all $k\in I$. If for all $i\neq j$, 
	\begin{equation*}
		\prob(A_{i}\cap A_{j})=\prob(A_{i})\prob(A_{j})
	\end{equation*} 
	then they are \textbf{pairwise independent}.\\
	If additionally, for all subsets $J\subseteq I$,
	\begin{equation*}
		\prob\left(\bigcap_{i\in J}A_{i}\right)=\prod_{i\in J}\prob(A_{i})
	\end{equation*}
	then they are \textbf{(mutually) independent}.
\end{sdefn}
\begin{sdefn}
	Let $A$ be a collection of subsets of $\Omega$. The \textbf{$\sigma$-field generated by $A$} is:
	\begin{equation*}
		\sigma(A)=\bigcap_{A\subseteq\mathcal{G}}\mathcal{G}
	\end{equation*}
	where $\mathcal{G}$ are also $\sigma$-field. $\sigma(A)$ is the smallest $\sigma$-field containing $A$.
\end{sdefn}
\begin{sdefn}
	\textbf{Product space} of two probability spaces $(\Omega_{1},\mathcal{F}_{1},\prob_{1})$ and $(\Omega_{2},\mathcal{F}_{2},\prob_{2})$ is the probability space $(\Omega_{1}\times\Omega_{2},\mathcal{G},\prob_{12})$ comprising:
	\begin{enumerate}
		\item a collection of ordered pairs $\Omega_{1}\times\Omega_{2}=\{(\omega_{1},\omega_{2}):\omega_{1}\in\Omega_{1},\omega_{2}\in\Omega_{2}\}$
		\item a $\sigma$-algebra $\mathcal{G}=\sigma(\mathcal{F}_{1}\times\mathcal{F}_{2})$ where $\mathcal{F}_{1}\times\mathcal{F}_{2}=\{A_{1}\times A_{2}:A_{1}\in\mathcal{F}_{1},A_{2}\in\mathcal{F}_{2}\}$ 
		\item a probability measure $\prob_{12}:\mathcal{F}_{1}\times\mathcal{F}_{2}\to [0,1]$ given by:
		\begin{equation*}
			\prob_{12}(A_{1}\times A_{2})=\prob_{1}(A_{1})\prob_{2}(A_{2})
		\end{equation*}
		for $A_{1}\in\mathcal{F}_{1},A_{2}\in\mathcal{F}_{2}$.
	\end{enumerate}
\end{sdefn}
\begin{sdefn}
	\textbf{Random variable} is a function $X:\Omega\to\mathbb{R}$ with the property that:
	\begin{equation*}
		X^{-1}((-\infty,x])=\{\omega\in\Omega:X(\omega)\leq x\}\in\mathcal{F}
	\end{equation*}
	for any $X\in\mathbb{R}$. We say the function is \textbf{$\mathcal{F}$-measurable}.
\end{sdefn}
\begin{sdefn}
	\textbf{Borel set} is a set which can be obtained by taking countable union, intersection or complement repeatedly.
\end{sdefn}
\begin{sdefn}
	\textbf{Borel $\sigma$-field} $\mathcal{B}(\mathbb{R})$ of $\mathbb{R}$ is a $\sigma$-field that is generated by all open sets. It is a collection of Borel sets.
\end{sdefn}
\begin{sdefn}
	\textbf{(Cumulative) distribution function} (CDF) of a random variable $X$ is a function $F_{X}:\mathbb{R}\to[0,1]$ given by
	\begin{equation*}
		F_{X}(x)=\prob(X\leq x)=\prob\circ X^{-1}((-\infty,x])
	\end{equation*}
	In \textbf{discrete} case, \textbf{probabilty mass function} (PMF) of discrete random variable $X$ is the function $f:\mathbb{R}\to[0,1]$ given by:
	\begin{align*}
		f_{X}(x)&=\prob(X=x)=\prob\circ X^{-1}(\{x\}) & F_{X}(x)&=\sum_{i:x_{i}\leq x}f(x_{i}) & f_{X}(x)&=F_{X}(x)-\lim_{y\to x^{-}}F_{X}(y)
	\end{align*}
	In \textbf{continuous} case, \textbf{probability density function} (PDF) of continuous random variable $X$ is the function $f:\mathbb{R}\to[0,\infty)$ given by:
	\begin{align*}
		F_{X}(x)&=\intlu{-\infty}{x}f(u)\,du & f_{X}(x)&=\pdv*{F_{X}(x)}{x}
	\end{align*}
\end{sdefn}
\begin{sdefn}
	The \textbf{$q$-th quantile} of a random variable $X$ is defined as a number $z_{q}$ such that:
	\begin{equation*}
		\prob(X\leq z_{q})=q
	\end{equation*}
\end{sdefn}

\newpage
\begin{sdefn}
	Let $X_{i}:\Omega\to\mathbb{R}$ for all $1\leq i\leq n$ be random variables. \textbf{Random vector} $\mathbf{X}=(X_{1},X_{2},\cdots,X_{n}):\Omega\to\mathbb{R}^{n}$ with properties:
	\begin{equation*}
		\mathbf{X}^{-1}(D)=\{\omega\in\Omega:\mathbf{X}(\omega)=(X_{1}(\omega),X_{2}(\omega),\cdots,X_{n}(\omega))\in D\}\in\mathcal{F}
	\end{equation*}
	for all $D\in\mathcal{B}(\mathbb{R}^{n})$.\\
	We can also say $\mathbf{X}$ is a random vector if
	\begin{equation*}
		X_{i}^{-1}(B)\in\mathcal{F}
	\end{equation*}
	for all $B\in\mathcal{B}(\mathbb{R})$ and $i$.
\end{sdefn}
\begin{sdefn}
	Given a random vector $(X,Y)$. \textbf{Joint distribution function} (JCDF) $F_{X,Y}:\mathbb{R}^{2}\to[0,1]$ is defined as:
	\begin{equation*}
		F_{X,Y}(x,y)=\prob(X\leq x,Y\leq y)=\prob\circ(X,Y)^{-1}((-\infty,x]\times(-\infty,y])
	\end{equation*}
	In discrete case, \textbf{joint probability mass function} (JPMF) of \textbf{jointly discrete} random variable $X$ and $Y$ is the function $f_{X,Y}:\mathbb{R}^{2}\to[0,1]$ given by:
	\begin{align*}
		f_{X,Y}(x,y)&=\prob((X,Y)=(x,y))=\prob\circ(X,Y)^{-1}(\{x,y\}) & F_{X,Y}(x,y)&=\sum_{u\leq x}\sum_{v\leq y}f(u,v)
	\end{align*}
	In continuous case, \textbf{joint probability density function} (JPDF) of \textbf{jointly continuous} random variable $X$ and $Y$ is the function $f_{X,Y}:\mathbb{R}^{2}\to[0,\infty)$ given by:
	\begin{align*}
		f_{X,Y}(x,y)&=\pdv*{F_{X,Y}(x,y)}{x,y} & F_{X,Y}(x,y)&=\intlu{-\infty}{y}\intlu{-\infty}{x}f_{X,Y}(u,v)\,du\,dv
	\end{align*}
\end{sdefn}
\begin{sdefn}
	Let $X$ and $Y$ be random variables. \textbf{Marginal distribution function} (Marginal CDF) is given by:
	\begin{equation*}
		F_{X}(x)=\prob(X^{-1}((-\infty,x])\cap Y^{-1}((-\infty,\infty)))=\lim_{y\to\infty}F_{X,Y}(x,y)
	\end{equation*}
	In discrete case, \textbf{marginal mass function} (Marginal PMF) is given by:
	\begin{equation*}
		f_{X}(x)=\sum_{y}f_{X,Y}(x,y)
	\end{equation*}
	In continuous case, \textbf{marginal density function} (Marginal PDF) is given by:
	\begin{equation*}
		f_{X}(x)=\intinfty f_{X,Y}(x,y)\,dy
	\end{equation*}
\end{sdefn}
\begin{sdefn}
	Given a random variable $X$. \textbf{Mean value}, \textbf{expectation}, or \textbf{expected value} of $X$ is given by:
	\begin{equation*}
		\expect X=\begin{cases}
			\sum_{x:f_{X}(x)>0}xf_{X}(x), &X\text{ is discrete}\\
			\intinfty xf_{X}(x)\,dx, &X\text{ is continuous}
		\end{cases}
	\end{equation*}
	If it is absolutely convergent.
\end{sdefn}
\begin{sdefn}
	Given $k\in\mathbb{N}_{+}$ and a random variable $X$. \textbf{$k$-th moment} $m_{k}$ is defined to be:
	\begin{equation*}
		\expect(X^{k})=\begin{cases}
			\sum_{x}x^{k}f_{X}(x), &X\text{ is discrete}\\
			\intinfty x^{k}f_{X}(x)\,dx, &X\text{ is continuous}
		\end{cases}
	\end{equation*}
	\textbf{$k$-th cnetral moment} $\alpha_{k}$ is defined to be
	\begin{equation*}
		\expect((X-\expect X)^{k})=\begin{cases}
			\sum_{x}(x-\expect X)^{k}f_{X}(x), &X\text{ is discrete}\\
			\intinfty(x-\expect X)^{k}f_{X}(x)\,dx, &X\text{ is continuous}
		\end{cases}
	\end{equation*}
	\textbf{Mean} $\mu$ is the $1$st moment $\mu=m_{1}=\expect X$.\\
	\textbf{Variance} is the $2$nd central moment $\alpha_{2}=\Var(X)=\expect((X-\expect X)^{2})=\expect(X^{2})-(\expect X)^{2}$.\\
	\textbf{Standard deviation} $\sigma$ is defined as $\sigma=\sqrt{\Var(X)}$.
\end{sdefn}
\begin{sdefn}
	Given two random variables $X$ and $Y$. \textbf{Conditional distribution function} (Conditional CDF) of $Y$ given $X=x$ for any $x$ is defined by:
	\begin{equation*}
		F_{Y|X}(y|x)=\prob(Y\leq y|X=x)=\begin{cases}
			\frac{\prob(Y\leq y,X=x)}{\prob(X=x)}, &X\text{ is discrete}\\
			\intlu{-\infty}{y}\frac{f_{X,Y}(x,v)}{f_{X}(x)}\,dv, &X\text{ is continuous}
		\end{cases}
	\end{equation*}
	In discrete case, \textbf{conditional mass function} (Conditional PMF) of $Y$ given $X=x$ is defined by:
	\begin{equation*}
		f_{Y|X}(y|x)=\begin{cases}
			\frac{\prob(Y=y,X=x)}{\prob(X=x)}, &X\text{ is discrete}\\
			\pdv*{F_{Y|X}(y|x)}{y}=\frac{f_{X,Y}(x,y)}{f_{X}(x)}, &X\text{ is continuous}
		\end{cases}
	\end{equation*}
\end{sdefn}
\begin{sdefn}
	Given two random variables $X$ and $Y$, and an event $X=x$ for some $X$. \textbf{Conditional expectation} of random variable $Y$ is defined by:
	\begin{equation*}
		\psi(x)=\expect(Y|X=x)=\begin{cases}
			\sum_{y}yf_{Y|X}(y|x), &X\text{ and }Y\text{ are discrete}\\
			\intinfty yf_{Y|X}(y|x)\,dy, &X\text{ and }Y\text{ are continuous}
		\end{cases}
	\end{equation*}
	Given a random variable $X$. Conditional expectation of random variable $Y$ is defined by:
	\begin{equation*}
		\psi(X)=\expect(Y|X)=\begin{cases}
			\sum_{x}\psi(x), &X\text{ and }Y\text{ are discrete}\\
			\intinfty\psi(x)\,dx, &X\text{ are continuous}
		\end{cases}
	\end{equation*}
\end{sdefn}
\begin{sdefn}
	Given $X\independent Y$. In discrete case, \textbf{convolution} $f_{X+Y}$ ($f_{X}*f_{Y}$) of PMFs of random variables $X$ and $Y$ is the PMF of $X+Y$:
	\begin{equation*}
		f_{X+Y}(z)=\prob(X+Y-z)=\sum_{x}f_{X}(x)f_{Y}(z=x)=\sum_{y}f_{X}(z-y)f_{Y}(y)
	\end{equation*}
	In continuous case, \textbf{convolution} of PDFs of random variables $X$ and $Y$ is the PDF of $X+Y$:
	\begin{equation*}
		f_{X+Y}(z)=\intinfty f_{X}(z-y)f_{Y}(y)\,dy=\intinfty f_{X}(x)f_{Y}(z-x)\,dx
	\end{equation*}
\end{sdefn}
\begin{sdefn}
	\textbf{Parametric distribution} of a random variable is a distribution where the PMF or PDF depends on one or more parameters.
\end{sdefn}

\newpage
\section*{Named Properties}
\begin{spro}(Fundamental Principle of Counting)
	Suppose that $m_{i}$ represents the number of outcomes of the $i$-th event. The total number of outcomes of $n$ independent events is the product of the number of each individual event:
	\begin{equation*}
		\prod_{i=1}^{n}m_{i}
	\end{equation*} 
\end{spro}
\begin{spro}(Pascal's Identity)
	Let $n$ and $k$ be integers with $0<k<n$. Then:
	\begin{equation*}
		\binom{n}{k}=\binom{n-1}{k-1}+\binom{n-1}{k}
	\end{equation*}
\end{spro}
\begin{spro}(Binomial Theorem)
	Let $n$ be a non-negative integer. We have:
	\begin{equation*}
		(x+y)^{n}=\sum_{k=0}^{n}\binom{n}{k}x^{k}y^{n-k}
	\end{equation*} 
	where $\binom{n}{k}$ for all $k$ are called the \textbf{binomial coefficient}.
\end{spro}
\begin{spro}(Vandermonde's Identity)
	Let $m,n,r\in\mathbb{Z}$ with $0\leq r\leq m$ and $0\leq r\leq n$. We have:
	\begin{equation*}
		\binom{m+n}{r}=\sum_{k=0}^{r}\binom{m}{r-k}\binom{n}{k}
	\end{equation*}
\end{spro}
\begin{spro}(Multinomial Theorem)
	Let $n$ be a non-negative integers. We have:
	\begin{equation*}
		(x_{1}+x_{2}+\cdots+x_{k})^{n}=\sum_{(n_{1},n_{2},\cdots,n_{k}):n_{1}+n_{2}+\cdots+n_{k}=n}\binom{n}{n_{1},n_{2},\cdots,n_{k}}x_{1}^{n_{1}}x_{2}^{n_{2}}\cdots x_{k}^{n_{k}}
	\end{equation*}
	where $(n_{1},n_{2},\cdots,n_{k})$ are all non-negative integer-valued vectors.
\end{spro}
\begin{spro}(Inclusion-exclusion formula)
	\begin{equation*}
		\prob\left(\bigcup_{i=1}^{n}A_{i}\right)=\sum_{i}\prob(A_{i})-\sum_{i<j}\prob(A_{i}\cap A_{j})+\cdots+(-1)^{n+1}\prob(A_{1}\cap A_{2}\cap\cdots\cap A_{n})
	\end{equation*}
\end{spro}
\begin{spro}(General Multiplication Rule)
	Let $A_{1},A_{2},\cdots,A_{n}$ be a sequence of events. We have:
	\begin{equation*}
		\prob\left(\bigcap_{i=1}^{n}A_{i}\right)=\prob(A_{1})\prob(A_{2}|A_{1})\prob(A_{3}|A_{1}\cap A_{2})\cdots\prob(A_{n}|A_{1}\cap A_{2}\cap\cdots\cap A_{n-1})
	\end{equation*}
\end{spro}
\begin{spro}(Law of total probability)
	Let $\{B_{1},B_{2},\cdots,B_{n}\}$ be a partition of $\Omega$. ($B_{i}\cap B_{j}=\emptyset$ for all $i\neq j$ and $\bigcup_{i=1}^{n}=\Omega$).\\
	If $\prob(B_{i})>0$ for all $i$, then:
	\begin{equation*}
		\prob(A)=\sum_{i=1}^{n}\prob(A|B_{i})\prob(B_{i})
	\end{equation*}
\end{spro}
\begin{spro}(Bayes' Theorem)
	Suppose that a sequence of events $A_{1},A_{2},\cdots,A_{n}$ is a partition of sample space. Assume further that $\prob(A_{i})>0$ for all $i$. Let $B$ be any event, then for any $i$:
	\begin{equation*}
		\prob(A_{i}|B)=\frac{\prob(B|A_{i})\prob(A_{i})}{\sum_{k=1}^{n}\prob(B|A_{k})\prob(A_{k})}
	\end{equation*}
\end{spro}
\begin{spro}(Law of total expectation)
	Let $\psi(X)=\expect(Y|X)$. Conditional expectation satisfies:
	\begin{equation*}
		\expect(\psi(X))=\expect(\expect(Y|X))=\expect(Y)
	\end{equation*}
\end{spro}
\begin{spro}(Tail sum formula)
	If discrete random variable $X$ has a PMF $f_{x}$ with $f_{X}(x)=0$ when $x<0$, then:
	\begin{equation*}
		\expect{X}=\sum_{k=0}^{\infty}\prob(X>k)
	\end{equation*}
	If continuous random variable $X$ has a PDF $f_{X}$ with $f_{X}(x)=0$ when $x<0$, and a CDF $F_{X}$, then:
	\begin{equation*}
		\expect X=\intlu{0}{\infty}(1-F_{X}(x))\,dx
	\end{equation*}
\end{spro}

\newpage
\section*{Distributions}
For discrete random variables,
\begin{seg}(Bernoulli distribution) $X\sim\Bern(p)$\\
	Suppose we perform $1$ Bernoulli trial. Let $p$ be probability of success and $X$ be number of successes.
	\begin{align*}
		F_{X}(x)&=\begin{cases}
			0, &x<0\\
			1-p, &0\leq x<1\\
			1, &x\geq 1
		\end{cases} & f_{X}(x)&=\begin{cases}
			1-p, &x=0\\
			p, &x=1\\
			0, &\text{Otherwise}
		\end{cases} & \expect X&=p & \Var(X)&=p(1-p)
	\end{align*}
\end{seg}
\begin{seg}(Binomial distribution) $Y\sim\Bin(n,p)$\\
	Suppose we perform $n$ independent Bernoulli trials. Let $p$ be the probability of success and $Y=X_{1}+X_{2}+\cdots+X_{n}$ be total number of successes.
	\begin{align*}
		f_{Y}(k)&=\binom{n}{k}p^{k}(1-p)^{n-k} & F_{Y}(k)&=\sum_{i=0}^{k}\binom{n}{i}p^{i}(1-p)^{n-i} & \expect X&=np & \Var(X)&=np(1-p)
	\end{align*}
\end{seg}
\begin{seg}(Trinomial distribution)\\
	Suppose we perform $n$ trials with three outcomes $A$, $B$ and $C$, where the probability of occurrence is $p$, $q$ and $1-p-q$ respectively. Let $X$ be number of occurrence of $A$ and $Y$ be number of occurrence of $B$.\\
	Probability of $x$ $A$'s, $y$ $B$'s and $n-x-y$ $C$'s is:
	\begin{equation*}
		f_{X,Y}(x,y)=\binom{n}{r,w,n-r-w}p^{x}q^{y}(1-p-q)^{n-x-y}
	\end{equation*}
\end{seg}
\begin{seg}(Geometric distribution) $W\sim\Geom(p)$ $X\sim\Geom(p)$\\
	Suppose we keep performing independent Bernoulli trials until the first success shows up. Let $p$ be probability of success.\\
	Let $W$ be the waiting time which elapses before first success. For $k\geq 1$,
	\begin{align*}
		f_{W}(k)&=p(1-p)^{k-1} & F_{W}(k)&=1-(1-p)^{k} & \expect W&=\frac{1}{p} & \Var(W)&=\frac{1-p}{p^{2}}
	\end{align*}
	Above is the conventional geometric distribution.\\
	Let $X$ be number of failures before first success. For $k\geq 0$,
	\begin{align*}
		f_{X}(k)&=p(1-p)^{k} & F_{X}(k)&=1-(1-p)^{k+1} & \expect X&=\frac{1-p}{p} & \Var(X)&=\frac{1-p}{p^{2}}
	\end{align*}
\end{seg}
\begin{seg}(Negative Binomial distribution) $W_{r}\sim\NBin(r,p)$ $X\sim\NBin(r,p)$\\
	Suppose we keep performing independent Bernoulli trials until the first success shows up. Let $p$ be the probability of success.\\
	Let $W_{r}$ be the waiting time which elapses before $r$-th success. For any $k\geq r$,
	\begin{align*}
		f_{W_{r}}(k)&=\binom{k-1}{r-1}p^{r}(1-p)^{k-r} & \expect W_{r}&=\frac{r}{p} & \Var(W_{r})&=\frac{r(1-p)}{p^{2}}
	\end{align*}
	Let $X$ be number of failures before the $r$-th success. For any $k\geq 0$,
	\begin{align*}
		f_{X}(k)&=\binom{k+r-1}{r-1}p^{r}(1-p)^{k} & \expect X&=\frac{r(1-p)}{p} & \Var(X)&=\frac{r(1-p)}{p^{2}}
	\end{align*}
\end{seg}
\begin{seg}(Poisson distribution) $X\sim\Poisson(\lambda)$\\
	Suppose we perform $n$ independent Bernoulli trials. Let $p$ be the probability of success, $\lambda=np$ and $X\sim\Bin(n,p)$. When $n$ is large, $p$ is small, and $np$ is moderate:
	\begin{align*}
		f_{X}(k)&=\binom{n}{k}p^{k}(1-p)^{n-k}\approx\frac{\lambda^{k}}{k!}e^{-\lambda} & F_{X}(k)&=\sum_{i=0}^{k}\frac{\lambda^{i}}{i!}e^{-\lambda} & \expect X&=\lambda & \Var(X)&=\lambda
	\end{align*}
\end{seg}

\newpage
\begin{seg}(Hypergeometric distribution) $X\sim\Hypergeometric(N,m,n)$\\
	Suppose that we have a set of $N$ balls. There are $m$ red balls and $N-m$ blue balls. We choose $n$ of these balls, without replacement. Let $X$ be the number of red balls in our sample. For $0\leq k\leq\min(m,n)$,
	\begin{align*}
		f_{X}(k)&=\frac{\binom{m}{k}\binom{N-m}{n-k}}{\binom{N}{n}} & \expect{X}&=\frac{mn}{N} & \Var(X)&=\frac{mn}{N}\left(\frac{(m-1)(n-1)}{N-1}+1-\frac{mn}{N}\right)
	\end{align*}
\end{seg}
For continuous random variables,
\begin{seg}(Uniform distribution) $X\sim\U[a,b]$\\
	Random variable $X$ is uniform on $[a,b]$ is PDF and CDF is:
	\begin{align*}
		f_{X}(x)&=\begin{cases}
			\frac{1}{b-a}, &a\leq x\leq b\\
			0, &\text{Otherwise}
		\end{cases} & F_{X}(x)&=\begin{cases}
			0, &x<a\\
			\frac{x-a}{b-a}, &a\leq x\leq b\\
			1, &x>b
		\end{cases}
	\end{align*}    
\end{seg}
\begin{seg}(Exponential distribution) $X\sim\Exp(\lambda)$\\
	Random variable $X$ is exponential with parameter $\lambda>0$ if PDF and CDF is:
	\begin{align*}
		f_{X}(x)&=\begin{cases}
			0, &x<0\\
			\lambda e^{-\lambda x}, &x\geq 0
		\end{cases} & F_{X}(x)&=\begin{cases}
			0, &x<0\\
			1-e^{-\lambda x}, &x\geq 0
		\end{cases}
	\end{align*}
\end{seg}
\begin{seg}(Normal distribution / Gaussian distribution) $X\sim\N(\mu,\sigma^{2})$\\
	Random variable $X$ is normal if it has two parameters $\mu$ and $\sigma^{2}$, and its PDF and CDF is:
	\begin{align*}
		f_{X}(x)&=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right) & F_{X}(x)&=\intlu{-\infty}{x}f_{X}(u)\,du & \expect X&=\mu & \Var(X)&=\sigma^{2}
	\end{align*}
	Random variable $X$ is standard normal if $\mu=0$ and $\sigma^{2}=1$. ($X\sim\N(0,1)$)
	\begin{align*}
		f_{X}(x)&=\phi(x)=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x^{2}}{2}\right) & F_{X}(x)&=\Phi(x)=\intlu{-\infty}{x}\phi(u)\,du & \expect X&=0 & \Var(X)&=1
	\end{align*}
\end{seg}
\begin{seg}(Bivariate normal distribution)
	Two random variables $X$ and $Y$ are bivariate normal with $\mu_{X}$ and $\mu_{Y}$, variance $\sigma_{X}^{2}$ and $\sigma_{Y}^{2}$, and population correlation coefficient $\rho$ if:
	\begin{equation*}
		f_{X,Y}(x,y)=\frac{1}{2\pi\sigma_{X}\sigma_{Y}\sqrt{1-\rho^{2}}}\exp\left(-\frac{1}{2(1-\rho^{2})}\left(\left(\frac{x-\mu_{X}}{\sigma_{X}}\right)^{2}-2\rho\left(\frac{x-\mu_{X}}{\sigma_{X}}\right)\left(\frac{y-\mu_{Y}}{\sigma_{Y}}\right)+\left(\frac{y-\mu_{Y}}{\sigma_{Y}}\right)^{2}\right)\right)
	\end{equation*}
	Two random variables $X$ and $Y$ are standard bivariate normal if $\mu_{X}=\mu_{Y}=0$ and $\sigma_{X}^{2}=\sigma_{Y}^{2}=1$.
	\begin{equation*}
		f_{X,Y}(x,y)=\frac{1}{2\pi\sqrt{1-\rho^{2}}}\exp\left(-\frac{x^{2}-2\rho xy+y^{2}}{2(1-\rho^{2})}\right)
	\end{equation*}
\end{seg}
\begin{seg}(Multivariate normal distrubution) $\mathbf{X}\sim\N_{p}(\boldsymbol{\mu},\mathbf{\Sigma})$\\
	Random vector $\mathbf{X}$ with dimension $p$ is $p$-dimensional normal with $p\times 1$ mean vector $\boldsymbol{\mu}$ and $p\times p$ variance-covariance matrix $\mathbf{\Sigma}$ if we have:
	\begin{equation*}
		f(\mathbf{x})=(2\pi)^{-\frac{p}{2}}\abs{\mathbf{\Sigma}}^{-\frac{1}{2}}e^{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T}\mathbf{\Sigma}(\mathbf{x}-\boldsymbol{\mu})}
	\end{equation*}
\end{seg}
\begin{seg}(Cauchy distribution) $X\sim\Cauchy(\theta)$\\
	Random variable $X$ has a Cauchy distribution with parameter $\theta$ if:
	\begin{align*}
		f_{X}(x)&=\frac{1}{\pi(1+(x-\theta)^{2})} & \expect|X|&=\intinfty\frac{|x|}{\pi(1+(x-\theta)^{2})}\,dx=\infty
	\end{align*}
\end{seg}

\newpage
\begin{seg}(Gamma distribution) $X\sim\Gam(\alpha,\lambda)$\\
	Random variable $X$ has a gamma distribution with parameters $\alpha$ and $\lambda$ if:
	\begin{align*}
		f_{X}(x)&=\begin{cases}
			0, &x<0\\
			\frac{1}{\Gamma(\alpha)}\lambda e^{-\lambda x}(\lambda x)^{\alpha-1}, &x\geq 0
		\end{cases} & \Gamma(\alpha)&=\intlu{0}{\infty}e^{-y}y^{\alpha-1}\,dy & \expect{X}&=\frac{\alpha}{\lambda} & \Var(X)&=\frac{\alpha}{\lambda^{2}}
	\end{align*}
	where $\Gamma(\alpha)$ is the gamma function with $\Gamma(\alpha)=(\alpha-1)\Gamma(\alpha-1)$. If $\alpha$ is a positive integer, then $\Gamma(\alpha)=(\alpha-1)!$.
\end{seg}
\begin{seg}(Chi-squared distribution) $Y\sim\chi^{2}(n)$\\
	Assume that $X_{1},X_{2},\cdots,X_{n}$ are independent standard normal random variables. Let $Y=\sum_{i=1}^{n}X_{i}^{2}$. Random variable $Y$ has a $\chi^{2}$-distribution with parameter $n$ if:
	\begin{align*}
		f_{Y}(x)&=\begin{cases}
			0, &x<0\\
			\frac{1}{\Gamma(\frac{n}{2})}2^{-\frac{n}{2}}x^{\frac{n}{2}-1}e^{-\frac{x}{2}}, &x\geq 0
		\end{cases} & \expect{Y}&=n & \Var(Y)&=2n
	\end{align*}
\end{seg}
\begin{seg}(Student's t-distribution) $W\sim t(n)$\\
	Given $Y\sim\chi^{2}(n)$ and $Z\sim\N(0,1)$. If $Y$ and $Z$ are independent, let
	\begin{equation*}
		W=\frac{Z}{\sqrt{\frac{Y}{n}}}
	\end{equation*}
	The random variable $W$ follows the $t$-distribution with $n$ degree of freedom and:
	\begin{align*}
		f(w)&=\frac{\Gamma\left(\frac{n+1}{2}\right)}{\sqrt{n\pi}\Gamma\left(\frac{n}{2}\right)}\left(1+\frac{w^{2}}{n}\right)^{-\frac{n+1}{2}} & \expect{W}&=\begin{cases}
			\text{Undefined}, &n\leq 1\\
			0, &n>1\\
		\end{cases} & \Var(W)&=\begin{cases}
			\text{Undefined}, &n\leq 1\\
			\infty, &1<n\leq 2\\
			\frac{n}{n-2}, &n>2
		\end{cases}
	\end{align*}
	where $\Gamma(\alpha)$ is the gamma function.
\end{seg}
\begin{seg}(Beta distribution) $X\sim\Beta(a,b)$\\
	Random variable $X$ has a beta distribution with parameters $a$ and $b$ if:
	\begin{align*}
		f_{X}(x)&=\begin{cases}
			\frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1}, &0<x<1\\
			0, &\text{Otherwise}
		\end{cases} & B(a,b)&=\intlu{0}{1}x^{a-1}(1-x)^{b-1}\,dx=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}\\
		\expect{X}&=\frac{a}{a+b} & \Var(X)&=\frac{ab}{(a+b)^{2}(a+b+1)}
	\end{align*}
	where $B(a,b)$ is the beta function.
\end{seg}
\begin{seg}(F distribution) $F\sim F(r_{1},r_{2})$\\
	Assume that $X$ and $Y$ are independent random variables with $X\sim\chi^{2}(r_{1})$ and $Y\sim\chi^{2}(r_{2})$. Let:
	\begin{equation*}
		F=\frac{\frac{X}{r_{1}}}{\frac{Y}{r_{2}}}
	\end{equation*}
	Then $F$ has a F-distribution with $r_{1}$ and $r_{2}$ degrees of freedom with:
	\begin{equation*}
		f_{F}(w)=\frac{\Gamma\left(\frac{r_{1}+r_{2}}{2}\right)}{\Gamma\left(\frac{r_{1}}{2}\right)\Gamma\left(\frac{r_{2}}{2}\right)}\left(\frac{r_{1}}{r_{2}}\right)^{\frac{r_{1}}{2}}w^{\frac{r_{1}}{2}-1}\left(1+\frac{r_{1}w}{r_{2}}\right)^{-\frac{r_{1}+r_{2}}{2}}
	\end{equation*}
	where $0<w<\infty$.
\end{seg}

\chapter{Generating function}
\section{Introduction of generating functions}
A sequence of number $a=\{a_{i}:i=0,1,2,\cdots\}$ may contain a lot of information. For example, values of PMF tells us the distribution of a discrete random variables. A concise way of storing this information is to wrap up the numbers together in a generating function. 
\begin{defn}
	For any sequence $\{a_{n}:n=0,1,2,\cdots\}$, we defined the generating function by
	\begin{equation*}
		G_{a}(s)=\sum_{i=0}^{\infty}a_{i}s^{i}=\lim_{N\to\infty}\sum_{i=0}^{N}a_{i}s^{i}
	\end{equation*}
	for $s\in\mathbb{R}$ if the limit exists. 
\end{defn}
\begin{rem}
	We can observe that
	\begin{equation*}
		a_{i}=\frac{G_{a}^{(i)}(0)}{i!}
	\end{equation*}
\end{rem}
\begin{eg}
	Sometimes, we cannot interchange countable sum with derivatives.\\
	Let $b_{n}(x)=\frac{\sin{nx}}{n}$ such that $a_{1}(x)=b_{1}(x)$ and $a_{n}(x)=b_{n}(x)-b_{n-1}(x)$.
	\begin{align*}
		\tag{Squeeze Theorem}
		\sum_{n=0}^{\infty}a_{n}(x)&=\lim_{n\to\infty}\sum_{i=0}^{n}a_{n}(x)=\lim_{n\to\infty}\frac{\sin{nx}}{n}=0\\
		\lim_{n\to\infty}\pdv*{\sum_{i=0}^{n}a_{i}(x)}{x}&=0\\
		\lim_{n\to\infty}\sum_{i=0}^{n}\pdv*{a_{n}(x)}{x}&=\lim_{n\to\infty}\cos{nx}\quad \text{does not exist}
	\end{align*}
\end{eg}
Convolutions are common in probability theory, and generating functions can provide a tool for studying them.
\begin{defn}
	Let $a=\{a_{i}:i\geq 0\}$ and $b=\{b_{i}:i\geq 0\}$ be two sequence of real numbers. \textbf{Convolution} $c=a*b=\{c_{i}:i\geq 0\}$ of $\{a_{i}\}$ and $\{b_{i}\}$ is defined by
	\begin{equation*}
		c_{n}=\sum_{i=0}^{n}a_{i}b_{n-i}
	\end{equation*}
\end{defn}
\begin{eg}
	If $a_{n}=f_{X}(n)$ and $b_{n}=f_{Y}(n)$, then $c_{n}=f_{X+Y}(n)$.
\end{eg}
\begin{lem}
	If sequences $a$ and $b$ have generating functions $G_{a}(s)$ and $G_{b}(s)$ respectively, then
	\begin{equation*}
		G_{c}(s)=G_{a}(s)G_{b}(s)
	\end{equation*}
\end{lem}
\begin{proofing}
	\begin{equation*}
		G_{c}(s)=\sum_{n=0}^{\infty}c_{n}s^{n}=\sum_{n=0}^{\infty}\sum_{i=0}^{n}a_{i}b_{n-i}s^{i}s^{n-i}=\sum_{i=0}^{\infty}a_{i}s^{i}\sum_{n=i}^{\infty}b_{n-i}s^{n-i}=\sum_{i=0}^{\infty}a_{i}s^{i}\sum_{j=0}^{\infty}b_{j}s^{j}=G_{a}(s)G_{b}(s)
	\end{equation*}
\end{proofing}
We can see from the definition of generating function that it is a power series. We may want to know whether the series is convergent.
\begin{defn}
	\textbf{Radius of convergence} $R$ of power series is the half size of an interval such that the power series $f(s)$ is convergent. If $s\in(-R,R)$, then $f(s)$ is convergent. If $s\in[-R,R]^{\complement}$, then $f(s)$ is divergent.\\
	We can obtain the radius of convergence by applying root test:
	\begin{equation*}
		R=\frac{1}{\limsup_{n\to\infty}\sqrt[n]{\abs{a_{n}}}}
	\end{equation*}
\end{defn}
\begin{rem}
	We need to perform additional tests to find whether the power series converges at $s=-R$ and $s=R$.
\end{rem}
\begin{rem}
	Sometimes, it is hard to compute $R$ using root test. One convenient way to compute $R$ is using the ratio test. If the limit exists,
	\begin{equation*}
		R=\lim_{n\to\infty}\abs{\frac{a_{n}}{a_{n+1}}}
	\end{equation*}
\end{rem}
Here are some properties of power series involving radius of convergence. We will not prove them since the proof is not important.
\begin{thm}
	If $R$ is the radius of convergence of $G_{a}(s)=\sum_{i=0}^{\infty}a_{i}s^{i}$, then
	\begin{enumerate}
		\item $G_{a}(s)$ converges absolutely for all $\abs{s}<R$ and diverges for all $\abs{s}>R$.
		\item $G_{a}(s)$ can be differentiated or integrated for any fixed number of times term by term if $\abs{s}<R$.
		\begin{equation*}
			\pdv*[order={i}]{\sum_{n=0}^{\infty}a_{n}s^{n}}{s}=\sum_{n=0}^{\infty}\pdv*[order={i}]{a_{n}s^{n}}{s}
		\end{equation*}
		\item If $R>0$ and $G_{a}(s)=G_{b}(s)$ for all $\abs{s}\leq R'$ for some $0<R'\leq R$, then $a_{n}=b_{n}$ for all $n$.
	\end{enumerate}
\end{thm}
\begin{rem}
	For any sequence $\{a_{n}:n\geq 0\}$, if radius of convergence of $G_{a}(s)$ is positive, then $\{a_{n}:n\geq 0\}$ is uniquely determined by $G_{a}(s)$ via
	\begin{equation*}
		a_{n}=\frac{1}{n!}G_{a}^{(n)}(0)
	\end{equation*}
\end{rem}
Suppose that $X$ is a discrete random variables taking values in the non-negative integers. We can see how the generating function works in probability.
\begin{defn}
	\textbf{Probability generating function} (PGF) of a non-negative random variable $X$ is
	\begin{equation*}
		G_{X}(s)=\expect s^{X}=\sum_{i=0}^{\infty}s^{i}f_{X}(i)
	\end{equation*}
\end{defn}
Using this, we can actually find what distribution a random variable has with the following theorem.
\begin{thm}
	Given two random variables $X$ and $Y$ with corresponding PGFs. If two PGFs are the same, then $X$ and $Y$ have the same distribution.
\end{thm}
This is particularly useful to find the distribution of a random variable.
\begin{eg}
	\label{Chapter 7 (Example) Two independent poisson PGF}
	Suppose that $X\independent Y$. Let $X\sim\Poisson(\lambda)$ and $Y\sim\Poisson(\mu)$. What is the distribution of $Z=X+Y$?\\
	Recall that $f_{Z}=f_{X}*f_{Y}$. We let $a_{n}=f_{X}(n)$ and $b_{n}=f_{Y}(n)$.
	\begin{align*}
		G_{X}(s)&=\sum_{i=0}^{\infty}\frac{\lambda^{i}e^{-\lambda}}{i!}s^{i}=e^{\lambda(s-1)} & G_{Y}(s)&=e^{\mu(s-1)} & G_{Z}(s)&=e^{(\lambda+\mu)(s-1)}
	\end{align*}
	We may conclude that $Z\sim\Poisson(\lambda+\mu)$.
\end{eg}
\begin{rem}
	If $a_{n}=f_{X}(n)$ for some random variables $X$, then $R\geq 1$ for $G_{X}(s)=G_{a}(s)$ since
	\begin{equation*}
		\sum_{n=0}^{\infty}f_{X}(n)s^{n}
	\end{equation*}
	converges when $s\in[-1,1]$.
\end{rem}

\newpage
\begin{eg}
	Let $X\sim\Poisson(\lambda)$ and  $a_{n}=f_{X}(n)=\frac{\lambda^{n}e^{-\lambda}}{n!}$. By ratio test, as $n\to\infty$,
	\begin{equation*}
		\frac{a_{n}}{a_{n+1}}=\frac{n+1}{\lambda}\to\infty
	\end{equation*}
	Therefore, $R=\infty$.
\end{eg}
\begin{eg}
	Let $X$ has a PMF $a_{n}=f_{X}(n)=\frac{c}{n^{2}}$. By ratio test, as $n\to\infty$,
	\begin{equation*}
		\frac{a_{n}}{a_{n+1}}=\frac{(n+1)^{2}}{n}\to 1
	\end{equation*}
	Therefore, $R=1$.
\end{eg}
There is an important theorem regarding $s=1$. Again, we are not going to prove it.
\begin{thm}(\textbf{Abel's Theorem})
	Suppose that $a_{n}\geq 0$ for all $n$. If $a$ has a generating function $G_{a}(s)$ and radius of convergence $R=1$, then if $\sum_{n=0}^{\infty}a_{n}$ converges in $\mathbb{R}\cup\{\infty\}$, we have
	\begin{equation*}
		\lim_{s\to 1^{-}}G_{a}(s)=\sum_{n=0}^{\infty}a_{n}\lim_{s\to 1^{-}}s^{n}=\sum_{n=0}^{\infty}a_{n}
	\end{equation*}
\end{thm}
\begin{eg}
	We have some PGF of random variable $X$.
	\begin{align*}
		X&\sim\Bern(p) & G_{X}(s)&=ps^{1}+(1-p)s^{0}=1-p+ps\\
		X&\sim\Bin(n,p) & G_{X}(s)&=(1-p+ps)^{n}\\
		X&\sim\Geom(p) & G_{X}(s)&=\sum_{n=1}^{\infty}(1-p)^{n-1}ps^{n}=\frac{ps}{1-s(1-p)}\\
		X&\sim\Poisson(\lambda) & G_{X}(s)&=e^{\lambda(s-1)}
	\end{align*}
\end{eg}
We already know that by computing the derivatives of $G$ at $s=0$, we can get the probability sequence. The following theorem shows that we can get the moment sequence by computing the derivatives of $G$ at $s=1$.
\begin{thm}
	If random variable $X$ has a PGF $G_{X}(s)$, then
	\begin{enumerate}
		\item $\expect X=\lim_{s\to 1^{-}}G'(s)=G'(1)$
		\item $\expect(X(X-1)\cdots(X-k+1))=G^{(k)}(1)$
		\item $\Var(X)=G''(1)+G'(1)-(G'(1))^{2}$
	\end{enumerate}
\end{thm}
\begin{proofing}
	\begin{enumerate}
		\item By having $s=1$,
		\begin{equation*}
			\left.\pdv*{G_{X}(s)}{s}\right|_{s=1}=\left.\pdv*{\sum_{k=0}^{\infty}f_{X}(k)s^{k}}{s}\right|_{s=1}=\left.\sum_{k=1}^{\infty}kf_{X}(k)s^{k-1}\right|_{s=1}=\sum_{k=1}^{\infty}kf_{X}(k)=\expect X
		\end{equation*}
		\item Let $s<1$.
		\begin{equation*}
			G^{(k)}(s)=\pdv*[order={k}]{\sum_{n}f_{X}(n)s^{n}}{s}=\sum_{n}n(n-1)\cdots(n-k+1)s^{n-k}f_{X}(n)=\expect(s^{X-k}X(X-1)\cdots(X-k+1))
		\end{equation*}
		By applying Abel's Theorem, we obtain
		\begin{equation*}
			G^{(k)}(1)=\expect(X(X-1)\cdots(X-k+1))
		\end{equation*}
		\item 
		\begin{equation*}
			\Var(X)=\expect(X^{2})-(\expect X)^{2}=\expect(X(X-1))+\expect X-(\expect X)^{2}=G''(1)+G'(1)-(G'(1))^{2}
		\end{equation*}
	\end{enumerate}
\end{proofing}

\newpage
Interestingly, we can also use generating function to deal with sum of random number of independent random variables.
\begin{thm}
	Let $X_{1},X_{2},\cdots$ be a sequence of independent identically distributed (i.i.d.) random variables with common PGF $G_{X}(s)$ and $N$ be a random variable independent of $X_{i}$ for all $i$ with PGF $G_{N}(s)$. If $T=X_{1}+X_{2}+\cdots+X_{N}$, then
	\begin{equation*}
		G_{T}(s)=G_{N}(G_{X}(s))
	\end{equation*}
\end{thm}
\begin{proofing}
	\begin{align*}
		G_{T}(s)=\expect s^{T}=\expect(\expect(s^{T}|N))=\sum_{n}\expect(s^{T}|N=n)\prob(N=n)
		=\sum_{n}\expect(s^{X_{1}+X_{2}+\cdots+X_{n}}|N=n)\prob(N=n)
		&=\sum_{n}(G_{X}(s))^{n}\prob(N=n)\\
		&=G_{N}(G_{X}(s))
	\end{align*}
\end{proofing}
\begin{eg}
	The sum of a Poisson number of independent Bernoulli random variables is still Poisson.\\
	Let $G_{N}(t)=e^{\lambda(t-1)}$ and $G_{X}(s)=1-p+ps$.
	\begin{equation*}
		G_{T}(s)=G_{N}(G_{X}(s))=e^{\lambda(1-p+ps-1)}=e^{\lambda p(s-1)}
	\end{equation*}
	Therefore, $T\sim\Poisson(\lambda p)$.
\end{eg}
When JPMF exists, there obviously will be a joint PGF.
\begin{defn}
	Let random variables $X_{1},X_{2}$ be both non-negative integer-valued, jointly discrete with JPMF $f_{X_{1},X_{2}}$.\\
	\textbf{Joint probability generating function} (JPGF) is defined by
	\begin{equation*}
		G_{X_{1},X_{2}}(s_{1},s_{2})=\expect(s_{1}^{X_{1}}s_{2}^{X_{2}})=\sum_{i=0}^{\infty}\sum_{j=0}^{\infty}s_{1}^{i}s_{2}^{j}f_{X_{1},X_{2}}(i,j)
	\end{equation*}
\end{defn}
\begin{rem}
	We can find that
	\begin{equation*}
		f_{X_{1},X_{2}}(i,j)=\left.\left(\pdv*[order={i}]{\pdv*[order={j}]{\frac{G_{X_{1},X_{2}}(s_{1},s_{2})}{i!j!}}{s_{2}}}{s_{1}}\right)\right|_{(s_{1},s_{2})=(0,0)}
	\end{equation*}
\end{rem}
\begin{thm}
	Random variables $X$ and $Y$ are independent if and only if $G_{X,Y}(s,t)=G_{X}(s)G_{Y}(t)$.
\end{thm}
\begin{proofing}
	If $X\independent Y$,
	\begin{equation*}
		G_{X,Y}(s,t)=\sum_{i=0}^{\infty}\sum_{j=0}^{\infty}s^{i}t^{j}f_{X,Y}(i,j)=\sum_{i=0}^{\infty}s^{i}f_{X}(i)\sum_{j=0}^{\infty}t^{j}f_{Y}(j)=G_{X}(s)G_{Y}(t)
	\end{equation*}
	If $G_{X,Y}(s,t)=G_{X}(s)G_{Y}(t)$, we consider the coefficient of terms $s^{i}t^{j}$ for all $i\geq 0$ and $j\geq 0$. We can see that
	\begin{equation*}
		f_{X,Y}(i,j)=f_{X}(i)f_{Y}(j)
	\end{equation*}
	Therefore, $X\independent Y$.
\end{proofing}
\begin{thm}
	If random variables $X$ and $Y$ are independent, then $G_{X+Y}(t)=G_{X}(t)G_{Y}(t)$
\end{thm}
\begin{proofing}
	\begin{equation*}
		G_{X+Y}(t)=\expect(t^{X+Y})=\expect(t^{X})\expect(t^{Y})=G_{X}(t)G_{Y}(t)
	\end{equation*}
\end{proofing}
\begin{rem}
	The converse does not necessarily be true.
\end{rem}
\section{Applications of generating functions}
The following example involves simple random walk, which is discussed in Appendix \ref{Appendix A Simple random walk}. Generating functions are particularly valuable when studying random walk. So far, we have only considered random variables $X$ taking finite values only. In this application, we encounter variables that can take the value $+\infty$. For such variables $X$, $G_{X}(s)$ converges so long as $\abs{s}<1$ and
\begin{equation*}
	\lim_{s\to 1^{-}}G_{X}(s)=\sum_{k}\prob(X=k)=1-\prob(X=\infty)
\end{equation*}
\begin{defn}
	A random variable $X$ is \textbf{defective} if $\prob(X=\infty)>0$.
\end{defn}
\begin{rem}
	It is no surprise that expectation is infinite when random variable is defective.
\end{rem}
With this generalization, we can start discussing random walk.
\begin{eg}(\textbf{Recurrence and transience of random walk})
	\label{Chapter 7 (Example) Simple random walk recurrence and transience}
	Let $Y_{n}$ the position of the particles after $n$ moves and $X_{i}$ be independent and identically distributed random variables mentioned in Appendix \ref{Appendix A Simple random walk}. For $n\geq 0$,
	\begin{align*}
		Y_{n}&=\sum_{i=1}^{n}X_{i} & Y_{0}&=0 & \prob(X_{i}=1)&=p & \prob(X_{i}=-1)&=q=1-p
	\end{align*}
	Let $T_{0}$ be number of moves until the particle makes its first return to the origin.
	\begin{equation*}
		T_{0}=\min\{i\geq 1:Y_{i}=0\}
	\end{equation*}
	Is $T_{0}$ a defective random variable? How do we calculate $\prob(T_{0}=\infty)$?\\
	Let $p_{0}(n)$ be the probability of the particle return to the origin at $n$ moves and $P_{0}$ be the generating function of $p_{0}$.\\
	Let $f_{0}(n)$ be the probability of the particle first return to the origin at $n$ moves and $F_{0}$ be the generating function of $f_{0}$.
	\begin{align*}
		p_{0}(n)&=\prob(Y_{n}=0)=\begin{cases}
			\binom{n}{\frac{n}{2}}p^{\frac{n}{2}}q^{\frac{n}{2}}, &n\text{ is even}\\
			0, &n\text{ is odd}
		\end{cases} & P_{0}(s)&=\lim_{N\to\infty}\sum_{n=0}^{N}p_{0}(n)s^{n}\\
		f_{0}(n)&=\prob(Y_{1}\neq 0,Y_{2}\neq 0,\cdots,Y_{n-1}\neq 0,Y_{n}=0)=\prob(T_{0}=n) & F_{0}(s)&=\lim_{N\to\infty}\sum_{n=1}^{N}f_{0}(n)s^{n}
	\end{align*}
\end{eg}
\begin{thm}
	\label{Chapter 7 (Theorem) Simple random walk particle return generating function}
	From the definitions in Example \ref{Chapter 7 (Example) Simple random walk recurrence and transience}, we have
	\begin{enumerate}
		\item $P_{0}(s)=1+P_{0}(s)F_{0}(s)$
		\item $P_{0}(s)=(1-4pqs^{2})^{-\frac{1}{2}}$
		\item $F_{0}(s)=1-(1-4pqs^{2})^{\frac{1}{2}}$
	\end{enumerate}
\end{thm}
\begin{proofing}
	\begin{enumerate}
		\item By using Law of total probability, 
		\begin{align*}
			p_{0}(n)&=\sum_{i=1}^{n}\prob(Y_{n}=0|Y_{1}\neq 0,Y_{2}\neq 0,\cdots,Y_{i-1}\neq 0,Y_{i}=0)f_{0}(i)\\
			\tag{Markov property in Lemma \ref{Appendix A (Lemma) Simple random walk properties}}
			&=\sum_{i=1}^{n}\prob(Y_{n}=0|Y_{i}=0)f_{0}(i)\\
			\tag{Temporarily homogeneous property in Lemma \ref{Appendix A (Lemma) Simple random walk properties}}
			&=\sum_{i=1}^{n}\prob(Y_{n-i}=0)f_{0}(i)\\
			&=\sum_{i=1}^{n}p_{0}(n-k)f_{0}(i)\\
			p_{0}(0)&=1\\
			P_{0}(s)=\sum_{k=0}^{\infty}p_{0}(k)s^{k}=1+\sum_{k=1}^{\infty}p_{0}(k)s^{k}&=1+\sum_{k=1}^{\infty}\sum_{i=1}^{k}p_{0}(k-i)f_{0}(i)s^{k}=1+\sum_{i=1}^{\infty}\sum_{k=i}^{\infty}p_{0}(k-i)s^{k-i}f_{0}(i)s^{i}=1+P_{0}(s)F_{0}(s)
		\end{align*}
		\item \textit{If you want to understand the proof, search "Central binomial coefficient" in Wikipedia}\\
		We know that $Y_{n}=0$ if $n$ is even. Therefore,
		\begin{equation*}
			\tag{$\binom{\frac{-1}{2}}{i}$ is a generalized binomial coefficient}
			P_{0}(s)=\lim_{N\to\infty}\sum_{n=0}^{N}p_{0}(n)s^{n}=\lim_{N\to\infty}\sum_{i=0}^{N}\binom{2i}{i}p^{i}q^{i}s^{2i}=\lim_{N\to\infty}\sum_{i=1}^{N}(-1)^{i}4^{i}\binom{\frac{-1}{2}}{i}p^{i}q^{i}s^{2i}=\frac{1}{\sqrt{1-4pqs^{2}}}
		\end{equation*}
		\item By applying (1) and (2), we can get
		\begin{equation*}
			F_{0}(s)=\frac{P_{0}(s)-1}{P_{0}(s)}=1-\sqrt{1-4pqs^{2}}
		\end{equation*}
	\end{enumerate}
\end{proofing}
From this theorem, we can get the following corollary.
\begin{cor}
	The probability that the particle ever returns to the origin is
	\begin{equation*}
		\sum_{n=1}^{\infty}f_{0}(n)=F_{0}(1)=1-\abs{p-q}
	\end{equation*}
	Probability that the particle will not return to origin ever is
	\begin{equation*}
		\prob(T_{0}=\infty)=\abs{p-q}
	\end{equation*}
\end{cor}
\begin{proofing}
	By using Theorem \ref{Chapter 7 (Theorem) Simple random walk particle return generating function}, since $p+q=1$,
	\begin{equation*}
		F_{0}(1)=1-(1-4pq)^{\frac{1}{2}}=1-(p^{2}-2pq+q^{2})^{\frac{1}{2}}=1-\abs{p-q}
	\end{equation*}
\end{proofing}
\begin{rem}
	Random walk is \textbf{recurrent} if it has at least one recurrent point. ($\prob(X<\infty)=1$)\\
	Random walk is \textbf{transient} if it has no recurrent points. ($\prob(X=\infty)>0$)\\
	Notice that when $p=q=\frac{1}{2}$, $\prob(T_{0}=\infty)$ and therefore random walk is recurrent.\\
	If $p\neq q$, then $\prob(T_{0}=\infty)\neq 0$ and so the random walk is transient.
\end{rem}
\begin{eg}
	We use the Example \ref{Chapter 7 (Example) Simple random walk recurrence and transience} again. How do we calculate $\expect T_{0}$ if $p=q=\frac{1}{2}$?
	\begin{align*}
		F_{0}(s)&=1-\sqrt{1-s^{2}} & F_{0}'(s)&=\frac{s}{\sqrt{1-s^{2}}} & \expect T_{0}&=\lim_{s\to 1^{-}}F_{0}'(s)=\infty
	\end{align*}
	This means that although we find that the particle almost certainly return to origin, the expectation for number of steps needed to return to origin is still infinite.
\end{eg}
We move on to our next important application, which is the Branching Process.\\
Many scientists have been interested in reproduction in a population. Accurate models for evolution are extremely difficult to handle, but some non-trivial models are tractable. We will investigate one of the models.
\begin{eg}(\textbf{Galton-Watson process}) 
	This process investigates a population that evolves in generations.\\
	Let $Z_{n}$ be number of individuals of the $n$-th generation and $X_{i}^{(m)}$ be number of offspring of the $i$-th individual of the $m$-th generation. We have:
	\begin{equation*}
		Z_{n+1}=\begin{cases}
			X_{1}^{(n)}+X_{2}^{(n)}+\cdots+X_{Z_{n}}^{(n)}, &Z_{n}\geq 1\\
			0, &Z_{n}=0
		\end{cases}
	\end{equation*}
	We make some following assumptions:
	\begin{enumerate}
		\item Family sizes of the individuals of the branching process form a collection of independent random variables.\\
		($X_{i}^{(k)}$'s are independent)
		\item All family sizes have the same probability mass function $f$ and generating function $G$. ($X_{i}^{(k)}$'s are identically distributed)
	\end{enumerate}
	Assume that $Z_{0}=1$. Note that $Z_{1}=X_{1}^{(0)}$
\end{eg}

\newpage
\begin{thm}
	\label{Chapter 7 (Theorem) Galton-Watson process PGF n-fold iterate properties}
	Let $G_{n}(s)=\expect s^{Z_{n}}$ and $G(s)=G_{1}(s)=\expect s^{Z_{1}}=\expect s^{X_{i}^{(m)}}$ for all $i$ and $m$. Then
	\begin{equation*}
		G_{n}(s)=G(G(\cdots(G(s))\cdots))=G(G_{n-1}(s))=G_{n-1}(G(s))
	\end{equation*}
	is the $n$-fold iteration of $G$.\\
	This further implies
	\begin{equation*}
		G_{m+n}(s)=G_{m}(G_{n}(s))=G_{n}(G_{m}(s))
	\end{equation*}
\end{thm}
\begin{proofing}
	When $n=2$, 
	\begin{equation*}
		G_{2}(s)=\expect s^{Z_{2}}=\expect s^{X_{1}^{(1)}+X_{2}^{(1)}+\cdots+X_{Z_{1}}^{(1)}}=G_{Z_{1}}\left(G_{X_{1}^{(1)}}(s)\right)=G(G(s))
	\end{equation*}.\\
	When $n=m+1$ for some $m$,
	\begin{equation*}
		G_{m+1}(s)=\expect s^{Z_{m+1}}=\expect s^{X_{1}^{(m)}+X_{2}^{(m)}+\cdots+X_{Z_{m}}^{(m)}}=G_{Z_{m}}\left(G_{X_{1}^{(m)}}(s)\right)=G_{m}(G(s))
	\end{equation*}
\end{proofing}
In principle, the above theorem tells us the distribution of $Z_{n}$. However, it may not be easy to compute $G_{n}(s)$.\\
The moments of $Z_{n}$ can be computed easier.
\begin{lem}
	Let $\expect Z_{1}=\expect X_{i}^{(m)}=\mu$ and $\Var(Z_{1})=\sigma^{2}$. Then
	\begin{align*}
		\expect Z_{n}&=\mu^{n} & \Var(Z_{n})&=\begin{cases}
			n\sigma^{2}, &\mu=1\\
			\frac{\sigma^{2}(\mu^{n}-1)\mu^{n-1}}{\mu-1}, &\mu\neq 1
		\end{cases}
	\end{align*}
\end{lem}
\begin{proofing}
	Using Theorem \ref{Chapter 7 (Theorem) Galton-Watson process PGF n-fold iterate properties}, we can get
	\begin{align*}
		\expect Z_{2}&=G_{2}'(1)=G'(G(1))G'(1)=G'(1)\mu=\mu^{2}\\
		\expect Z_{n}&=G_{n}'(1)=G'(G_{n-1}(1))G_{n-1}'(1)=G'(1)\mu^{n-1}=\mu^{n}\\
		G_{1}''(1)&=\sigma^{2}+(G'(1))^{2}-G'(1)=\sigma^{2}+\mu^{2}-\mu\\
		G_{2}''(1)&=G''(G(1))(G'(1))^{2}+G'(G(1))G''(1)=G''(1)(\mu^{2}+\mu)\\
		G_{n}''(1)&=G''(G_{n-1}(1))(G_{n-1}'(1))^{2}+G'(G_{n-1}(1))G_{n-1}''(1)\\
		&=(\sigma^{2}+\mu^{2}-\mu)\mu^{2n-2}+\mu G_{n-1}''(1)\\
		&=\mu^{2n-2}(\sigma^{2}+\mu^{2}-\mu)+\mu^{2n-3}(\sigma^{2}+\mu^{2}-\mu)+\cdots+\mu^{n-1}(\sigma^{2}+\mu^{2}-\mu)\\
		&=\frac{\mu^{n-1}(\sigma^{2}+\mu^{2}-\mu)(\mu^{n}-1)}{\mu-1}
	\end{align*}
	If $\mu=1$,
	\begin{equation*}
		\Var(Z_{n})=G_{n}''(1)+G_{n}'(1)-(G_{n}'(1))^{2}=\sigma^{2}+G_{n-1}''(1)+1-1=n\sigma^{2}
	\end{equation*}
	If $\mu\neq 1$,
	\begin{equation*}
		\Var(Z_{n})=G_{n}''(1)+G_{n}'(1)-(G_{n}'(1))^{2}=\frac{\mu^{n-1}(\sigma^{2}+\mu^{2}-\mu)(\mu^{n}-1)}{\mu-1}+\mu^{n}-\mu^{2n}=\frac{\mu^{n-1}\sigma^{2}(\mu^{n}-1)}{\mu-1}
	\end{equation*}
\end{proofing}
\begin{eg}
	Does this process eventually lead to extinct?\\
	Note that
	\begin{align*}
		\{\text{ultimate extinction}\}&=\bigcup_{n}\{Z_{n}=0\}=\lim_{n\to\infty}\{Z_{n}=0\}\\
		\prob(\text{ultimate extinction})&=\prob\left(\lim_{n\to\infty}\{Z_{n}=0\}\right)=\lim_{n\to\infty}\prob(Z_{n}=0)=\lim_{n\to\infty}G_{n}(0)
	\end{align*}
	Let $\eta_{n}=G_{n}(0)$ and $\eta=\lim_{n\to\infty}\eta_{n}$.
\end{eg}

\newpage
\begin{thm}
	We have that $\eta$ is the smallest non-negative root of the equation
	\begin{equation*}
		s=G(s)
	\end{equation*}
	Furthermore,
	\begin{enumerate}
		\item $\eta=1$ if $\mu<1$
		\item $\eta<1$ if $\mu>1$
		\item $\eta=1$ if $\mu=1$ and $\sigma^{2}>0$
		\item $\eta=0$ if $\mu=1$ and $\sigma^{2}=0$
	\end{enumerate}
\end{thm}
\begin{proofing}
	\begin{equation*}
		\eta_{n}=G_{n}(0)=G(G_{n-1}(0))=G(\eta_{n-1})
	\end{equation*}
	We know that $\eta_{n}$ is bounded. Therefore, $\eta_{n}\to\eta$ for some $\eta\in[0,1]$.
	\begin{equation*}
		\eta=\lim_{n\to\infty}\eta_{n}=\lim_{n\to\infty}G(\mu_{n-1})=G\left(\lim_{n\to\infty}\eta_{n-1}\right)=G(\eta)
	\end{equation*}
	Suppose that there exists another non-negative root $\psi$.
	\begin{align*}
		\eta_{1}&=G(0)\leq G(\psi)=\psi\\
		\eta_{2}&=G(\eta_{1})\leq G(\psi)=\psi
	\end{align*}
	By induction, $\eta_{n}\leq\psi$ for all $n$ and therefore $\eta\leq\psi$. Therefore, $\eta$ is the smallest non-negative root of the equation $s=G(s)$.
	\begin{equation*}
		G''(s)=\sum_{i=2}^{\infty}i(i-1)s^{i-2}\prob(Z_{1}=i)\geq 0
	\end{equation*}
	Therefore, $G$ is non-decreasing and also either convex or a straight line.\\
	When $\mu\neq 1$, we can find that two curves $y=G(s)$ and $y=s$ intersects at $s=1$ and $s=k\in\mathbb{R}$.\\
	We know that $\eta\leq 1$ since $\eta$ is the smallest root. In order to intersect at $s=\eta$, $G'(\eta)\leq 1$.\\
	If $\mu=G'(1)<1$, then $\eta=1$.\\
	If $\mu=G'(1)>1$, then $\eta=k$ such that $G'(k)\leq 1$.\\
	In the case when $\mu=G'(1)=1$, we need to further analyse whether $y=G(s)$ intersects $y=s$ at $1$ point or infinite points.
	\begin{equation*}
		\sigma^{2}=G''(1)+G'(1)-(G'(1))^{2}=G''(1)
	\end{equation*}
	If $\sigma^{2}=G''(1)>0$, then $\eta=1$.\\
	If $\sigma^{2}=G''(1)=0$, then $\eta=0$.
\end{proofing}

\section{Moment generating function and Characteristic function}
Recall that we can unify both discrete and continuous distribution into one. We can change how we define PGF.
\begin{defn}
	\textbf{Probability generating function} of a random variable $X$ is given by:
	\begin{equation*}
		\expect s^{X}=\int s^{x}\,dF_{X}
	\end{equation*}
\end{defn}
For a more general variables $X$, it is best if we substitute $s=e^{t}$. We get the following definition.
\begin{defn}
	\textbf{Moment generating function} (MGF) of a random variable $X$ is the function $M:\mathbb{R}\to[0,\infty)$ given by:
	\begin{equation*}
		M_{X}(t)=\expect(e^{tX})=\int e^{tx}\,dF_{X}
	\end{equation*}
\end{defn}
\begin{rem}
	The definition of MGF only requires replacing $s$ by $e^{t}$ in PGF. MGF is easier for computing moments, but less convenient for computing distribution.
\end{rem}
\begin{rem}
	MGFs are related to Laplace transforms.
\end{rem}
\begin{defn}
	\textbf{Joint moment generating function} (JMGF) of two random variable $X$ and $Y$ is given by:
	\begin{equation*}
		M_{X,Y}(s,t)=\expect(e^{sX+tY})
	\end{equation*}
\end{defn}
We can easily get the following lemma.
\begin{lem}
	Given a MGF $M_{X}(t)$ of a random variable $X$. 
	\begin{enumerate}
		\item For any $k\geq 0$,
		\begin{equation*}
			\expect X^{k}=M_{X}^{(k)}(0)
		\end{equation*}
		\item The function $M$ can be expanded via Taylor's Theorem within its radius of convergence.
		\begin{equation*}
			M_{X}(t)=\sum_{i=0}^{\infty}\frac{\expect X^{k}}{k!}t^{k}
		\end{equation*}
		\item If $X$ and $Y$ are independent, then
		\begin{equation*}
			M_{X+Y}(t)=M_{X}(t)M_{Y}(t)
		\end{equation*}
	\end{enumerate}
\end{lem}
\begin{proofing}
	\begin{enumerate}
		\item 
		\begin{equation*}
			M^{(k)}(0)=\left.\pdv*[order={k}]{\int e^{tx}\,dF_{X}(x)}{t}\right|_{t=0}=\left.\int x^{k}e^{tx}\,dF_{X}(x)\right|_{t=0}=\int x^{k}\,dF_{X}(x)=\expect X^{k}
		\end{equation*}
		\item Just using (1) and Taylor's Theorem and you get the answer.
		\item Substitute $s=e^{t}$ into Theorem \ref{Chapter 7 (Theorem) PGF for sum of random independent variables}.
	\end{enumerate}
\end{proofing}
\begin{lem}
	If $X_{1},X_{2},\cdots,X_{n}$ are independent, then:
	\begin{equation*}
		M_{X_{1},X_{2},\cdots,X_{n}}(t_{1},t_{2},\cdots,t_{n})=M_{X_{1}}(t_{1})M_{X_{2}}(t_{2})\cdots M_{X_{n}}(t_{n})
	\end{equation*}
\end{lem}
\begin{proofing}
	By independence,
	\begin{equation*}
		M_{X_{1},X_{2},\cdots,X_{n}}(t_{1},t_{2},\cdots,t_{n})=\expect(e^{t_{1}X_{1}+t_{2}X_{2}+\cdots+t_{n}X_{n}})=\expect(e^{t_{1}X_{1}})\expect(e^{t_{2}X_{2}})\cdots\expect(e^{t_{n}X_{n}})=M_{X_{1}}(t_{1})M_{X_{2}}(t_{2})\cdots M_{X_{n}}(t_{n})
	\end{equation*}
\end{proofing}
\begin{rem}
	$M_{X}(0)=1$ for all random variables $X$.
\end{rem}
\begin{eg}
	Let $X\sim\Bern(p)$. We have:
	\begin{equation*}
		M_{X}(t)=\expect(e^{tX})=q+pe^{t}
	\end{equation*}
\end{eg}
\begin{eg}
	Let $X\sim\Bin(n,p)$. We have:
	\begin{equation*}
		M_{X}(t)=(q+pe^{t})^{n}
	\end{equation*}
\end{eg}
\begin{eg}
	Let $X\sim\Geom(p)$. We have:
	\begin{align*}
		f_{X}(x)&=p(1-p)^{x-1} & M_{X}(t)&=\sum_{k=1}^{\infty}e^{tk}p(1-p)^{k-1}=\frac{pe^{t}}{1-e^{t}(1-p)}\\
		f_{X}(x)&=p(1-p)^{x} & M_{X}(t)&=\sum_{k=0}^{\infty}e^{tk}p(1-p)^{k}=\frac{p}{1-e^{t}(1-p)}
	\end{align*}
\end{eg}

\newpage
\begin{eg}
	Let $X\sim\NBin(r,p)$. We have:
	\begin{align*}
		f_{X}(x)&=\binom{x-1}{r-1}p^{r}(1-p)^{x-r} & M_{X}(t)&=\left(\frac{pe^{t}}{1-e^{t}(1-p)}\right)^{n}\\
		f_{X}(x)&=\binom{x+r-1}{r-1}p^{r}(1-p)^{x} & M_{X}(t)&=\left(\frac{p}{1-e^{t}(1-p)}\right)^{n}
	\end{align*}
\end{eg}
\begin{eg}
	Let $X\sim\Poisson(\lambda)$. We have:
	\begin{equation*}
		M_{X}(t)=\sum_{k=0}^{\infty}\frac{\lambda^{k}e^{tk-\lambda}}{k!}=e^{\lambda(e^{t}-1)}
	\end{equation*}
\end{eg}
\begin{eg}
	Let $X\sim\U[a,b]$ for some $a<b$. We have:
	\begin{equation*}
		M_{X}(t)=\begin{cases}
			\int_{a}^{b}\frac{e^{tx}}{b-a}\,dx=\frac{e^{tb}-e^{ta}}{t(b-a)}, &t\neq 0\\
			1, &t=0
		\end{cases}
	\end{equation*}
\end{eg}
\begin{eg}
	Let $X\sim\Exp(\lambda)$. For $t<\lambda$, we have:
	\begin{equation*}
		M_{X}(t)=\int_{0}^{\infty}\lambda e^{x(t-\lambda)}\,dx=\frac{\lambda}{\lambda-t}
	\end{equation*}
\end{eg}
\begin{eg}
	Let $X\sim\N(\mu,\sigma^{2})$. We have:
	\begin{align*}
		M_{X}(t)&=\frac{1}{\sqrt{2\pi\sigma^{2}}}\int_{-\infty}^{\infty}e^{tx}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right)\,dx\\
		&=\frac{1}{\sqrt{2\pi\sigma^{2}}}\int_{-\infty}^{\infty}\exp\left(-\frac{x^{2}-2(\mu-\sigma^{2}t)x+\mu^{2}}{2\sigma^{2}}\right)\,dx\\
		&=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(\frac{(\mu-\sigma^{2}t)^{2}-\mu^{2}}{2\sigma^{2}}\right)\int_{-\infty}^{\infty}\exp\left(-\frac{(x-(\mu-\sigma^{2}t))^{2}}{2\sigma^{2}}\right)\,dx\\
		&=\exp\left(\frac{-2\mu\sigma^{2}t+\sigma^{4}t^{2}}{2\sigma^{2}}\right)\\
		&=\exp\left(\frac{1}{2}\sigma^{2}t^{2}-\mu t\right)
	\end{align*}	
\end{eg}
\begin{eg}
	Let $X\sim\Cauchy(0)$.
	\begin{align*}
		f_{X}(x)&=\frac{1}{\pi(1+x^{2})} & M_{X}(t)&=\frac{1}{\pi}\intinfty\frac{e^{tx}}{1+x^{2}}\,dx
	\end{align*}
	$M_{X}(t)$ exists only at $t=0$. We get $M_{X}(0)=1$.
\end{eg}
\begin{eg}
	Let $X\sim\Gam(\alpha,\lambda)$. If $t<\lambda$, we have:
	\begin{equation*}
		M_{X}(t)=\left(1-\frac{t}{\lambda}\right)^{-\alpha}
	\end{equation*}
\end{eg}
\begin{eg}
	Let $X\sim\chi^{2}(k)$. If $t<\frac{1}{2}$, we have:
	\begin{equation*}
		M_{X}(t)=(1-2t)^{-\frac{k}{2}}
	\end{equation*}
\end{eg}
\begin{eg}
	Let $X\sim\Beta(\alpha,\beta)$. We have:
	\begin{equation*}
		M_{X}(t)=1+\sum_{k=1}^{\infty}\left(\prod_{r=0}^{k-1}\frac{\alpha+r}{\alpha+\beta+r}\right)\frac{t^{k}}{k!}
	\end{equation*}
\end{eg}
\begin{rem}
	Not all distributions have MGF.
\end{rem}
\begin{eg}
	MGF of $X\sim t(n)$ is undefined.
\end{eg}
Moment generating functions provide a useful technique but the integrals used to define may not be finite. There is another class of functions which finiteness is guaranteed.
\begin{defn}
	\textbf{Characteristic function} (CF) of a random variable $X$ is the function $\phi_{X}:\mathbb{R}\to\mathbb{C}$ given by:
	\begin{align*}
		\phi_{X}(t)&=\expect(e^{itX})=\int e^{itx}\,dF_{X}(x)=\expect\cos(tX)+i\expect\sin(tX) & i&=\sqrt{-1}
	\end{align*}
\end{defn}
\begin{rem}
	$\phi_{X}(t)$ is essentially a Fourier Transform.
\end{rem}
\begin{lem}
	CF $\phi_{X}$ of a random variable $X$ has the following properties:
	\begin{enumerate}
		\item $\phi_{X}(0)=1$. $\abs{\phi_{X}(t)}\leq 1$ for all $t$
		\item $\phi_{X}(t)$ is uniformly continuous
	\end{enumerate}
\end{lem}
\begin{proofing}
	\begin{enumerate}
		\item For all $t$,
		\begin{align*}
			\phi_{X}(0)&=\int\,dF_{X}(x)=1 & \abs{\phi_{X}(t)}&=\abs{\int(\cos(tx)+i\sin(tx))\,dF_{X}(x)}\leq\int\abs{\cos(tx)+i\sin(tx)}\,dF_{X}(x)=\int\,dF_{X}(x)=1
		\end{align*}
		\item
		\begin{equation*}
			\sup_{t}\abs{\phi_{X}(t+c)-\phi_{X}(t)}=\sup_{t}\abs{\int(e^{i(t+c)x}-e^{itx})\,dF_{X}(x)}\leq\sup_{t}\left(\int\abs{e^{itx}}\abs{e^{icx-1}}\,dF_{X}(x)\right)
		\end{equation*}
		When $c\to 0$, the supremum $\to 0$. Therefore, $\phi_{X}(t)$ is uniformly continuous.
	\end{enumerate}
\end{proofing}
\begin{thm}
	There are some properties of $\phi_{X}$ of a random variable $X$ regarding derivatives and moments.
	\begin{enumerate}
		\item If $\phi_{X}^{(k)}(0)$ exists, then
		\begin{equation*}
			\begin{cases}
				\expect\abs{X}^{k}<\infty, &k\text{ is even}\\
				\expect\abs{X}^{k-1}<\infty, &k\text{ is odd}
			\end{cases}
		\end{equation*}
		\item If $\expect\abs{X}^{k}<\infty$, then $\phi_{X}^{(k)}(0)$ exists. We have
		\begin{equation*}
			\phi_{X}(t)=\sum_{j=0}^{k}\frac{\phi_{X}^{(j)}(0)}{j!}t^{j}+o(t^{k})=\sum_{j=0}^{k}\frac{\expect X^{j}}{j!}(it)^{j}+o(t^{k})
		\end{equation*}
	\end{enumerate}
\end{thm}
\begin{proofing}
	We use the Taylor's Theorem.
	\begin{equation*}
		\phi_{X}(t)=\sum_{j=0}^{k}\frac{\phi_{X}^{(j)}(0)}{j!}t^{j}+o(t^{k})=\sum_{j=0}^{k}\frac{\expect X^{j}}{j!}(it)^{j}+o(t^{k})
	\end{equation*}
	\begin{enumerate}
		\item 
		\begin{equation*}
			\phi_{X}^{(k)}(0)=i^{k}\expect X^{k}
		\end{equation*}
		If $k$ is even, we have $\phi_{X}^{(k)}(0)=(-1)^{\frac{k}{2}}\expect X^{k}=(-1)^{\frac{k}{2}}\expect\abs{X}^{k}$ exists. Therefore, $\expect\abs{X}^{k}<\infty$.\\
		If $k$ is odd, we know that $\phi_{X}^{(k-1)}(0)$ exists if $\phi_{X}^{(k)}(0)$ exists.\\
		Therefore, with $\phi_{X}^{(k-1)}(0)=(-1)^{\frac{k-1}{2}}\expect X^{k-1}=(-1)^{\frac{k-1}{2}}\expect\abs{X}^{k-1}$, $\expect\abs{X}^{k-1}<\infty$.
		\item Again using the formula in (1). We have
		\begin{equation*}
			\frac{\phi_{X}^{(k)}(0)}{i^{k}}=\expect X^{k}\leq\expect\abs{X}^{k}<\infty
		\end{equation*}
		Therefore, $\phi_{X}^{(k)}(0)$ exists. The formula can be obtained from the Taylor's theorem formula.
	\end{enumerate}
\end{proofing}
\begin{thm}
	If $X\independent Y$, then $\phi_{X+Y}(t)=\phi_{X}(t)\phi_{Y}(t)$
\end{thm}
\begin{proofing}
	\begin{equation*}
		\phi_{X+Y}(t)=\expect(e^{it(X+Y)})=\expect(e^{itX})\expect(e^{itY})=\phi_{X}(t)\phi_{Y}(t)
	\end{equation*}
\end{proofing}
Again and again, we have a joint characteristic function.
\begin{defn}
	\textbf{Joint characteristic function} (JCF) $\phi_{X,Y}$ of two random variables $X,Y$ is given by
	\begin{equation*}
		\phi_{X,Y}(s,t)=\expect(e^{i(sX+tY)})
	\end{equation*}
\end{defn}
We have another way to prove that two random variables are independent.
\begin{thm}
	\label{Chapter 7 (Theorem) Independence via CF}
	Two random variables $X,Y$ are independent if and only if for all $s$ and $t$,
	\begin{equation*}
		\phi_{X,Y}(s,t)=\phi_{X}(s)\phi_{Y}(t)
	\end{equation*}
\end{thm}
\begin{proofing}
	If $X\independent Y$,
	\begin{equation*}
		\phi_{X,Y}(s,t)=\expect(e^{i(sX+tY)})=\expect(e^{isX})\expect(e^{itY})=\phi_{X}(s)\phi_{Y}(t)
	\end{equation*}
	Currently, it is not suffice to prove the inverse. We will need to use a theorem later. (Example \ref{Chapter 7 (Example) Proof of Independence via CF})
\end{proofing}
\begin{eg}
	Let $X\sim\Bern(p)$. We have
	\begin{equation*}
		\phi_{X}(t)=\expect(e^{itX})=q+pe^{it}
	\end{equation*}
\end{eg}
\begin{eg}
	Let $X\sim\Bin(n,p)$. We have
	\begin{equation*}
		\phi_{X}(t)=(q+pe^{it})^{n}
	\end{equation*}
\end{eg}
\begin{eg}
	Let $X\sim\Exp(1)$. We have
	\begin{equation*}
		\phi_{X}(t)=\int e^{(it-1)x}\,dx=\frac{1}{1-it}
	\end{equation*}
\end{eg}
\begin{eg}
	Let $X\sim\Cauchy$. We have
	\begin{equation*}
		\phi_{X}(t)=e^{-\abs{t}}
	\end{equation*}
\end{eg}
\begin{eg}
	Let $X\sim\N(\mu,\sigma^{2})$. Using the fact that for any $u\in\mathbb{C}$, not just in $\mathbb{R}$,
	\begin{equation*}
		\frac{1}{\sqrt{2\pi\sigma^{2}}}\intinfty\exp\left(-\frac{(x-u)^{2}}{2\sigma^{2}}\right)\,dx=1
	\end{equation*}
	We have
	\begin{align*}
		\phi_{X}(t)&=\frac{1}{\sqrt{2\pi\sigma^{2}}}\intinfty e^{itx}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right)\,dx\\
		&=\frac{1}{\sqrt{2\pi\sigma^{2}}}\intinfty\exp\left(-\frac{x^{2}-(2\mu+2\sigma^{2}it)x+\mu^{2}}{2\sigma^{2}}\right)\,dx\\
		&=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(\frac{(\mu+\sigma^{2}it)^{2}-\mu^{2}}{2\sigma^{2}}\right)\intinfty\exp\left(-\frac{(x-(\mu+\sigma^{2}it))^{2}}{2\sigma^{2}}\right)\,dx\\
		&=\exp\left(\frac{\mu^{2}+2\sigma^{2}i\mu t-\sigma^{4}t^{2}-\mu^{2}}{2\sigma^{2}}\right)\\
		&=\exp\left(i\mu t-\frac{1}{2}\sigma^{2}t^{2}\right)
	\end{align*}
\end{eg}
\begin{rem}
	We have a function called \textbf{cumulant generating function} defined by $\log\phi_{X}(t)$. Normal distribution is the only distribution we have learnt whose cumulant generating function has finite terms, which is:
	\begin{equation*}
		\log\phi_{X}(t)=i\mu t-\frac{1}{2}\sigma^{2}t^{2}
	\end{equation*}
\end{rem}
\newpage
\section{Inversion and continuity theorems}
There are two major ways that characteristic functions are useful. One of them is that we can use characteristic function of a random variable to generate a probability density function of that random variable.
\begin{thm}(Fourier Inverse Transform for continuous case)
	If a random variable $X$ is continuous with a PDF $f_{X}$ and a CF $\phi_{X}$, then
	\begin{equation*}
		f_{X}(x)=\frac{1}{2\pi}\intinfty e^{-itx}\phi_{X}(t)\,dt
	\end{equation*}
	at all point $x$ which $f_{X}$ is differentiable.\\
	If $X$ has a CDF $F_{X}$, then
	\begin{equation*}
		F_{X}(b)-F_{X}(a)=\frac{1}{2\pi}\intinfty\intlu{a}{b}e^{-itx}\phi_{X}(t)\,dx\,dt
	\end{equation*}
\end{thm}
\begin{proofing}[Proof (Non-rigorous)]
	Let
	\begin{align*}
		I(x)&=\frac{1}{2\pi}\intinfty e^{itx}\phi_{X}(t)\,dt=\frac{1}{2\pi}\intinfty e^{-itx}\intinfty e^{ity}f_{X}(y)\,dy\,dt\\
		I_{\varepsilon}(x)&=\frac{1}{2\pi}\intinfty e^{-itx}\intinfty e^{ity}f_{X}(y)\,dy\,e^{-\frac{1}{2}\varepsilon^{2}t^{2}}\,dt
	\end{align*}
	We want to show that $I_{\varepsilon}(x)\to I(x)$ when $\varepsilon\to 0$.
	\begin{align*}
		I_{\varepsilon}(x)&=\frac{1}{2\pi}\intinfty\intinfty e^{-\frac{1}{2}\varepsilon^{2}t^{2}+i(y-x)t}f_{X}(y)\,dt\,dy\\
		&=\frac{1}{\sqrt{2\pi\varepsilon^{2}}}\left(\frac{1}{\sqrt{2\pi\frac{1}{\varepsilon^{2}}}}\right)\intinfty\exp\left(-\frac{(y-x)^{2}}{2\varepsilon^{2}}\right)f_{X}(y)\intinfty\exp\left(-\frac{-\left(t-i\frac{y-x}{\varepsilon}\right)^{2}}{2\left(\frac{1}{\varepsilon^{2}}\right)}\right)\,dt\,dy\\
		&=\frac{1}{\sqrt{2\pi\varepsilon^{2}}}\intinfty\exp\left(-\frac{(y-x)^{2}}{2\epsilon}\right)f_{X}(y)\,dy
	\end{align*}
	Let $Z\sim\N(0,1)$ and $Z_{\varepsilon}=\varepsilon Z$. $I_{\varepsilon}(x)$ is the PDF of $\varepsilon Z+X$. Therefore, we can say that $f_{\varepsilon Z+X}(x)\to f_{X}(x)$ when $\varepsilon\to 0$.
\end{proofing}
\begin{thm}(\textbf{Inversion Theorem})
	If a random variable $X$ have a CDF $F_{X}$ and a CF $\phi_{X}$, we define $\overline{F}_{X}:\mathbb{R}\to[0,1]$ by
	\begin{equation*}
		\overline{F}_{X}(x)=\frac{1}{2}\left(F_{X}(x)+F_{X}(x^{-})\right)
	\end{equation*}
	Then for all $a\leq b$,
	\begin{equation*}
		\overline{F}_{X}(b)-\overline{F}_{X}(a)=\intinfty\frac{e^{-iat}-e^{-ibt}}{2\pi it}\phi_{X}(t)\,dt
	\end{equation*}
\end{thm}
\begin{rem}
	We can say $\overline{F}_{X}$ represents the average of limit going from two directions.
\end{rem}
\begin{eg}
	\label{Chapter 7 (Example) Proof of Independence via CF}
	With the Inversion Theorem, we can now prove Theorem \ref{Chapter 7 (Theorem) Independence via CF}.\\
	Given two random variables $X,Y$. We want to first extend the Fourier Inverse Transform into multivariable case.\\
	If $\phi_{X,Y}(s,t)=\phi_{X}(s)\phi_{Y}(t)$, then for any $a\leq b$ and $c\leq d$,
	\begin{align*}
		\overline{F}_{X,Y}(b,d)-\overline{F}_{X,Y}(b,c)-\overline{F}_{X,Y}(a,d)+\overline{F}_{X,Y}(a,c)&=\intinfty\intinfty\frac{(e^{-ias}-e^{-ibs})(e^{-ict}-e^{-idt})}{-4\pi^{2}t^{2}}\phi_{X}(s)\phi_{Y}(t)\,ds\,dt\\
		&=(\overline{F}_{X}(b)-\overline{F}_{X}(a))\intinfty\frac{e^{-ict}-e^{-idt}}{2\pi it}\phi_{Y}(t)\,dt\\
		&=(\overline{F}_{X}(b)-\overline{F}_{X}(a))(\overline{F}_{Y}(d)-\overline{F}_{Y}(c))\\
		&=\overline{F}_{X}(b)\overline{F}_{Y}(d)-\overline{F}_{X}(b)\overline{F}_{Y}(c)-\overline{F}_{X}(a)\overline{F}_{Y}(d)+\overline{F}_{X}(a)\overline{F}_{Y}(c)
	\end{align*}
	From the definition of independent random variables, we prove that $X\independent Y$ if $\phi_{X,Y}(s,t)=\phi_{X}(s)\phi_{Y}(t)$.
\end{eg}
\newpage
Another way is to evaluate the convergence of a sequence of cumulative distribution function.
\begin{defn}(Convergence of distribution function sequence [Weak convergence])
	A sequence of CDF 
	$F_{1},F_{2},\cdots$ \textbf{converges} to a CDF $F$, written as $F_{n}\to F$, if at each point $x$ where $F$ is continuous,
	\begin{equation*}
		F_{n}(x)\to F(x)
	\end{equation*}
\end{defn}
\begin{eg}
	Assume we have two sequences of CDF.
	\begin{align*}
		F_{n}(x)&=\begin{cases}
			0, &x<\frac{1}{n}\\
			1, & x\geq\frac{1}{n}
		\end{cases} & G_{n}(x)&=\begin{cases}
			0, &x<-\frac{1}{n}\\
			1, & x\geq-\frac{1}{n}
		\end{cases}
	\end{align*}
	If we have $n\to\infty$, we get
	\begin{align*}
		F(x)&=\begin{cases}
			0, &x\leq 0\\
			1, &x>0
		\end{cases} & G(x)&=\begin{cases}
			0, &x<0\\
			1, &x\geq 0
		\end{cases}
	\end{align*}
	This is problematic because $F(x)$ in this case is not a distribution function because it is not right-continuous.\\
	Therefore, it is needed to define the convergence so that both sequences $\{F_{n}\}$ and $\{G_{n}\}$ have the same limit.
\end{eg}
We can modify a bit on the definition to say each distribution function in the sequence represents a different random variable.
\begin{defn}(Convergence in distribution for random variables)
	Let $X,X_{1},X_{2},\cdots$ be a family of random variables with PDF $F,F_{1},F_{2},\cdots$, we say $X_{n}\to X$, written as $X_{n}\xrightarrow{D}X$ or $X_{n}\Rightarrow X$, if $F_{n}\to F$.
\end{defn}
\begin{rem}
	For this convergence definition, we do not care about the closeness of $X_{n}$ and $X$ as functions of $\omega$.
\end{rem}
\begin{rem}
	Sometimes, we also write $X_{n}\Rightarrow F$ or $X_{n}\xrightarrow{D}F$.
\end{rem}
With the definition, sequence of characteristic functions can be used to determine whether the sequence of cumulative distribution function converges.
\begin{thm}(\textbf{L\'evy continuity theorem})
	Suppose that $F_{1},F_{2},\cdots$ is a sequence of CDF with CF $\phi_{1},\phi_{2},\cdots$, then
	\begin{enumerate}
		\item If $F_{n}\to F$ for some CDF $F$ with CF $\phi$, then $\phi_{n}\to\phi$ pointwise.
		\item If $\phi_{n}\to\phi$ pointwise for some CF $\phi$, and $\phi$ is continuous at $O$ ($t=0$), then $\phi$ is the CF of some CDF $F$ and $F_{n}\to F$.
	\end{enumerate}
\end{thm}
We have a more general definition of convergence.
\begin{defn}(\textbf{Vague convergence})
	Given a sequence of CDF $F_{1},F_{2},\cdots$. Suppose that $F_{n}(x)\to G(x)$ at all continuity point of $G$ but $G$ may not be a CDF. Then we say $F_{n}\to G$ \textbf{vaguely}, written as $F_{n}\xrightarrow{v}G$.
\end{defn}
\begin{eg}
	If
	\begin{align*}
		F_{n}(x)&=\begin{cases}
			0, &x<\frac{1}{n}\\
			\frac{1}{2}, &\frac{1}{n}\leq x<n\\
			1, &x\geq n
		\end{cases} & G(x)&=\begin{cases}
			0, &x<0\\
			\frac{1}{2}, &x\geq 0
		\end{cases}
	\end{align*}
	We can see that $F_{n}\xrightarrow{v}G$ if $n\to\infty$ and $G$ is not a CDF.
\end{eg}
\begin{rem}
	In L\'evy Continuity Theorem (2), the statement that $\phi$ is continuous at $O$ can be replaced by any of the following statements:
	\begin{enumerate}
		\item $\phi(t)$ is a continuous function of $t$
		\item $\phi(t)$ is a CF of some CDF
		\item The sequence $\{F_{n}\}_{n=1}^{\infty}$ is tight, i.e. for all $\epsilon>0$, there exists $M_{\epsilon}>0$ such that
		\begin{equation*}
			\sup_{n}(F_{n}(-M_{\epsilon})+1-F_{n}(M_{\epsilon}))\leq\epsilon
		\end{equation*}
	\end{enumerate}
\end{rem}
\begin{eg}
	Let $X_{n}\sim\N(0,n^{2})$ and let $\phi_{n}$ be the CF of $X_{n}$. Then
	\begin{equation*}
		\phi_{n}(t)=\exp\left(-\frac{1}{2}n^{2}t^{2}\right)\to\phi(t)=\begin{cases}
			0, &t\neq 0\\
			1, &t=0
		\end{cases}
	\end{equation*}
\end{eg}
\section{Two limit theorems}
In this section, we introduce two fundamental theorems in probability theory, the Law of Large Numbers and the Central Limit Theorem.
\begin{thm}(\textbf{Weak Law of Large Numbers} [WLLN])
	Let $X_{1},X_{2},\cdots$ be i.i.d. random variables. Assume that $\expect\abs{X_{1}}<\infty$ and $\expect X_{1}=\mu$. We have:
	\begin{equation*}
		\frac{1}{n}\sum_{i=1}^{n}X_{i}\xrightarrow{D}\mu
	\end{equation*}
\end{thm}
\begin{proofing}
	We recall the Taylor expansion of $\phi_{\xi}(s)$ at $0$. If $\expect\abs{\xi}^{k}<\infty$ and $s$ is small, then
	\begin{equation*}
		\phi_{\zeta}(s)=\sum_{j=0}^{k}\frac{\expect\xi^{j}}{j!}(is)^{j}+o(s^{k})
	\end{equation*}
	For any $t\in\mathbb{R}$, let $\phi_{X_{1}}(s)=\expect(e^{isX_{1}})$.
	\begin{align*}
		\phi_{n}(t)=\expect\left(\exp\left(\frac{it}{n}\sum_{i=1}^{n}X_{i}\right)\right)=\expect\left(\prod_{i=1}^{n}\exp\left(\frac{itX_{i}}{n}\right)\right)=\left(\expect\left(\exp\left(\frac{itX_{1}}{n}\right)\right)\right)^{n}=\left(\phi_{X_{1}}\left(\frac{t}{n}\right)\right)^{n}&=\left(1+\frac{it}{n}\expect X_{1}+o\left(\frac{t}{n}\right)\right)^{n}\\
		&=\left(1+\frac{i\mu t}{n}+o\left(\frac{t}{n}\right)\right)^{n}\\
		&\to e^{i\mu t}
	\end{align*}
	By L\'evy continuity theorem, we get that $\frac{1}{n}\sum_{i=1}^{n}X_{i}\xrightarrow{D}\mu$.
\end{proofing}
\begin{thm}(\textbf{Central Limit Theorem} [CLT])
	Let $X_{1},X_{2},\cdots$ be i.i.d. random variables with $\expect\abs{X_{1}}^{2}<\infty$ and $\expect X_{1}=\mu$, $\Var(X_{1})=\sigma^{2}$. Then
	\begin{equation*}
		\frac{1}{\sigma}\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}-\mu\right)=\frac{\sum_{i=1}^{n}X_{i}-n\mu}{\sqrt{n}\sigma}\xrightarrow{D}\N(0,1)
	\end{equation*}
\end{thm}
\begin{proofing}
	Let $Y_{i}=\frac{X_{i}-\mu}{\sigma}$. We have $\expect Y_{i}=0$ and $\Var(Y_{i})=1$.
	\begin{align*}
		\frac{\sum_{i=1}^{n}X_{i}-n\mu}{\sqrt{n}\sigma}&=\sum_{i=1}^{n}\frac{1}{\sqrt{n}}\frac{X_{i}-\mu}{\sigma}=\sum_{i=1}^{n}\frac{Y_{i}}{\sqrt{n}}\\
		\phi_{n}(t)&=\expect\left(\exp\left(it\sum_{\ell-1}^{n}\frac{Y_{\ell}}{\sqrt{n}}\right)\right)\\
		&=\left(\expect\left(\exp\left(\frac{itY_{1}}{\sqrt{n}}\right)\right)\right)^{n}\\
		&=\left(\phi_{Y_{1}}\left(\frac{t}{\sqrt{n}}\right)\right)^{n}\\
		\tag{Taylor expansion}
		&=\left(1+\frac{it}{\sqrt{n}}\expect Y_{1}+\frac{1}{2}\left(\frac{it}{\sqrt{n}}\right)^{2}\expect(Y_{i}^{2})+o\left(\frac{t^{2}}{n}\right)\right)^{n}\\
		&=\left(1-\frac{t^{2}}{2n}+o\left(\frac{t^{2}}{n}\right)\right)^{n}\\
		&\to e^{-\frac{1}{2}t^{2}}
	\end{align*}
	By L\'evy continuity theorem, $\frac{\sum_{i=1}^{n}X_{i}-n\mu}{\sqrt{n}\sigma}\xrightarrow{D}\N(0,1)$.
\end{proofing}
\newpage
Central Limit Theorem can be generalized in several directions, one of which concerns about independent random variables instead of i.i.d. random variables.
\begin{thm}
	Let $X_{1},X_{2},\cdots$ be independent random variables satisfying $\expect X_{i}=0$, $\Var(X_{i})=\sigma_{i}^{2}$, $\expect\abs{X_{i}}^{3}<\infty$ and such that
	\begin{equation*}
		\tag{*}
		\frac{1}{(\sigma(n))^{3}}\sum_{i=1}^{n}\expect\abs{X_{i}^{3}}\to 0 \text{ as }n\to\infty
	\end{equation*}
	where $(\sigma(n))^{2}=\Var(\sum_{i=1}^{n}X_{i})=\sum_{i=1}^{n}\sigma_{i}^{2}$. Then
	\begin{equation*}
		\frac{1}{\sigma(n)}\sum_{i=1}^{n}X_{i}\xrightarrow{D}\N(0,1)
	\end{equation*}
\end{thm}
\begin{rem}
	The condition (*) means that none of the random variables $X_{i}$ can be significant in the sum.
	\begin{equation*}
		\frac{1}{(\sigma(n))^{3}}\sum_{i=1}^{n}\abs{X_{i}}^{3}\lesssim\frac{1}{\sigma(n)}\max_{i=1,2,\cdots,n}\abs{X_{i}}\left(\frac{1}{(\sigma(n))^{2}}\right)\sum_{i=1}^{n}(X_{i})^{2}\approx\frac{1}{\sigma(n)}\max_{i=1,2,\cdots,n}\abs{X_{i}}\to 0
	\end{equation*}
\end{rem}
This theorem is a special case of Central Limit Theorem. It is more about the sum of Bernoulli random variables converges to a normal distribution.
\begin{thm}(\textbf{De Moivre-Laplace Limit Theorem}) Suppose that $X\sim\Bin(n,p)$. Then for any $a<b$, as $n\to\infty$,
	\begin{equation*}
		\prob\left(a<\frac{X-np}{\sqrt{np(1-p)}}\leq b\right)\to\Phi(b)-\Phi(a)
	\end{equation*}	
\end{thm}
\begin{proofing}
	Before we start the proof, we need to know about the Stirling's formula:
	\begin{equation*}
		n!\sim\sqrt{2\pi n}\left(\frac{n}{e}\right)^{n}
	\end{equation*}
	Our target is to transform the PMF of Binomial random variable into the PDF of standard normal distribution. For $0\leq k\leq n$,
	\begin{align*}
		\binom{n}{k}p^{k}(1-p)^{n-k}&=\frac{n!}{k!(n-k)!}p^{k}(1-p)^{n-k}\\
		\tag{Using Stirling's formula}
		&\sim\sqrt{\frac{n}{2\pi k(n-k)}}\frac{n^{n}}{k^{k}(n-k)^{n-k}}p^{k}(1-p)^{n-k}\\
		&=\sqrt{\frac{n}{2\pi k(n-k)}}\left(\frac{np}{k}\right)^{k}\left(\frac{n(1-p)}{n-k}\right)^{n-k}\\
		\tag{$\frac{k}{n}\to p$}
		&\sim\sqrt{\frac{1}{2\pi np(1-p)}}\exp\left(-k\ln\left(\frac{k}{np}\right)+(k-n)\ln\left(\frac{n-k}{n(1-p)}\right)\right)
	\end{align*}
	We know that $\expect{X}=np$ and $\Var(X)=np(1-p)$.\\
	For any integer $k$ we choose between $0$ and $n$, there exists an arbitrary finite point $c$ such that $k=np+c\sqrt{np(1-p)}$.\\
	To simplify, let $q=1-p$. Using the Taylor series of $\ln(1+x)=x-\frac{x^{2}}{2}+\frac{x^{3}}{3}+o(x^{3})$, we get:
	\begin{align*}
		\binom{n}{k}p^{k}q^{n-k}&\sim\sqrt{\frac{1}{2\pi npq}}\exp\left((-np-c\sqrt{npq})\ln\left(\frac{np+c\sqrt{npq}}{np}\right)+(np+c\sqrt{npq}-n)\ln\left(\frac{n-np-c\sqrt{npq}}{nq}\right)\right)\\
		&=\frac{1}{\sqrt{2\pi npq}}\exp\left((-np-c\sqrt{npq})\ln\left(1+c\sqrt{\frac{q}{np}}\right)+(c\sqrt{npq}-nq)\ln\left(1-c\sqrt{\frac{p}{nq}}\right)\right)\\
		&=\frac{1}{\sqrt{2\pi npq}}\exp\left((-np-c\sqrt{npq})\left(c\sqrt{\frac{q}{np}}-\frac{c^{2}q}{2np}+o(n^{-1})\right)+(c\sqrt{npq}-nq)\left(-c\sqrt{\frac{p}{nq}}-\frac{c^{2}p}{2nq}+o(n^{-1})\right)\right)\\
		&=\frac{1}{\sqrt{2\pi npq}}\exp\left((-c\sqrt{npq}-c^{2}q+\frac{1}{2}c^{2}q+o(1))+(-c^{2}p+c\sqrt{npq}-\frac{1}{2}c^{2}p+o(1))\right)\\
		&\sim\frac{1}{\sqrt{2\pi npq}}\exp\left(\frac{1}{2}c^{2}\right)\\
		&=\frac{1}{\sqrt{2\pi npq}}e^{-\frac{(k-np)^{2}}{2npq}}
	\end{align*}
	Therefore, as $n\to\infty$, $\frac{X-np}{\sqrt{np(1-p)}}\xrightarrow{D}N(0,1)$ and the theorem is proven.
\end{proofing}
\begin{rem}
	For better consideration, we may let $X_{1},X_{2},\cdots,X_{n}$ be random sample of a population $X\sim\Bern(p)$. We can modify the above theorem into:
	\begin{equation*}
		\prob\left(a<\frac{\overline{X}-p}{\sqrt{\frac{p(1-p)}{n}}}\leq b\right)\to\Phi(b)-\Phi(a)
	\end{equation*}
\end{rem}
\section{Sampling}
In a lot of cases, we do not know the actual distribution of the population. We can only predict the distribution based on the samples we can get. This section is more close to statistics than probability, so we will not talk a lot about this.
\begin{defn}
	A set of random variables $\{X_{1},X_{2},\cdots,X_{n}\}$ are called \textbf{random sample} of a random variable $X$ with PMF or PDF $f_{X}(x)$ and CDF $F_{X}(x)$ if they are independent and identically distributed (i.i.d.).
	\begin{enumerate}
		\item \textbf{Sample mean} of $X$, denoted by $\overline{X}$, is defined by:
		\begin{equation*}
			\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}
		\end{equation*}
		\item \textbf{Sample variance} of $X$, denoted by $S_{n-1}^{2}$, is defined by:
		\begin{equation*}
			S_{n-1}^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}
		\end{equation*}
	\end{enumerate}
\end{defn}
\begin{rem}
	Notice that the denominator is $n-1$.
\end{rem}
\begin{thm}
	\label{Chapter 7 (Theorem) Expectation and variance of sample mean}
	Given a sample mean $\overline{X}$ of a random variable $X$. We have $\expect{\overline{X}}=\mu$ and $\Var(\overline{X})=\frac{\sigma^{2}}{n}$.
\end{thm}
\begin{proofing}
	\begin{align*}
		\expect{\overline{X}}&=\expect\left(\frac{1}{n}\sum_{k=1}^{n}X_{k}\right)=\frac{1}{n}\sum_{k=1}^{n}\expect{X_{k}}=\frac{1}{n}\sum_{k=1}^{n}\mu=\mu\\
		\Var(\overline{X})&=\frac{1}{n^{2}}\sum_{k=1}^{n}\Var(X_{k})=\frac{n\sigma^{2}}{n^{2}}=\frac{1}{n}\sigma^{2}
	\end{align*}
\end{proofing}
\begin{thm}
	Given a sample variance $S_{n-1}^{2}$ of a random variable $X$. We have $\expect{S_{n-1}^{2}}=\sigma^{2}$.
\end{thm}
\begin{proofing}
	\begin{align*}
		\expect{S_{n-1}^{2}}&=\frac{1}{n-1}\sum_{i=1}^{n}\expect(X_{i}-\overline{X})^{2}\\
		&=\frac{1}{n-1}\sum_{i=1}^{n}(\expect(X_{i}-\mu)^{2}+\expect(\overline{X}-\mu)^{2}-2\expect((X_{i}-\mu)(\overline{X}-\mu)))\\
		&=\frac{1}{n-1}\sum_{i=1}^{n}(\Var(X_{i})+\Var(\overline{X})-2\cov(X_{i},\overline{X}))\\
		&=\frac{n\sigma^{2}}{n-1}+\frac{\sigma^{2}}{n-1}-\frac{2}{n-1}\sum_{i=1}^{n}\cov\left(X_{i},\frac{1}{n}\sum_{j=1}^{n}X_{j}\right)\\
		&=\frac{n\sigma^{2}}{n-1}+\frac{\sigma^{2}}{n-1}-\frac{2}{n(n-1)}\sum_{i=1}^{n}\sum_{j=1}^{n}\cov(X_{i},X_{j})\\
		&=\frac{n\sigma^{2}}{n-1}+\frac{\sigma^{2}}{n-1}-\frac{2\sigma^{2}}{n-1}=\sigma^{2}
	\end{align*}
\end{proofing}
By using CLT, we can use this theorem to find an estimation of $\mu_{X}$ if we know that value of $\sigma_{X}^{2}$.
\begin{thm}
	\label{Chapter 7 (Theorem) Sampling distribution with sample mean}
	Let $X_{1},X_{2},\cdots,X_{n}$ be random sample from the population $X\sim\N(\mu_{X},\sigma_{X}^{2})$. We have:
	\begin{equation*}
		\overline{X}\sim\N\left(\mu_{X},\frac{\sigma_{X}^{2}}{n}\right)
	\end{equation*}
\end{thm}
\begin{proofing}
	By Theorem \ref{Chapter 5 (Theorem) Additivity of Normal Distribution} and the properties of Normal distribution,
	\begin{equation*}
		X_{1}+X_{2}+\cdots+X_{n}\sim\N(n\mu_{X},n\sigma_{X}^{2})
	\end{equation*}
	By Lemma \ref{Chapter 5 (Lemma) Properties of Normal distribution}, we have:
	\begin{equation*}
		\overline{X}=\frac{1}{n}(X_{1}+X_{2}+\cdots+X_{n})\sim\N\left(\mu_{X},\frac{\sigma_{X}^{2}}{n}\right)
	\end{equation*}
\end{proofing}
What if instead we want to find an estimation of $\sigma_{X}^{2}$ using $\mu_{X}$?
\begin{thm}
	\label{Chapter 7 (Theorem) Sampling distribution with known mean}
	Let $X_{1},X_{2},\cdots,X_{n}$ be random sample from the population $X\sim\N(\mu_{x},\sigma_{X}^{2})$. Then we have:
	\begin{equation*}
		\sum_{i=1}^{n}\left(\frac{X_{i}-\mu_{X}}{\sigma_{X}}\right)^{2}\sim\chi^{2}(n)
	\end{equation*}
\end{thm}
\begin{proofing}
	Using the properties of normal distribution, for any $i=1,\cdots,n$, we have:
	\begin{equation*}
		\frac{X_{i}-\mu_{X}}{\sigma_{X}}\sim\N(0,1)
	\end{equation*}
	Therefore, by definition of t-distribution,
	\begin{equation*}
		\sum_{i=1}^{n}\left(\frac{X_{i}-\mu_{X}}{\sigma_{X}}\right)^{2}\sim\chi^{2}(n)
	\end{equation*}
\end{proofing}
Most often, we won't even know the other parameter. How do we find $\sigma_{X}$ if $\mu_{X}$ is unknown? We can use the following theorem.
\begin{thm}
	\label{Chapter 7 (Theorem) Sampling distribution with sample variance}
	Let $X_{1},X_{2},\cdots,X_{n}$ be random sample from the population $X\sim\N(\mu_{X},\sigma_{X}^{2})$. Then we have:
	\begin{enumerate}
		\item $\overline{X}$ and $S_{n-1}^{2}$ are independent.
		\item \begin{equation*}
			\frac{(n-1)S_{n-1}^{2}}{\sigma_{X}^{2}}\sim\chi^{2}(n-1)
		\end{equation*}
	\end{enumerate}
\end{thm}
\begin{proofing}
	\begin{enumerate}
		\item It suffices to prove that $\overline{X}$ and $X_{i}-\overline{X}$ are independent for any $i=1,2,\cdots,n$.\\
		We know that $\overline{X}\sim\N(\mu_{X},\frac{\sigma_{X}^{2}}{n})$ and $X_{i}-\overline{X}\sim\N(0,\frac{(n+1)\sigma_{X}^{2}}{n})$.
		\begin{equation*}
			\cov(\overline{X},X_{i}-\overline{X})=
		\end{equation*}
		\item We may find that:
		\begin{align*}
			\frac{(n-1)S_{n-1}^{2}}{\sigma_{X}^{2}}=\sum_{i=1}^{n}\frac{(X_{i}-\overline{X})^{2}}{\sigma_{X}^{2}}=\sum_{i=1}^{n}\frac{((X_{i}-\mu_{X})+(\mu_{X}-\overline{X}))^{2}}{\sigma_{X}^{2}}&=\sum_{i=1}^{n}\frac{(X_{i}-\mu_{X})^{2}}{\sigma_{X}^{2}}-\frac{n(\overline{X}-\mu_{X})^{2}}{\sigma_{X}^{2}}\\
			&=\sum_{i=1}^{n}\left(\frac{X_{i}-\mu_{X}}{\sigma_{X}}\right)^{2}-\left(\frac{\overline{X}-\mu_{X}}{\frac{\sigma_{X}}{\sqrt{n}}}\right)^{2}
		\end{align*}
		We know that $\frac{X_{i}-\mu_{X}}{\sigma_{X}}\sim\N(0,1)$. Let $U=\frac{(n-1)S_{n-1}^{2}}{\sigma_{X}^{2}}$ and $V=\left(\frac{\overline{X}-\mu_{X}}{\sigma_{X}/\sqrt{n}}\right)$. Use Theorem \ref{Chapter 7 (Theorem) Sampling distribution with known mean} and by definition, we have:
		\begin{align*}
			U+V&\sim\chi^{2}(n) & V&\sim\chi^{2}(1)
		\end{align*} 
		We can use MGF to prove the theorem. Note that for any $i=1,2,\cdots,n$, $\overline{X}$ and $X_{i}-\overline{X}$ are independent.
		\begin{equation*}
			M_{U}(t)=\frac{M_{U+V}(t)}{M_{V}(t)}=(1-2t)^{-\frac{n-1}{2}}\implies U=\frac{(n-1)S_{n-1}^{2}}{\sigma_{X}^{2}}\sim\chi^{2}(n-1)
		\end{equation*}
	\end{enumerate}
\end{proofing}
\begin{thm}
	Let $X_{1},X_{2},\cdots,X_{n}$ be random sample from the population $X\sim\N(\mu_{X},\sigma_{X}^{2})$. We have:
	\begin{equation*}
		\frac{\overline{X}-\mu_{X}}{\frac{S_{n-1}}{\sqrt{n}}}\sim t(n-1)
	\end{equation*}
\end{thm}
\begin{proofing}
	By Theorem \ref{Chapter 7 (Theorem) Sampling distribution with sample mean} and \ref{Chapter 7 (Theorem) Sampling distribution with sample variance}, we let:
	\begin{align*}
		U&=\frac{\overline{X}-\mu_{X}}{\frac{\sigma_{X}}{\sqrt{n}}}\sim\N(0,1) & V&=\frac{(n-1)S_{n-1}^{2}}{\sigma_{X}^{2}}\sim\chi^{2}(n-1)
	\end{align*}
	By definition of $t$-distribution, we have:
	\begin{equation*}
		\frac{\overline{X}-\mu_{X}}{\frac{S_{n-1}}{\sqrt{n}}}=\frac{U}{\sqrt{\frac{V}{n-1}}}\sim t(n-1)
	\end{equation*}
\end{proofing}

\chapter{Convergence of random variables}
We have mentioned the convergence in distribution in Chapter $5$. However, this is not the only important type of convergence mode of random variables. In this chapter, we will introduce some other convergence modes.
\section{Modes of convergence}
Many modes of convergence of a sequence of random variables will be discussed.\\
Let us recall the convergence mode of real function. Let $f,f_{1},f_{2},\cdots:[0,1]\to\mathbb{R}$.
\begin{enumerate}
	\item Pointwise convergence\\
	We say $f_{n}\to f$ pointwise if for all $x\in[0,1]$,
	\begin{equation*}
		f_{n}(x)\to f(x)\text{ as }n\to\infty
	\end{equation*}
	\item Convergence in norm $\norm{\cdot}$\\
	We say $f_{n}\to f$ in norm $\norm{\cdot}$ if
	\begin{equation*}
		\norm{f_{n}-f}\to 0\text{ as }n\to\infty
	\end{equation*}
	\item Convergence in Lebesgue (uniform) measure\\
	We say $f_{n}\to f$ in uniform measure $\mu$ if for all $\varepsilon>0$,
	\begin{equation*}
		\mu\left(\{x\in[0,1]:\abs{f_{n}(x)-f(x)}>\varepsilon\}\right)\to 0\text{ as }n\to\infty
	\end{equation*}
\end{enumerate}
We can use these definitions to define convergence modes of random variables.
\begin{defn}(\textbf{Almost sure convergence})
	We say $X_{n}\to X$ \textbf{almost surely}, written as $X_{n}\xrightarrow{\text{a.s.}}X$, if
	\begin{align*}
		\prob(\{\omega\in\Omega:X_{n}(\omega)\to X(\omega)\text{ as }n\to\infty\})&=1 & &\text{or} & \prob(\{\omega\in\Omega:X_{n}(\omega)\not\to X(\omega)\text{ as }n\to\infty\})&=0
	\end{align*}
\end{defn}
\begin{rem}
	$X_{n}\xrightarrow{\text{a.s.}}X$ almost surely is an adaptation to the pointwise convergence for function.
\end{rem}
\begin{rem}
	Very often, we also call almost surely convergence:
	\begin{enumerate}
		\item $X_{n}\to X$ almost everywhere ($X_{n}\xrightarrow{\text{a.e.}}X$)
		\item $X_{n}\to X$ with probability $1$ ($X_{n}\to X$ w.p. $1$)
	\end{enumerate}
\end{rem}
\begin{defn}(Convergence in $r$-th mean)
	Let $r\geq 1$. We say $X_{n}\to X$ \textbf{in $r$-th mean}, written as $X_{n}\xrightarrow{r}X$, if
	\begin{equation*}
		\expect\abs{X_{n}-X}^{r}\to 0\text{ as }n\to\infty
	\end{equation*}
\end{defn}
\begin{eg}
	If $r=1$, we say $X_{n}\to X$ in mean or expectation. If $r=2$, we say $X_{n}\to X$ in mean square.
\end{eg}
\begin{defn}(Convergence in probability)
	We say $X_{n}\to X$ \textbf{in probability}, written as $X_{n}\xrightarrow{\prob}X$, if for all $\varepsilon>0$,
	\begin{equation*}
		\prob(\abs{X_{n}-X}>\varepsilon)\to 0\text{ as }n\to\infty
	\end{equation*}
\end{defn}
\begin{defn}(Convergence in distribution)
	We say that $X_{n}\to X$ \textbf{in distribution}, written as $X_{n}\xrightarrow{D}X$, if at continuity point of $\prob(X\leq x)$,
	\begin{equation*}
		F_{n}(x)=\prob(X_{n}\leq x)\to\prob(X\leq x)=F(x)\text{ as }n\to\infty
	\end{equation*}
\end{defn}
Before we tackle the relationships between different convergence mode, we first need to introduce some formulas.
\begin{lem}(Markov's inequality)
	If $X$ is any random variables with finite mean, then for all $a>0$,
	\begin{equation*}
		\prob(\abs{X}\geq a)\leq\frac{\expect\abs{X}}{a}
	\end{equation*}
\end{lem}
\begin{proofing}
	\begin{equation*}
		\prob(\abs{X}\geq a)=\expect(\mathbf{1}_{\abs{X}\geq a})\leq\expect\left(\frac{\abs{X}}{a}\mathbf{1}_{\abs{X}>a}\right)\leq\frac{\expect\abs{X}}{a}
	\end{equation*}
\end{proofing}
\begin{rem}
	For any non-negative function $\varphi$ that is increasing on $[0,\infty)$,
	\begin{equation*}
		\prob(\abs{X}\geq a)=\prob(\varphi(\abs{X})\geq\varphi(a))\leq\frac{\expect(\varphi(\abs{X}))}{\varphi(a)}
	\end{equation*}
\end{rem}
Following inequality needs H\"older inequality (In Appendix $C$) in order to be proven. Therefore, we will not prove it here.
\begin{lem}(Lyapunov's inequality)
	Let $Z$ be any random variables. For all $r\geq s>0$,
	\begin{equation*}
		(\expect\abs{Z}^{s})^{\frac{1}{s}}\leq(\expect\abs{Z}^{r})^{\frac{1}{r}}
	\end{equation*}
\end{lem}
We also need to know how we can obtain almost sure convergence.
\begin{lem}
	\label{Chapter 8 (Lemma) Obtaining almost sure convergence}
	Let
	\begin{align*}
		A_{n}(\varepsilon)&=\{\omega\in\Omega:\abs{X_{n}(\omega)-X(\omega)}>\varepsilon\} & B_{m}(\varepsilon)&=\bigcup_{n=m}^{\infty}A_{n}(\varepsilon)
	\end{align*}
	We have
	\begin{enumerate}
		\item $X_{n}\xrightarrow{\text{a.s.}}X$ if and only if $\lim_{m\to\infty}\prob(B_{m}(\varepsilon))=0$ for all $\varepsilon>0$
		\item $X_{n}\xrightarrow{\text{a.s.}}X$ if $\sum_{n=1}^{\infty}\prob(A_{n}(\varepsilon))<\infty$ for all $\varepsilon>0$
	\end{enumerate}
\end{lem}
\begin{proofing}
	\begin{enumerate}
		\item We denote $C=\{\omega\in\Omega:X_{n}(\omega)\to X(\omega)\text{ as }n\to\infty\}$.\\
		If $\omega\in C$, that means for all $\varepsilon>0$, there exists $n_{0}>0$ such that $\abs{X_{n}(\omega)-X(\omega)}\leq\varepsilon$ for all $n\geq n_{0}$.\\
		This also means that for all $\varepsilon>0$, $\abs{X_{n}(\omega)-X(\omega)}>\varepsilon$ for finitely many $n$.\\
		If $\omega\in C^{\complement}$, that means that for all $\varepsilon>0$, $\abs{X_{n}(\omega)-X(\omega)}>\varepsilon$ for infinitely many $n$. ($\omega\in\bigcap_{m=1}^{\infty}\bigcup_{n=m}^{\infty}A_{n}(\varepsilon)$)\\
		Therefore,
		\begin{equation*}
			C^{\complement}=\bigcup_{\varepsilon>0}\bigcap_{m=1}^{\infty}\bigcup_{n=m}^{\infty}A_{n}(\varepsilon)
		\end{equation*}
		If $\prob(C^{\complement})=0$, then for all $\varepsilon>0$,
		\begin{equation*}
			\prob\left(\bigcap_{m=1}^{\infty}\bigcup_{n=m}^{\infty}A_{n}(\varepsilon)\right)=0
		\end{equation*}
		We can also find that
		\begin{align*}
			\prob\left(\bigcap_{m=1}^{\infty}\bigcup_{n=m}^{\infty}A_{n}(\varepsilon)\right)&=0 & &\implies & \prob(C^{\complement})&=\prob\left(\bigcup_{\varepsilon>0}\bigcap_{m=1}^{\infty}\bigcup_{n=m}^{\infty}A_{n}(\varepsilon)\right)=\prob\left(\bigcup_{k=1}^{\infty}\bigcap_{m=1}^{\infty}\bigcup_{n=m}^{\infty}A_{n}\left(\frac{1}{k}\right)\right)=0
		\end{align*}
		Therefore, $X_{n}\xrightarrow{\text{a.s.}}X$ if and only if $\lim_{m\to\infty}\prob(B_{m}(\varepsilon))=0$ for all $\varepsilon>0$
		\item From (1), for all $\varepsilon>0$,
		\begin{equation*}
			\sum_{n=1}^{\infty}\prob(A_{n}(\varepsilon))<\infty\implies\lim_{m\to\infty}\sum_{n=m}^{\infty}\prob(A_{n}(\varepsilon))=0\implies\lim_{m\to\infty}\prob(B_{m}(\varepsilon))=0\implies(X_{n}\xrightarrow{\text{a.s.}}X)
		\end{equation*}
	\end{enumerate}
\end{proofing}
\begin{lem}
	\label{Chapter 8 (Lemma) Non-relationship between almost surely convergence and mean}
	There exist sequences that
	\begin{enumerate}
		\item converge almost surely but not in mean
		\item converge in mean but not almost surely
	\end{enumerate}
\end{lem}
\begin{proofing}
	\begin{enumerate}
		\item We consider
		\begin{equation*}
			X_{n}=\begin{cases}
				n^{3}, &\text{Probability }=n^{-2}\\
				0, &\text{Probability }=1-n^{-2}
			\end{cases}
		\end{equation*}
		By applying Lemma \ref{Chapter 8 (Lemma) Obtaining almost sure convergence}, for some $\varepsilon>0$.
		\begin{align*}
			\prob(\abs{X_{n}(\omega)-X(\omega)}>\varepsilon)&=\frac{1}{n^{2}} & \sum_{n=1}^{\infty}\prob(\abs{X_{n}(\omega)-X(\omega)}>\varepsilon)&<\infty
		\end{align*}
		Therefore, the sequence converges almost surely. However,
		\begin{equation*}
			\expect\abs{X_{n}-X}=n^{3}\left(\frac{1}{n^{2}}\right)=n\to\infty
		\end{equation*}
		Therefore, the sequence does not converge in mean.
		\item We consider
		\begin{equation*}
			X_{n}=\begin{cases}
				1, &\text{Probability }=n^{-1}\\
				0, &\text{Probability }=1-n^{-1}
			\end{cases}
		\end{equation*}
		In mean, as $n\to\infty$ we have
		\begin{equation*}
			\expect\abs{X_{n}-X}=1\left(\frac{1}{n}\right)=\frac{1}{n}\to 0
		\end{equation*}
		However, by applying Lemma \ref{Chapter 8 (Lemma) Obtaining almost sure convergence}, if $\varepsilon\in(0,1)$, for all $n$
		\begin{align*}
			\prob(B_{m}(\varepsilon))&=1-\lim_{r\to\infty}\prob(X_{n}=0\text{ for all }n\text{ such that }m\leq n\leq r)\\
			&=1-\lim_{r\to\infty}\prod_{i=m}^{r}\frac{i-1}{i}\\
			&=1-\lim_{r\to\infty}\frac{m-1}{r}\to 1\neq 0
		\end{align*}
		Therefore, the sequence does not converge almost surely.
	\end{enumerate}
\end{proofing}
We can now deduce the following implications. Roughly speaking, convergence in distribution is the weakest among all convergence modes, since it only cares about the distribution of $X_{n}$.
\begin{thm}
	\label{Chapter 8 (Theorem) implications of different convergence modes}
	The following implications hold:
	\begin{enumerate}
		\item \begin{enumerate}
			\item $(X_{n}\xrightarrow{\text{a.s.}}X)\implies(X_{n}\xrightarrow{\prob}X)$
			\item $(X_{n}\xrightarrow{r}X)\implies(X_{n}\xrightarrow{\prob}X)$
			\item $(X_{n}\xrightarrow{\prob}X)\implies(X_{n}\xrightarrow{D}X)$
		\end{enumerate}
		\item If $r\geq s\geq 1$, then $(X_{n}\xrightarrow{r}X)\implies(X_{n}\xrightarrow{s}X)$
		\item No other implications holds in general.
	\end{enumerate}
\end{thm}
\newpage
\begin{proofing}
	\begin{enumerate}
		\item \begin{enumerate}
			\item From Lemma \ref{Chapter 8 (Lemma) Obtaining almost sure convergence}, for all $\varepsilon>0$,
			\begin{equation*}
				\prob(A_{m}(\varepsilon))\leq\prob\left(\bigcup_{n=m}^{\infty}A_{n}(\varepsilon)\right)=\prob(B_{m}(\varepsilon))\to 0
			\end{equation*}
			Therefore, $(X_{n}\xrightarrow{\text{a.s.}}X)\implies(X_{n}\xrightarrow{\prob}X)$
			\item From Markov's inequality, since $r\geq 1$,
			\begin{equation*}
				0\leq\prob(\abs{X-X_{n}}>\varepsilon)=\prob(\abs{X-X_{n}}^{r}>\varepsilon^{r})\leq\frac{\expect\abs{X_{n}-X}^{r}}{\varepsilon^{r}}
			\end{equation*}
			Therefore, if $X_{n}\xrightarrow{r}X$, then $\expect\abs{X_{n}-X}^{r}\to 0$. We have $\prob(\abs{X-X_{n}}>\varepsilon)\to 0$ and thus $X_{n}\xrightarrow{\prob}X$.
			\item 
			\begin{align*}
				\prob(X_{n}\leq x)&=\prob(X_{n}\leq x,X\leq x+\varepsilon)+\prob(X_{n}\leq x,X>x+\varepsilon)\leq\prob(X\leq x+\varepsilon)+\prob(\abs{X_{n}-X}>\varepsilon)\\
				\prob(X\leq y)&\leq\prob(X_{n}\leq y+\varepsilon)+\prob(\abs{X_{n}-X}>\varepsilon)\\
				\tag{$y=x-\varepsilon$}
				\prob(X_{n}\leq x)
				&\geq\prob(X\leq x-\varepsilon)-\prob(\abs{X_{n}-X}>\varepsilon)
			\end{align*}
			Since $X_{n}\xrightarrow{\prob}X$, $\prob(\abs{X_{n}-X}>\varepsilon)\to 0$ for all $\varepsilon>0$. Therefore,
			\begin{equation*}
				\prob(X\leq x-\varepsilon)\leq\liminf_{n\to\infty}\prob(X_{n}\leq x)\leq\limsup_{n\to\infty}\prob(X_{n}\leq x)\leq\prob(X\leq x+\varepsilon)
			\end{equation*}
			By having $\varepsilon\to 0$,
			\begin{equation*}
				\prob(X\leq x)\leq\liminf_{n\to\infty}\prob(X_{n}\leq x)\leq\limsup_{n\to\infty}\prob(X_{n}\leq x)\leq\prob(X\leq x)
			\end{equation*}
			Therefore, $\lim_{n\to\infty}\prob(X_{n}\leq x)=\prob(X\leq x)$ and thus $X_{n}\xrightarrow{D}X$.
		\end{enumerate}
		\item Since $X_{n}\xrightarrow{r}X$, $\expect\abs{X_{n}-X}\to 0$ as $n\to\infty$. By Lyapunov's inequality, if $r\geq s$,
		\begin{equation*}
			\expect\abs{X_{n}-X}^{s}\leq(\expect\abs{X_{n}-X}^{r})^{\frac{s}{r}}\to 0
		\end{equation*}
		\item Let $\Omega=\{H,T\}$ and $\prob(H)=\prob(T)=\frac{1}{2}$. Let
		\begin{align*}
			X_{2m}(\omega)&=\begin{cases}
				1, &\omega=H\\
				0, &\omega=T
			\end{cases} & X_{2m+1}(\omega)=\begin{cases}
				0, &\omega=H\\
				1, &\omega=T
			\end{cases}
		\end{align*}
		Since $F(x)$ and $F_{n}(x)$ for all $n$ are all the same, $X_{n}\xrightarrow{D}X$. However, for $\varepsilon\in[0,1]$, $\prob(\abs{X_{n}-X}>\varepsilon)\not\to 0$.\\
		Therefore, $(X_{n}\xrightarrow{D}X)\centernot\implies(X_{n}\xrightarrow{\prob}X)$.\\
		Let $r=1$ and
		\begin{align*}
			X_{n}&=\begin{cases}
				n, &\text{probability }=\frac{1}{n}\\
				0, &\text{probability }=1-\frac{1}{n}
			\end{cases} & X&=0
		\end{align*}
		We get that $\prob(\abs{X_{n}-X}>\varepsilon)=\frac{1}{n}\to 0$. However, $\expect\abs{X_{n}-X}=n\left(\frac{1}{n}\right)=1\not\to 0$. Therefore, $(X_{n}\xrightarrow{\prob}X)\centernot\implies(X_{n}\xrightarrow{r}X)$.\\
		Let $\Omega=[0,1]$, $\mathcal{F}=\mathcal{B}([0,1])$ and $\prob$ be uniform.\\
		Let $I_{i}$ be such that $I_{\frac{1}{2}m(m-1)+1}, I_{\frac{1}{2}m(m-1)+2}, \cdots, I_{\frac{1}{2}m(m-1)+m}$ is a partition of $[0,1]$ for all $m$.\\
		We have $I_{1}=[0,1], I_{2}\cup I_{3}=[0,1],\cdots$. Let
		\begin{align*}
			X_{n}(\omega)&=\mathbf{1}_{I_{n}(\omega)}=\begin{cases}
				1, &\omega\in I_{n}\\
				0, &\omega\in I_{n}^{\complement}
			\end{cases} & X(\omega)&=0\text{ for all }\omega\in\Omega
		\end{align*}
		For all $\varepsilon\in[0,1]$, $\prob(\abs{X_{n}-X}>\varepsilon)=\prob(I_{n})=\frac{1}{n}\to 0$ for some $n$ if $n\to\infty$.\\
		However, for any given $\omega\in\Omega$, although $1$ becomes less often due to decreasing probability, it never dies out.\\
		Therefore, $X_{n}(\omega)\not\to 0=X(\omega)$ and $\prob(\{\omega\in\Omega:X_{n}(\omega)\to X(\omega)\text{ as }n\to\infty\})=0$, and thus, $(X_{n}\xrightarrow{\prob}X)\centernot\implies(X_{n}\xrightarrow{\text{a.s.}}X)$.\\
		If $r\geq s\geq 1$, let
		\begin{align*}
			X_{n}&=\begin{cases}
				n, &\text{probability }=n^{-\left(\frac{r+s}{2}\right)}\\
				0, &\text{probability }=1-n^{-\left(\frac{r+s}{2}\right)}
			\end{cases} & X&=0
		\end{align*}
		
		\begin{align*}
			\expect\abs{X_{n}-X}^{s}&=n^{s}\left(n^{-\left(\frac{r+s}{2}\right)}\right)=n^{\frac{s-r}{2}}\to 0 & \expect\abs{X_{n}-X}^{r}&=n^{r}\left(n^{-\left(\frac{r+s}{2}\right)}\right)=n^{\frac{r-s}{2}}\to\infty
		\end{align*}
		Therefore, if $r\geq s\geq 1$, $(X_{n}\xrightarrow{s}X)\centernot\implies(X_{n}\xrightarrow{r}X)$.\\
		We have proven that $(X_{n}\xrightarrow{\text{a.s.}}X)\centernot\implies(X_{n}\xrightarrow{r}X)$ and $(X_{n}\xrightarrow{r}X)\centernot\implies(X_{n}\xrightarrow{\text{a.s.}}X)$ in Lemma \ref{Chapter 8 (Lemma) Non-relationship between almost surely convergence and mean}.
	\end{enumerate}
\end{proofing}
We can easily obtain this lemma.
\begin{lem}
	The following implications hold:
	\begin{enumerate}
		\item $(X_{n}\xrightarrow{1}X)\implies(X_{n}\xrightarrow{\prob}X)$
	\end{enumerate}
\end{lem}
\begin{proofing}
	Just use the Theorem \ref{Chapter 8 (Theorem) implications of different convergence modes} with $r=1$ and you get the answer.
\end{proofing}
Some of the implications does not hold in general but they hold if we apply some restrictions.
\begin{thm}(Partial converse statements)
	\label{Chapter 8 (Theorem) Partial converse statements}
	The following implications hold:
	\begin{enumerate}
		\item If $X_{n}\xrightarrow{D}c$, where $c$ is a constant, then $X_{n}\xrightarrow{\prob}c$.
		\item If $X_{n}\xrightarrow{\prob}X$ and $\prob(\abs{X_{n}}\leq k)=1$ for all $n$ with some fixed constant $k>0$, then $X_{n}\xrightarrow{r}X$ for all $r\geq 1$.
	\end{enumerate}
\end{thm}
\begin{proofing}
	\begin{enumerate}
		\item Since $X_{n}\xrightarrow{D}X$, $\prob(X_{n}\leq x)\to\prob(c\leq x)$ as $n\to\infty$. For all $\varepsilon>0$,
		\begin{equation*}
			\prob(\abs{X_{n}-c}\geq\varepsilon)=\prob(X_{n}\leq c-\varepsilon)+\prob(X_{n}\geq c+\varepsilon)=\prob(X_{n}\leq c-\varepsilon)+1-\prob(X_{n}<c+\varepsilon)
		\end{equation*}
		We can get that $\prob(X_{n}\leq c-\varepsilon)\to\prob(c\leq c-\varepsilon)=0$. For $\prob(X_{n}<c+\varepsilon)$,
		\begin{equation*}
			\prob\left(X_{n}\leq c+\frac{\varepsilon}{2}\right)\leq\prob(X_{n}<c+\varepsilon)\leq\prob(X_{n}\leq c+2\varepsilon)
		\end{equation*}
		\begin{align*}
			\prob\left(X_{n}\leq c+\frac{\varepsilon}{2}\right)&\to\prob\left(c\leq c+\frac{\varepsilon}{2}\right)=1 & \prob(X_{n}\leq c+2\varepsilon)&\to\prob(c\leq c+2\varepsilon)=1
		\end{align*}
		Therefore, $\prob(X_{n}<c+\varepsilon)\to 1$. We have
		\begin{equation*}
			\prob(\abs{X_{n}-c}\geq\varepsilon)\to 0+1-1 = 0
		\end{equation*}
		Therefore, $X_{n}\xrightarrow{\prob}c$.
		\item Since $X_{n}\xrightarrow{\prob}X$, $X_{n}\xrightarrow{D}X$. We have $\prob(\abs{X_{n}}\leq k)\to\prob(\abs{X}\leq k)=1$.\\
		Therefore, for all $\varepsilon>0$, if $\abs{X_{n}-X}\leq\varepsilon$, $\abs{X_{n}-X}\leq\abs{X_{n}}+\abs{X}\leq 2k$.
		\begin{align*}
			\expect\abs{X_{n}-X}^{r}&=\expect\left(\abs{X_{n}-X}^{r}\mathbf{1}_{\abs{X_{n}-X}\leq\varepsilon}\right)+\expect\left(\abs{X_{n}-X}^{r}\mathbf{1}_{\abs{X_{n}-X}>\varepsilon}\right)\\
			&\leq\varepsilon^{r}\expect\left(\mathbf{1}_{\abs{X_{n}-X}\leq\varepsilon}\right)+(2k)^{r}\expect\left(\mathbf{1}_{\abs{X_{n}-X}>\varepsilon}\right)\\
			&\leq\varepsilon^{r}+((2k)^{r}-\varepsilon^{r})\prob(\abs{X_{n}-X}>\varepsilon)
		\end{align*}
		Since $X_{n}\xrightarrow{\prob}X$, as $n\to\infty$, $\expect\abs{X_{n}-X}^{r}\to\varepsilon^{r}$. If we send $\varepsilon\to 0$, $\expect\abs{X_{n}-X}^{r}\to 0$ and therefore $X_{n}\xrightarrow{r}X$.
	\end{enumerate}
\end{proofing}
Note that any sequence $\{X_{n}\}$ which satisfies $X_{n}\xrightarrow{\prob}X$ necessarily contains a subsequence $\{X_{n_{i}}:1\leq i<\infty\}$ which converges almost surely.
\begin{thm}
	If $X_{n}\xrightarrow{\prob}X$, then there exists a non-random increasing sequence of integers $n_{1},n_{2},\cdots$ such that as $i\to\infty$,
	\begin{equation*}
		X_{n_{i}}\xrightarrow{\text{a.s.}}X
	\end{equation*}
\end{thm}
\begin{proofing}
	Since $X_{n}\xrightarrow{\prob}X$, $\prob(\abs{X_{n}-X}>\varepsilon)\to 0$ as $n\to\infty$ for all $\varepsilon>0$.\\
	We can pick an increasing sequence $n_{1},n_{2},\cdots$ of positive integers such that
	\begin{equation*}
		\prob(\abs{X_{n_{i}}-X}>i^{-1})\leq i^{-2}
	\end{equation*}
	For any $\varepsilon>0$,
	\begin{equation*}
		\sum_{i>\varepsilon^{-1}}\prob(\abs{X_{n_{i}}-X}>\varepsilon)\leq\sum_{i>\varepsilon^{-1}}\prob(\abs{X_{n_{i}}-X}>i^{-1})\leq\sum_{i}i^{-2}<\infty
	\end{equation*}
	By Lemma \ref{Chapter 8 (Lemma) Obtaining almost sure convergence}, we get the $X_{n_{i}}\xrightarrow{\text{a.s.}}X$ as $i\to\infty$
\end{proofing}

\newpage
In all above cases, we only deal with one random variable. What can we do if it involves two or more random variables? We first try to deal with cases when one converges to a constant.
\begin{thm}(\textbf{Slutsky's Theorem})
	If $X_{n}\xrightarrow{D}X$ and $Y_{n}\xrightarrow{\prob}c$, for a constant $c$, then:
	\begin{enumerate}
		\item $X_{n}+Y_{n}\xrightarrow{D}X+c$
		\item $X_{n}Y_{n}\xrightarrow{D}Xc$
		\item $\frac{X_{n}}{Y_{n}}\xrightarrow{D}\frac{X}{c}$ if $c\neq 0$.
	\end{enumerate}
\end{thm}
\begin{proofing}
	\begin{enumerate}
		\item Suppose that $c>0$ and pick $\delta$ such that $0<\delta<c$. We can find $N$ such that $\prob(\abs{Y_{n}-c}>\delta)<\delta$ for $n\geq N$. For all $x$, we have:
		\begin{align*}
			\prob(X_{n}+Y_{n}\leq x)&\leq\prob(X_{n}+Y_{n}\leq x,\abs{Y_{n}-c}\leq\delta)+\prob(\abs{Y_{n}-c}>\delta)\leq\prob(X_{n}\leq x-c+\delta)+\delta\\
			\prob(X_{n}+Y_{n}>x)&\leq\prob(X_{n}+Y_{n}>x,\abs{Y_{n}-c}\leq\delta)+\delta\leq\prob(X_{n}>x-c-\delta)+\delta
		\end{align*}
		By sending $n\to\infty$ and $\delta\to 0$, we find that $\prob(X_{n}+Y_{n}\leq x)\to\prob(X+c\leq x)$ when $c>0$.\\
		We can use the similar argument to prove that $\prob(X_{n}+Y_{n}\leq x)\to\prob(X+c\leq x)$ when $c<0$.\\
		Suppose that $c=0$. We may define an arbitrary small $\delta>0$ and a number $N$ such that $\prob(\abs{Y_{n}}>\delta)<\delta$ for $n\geq N$. For all $x$, we have:
		\begin{align*}
			\prob(X_{n}+Y_{n}\leq x)&\leq\prob(X_{n}+Y_{n}\leq x,\abs{Y_{n}}\leq\delta)+\prob(\abs{Y_{n}}>\delta)\leq\prob(X_{n}\leq x+\delta)+\delta\\
			\prob(X_{n}+Y_{n}>x)&\leq\prob(X_{n}+Y_{n}\leq x,\abs{Y_{n}}\leq\delta)+\prob(\abs{Y_{n}}>\delta)\leq\prob(X_{n}\leq x-\delta)+\delta
		\end{align*}
		By sending $n\to\infty$ and $\delta\to 0$, we find that $\prob(X_{n}+Y_{n}\leq x)\to\prob(X+c\leq x)$ when $c=0$.\\
		Therefore, $X_{n}+Y_{n}\xrightarrow{D}X+c$.
		\item Suppose that $c>0$ and pick $\delta$ such that $0<\delta<c$. We can find $N$ such that $\prob(\abs{Y_{n}-c}>\delta)<\delta$ for $n\geq N$.\\
		For all $x$, we have:
		\begin{align*}
			\prob(X_{n}Y_{n}\leq x)&\leq\prob(X_{n}Y_{n}\leq x,\abs{Y_{n}-c}\leq\delta)+\prob(\abs{Y_{n}-c}>\delta)\leq\prob\left(X_{n}\leq \frac{x}{c-\delta}\right)+\delta\\
			\prob(X_{n}Y_{n}>x)&\leq\prob(X_{n}Y_{n}>x,\abs{Y_{n}-c}\leq\delta)+\delta\leq\prob\left(X_{n}>\frac{x}{c+\delta}\right)+\delta
		\end{align*}
		By sending $n\to\infty$ and $\delta\to 0$, we find that $\prob(X_{n}Y_{n}\leq x)\to\prob(Xc\leq x)$ when $c>0$.\\
		We can use the similar argument to prove that $\prob(X_{n}Y_{n}\leq x)\to\prob(Xc\leq x)$ when $c<0$.\\
		Suppose that $c=0$. We many choose an arbitrary small number $\delta>0$ and a number $N$ such that $\prob(\abs{Y_{n}}>\delta)<\delta$ for $n\geq N$. For all $x$, we have:
		\begin{align*}
			\prob(X_{n}Y_{n}\leq x)&\leq\prob(X_{n}Y_{n}\leq x,\abs{Y_{n}}\leq\delta)+\prob(\abs{Y_{n}}>\delta)\leq\prob(-\delta X_{n}\leq x)+\delta\\
			\prob(X_{n}Y_{n}> x)&\leq\prob(X_{n}Y_{n}>x,\abs{Y_{n}}\leq\delta)+\prob(\abs{Y_{n}}>\delta)\leq\prob(\delta X_{n}>x)+\delta
		\end{align*}
		By sending $n\to\infty$ and $\delta\to 0$, we find that $\prob(X_{n}Y_{n}\leq x)\to\prob(0\leq x)$ when $c=0$.\\
		Therefore, $X_{n}Y_{n}\xrightarrow{D}Xc$.
		\item It suffices to prove that $Y_{n}^{-1}\xrightarrow{\prob}c^{-1}$ if $Y_{n}\xrightarrow{\prob}c$, or equivalently by Theorem \ref{Chapter 8 (Theorem) Partial converse statements}, $Y_{n}\xrightarrow{D}c$.\\
		If $Y_{n}\xrightarrow{D}c$, then $\prob(Y_{n}\leq x)\to\prob(c\leq x)$ as $n\to\infty$ for all $x$. When $x\geq 0$, as $n\to\infty$,
		\begin{equation*}
			\prob\left(\frac{1}{Y_{n}}\leq x\right)=\prob(Y_{n}<0)+\prob\left(Y_{n}\geq\frac{1}{x}\right)\to\prob(c<0)+\prob\left(c\geq\frac{1}{x}\right)=\prob\left(\frac{1}{c}\leq x\right)
		\end{equation*}
		When $x<0$, as $n\to\infty$,
		\begin{equation*}
			\prob\left(\frac{1}{Y_{n}}\leq x\right)=\prob\left(\frac{1}{x}\leq Y_{n}<0\right)\to\prob\left(\frac{1}{x}\leq c<0\right)=\prob\left(\frac{1}{c}\leq x\right)
		\end{equation*}
		Therefore, $\frac{1}{Y_{n}}\xrightarrow{D}\frac{1}{c}$ and thus $\frac{1}{Y_{n}}\xrightarrow{\prob}\frac{1}{c}$.\\
		By using (2), $\frac{X_{n}}{Y_{n}}\xrightarrow{D}\frac{X}{c}$. \end{enumerate}
\end{proofing}

\newpage
\begin{thm}(\textbf{Continuous mapping theorem})
	Given a sequence of random variables $\{X_{n}\}$ and a random variable $X$. Let $f$ be a function has the set of discontinuity points $D_{f}$ such that $\prob(X\in D_{f})=0$. We have:
	\begin{enumerate}
		\item If $X_{n}\xrightarrow{D}X$, then $f(X_{n})\xrightarrow{D}f(X)$.
		\item If $X_{n}\xrightarrow{\prob}X$, then $f(X_{n})\xrightarrow{\prob}f(X)$.
		\item If $X_{n}\xrightarrow{\text{a.s.}}X$, then $f(X_{n})\xrightarrow{\text{a.s.}}f(X)$.
	\end{enumerate}
\end{thm}
\begin{proofing}
	\begin{enumerate}
		\item \textit{Current knowledge in this notes does not suffice to prove (1). Search Weak convergence of measures in Wikipedia to start.}
		\item We fix an arbitrary $\varepsilon>0$. For any $\delta>0$, we consider a set $B_{\delta}$ defined as:
		\begin{equation*}
			B_{\delta}=\{x:x\not\in D_{f}\text{ and there exists }y\text{ such that }\abs{x-y}<\delta\text{ and }\abs{f(x)-f(y)}>\varepsilon\}
		\end{equation*}
		Suppose that $\abs{f(X)-f(X_{n})}>\varepsilon$. This means either $\abs{X-X_{n}}\geq\delta$, $X\in D_{f}$ or $X\in B_{\delta}$. Therefore,
		\begin{equation*}
			\prob(\abs{f(X_{n})-f(X)}>\varepsilon)\leq\prob(\abs{X_{n}-X}\geq\delta)+\prob(X\in D_{f})+\prob(X\in B_{\delta})
		\end{equation*}
		Since $X_{n}\xrightarrow{\prob}X$, $\prob(\abs{X_{n}-X}>\delta)\to 0$ as $n\to\infty$. As $\delta\to 0$, $B_{\delta}$ reduces to an empty set. By assumption, $\prob(X\in D_{f})=0$.\\
		Therefore, as $n\to\infty$ and $\delta\to 0$,
		\begin{equation*}
			\prob(\abs{f(X_{n})-f(X)}>\varepsilon)\to 0
		\end{equation*}
		We generalize to all $\varepsilon>0$, we get $f(X_{n})\xrightarrow{\prob}f(X)$.
		\item By definition of limit,
		\begin{align*}
			\prob(\{\omega\in\Omega:f(X_{n}(\omega))\to f(X(\omega))\text{ as }n\to\infty\})&\geq\prob(\{\omega\in\Omega:f(X_{n})\to f(X)\text{ as }n\to\infty\text{ and }X\not\in D_{f}\})\\
			&\geq\prob(\{\omega\in\Omega:X_{n}(\omega)\to X(\omega)\text{ as }n\to\infty\text{ and }X\not\in D_{f}\})
		\end{align*}
		Since $\prob(X\in D_{f})=0$ by assumption, since $X_{n}\xrightarrow{\text{a.s.}}X$,
		\begin{equation*}
			\prob(\{\omega\in\Omega:X_{n}\to X\text{ as }n\to\infty\text{ and }X\not\in D_{f}\})=\prob(\{\omega\in\Omega:X_{n}(\omega)\to X(\omega)\text{ as }n\to\infty\})=1
		\end{equation*}
		Therefore, $\prob(\{\omega\in\Omega:f(X_{n}(\omega))\to f(X(\omega))\text{ as }n\to\infty\})=1$ and thus $f(X_{n})\xrightarrow{\text{a.s.}}f(X)$.
	\end{enumerate}
\end{proofing}
\section{Other versions of Weak Law of Large Numbers}
We can revisit and introduce some other versions of weak law of large numbers and their applications.
\begin{thm}($L^{2}$-WLLN) 
	Let $X_{1},X_{2},\cdots,X_{n}$ be uncorrelated random variables with $\expect X_{i}=\mu$, $\Var(X_{i})\leq c<\infty$ for all $i$. We have:
	\begin{equation*}
		\frac{1}{n}\sum_{i=1}^{n}X_{i}\xrightarrow{2}\mu
	\end{equation*}
\end{thm}
\begin{proofing}
	\begin{equation*}
		\expect\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}-\mu\right)^{2}=\frac{1}{n^{2}}\expect\left(\sum_{i=1}^{n}X_{i}-\expect \left(\sum_{i=1}^{n}X_{i}\right)\right)^{2}=\frac{1}{n^{2}}\Var\left(\sum_{i=1}^{n}X_{i}\right)=\frac{1}{n^{2}}\sum_{i=1}^{n}\Var(X_{i})\leq\frac{c}{n}\to 0
	\end{equation*}
	Therefore, $\frac{1}{n}\sum_{i=1}^{n}\xrightarrow{2}\mu$.
\end{proofing}
\begin{rem}
	From this theorem, we can immediately find that
	\begin{equation*}
		\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\xrightarrow{2}\mu\right)\implies\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\xrightarrow{\prob}\mu\right)
	\end{equation*}
\end{rem}
\begin{rem}
	Note that in the i.i.d. case, we do not require the existence of variance.
\end{rem}

\newpage
There are wide range of applications for just Weak Law of Large Numbers.
\begin{eg}(Bernstein approximation)
	Let $f$ be continuous on $[0,1]$ and let
	\begin{equation*}
		\tag{Bernstein polynomial}
		f_{n}(x)=\sum_{m=0}^{n}\binom{n}{m}x^{n}(1-x)^{n-m}f\left(\frac{m}{n}\right)
	\end{equation*}
	We want to show that as $n\to\infty$.
	\begin{equation*}
		\sup_{x\in[0,1]}\abs{f_{n}(x)-f(x)}\to 0
	\end{equation*}
\end{eg}
\begin{rem}
	Let $x\in[0,1]$. To better approach this question, we can let $X_{1,x},X_{2,x},\cdots,X_{n,x}\sim\Bern(x)$ be i.i.d. random variables. Let $Y_{n,x}=\sum_{i=1}^{n}X_{i,x}\sim\Bin(n,x)$.
	\begin{align*}
		\prob(Y_{n,x}=m)&=\binom{n}{m}x^{m}(1-x)^{n-m}\\
		f_{n}(x)&=\sum_{m=0}^{n}\prob(Y_{n,x}=m)f\left(\frac{m}{n}\right)=\expect\left(f\left(\frac{Y_{n,x}}{n}\right)\right)
	\end{align*}
	We know that by WLLN, $\frac{Y_{n,x}}{n}\xrightarrow{\prob}x$.\\
	By continuous mapping theorem, $f\left(\frac{Y_{n,x}}{n}\right)\xrightarrow{\prob}f(x)$.
\end{rem}
\begin{eg}
	By obtaining that $f\left(\frac{Y_{n,x}}{n}\right)\xrightarrow{\prob}f(x)$, since there exists a number $M$ such that $\norm{f}_{\infty}\leq M$ (due to $f$ being continuous at $[0,1]$),
	\begin{align*}
		\abs{\expect\left(f\left(\frac{Y_{n,x}}{n}\right)\right)-f(x)}\leq\expect\abs{f\left(\frac{Y_{n,x}}{n}\right)-f(x)}&=\expect\left(\abs{f\left(\frac{Y_{n,x}}{n}\right)-f(x)}\mathbf{1}_{\abs{\frac{Y_{n,x}}{n}-x}\leq\delta_{\varepsilon}}\right)+\expect\left(\abs{f\left(\frac{Y_{n,x}}{n}\right)-f(x)}\mathbf{1}_{\abs{\frac{Y_{n,x}}{n}-x}>\delta_{\varepsilon}}\right)\\
		&\leq\varepsilon+2M\prob\left(\abs{\frac{Y_{n,x}}{n}-x}>\delta_{\varepsilon}\right)
	\end{align*}
	\begin{align*}
		\sup_{x\in[0,1]}\abs{\expect\left(f\left(\frac{Y_{n,x}}{n}\right)\right)-f(x)}&=\varepsilon+2M\sup_{x\in [0,1]}\left(\prob\left(\abs{\frac{Y_{n,x}-nx}{n}}>\delta_{\varepsilon}\right)\right)\\
		\tag{Markov's inequality and Lyapunov's inequality}
		&\leq\varepsilon+2M\sup_{x\in[0,1]}\left(\frac{\expect\abs{Y_{n,x}-nx}^{2}}{n^{2}\delta_{\varepsilon}^{2}}\right)\\
		\tag{$\expect Y_{n,x}=nx$}
		&\leq\varepsilon+2M\sup_{x\in[0,1]}\left(\frac{\Var(Y_{n,x})}{n^{2}\delta_{\varepsilon}^{2}}\right)=\varepsilon+2M\sup_{x\in[0,1]}\left(\frac{x(1-x)}{n\delta_{\varepsilon}^{2}}\right)\\
		&\leq\varepsilon+\frac{M}{2n\delta_{\varepsilon}^{2}}\\
		\limsup_{n\to\infty}\sup_{x\in[0,1]}\abs{\expect\left(f\left(\frac{Y_{n,x}}{n}\right)\right)-f(x)}&\leq\varepsilon\to 0
	\end{align*}
	Therefore, we can find that $\sup_{x\in[0,1]}\abs{f_{n}(x)-f(x)}\to 0$ as $n\to\infty$.
\end{eg}

\newpage
\begin{eg}(Borel's geometric concentration)
	Let $\mu_{n}$ be the uniform probability measure on the $n$-dimensional cube $[-1,1]^{n}$. Let $\mathcal{H}$ be a hyperplane that is orthogonal to a principal diagonal of $[-1,1]$ ($\mathcal{H}=(1,\cdots,1)^{\perp}$).\\
	Let $\mathcal{H}_{r}=\{x\in[-1,1]^{n}:\dist(x:\mathcal{H})\leq r\}$.\\
	Then for any given $\varepsilon>0$, $\mu_{n}(\mathcal{H}_{\varepsilon\sqrt{n}})\to 1$ as $n\to\infty$.\\
	We can prove this by letting $X_{1},X_{2},\cdots\sim\U[-1,1]$ be i.i.d. random variables and $\expect X_{i}=0$. Let $X=(X_{1},X_{2},\cdots,X_{n})$.\\
	For all $B\in[-1,1]^{n}$, $\mu_{n}(B)=\prob(X\in B)=\prob\circ X^{-1}(B)$.
	\begin{align*}
		\mu_{n}(\mathcal{H}_{\varepsilon\sqrt{n}})&=\prob(\dist(X,\mathcal{H})\leq\varepsilon\sqrt{n})\\
		&=\prob\left(\frac{\abs{\left<X,(1,\cdots,1)\right>}}{\norm{(1,\cdots,1)}_{2}}\leq\varepsilon\sqrt{n}\right)\\
		&=\prob\left(\abs{\frac{\sum_{i=1}^{n}X_{i}}{n}}\leq\varepsilon\right)\\
		&=\prob\left(\abs{\frac{\sum_{i=1}^{n}X_{i}}{n}-\expect X_{1}}\leq\varepsilon\right)\\
		\tag{WLLN}
		&\to 1
	\end{align*}
\end{eg}
We do not necessarily need to stick to a given sequence of random variables $X_{1},X_{2},\cdots$ in Law of Large Numbers.
\begin{thm}(WLLN for triangular array)
	Let $\{X_{n,j}\}_{1\leq j\leq n<\infty}$ be a triangular array. Let $Y_{n}=\sum_{i=1}^{n}X_{n,i}$, $\mu_{n}=\expect Y_{n}$ and $\sigma_{n}^{2}=\Var(Y_{n})$. Suppose that for some sequences $b_{n}$,
	\begin{equation*}
		\frac{\sigma_{n}^{2}}{b_{n}^{2}}=\expect\left(\frac{Y_{n}-\mu_{n}}{b_{n}}\right)^{2}\to 0
	\end{equation*}
	Then we have
	\begin{equation*}
		\frac{Y_{n}-\mu_{n}}{b_{n}}\xrightarrow{\prob}0
	\end{equation*}
\end{thm}
\begin{proofing}
	\begin{equation*}
		\expect\left(\frac{Y_{n}-\mu_{n}}{b_{n}}\right)^{2}=\frac{\Var(Y_{n})}{b_{n}^{2}}\to 0
	\end{equation*}
	Therefore, $\frac{Y_{n}-\mu_{n}}{b_{n}}\xrightarrow{2}0$ and thus $\frac{Y_{n}-\mu_{n}}{b_{n}}\xrightarrow{\prob}0$.
\end{proofing}
\begin{rem}
	We should choose $b_{n}$ that no larger than $\expect Y_{n}$ if possible.
\end{rem}

\newpage
\begin{eg}(Coupon collector's problem) 
	Let $X_{1},X_{2},\cdots$ be i.i.d. uniform random variables on $\{1,2,\cdots,n\}$.\\
	Let $\tau_{k}^{n}=\inf\{m:\abs{\{X_{1},X_{2},\cdots,X_{m}\}}=k\}$ be the waiting time for picking $k$ distinct types.\\
	What is the asymptotic behavior of $\tau_{n}^{n}$?\\
	It is easy to see that $\tau_{1}^{n}=1$. By convention, $\tau_{0}^{n}=0$.\\
	For $1\leq k\leq n$, let $X_{n,k}=\tau_{k}^{n}-\tau_{k-1}^{n}$ be the additional waiting time for picking $k$ distinct types when we have $k-1$ types.\\
	Notice that
	\begin{equation*}
		\tau_{n}^{n}=\sum_{k=1}^{n}X_{n,k}
	\end{equation*}
	We know that
	\begin{align*}
		\prob(X_{n,k}&=\ell)=\left(\frac{k-1}{n}\right)^{\ell-1}\left(1-\frac{k-1}{n}\right) & &\implies & X_{n,k}&\sim\Geom\left(1-\frac{k-1}{n}\right)
	\end{align*}
	We claim that $X_{n,k}$ are independent for all $k$. For a constant $c$,
	\begin{align*}
		\expect\tau_{n}^{n}&=\sum_{k=1}^{n}\expect X_{n,k}=\sum_{k=1}^{n}\left(1-\frac{k-1}{n}\right)^{-1}=\sum_{m=1}^{n}\frac{n}{m}\sim n\log n\\
		\Var(\tau_{n}^{n})&=\sum_{k=1}^{n}\Var(X_{n,k})=\sum_{k=1}^{n}\left(\left(1-\frac{k-1}{n}\right)^{-2}-\left(1-\frac{k-1}{n}\right)^{-1}\right)\leq\sum_{k=1}^{n}\left(1-\frac{k-1}{n}\right)^{-2}=\sum_{m=1}^{n}\frac{n^{2}}{m^{2}}\leq cn^{2}
	\end{align*}
	By WLLN, if we choose $b_{n}=n\log n$, then we have
	\begin{equation*}
		\frac{\Var(\tau_{n}^{n})}{b_{n}^{2}}\to 0\implies\frac{\tau_{n}^{n}-\sum_{m=1}^{n}\frac{n}{m}}{n\log n}\xrightarrow{\prob}0
	\end{equation*}
	Therefore, $\frac{\tau_{n}^{n}}{n\log n}\xrightarrow{\prob}1$
\end{eg}
\begin{eg}(An occupancy problem)
	$r$ balls are put at random into $n$ bins. All $n^{r}$ are equally likely.\\
	Let $A_{i}$ be event that the $i$-th bin is empty and $N_{n}$ be number of empty bins $=\sum_{i=1}^{n}\mathbf{1}_{A_{i}}$.\\
	How to prove that if $\frac{r}{n}\to c$ as $n\to\infty$,
	\begin{equation*}
		\frac{N_{n}}{n}\xrightarrow{\prob}e^{-c}
	\end{equation*}
	We can see that
	\begin{align*}
		\frac{\expect N_{n}}{n}&=\frac{1}{n}\sum_{i=1}^{n}\expect\mathbf{1}_{A_{i}}=\prob(A_{i})=\left(1-\frac{1}{n}\right)^{r}\to e^{-c}\\
		\Var(N_{n})&=\expect(N_{n}^{2})-(\expect N_{n})^{2}\\
		&=\expect\left(\sum_{i=1}^{n}\mathbf{1}_{A_{i}}\right)^{2}-\left(\expect\left(\sum_{i=1}^{n}\mathbf{1}_{A_{i}}\right)\right)^{2}\\
		&=\sum_{i=1}^{n}(\prob(A_{1})-(\prob(A_{1}))^{2})+\sum_{i\neq j}(\prob(A_{i}\cap A_{j})-(\prob(A_{1}))^{2})\\
		&=n\left(\left(1-\frac{1}{n}\right)^{r}-\left(1-\frac{1}{n}\right)^{2r}\right)+n(n-1)\left(\left(1-\frac{2}{n}\right)^{r}-\left(1-\frac{1}{n}\right)^{2r}\right)\\
		&=o(n^{2})
	\end{align*}
	By using WLLN, let $b_{n}=n$,
	\begin{equation*}
		\frac{\Var(N_{n})}{b_{n}^{2}}\to 0\implies\frac{N_{n}-\expect N_{n}}{n}\xrightarrow{\prob}0
	\end{equation*}
	Therefore, $\frac{N_{n}}{n}\xrightarrow{\prob}e^{-c}$.
\end{eg}

\newpage
\section{Borel-Cantelli Lemmas}
Let $A_{1},A_{2},\cdots$ be a sequence of events in $(\Omega,\mathcal{F})$. We are more interested in
\begin{equation*}
	\limsup_{n\to\infty}A_{n}=\{A_{n}\text{ i.o}\}=\bigcap_{m}\bigcup_{n=m}^{\infty} A_{n}
\end{equation*}
\begin{thm}(Borel-Cantelli Lemmas)
	For any sequence of events $A_{n}\in\mathcal{F}$,
	\begin{enumerate}
		\item (BCI) If $\sum_{n=1}^{\infty}\prob(A_{n})<\infty$, then
		\begin{equation*}
			\prob(A_{n}\text{ i.o.})=0
		\end{equation*}
		\item (BCII) If $\sum_{n=1}^{\infty}\prob(A_{n})=\infty$ and $A_{n}$'s are independent, then
		\begin{equation*}
			\prob(A_{n}\text{ i.o.})=1
		\end{equation*}
	\end{enumerate}
\end{thm}
\begin{proofing}
	\begin{enumerate}
		\item If $\sum_{n=1}^{\infty}\prob(A_{n})<\infty$,
		\begin{equation*}
			\prob(A_{n}\text{ i.o.})=\lim_{m\to\infty}\prob\left(\bigcup_{n=m}^{\infty}A_{n}\right)\leq\lim_{m\to\infty}\sum_{n=m}^{\infty}\prob(A_{n})=0
		\end{equation*}
		\item If $\sum_{n=1}^{\infty}\prob(A_{n})=\infty$ and $A_{n}$'s are independent, we have
		\begin{align*}
			\prob\left(\bigcup_{m=1}^{\infty}\bigcap_{n=m}^{\infty}A_{n}^{\complement}\right)&=\lim_{m\to\infty}\prob\left(\bigcap_{n=m}^{\infty}A_{n}^{\complement}\right)=\lim_{m\to\infty}\lim_{r\to\infty}\prob\left(\bigcap_{n=m}^{r}A_{n}^{\complement}\right)=\lim_{m\to\infty}\lim_{r\to\infty}\prod_{n=m}^{r}\prob(A_{n}^{\complement})=\lim_{m\to\infty}\prod_{n=m}^{\infty}(1-\prob(A_{n}))\\
			\tag{$1-x\leq e^{-x}$ if $x\geq 0$}
			&\leq\lim_{m\to\infty}\prod_{n=m}^{\infty}e^{-\prob(A_{n})}=\lim_{m\to\infty}\exp\left(-\sum_{n=m}^{\infty}\prob(A_{n})\right)=0\\
			\prob(A_{n}\text{ i.o.})&=\prob\left(\bigcap_{m=1}^{\infty}\bigcup_{n=m}^{\infty}A_{n}\right)=1-\prob\left(\bigcup_{m=1}^{\infty}\bigcap_{n=m}^{\infty}A_{n}^{\complement}\right)=1
		\end{align*}
	\end{enumerate}
\end{proofing}
\begin{rem}
	We can say that BCII is a partial converse statement of BCI.
\end{rem}
\begin{rem}
	i.o. is an abbreviation of "infinitely often". Similarly, f.o. is an abbreviation if "finitely often".
\end{rem}
We will now explore how we can apply Borel-Cantelli Lemmas in multiple applications.
\begin{eg}(Infinite Monkey Problem)
	Assume there is a keyboard with $N$ keys, each with distinct letters. Given a string of letters $S$ of length $m$. We have a monkey which randomly hits any key at any round.\\
	How do we prove that almost surely, the monkey will type up the given string $S$ for infinitely many times?\\
	Let $E_{k}$ be the event that the $m$-string $S$ is typed starting from the $k$-th hit. Note that $E_{k}$'s are not independent.\\
	In order to produce an independent sequence, we can consider $E_{mk+1}$. where each string is $m$ letters apart from next one.\\
	For any $i$, $\prob(E_{i})=\left(\frac{1}{N}\right)^{m}$. By BCII,
	\begin{equation*}
		\sum_{k=0}^{\infty}\prob(E_{mk+1})=\infty\implies\prob(E_{mk+1}\text{ i.o.})=1
	\end{equation*}
	Therefore, $\prob(E_{k}\text{ i.o.})=1$
\end{eg}
\newpage
Recall that if $X_{n}\xrightarrow{\prob}X$, there exists a non-random increasing sequence of integers $n_{1},n_{2},\cdots$ such that $X_{n_{i}}\xrightarrow{\text{a.s.}}X$ as $i\to\infty$.\\
We can use Borel-Cantelli Lemmas to prove a theorem that is pretty similar.
\begin{thm}
	$X_{n}\xrightarrow{\prob}X$ if and only if for all subsequence $X_{n(m)}$, there is a further subsequence
	\begin{equation*}
		X_{n(m_{k})}\xrightarrow{\text{a.s.}}X
	\end{equation*}
\end{thm}
\begin{proofing}
	\begin{figure}[h!]
		\begin{subfigure}[b]{0.05\textwidth}
			($\Longrightarrow$)
		\end{subfigure}
		\begin{subfigure}[t]{0.9\textwidth}
			Let $\varepsilon_{k}$ be a sequence of positive numbers such that $\varepsilon_{k}\to 0$ if $k\to\infty$. For any $k$, there exists an $n(m_{k})>n(m_{k-1})$ such that
			\begin{equation*}
				\tag{$X_{n}\xrightarrow{\prob}X$}
				\prob(\abs{X_{n(m_{k})}-X}>\varepsilon_{k})\leq 2^{-k}
			\end{equation*}
			Since $\sum_{k=1}^{\infty}\prob(\abs{X_{n(m_{k})}-X}>\varepsilon_{k})<\infty$, By BCI,
			\begin{align*}
				\prob(\abs{X_{n(m_{k})}-X}>\varepsilon_{k}\text{ i.o.})&=0 & \prob(\abs{X_{n(m_{k})}-X}>\varepsilon_{k}\text{ f.o.})&=1
			\end{align*}
			For all $\varepsilon>0$, $\varepsilon_{k}\leq\varepsilon$ for all $k\geq k_{0}$. If $\varepsilon_{k}\leq\varepsilon$,
			\begin{equation*}
				\{\abs{X_{n(m_{k})}-X}>\varepsilon_{k}\}\supseteq\{\abs{X_{n(m_{k})}-X}>\varepsilon\}
			\end{equation*}
			If $\omega\in\{\abs{X_{n(m_{k})}-X}>\varepsilon_{k}\}$ for finitely many $k$, then $\omega\in\{\abs{X_{n(m_{k})}-X}>\varepsilon\}$ for finitely many $k$. Therefore, for all $\varepsilon >0$
			\begin{equation*}
				\prob(\abs{X_{n(m_{k})}-X}>\varepsilon\text{ i.o.})=0
			\end{equation*}
		\end{subfigure}
	\end{figure}
	\begin{figure}[h!]
		\begin{subfigure}[b]{0.05\textwidth}
			($\Longleftarrow$)
		\end{subfigure}
		\begin{subfigure}[t]{0.9\textwidth}
			For all $\varepsilon>0$, let $a_{n}=\prob(\abs{X_{n}-X}>\varepsilon)$.\\
			For all $n(m)$, there exists $n(m_{k})$ such that $X_{n(m_{k})}\xrightarrow{\text{a.s.}}X$. We have
			\begin{equation*}
				(X_{n(m_{k})}\xrightarrow{\text{a.s.}}X)\implies(X_{n(m_{k})}\xrightarrow{\prob}X)\implies a_{n(m_{k})}\to 0
			\end{equation*}
			Therefore, for any $a_{n}$ and $a_{n(m)}$, there exists further $a_{n(m_{k})}\to 0$.\\
			We have $a_{n}\to 0\implies(X_{n}\xrightarrow{\prob}X)$.
		\end{subfigure}
	\end{figure}
\end{proofing}
We have a theorem that have conditions quite similar to Law of Large Numbers. However, notice that $\expect\abs{X_{1}}=\infty$ here.
\begin{thm}
	If $X_{1},X_{2},\cdots$ are i.i.d. random variables with $\expect\abs{X_{i}}=\infty$, then
	\begin{align*}
		\prob(\abs{X_{n}}\geq n\text{ i.o.})&=1 & \prob\left(\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n}X_{i}\text{ exists in }(-\infty,\infty)\right)&=0
	\end{align*}
\end{thm}
\begin{proofing}
	\begin{equation*}
		\expect\abs{X_{1}}=\int_{0}^{\infty}\prob(\abs{X_{1}}>t)\,dt\leq\sum_{n=0}^{\infty}\prob(\abs{X_{1}}>n)
	\end{equation*}
	Since $\{\abs{X_{n}}>n\}$ is a collection of independent events, by BCII, $\prob(\abs{X_{n}}>n\text{ i.o.})=1$.\\
	For the second statement, let $Y_{n}=\sum_{i=1}^{n}X_{i}$ and $C=\{\omega\in\Omega:\lim_{n\to\infty}\frac{1}{n}Y_{n}(\omega)\text{ exists in }\mathbb{R}\}$.\\
	Assume that $\omega\in C$, then
	\begin{equation*}
		\frac{Y_{n}(\omega)}{n}-\frac{Y_{n+1}(\omega)}{n+1}=\frac{Y_{n}(\omega)}{n(n+1)}-\frac{X_{n+1}(\omega)}{n+1}
	\end{equation*}
	Since $\frac{Y_{n}}{n}$ converges, $\frac{Y_{n}(\omega)}{n}-\frac{Y_{n+1}(\omega)}{n+1}\to 0$ and $\frac{Y_{n}(\omega)}{n(n+1)}\to 0$. We get that $\frac{X_{n+1}(\omega)}{n+1}\to 0$.\\
	However, that means $\abs{X_{n+1}}<n+1$ for an arbitrary large $n$. Therefore, $\omega\not\in\{\abs{X_{n}}\geq n\text{ i.o.}\}$.\\
	From that, we get that $\prob(C)=0$ since $\prob(\abs{X_{n}}\geq n\text{ i.o.})=1$.
\end{proofing}
\newpage
The next result extends BCII.
\begin{thm}
	If $A_{1},A_{2},\cdots$ are pairwise independent and $\sum_{n=1}^{\infty}\prob(A_{n})=\infty$, then as $n\to\infty$,
	\begin{equation*}
		\frac{\sum_{m=1}^{n}\mathbf{1}_{A_{m}}}{\sum_{m=1}^{n}\prob(A_{m})}\xrightarrow{\text{a.s.}}1
	\end{equation*}
\end{thm}
\begin{proofing}
	Let $X_{n}=\mathbf{1}_{A_{n}}$, $Y_{n}=\sum_{i=1}^{n}X_{i}$ and $\expect Y_{n}=\sum_{m=1}^{n}\prob(A_{m})$.\\
	Notice that pairwise independence is already enough for $\cov(X_{i},X_{j})=0$ for all $i\neq j$.\\
	Using Markov's inequality, for any $\varepsilon>0$, we get as $n\to\infty$
	\begin{align*}
		\prob\left(\abs{\frac{Y_{n}-\expect Y_{n}}{\expect Y_{n}}}>\varepsilon\right)&\leq\frac{\expect(Y_{n}-\expect Y_{n})^{2}}{\varepsilon^{2}(\expect Y_{n})^{2}}=\frac{\Var(Y_{n})}{\varepsilon^{2}(\expect Y_{n})^{2}}=\sum_{m=1}^{n}\frac{\Var(\mathbf{1}_{A_{m}})}{\varepsilon^{2}(\expect Y_{n})^{2}}=\sum_{m=1}^{n}\frac{\expect\mathbf{1}_{A_{m}}}{\varepsilon^{2}(\expect Y_{n})^{2}}=\frac{1}{\varepsilon^{2}\expect Y_{n}}\to 0
	\end{align*}
	Therefore, we get that $\frac{Y_{n}-\expect Y_{n}}{\expect Y_{n}}\xrightarrow{\prob}0$.\\
	Now, we can choose a desirable subsequence to prove almost surely convergence. Let $n_{k}=\inf\{n:\expect Y_{n}\geq k^{2}\}$.\\
	We can get that $\expect Y_{n_{k}}\geq k^{2}$ and $\expect Y_{n_{k}}=\expect Y_{n_{k}-1}+\expect\mathbf{1}_{A_{n_{k}}}<k^{2}+1$. Again by Markov's inequality,
	\begin{equation*}
		\sum_{k=1}^{\infty}\prob\left(\abs{\frac{Y_{n_{k}}-\expect Y_{n_{k}}}{\expect Y_{n_{k}}}}>\varepsilon\right)\leq\sum_{k=1}^{\infty}\frac{1}{\varepsilon^{2}\expect Y_{n_{k}}}\leq\sum_{k=1}^{\infty}\frac{1}{\varepsilon^{2}(k^{2}+1)}<\infty
	\end{equation*}
	By BCI, we have that as $k\to\infty$,
	\begin{align*}
		\frac{Y_{n_{k}}}{\expect Y_{n_{k}}}&\xrightarrow{\text{a.s.}}1 & \prob\left(\frac{Y_{n_{k}}}{\expect Y_{n_{k}}}\to 1\text{ as }k\to\infty\right)=1
	\end{align*}
	Let $C=\{\omega\in\Omega:\frac{Y_{n_{k}}(\omega)}{\expect Y_{n_{k}}}\to 1\text{ as }k\to\infty\}$. For $\omega\in C$, for all $n_{k}\leq n<n_{k+1}$, we have $Y_{n_{k}}(\omega)\leq Y_{n}(\omega)\leq Y_{n_{k+1}}(\omega)$.
	\begin{equation*}
		\frac{Y_{n_{k}}(\omega)}{\expect Y_{n_{k}+1}}\leq\frac{Y_{n}(\omega)}{\expect Y_{n}}\leq\frac{Y_{n_{k}+1}(\omega)}{\expect Y_{n_{k}}}
	\end{equation*}
	Since $\frac{Y_{n_{k}}(\omega)}{\expect Y_{n_{k}+1}}=\frac{Y_{n_{k}}(\omega)}{\expect Y_{n_{k}}}\left(\frac{\expect Y_{n_{k}}}{\expect Y_{n_{k}+1}}\right)\to 1$ and $\frac{Y_{n_{k}+1}(\omega)}{\expect Y_{n_{k}+1}}=\frac{Y_{n_{k}+1}(\omega)}{\expect Y_{n_{k}+1}}\left(\frac{\expect Y_{n_{k}+1}}{\expect Y_{n_{k}}}\right)\to 1$, we get that for any $\omega\in C$,
	\begin{equation*}
		\frac{Y_{n}(\omega)}{\expect Y_{n}}\to 1
	\end{equation*}
	Therefore, we have
	\begin{equation*}
		\prob\left(\frac{Y_{n}}{\expect Y_{n}}\to 1\right)\geq\prob\left(\frac{Y_{n_{k}}}{\expect Y_{n_{k}}}\to 1\text{ as }k\to\infty\right)=1
	\end{equation*}
	As a result, we get that
	\begin{equation*}
		\frac{Y_{n}}{\expect Y_{n}}\xrightarrow{\text{a.s.}}1
	\end{equation*}
\end{proofing}
If the events $A_{1}, A_{2},\cdots$ in the Borel-Cantelli Lemmas are independent, then $\prob(A)$ is either $0$ or $1$ depending on whether $\sum\prob(A_{n})$ converges. The following is a simple version.
\begin{thm}(Borel Zero-one Law)
	Let $A_{1},A_{2},\cdots\in\mathcal{F}$ and $\mathcal{A}=\sigma(A_{1},A_{2},\cdots)$. Suppose that
	\begin{enumerate}
		\item $A\in\mathcal{A}$
		\item $A$ is independent with any finite collection of $A_{1},A_{2},\cdots$
	\end{enumerate}
	Then $\prob(A)=0$ or $1$.
\end{thm}
\begin{proofing}[Proof (Non-rigorous)]
	Suppose that $A_{1},A_{2},\cdots$ are independent. Let $A=\limsup_{n}A_{n}$.\\
	We know that $A=\bigcap_{m=1}^{\infty}\bigcup_{n=m}^{\infty}A_{n}$. Therefore, $A\in\mathcal{A}=\sigma(A_{1},A_{2},\cdots)$.\\
	For all $k$, we can also have $A=\bigcap_{m=k+1}^{\infty}\bigcup_{n=m}^{\infty}A_{n}$. Therefore, $A$ is independent with any $A_{i}\in\sigma(A_{1},A_{2},\cdots,A_{k})$.\\
	Setting $k\to\infty$, we have that $A$ is independent of all elements in $\mathcal{A}$, which also include itself.\\
	Therefore, $\prob(A)=\prob(A\cap A)=(\prob(A))^{2}\implies\prob(A)=0$ or $1$.
\end{proofing}
\newpage
Let $X_{1},X_{2},\cdots$ be a collection of random variables. For any subcollection $\{X_{i}:i\in I\}$, write $\sigma(X_{i}:i\in I)$ for the smallest $\sigma$-field with reference to which each of $X_{i}$ is measurable.
\begin{defn}
	Let $\mathcal{H}_{n}=\sigma(X_{n+1},X_{n+2},\cdots)$. We have $\mathcal{H}_{n}\supseteq\mathcal{H}_{n+1}\supseteq\cdots$. \textbf{Tail $\sigma$-field} is defined as
	\begin{equation*}
		\mathcal{H}_{\infty}=\bigcap_{n}\mathcal{H}_{n}
	\end{equation*}
\end{defn}
\begin{rem}
	If $E\in\mathcal{H}_{\infty}$, then $E$ is called \textbf{tail event}.
\end{rem}
\begin{eg}
	$\{\limsup_{n\to\infty}X_{n}=\infty\}$ is a tail event.
\end{eg}
\begin{eg}
	$\{\sum_{n}X_{n}\text{ converges}\}$ is a tail event.
\end{eg}
\begin{eg}
	$\{\sum_{n}X_{n}\text{ converges to }1\}$ is not a tail event.
\end{eg}
We get another version of zero-one law.
\begin{thm}(Kolmogorov's zero-one law)
	If $H\in\mathcal{H}_{\infty}$, then $\prob(H)=0$ or $1$.
\end{thm}
We continue to explore more into tail events.
\begin{defn}
	We define \textbf{tail function} to be $Y:\Omega\to\mathbb{R}\cup\{-\infty,\infty\}$, which is a generalized random variables that is a function of $X_{1},X_{2},\cdots$. It is independent of any finite collection of $X_{i}$'s and is $\mathcal{H}_{\infty}$-measurable.
\end{defn}
\begin{eg}
	Let $Y(\omega)=\limsup_{n\to\infty}X_{n}(\omega)$ for all $\omega\in\Omega$. $F_{Y}(y)=\prob(Y\leq y)=0$ or $1$ for all $y\in\mathbb{R}\cup\{-\infty,\infty\}$.\\
	$\{Y\leq y\}$ is a tail event.
\end{eg}
\begin{thm}
	If $Y$ is a tail function of independent sequence of random variables $X_{1},X_{2},\cdots$, then there exists $-\infty\leq k\leq\infty$,
	\begin{equation*}
		\prob(Y=k)=1
	\end{equation*}
\end{thm}
Again let $X_{1},X_{2},\cdots$ be i.i.d. random variables and let $Y_{n}=\sum_{i=1}^{n}X_{i}$.\\
Recall that if $\expect\abs{X_{1}}<\infty$,
\begin{equation*}
	\prob\left(\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n}X_{i}=\expect X_{1}\right)=1
\end{equation*}
If $\expect\abs{X_{1}}=\infty$,
\begin{equation*}
	\prob\left(\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n}X_{i}\text{ exists}\right)=0
\end{equation*}
Using tail function, the random variables are not necessarily identically distributed.
\begin{thm}
	Let $X_{1},X_{2},\cdots$ be independent random variables. We have:
	\begin{equation*}
		\prob\left(\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n}X_{i}\text{ exists}\right)=0\text{ or }1
	\end{equation*}
\end{thm}
\begin{proofing}
	Let $Z_{1}=\limsup_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n}X_{i}$ and $Z_{2}=\liminf_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n}X_{i}$. We claim that both $Z_{1}$ and $Z_{2}$ are tail functions of $X_{i}$'s. For any $k$,
	\begin{align*}
		Z_{1}(\omega)&=\limsup_{n\to\infty}\left(\frac{1}{n}\sum_{i=1}^{k}X_{i}(\omega)+\frac{1}{n}\sum_{i=k+1}^{n}X_{i}(\omega)\right) & Z_{2}(\omega)&=\liminf_{n\to\infty}\left(\frac{1}{n}\sum_{i=1}^{k}X_{i}(\omega)+\frac{1}{n}\sum_{i=k+1}^{n}X_{i}(\omega)\right)
	\end{align*}
	Therefore, both $Z_{1}$ and $Z_{2}$ do not depend on any finite collection of $X_{i}$. We say that $\{Z_{1}=Z_{2}\}$ is a tail event.\\
	Therefore, by Kolmogorov's zero-one law.
	\begin{equation*}
		\prob\left(\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n}X_{i}\text{ exists}\right)=\prob(Z_{1}=Z_{2})=0\text{ or }1
	\end{equation*}
\end{proofing}
\begin{eg}(Random power series)
	Let $X_{1},X_{2},\cdots$ be i.i.d. exponential random variables with parameter $\lambda=1$. We consider a random power series
	\begin{equation*}
		p(z;\omega)=\sum_{n=0}^{\infty}X_{n}(\omega)z^{n}
	\end{equation*}
	The formula for radius of convergence is
	\begin{equation*}
		R(\omega)=\frac{1}{\limsup_{n\to\infty}\abs{X_{n}(\omega)}^{\frac{1}{n}}}
	\end{equation*}
	We can get that $R(\omega)$ is a tail function of $X_{i}$'s. Therefore, there exists $C$ such that $\prob(R=C)=1$ ($R=C$ almost surely)\\
	We want to find the value of $C$.\\
	We claim that $C=1$.
	\begin{equation*}
		\prob\left(\limsup_{n\to\infty}\abs{X_{n}}^{\frac{1}{n}}=1\right)=1
	\end{equation*}
	It suffices to show that for all $\varepsilon>0$,
	\begin{align*}
		\prob\left(\limsup_{n\to\infty}\abs{X_{n}}^{\frac{1}{n}}\leq 1+\varepsilon\right)&=1 & \prob\left(\limsup_{n\to\infty}\abs{X_{n}}^{\frac{1}{n}}\geq 1-\varepsilon\right)&=1
	\end{align*}
	We first prove the first one.
	\begin{equation*}
		\sum_{n=1}^{\infty}\prob\left(\abs{X_{n}}^{\frac{1}{n}}>1+\varepsilon\right)=\sum_{n=1}^{\infty}\prob(\abs{X_{n}}>(1+\varepsilon)^{n})=\sum_{n=1}^{\infty}e^{-(1+\varepsilon)^{n}}<\infty
	\end{equation*}
	Therefore, by BCI,
	\begin{equation*}
		\prob(\abs{X_{n}}^{\frac{1}{n}}>1+\varepsilon\text{ i.o.})=0\implies\prob\left(\limsup_{n\to\infty}\abs{X_{n}}^{\frac{1}{n}}\leq 1+\varepsilon\right)=1
	\end{equation*}
	Similarly,
	\begin{equation*}
		\sum_{n=1}^{\infty}\prob\left(\abs{X_{n}}^{\frac{1}{n}}>1-\varepsilon\right)=\sum_{n=1}^{\infty}\prob(\abs{X_{n}}>(1-\varepsilon)^{n})=\sum_{n=1}^{\infty}e^{-(1-\varepsilon)^{n}}=\infty
	\end{equation*}
	Therefore, by BCII,
	\begin{equation*}
		\prob(\abs{X_{n}}^{\frac{1}{n}}>1-\varepsilon\text{ i.o.})=1\implies\prob\left(\limsup_{n\to\infty}\abs{X_{n}}^{\frac{1}{n}}\geq 1-\varepsilon\right)=1
	\end{equation*}
	By sending $\varepsilon\to 0$, we get
	\begin{equation*}
		\prob\left(\limsup_{n\to\infty}\abs{X_{n}}^{\frac{1}{n}}=1\right)=1
	\end{equation*}
	Therefore, $C=1$.
\end{eg}

\section{Strong Law of Large Numbers}
We recall the Weak Law of Large Numbers. Let $X_{1},X_{2},\cdots$ be a sequence of i.i.d. random variables with $\expect(X_{1})=\mu$. As $n\to\infty$,
\begin{align*}
	\frac{1}{n}\sum_{i=1}^{n}X_{i}&\xrightarrow{D}\mu & \frac{1}{n}\sum_{i=1}^{n}X_{i}&\xrightarrow{\prob}\mu
\end{align*}
By name, WLLN indeed has a stronger version. It is called Strong Law of Large Numbers. We prove one of the versions of SLLN.
\begin{thm}(\textbf{Strong Law of Large Numbers} [SLLN])
	Let $X_{1},X_{2},\cdots$ be i.i.d. random variables with $\expect X_{1}=\mu$ and $\expect\abs{X_{1}}<\infty$. We have:
	\begin{equation*}
		\frac{1}{n}\sum_{i=1}^{n}X_{i}\xrightarrow{\text{a.s.}}\mu
	\end{equation*}
\end{thm}
Note that the proof for SLLN is very complicated, and we will not prove it here. Instead, we will prove a different version of SLLN.
\newpage
\begin{thm}(SLLN with $\expect X_{i}^{4}<\infty$)
	Let $X_{1},X_{2},\cdots$ be i.i.d. random variables with $\expect X_{1}=0$ and $\expect(X_{1}^{4})<\infty$. We have:
	\begin{equation*}
		\frac{1}{n}\sum_{i=1}^{n}X_{i}\xrightarrow{\text{a.s.}}0
	\end{equation*}
\end{thm}
\begin{proofing}
	\begin{equation*}
		\expect\left(\sum_{i=1}^{n}X_{i}\right)^{4}=\sum_{i,j,k,\ell=1}^{n}\expect X_{i}X_{j}X_{k}X_{\ell}
	\end{equation*}
	The expectation is non-zero if there are $2$ pairs of the random variables with same value.
	\begin{align*}
		\expect Y_{n}^{4}&=3\sum_{i\neq j}\expect X_{i}^{2}\expect X_{j}^{2}+\sum_{i}\expect X_{i}^{4}=O(n^{2})\\
		\tag{Markov's inequality}
		\prob\left(\abs{\frac{1}{n}\sum_{i=1}^{n}X_{i}}\geq\varepsilon\right)&\leq\frac{1}{(n\varepsilon)^{4}}\expect\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right)^{4}=O\left(\frac{1}{n^{2}}\right)
	\end{align*}
	Therefore, we get that for all $\varepsilon>0$
	\begin{equation*}
		\sum_{n=1}^{\infty}\prob\left(\abs{\frac{1}{n}\sum_{i=1}^{n}X_{i}}>\varepsilon\right)<\infty
	\end{equation*}
	Therefore, $\frac{1}{n}\sum_{i=1}^{n}X_{i}\xrightarrow{\text{a.s.}}0$.
\end{proofing}
\begin{thm}(SLLN with $\expect X_{1}^{2}<\infty$) Let $X_{1},X_{2},\cdots$ be i.i.d. random variables with $\expect X_{1}^{2}<\infty$ and $\expect X_{i}=\mu$. We have:
	\begin{align*}
		\frac{1}{n}\sum_{i=1}^{n}X_{i}&\xrightarrow{2}\mu & \frac{1}{n}\sum_{i=1}^{n}X_{i}&\xrightarrow{\text{a.s.}}\mu
	\end{align*}
\end{thm}
\begin{proofing}
	Let $Y_{n}=\sum_{i=1}^{n}X_{i}$. We first show convergence in mean square. Since $\expect X_{1}^{2}<\infty$, as $n\to\infty$,
	\begin{equation*}
		\expect\left(\frac{Y_{n}}{n}-\mu\right)^{2}=\frac{\expect(Y_{n}-n\mu)^{2}}{n^{2}}=\frac{\Var(Y_{n})}{n^{2}}=\frac{\Var(X_{1})}{n}\to 0
	\end{equation*}
	For the almost sure convergence, we know that convergence in probability implies the existence of almost sure convergence of some subsequence of $\frac{Y_{n}}{n}$ to $\mu$. We write $n_{i}=i^{2}$. By using Markov's inequality, for all $\varepsilon>0$,
	\begin{equation*}
		\sum_{i}\prob\left(\frac{\abs{Y_{i^{2}}-i^{2}\mu}}{i^{2}}>\varepsilon\right)\leq\sum_{i}\frac{\expect\abs{Y_{i^{2}}-i^{2}\mu}^{2}}{i^{4}\varepsilon^{2}}=\sum_{i}\frac{\Var(Y_{i^{2}})}{i^{4}\varepsilon^{2}}=\sum_{i}\frac{\Var(X_{1})}{i^{2}\varepsilon^{2}}<\infty
	\end{equation*}
	Therefore, we get that $\frac{Y_{i^{2}}}{i^{2}}\xrightarrow{\text{a.s.}}\mu$. However, we need to fill the gaps.\\
	We suppose the $X_{i}$ are non-negative. We have $Y_{i^{2}}\leq Y_{n}\leq Y_{(i+1)^{2}}$ if $i^{2}\leq n\leq (i+1)^{2}$.\\
	We can get that
	\begin{equation*}
		\frac{Y_{i^{2}}}{(i+1)^{2}}\leq\frac{Y_{n}}{n}\leq\frac{Y_{(i+1)^{2}}}{i^{2}}
	\end{equation*}
	Since we get that $\frac{Y_{i^{2}}}{i^{2}}\xrightarrow{\text{a.s.}}\mu$, by having $\frac{i^{2}}{(i+1)^{2}}\to 1$ as $i\to\infty$, we get that whenever $X_{i}$ are non-negative, as $n\to\infty$
	\begin{equation*}
		\frac{Y_{n}}{n}\xrightarrow{\text{a.s.}}\mu
	\end{equation*}
	For general $X_{i}$, we can write $X_{n}=X_{n}^{+}-X_{n}^{-}$ where
	\begin{align*}
		X_{n}^{+}(\omega)&=\max\{X_{n}(\omega),0\} & X_{n}^{-}(\omega)&=-\min\{X_{n}(\omega),0\}
	\end{align*}
	Therefore, both $X_{n}^{+}(\omega)$ and $X_{n}^{-}(\omega)$ are non-negative.\\
	Furthermore, $X_{n}^{+}\leq\abs{X_{n}}$ and $X_{n}^{-}\leq\abs{X_{n}}$. Therefore, $\expect(X_{n}^{+})^{2}<\infty$ and $\expect(X_{n}^{-})^{2}<\infty$.
	By previous conclusion for non-negative random variables, we get as $n\to\infty$,
	\begin{equation*}
		\frac{Y_{n}}{n}=\frac{1}{n}\left(\sum_{i=1}^{n}X_{i}^{+}-\sum_{i=1}^{n}X_{i}^{-}\right)\xrightarrow{\text{a.s.}}\expect X_{1}^{+}-\expect X_{1}^{-}=\expect X_{1}
	\end{equation*}
	Therefore, $\frac{Y_{n}}{n}\xrightarrow{\text{a.s.}}\mu$.
\end{proofing}
\newpage
Why do we need SLLN? There are a lot of applications that specifically need SLLN.
\begin{eg}(Renewal Theory) Assume that we have a light bulb. We change it immediately when it burnt out.\\
	Let $X_{i}$ be the lifetime of $i$-th bulb and $T_{n}=X_{1}+X_{2}+\cdots+X_{n}$ be the time to replace the $n$-th bulb.\\
	Let $N_{t}=\sup\{n:T_{n}\leq t\}$ be number of bulbs that have burnt out by time $t$. $T_{N_{t}}$ is the exact time that $N_{t}$'s bulb burnt out.\\
	Since we are dealing with practical bulb, assume that $X_{1},X_{2},\cdots$ are i.i.d. random variables with $0<X_{i}<\infty$ and $\expect X_{1}<\infty$.
\end{eg}
\begin{thm}
	Let $\expect X_{1}=\mu$. As $t\to\infty$,
	\begin{equation*}
		\frac{t}{N_{t}}\xrightarrow{\text{a.s.}}\mu
	\end{equation*}
\end{thm}
\begin{proofing}
	Since $T_{N_{t}}\leq t<T_{N_{t}+1}$,
	\begin{equation*}
		\frac{T_{N_{t}}}{N_{t}}\leq\frac{t}{N_{t}}<\frac{T_{N_{t}+1}}{N_{t}+1}\left(\frac{N_{t}+1}{N_{t}}\right)
	\end{equation*}
	By SLLN, we know that $\frac{T_{n}}{n}\xrightarrow{\text{a.s.}}\mu$. Since $\frac{T_{n}}{n}$ and $\frac{T_{N_{t}}}{N_{t}}$ are the same sequence, we get that
	\begin{align*}
		\frac{T_{N_{t}}}{N_{t}}&\xrightarrow{\text{a.s.}}\mu & \frac{T_{N_{t}+1}}{N_{t}+1}&\xrightarrow{\text{a.s.}}\mu
	\end{align*}
	For all $\omega\in\Omega$, $t<T_{N_{t}+1}=X_{1}(\omega)+X_{2}(\omega)+\cdots+X_{N_{t}(\omega)+1}(\omega)$.\\
	As $t\to\infty$, it forces $N_{t}(\omega)\to\infty$. Therefore, $\frac{N_{t}+1}{N_{t}}\xrightarrow{\text{a.s.}}1$.
	Combining all of this, we get $\frac{t}{N_{t}}\xrightarrow{\text{a.s.}}\mu$.
\end{proofing}
\begin{cla}
	If $X_{n}\xrightarrow{\prob}X_{\infty}$, then $N_{m}\xrightarrow{\text{a.s.}}\infty$ as $m\to\infty$.
\end{cla}
\begin{rem}
	For this claim, it is not necessary that $X_{N_{m}}\xrightarrow{\text{a.s}}X_{\infty}$ or $X_{N_{m}}\xrightarrow{\prob}X_{\infty}$.
\end{rem}
\begin{eg}
	Recall the example that we use in Theorem \ref{Chapter 8 (Theorem) implications of different convergence modes} to prove $(X_{n}\xrightarrow{\prob}X)\centernot\implies(X_{n}\xrightarrow{\text{a.s.}}X)$. Let $\Omega=[0,1]$. Let
	\begin{equation*}
		Y_{m,k}=\mathbf{1}_{I_{m,k}}=\begin{cases}
			1, &\omega\in\left[\frac{k-1}{m},\frac{k}{m}\right]\\
			0, &\text{Otherwise}
		\end{cases}
	\end{equation*}
	Let $X_{n}$ be the enumeration of $Y_{m,k}$. i.e. $X_{1}=Y_{1,1}$, $X_{2}=Y_{2,1}$, $X_{3}=Y_{2,2}$, $\cdots$.\\
	From the proof of the theorem, we got that $X_{n}\xrightarrow{\prob}X_{\infty}=0$ but $X_{n}\centernot{\xrightarrow{\text{a.s.}}}X_{\infty}$.\\
	For each $\omega\in\Omega$, and each $m\geq 1$, there exists $k$ such that $\omega\in\left[\frac{k-1}{m},\frac{k}{m}\right]$. We denote these as $k_{m}(\omega)$.\\
	Let $N_{m}(\omega)=\sum_{i=1}^{m-1}i+k_{m}(\omega)$. We get that $X_{N_{m}(\omega)}(\omega)=Y_{m,k_{m}(\omega)}(\omega)=1$.\\
	However, $X_{\infty}=0$. That means, $X_{N_{m}}\centernot{\xrightarrow{\prob}}X_{\infty}$ and $X_{N_{m}}\centernot{\xrightarrow{\text{a.s.}}}X_{\infty}$.
\end{eg}
We move to our next examples, which is the Glivenko-Cantelli Theorem. It is also called the Fundamental Theorem of Statistics.
\begin{thm}(Glivenko-Cantelli Theorem)
	Assume that $X\sim F(x)$ where $F(x)$ is unknown. Let $X_{1},X_{2},\cdots$ be i.i.d. random samples of $X$. We define the empirical distribution function, which is also a distribution function of a histogram.
	\begin{align*}
		F_{N}(x)&=\frac{1}{N}\sum_{i=1}^{N}\mathbf{1}_{X_{i}\leq x} & F_{N}(x;\omega)&=\frac{1}{N}\sum_{i=1}^{N}\mathbf{1}_{X_{i}(\omega)\leq x}
	\end{align*}
	We have that
	\begin{equation*}
		\sup_{x}\abs{F_{n}(x)-F(x)}\xrightarrow{\text{a.s.}}0
	\end{equation*}
\end{thm}
\begin{proofing}
	We only proof for the case when $F(x)$ is continuous.\\
	For each $m$, we have a partition $-\infty=x_{0}<x_{1}<\cdots<x_{m}=\infty$ such that $F(x_{i})-F(x_{i-1})=\frac{1}{m}$.\\
	For all $x\in[x_{i-1},x_{i})$,
	\begin{align*}
		F_{N}(x)-F(x)&\leq F_{N}(x_{i})-F(x_{i-1})=F_{N}(x_{i})-F(x_{i})+\frac{1}{m}\\
		F_{N}(x)-F(x)&\geq F_{N}(x_{i-1})-F(x_{i})=F_{N}(x_{i-1})-F(x_{i-1})-\frac{1}{m}
	\end{align*}
	From this, we get
	\begin{equation*}
		-\sup_{i}\abs{F_{N}(x_{i})-F(x_{i})}-\frac{1}{m}\leq F_{N}(x)-F(x)\leq\sup_{i}\abs{F_{N}(x_{i})-F(x_{i})}+\frac{1}{m}\implies\sup_{x}\abs{F_{N}(x)-F(x)}\leq\sup_{i}\abs{F_{N}(x_{i})-F(x_{i})}+\frac{1}{m}
	\end{equation*}
	By SLLN, when we fix $x$, we get
	\begin{align*}
		F_{N}(x)=\frac{1}{N}\sum_{i=1}^{N}\mathbf{1}_{X_{i}\leq x}&\xrightarrow{\text{a.s.}}\expect\mathbf{1}_{X_{1}\leq x}=\prob(X_{1}\leq x)=F(x) & \prob(\{\omega\in\Omega:F_{N}(x;\omega)\to F(x)\text{ as }N\to\infty\})&=1
	\end{align*}
	Let $C_{x}=\{\omega\in\Omega:F_{N}(x;\omega)\to F(x)\text{ as }N\to\infty\}$. Notice that if $\omega\in\bigcap_{i=1}^{\infty}C_{x_{i}}$, $\sup_{i}\abs{F_{N}(x_{i};\omega)-F(x_{i})}\to 0$.
	\begin{equation*}
		\limsup_{N}\sup_{x}\abs{F_{N}(x)-F(x)}\leq\frac{1}{m}
	\end{equation*}
	If $\omega\in\bigcap_{m=1}^{\infty}\bigcap_{i=1}^{m}C_{x_{i}}$,
	\begin{equation*}
		\limsup_{N}\sup_{x}\abs{F_{N}(x)-F(x)}=0
	\end{equation*}
	Therefore, since $\bigcap_{m=1}^{\infty}\bigcap_{i=1}^{m}C_{x_{i}}\subseteq\{\omega\in\Omega:\sup_{x}\abs{F_{N}(x;\omega)-F(x)}\to 0\text{ as }N\to\infty\}$ and $\prob(C_{x_{i}})=1$ by SLLN,
	\begin{equation*}
		\prob(\{\omega\in\Omega:\sup_{x}\abs{F_{N}(x;\omega)-F(x)}\to 0\text{ as }N\to\infty\})=1
	\end{equation*}
\end{proofing}
We will end here. Of course, there are still a lot of examples that we haven't explored (including some mentioned during the lectures that I'm too lazy to include here). We also skipped a lot of proofs in some of the theorems. It is up to you to explore further, either in other courses or in the future world of mathematics.

\appendix
\renewcommand{\thechapter}{\Alph{chapter}}

\chapter{Random walk}
\label{Appendix A Simple random walk}
\begin{eg}(\textbf{Simple random walk})
	Consider a particle moving on the real line. Every step it moves to the right by $1$ with probability $p$, and to the left by $1$ with probability $q=1-p$. Let $Y_{n}$ be the position of the particles after $n$ moves and let $Y_{0}=a$. Then:
	\begin{equation*}
		Y_{n}=a+\sum_{i=1}^{n}X_{i}
	\end{equation*}
	where $\{X_{1},X_{2},\cdots\}$ is a sequence of independently random variables taking $1$ with probability $p$ and $-1$ with probability $q$.\\
	Random walk is \textbf{symmetric} if $p=q=\frac{1}{2}$.
\end{eg}
\begin{lem}
	\label{Appendix A (Lemma) Simple random walk properties}
	Simple random walk has the following properties:
	\begin{enumerate}
		\item It is \textbf{spatially homogeneous}: $\prob(Y_{n}=j|Y_{0}=a)=\prob(Y_{n}=j+b|Y_{0}=a+b)$.
		\item It is \textbf{temporarily homogeneous}: $\prob(Y_{n}=j|Y_{0}=a)=\prob(Y_{m+n}=j|Y_{m}=a)$.
		\item It has \textbf{Markov property}: $\prob(Y_{m+n}=j|Y_{0},Y_{1},\cdots,Y_{m})=\prob(Y_{m+n}=j|Y_{m})$, $n\geq 0$.
	\end{enumerate}
\end{lem}
\begin{proofing}
	\begin{enumerate}
		\item $\prob(Y_{n}=j|Y_{0}=a)=\prob(\sum_{i=1}^{n}X_{i}=j-a)=\prob(Y_{n}=j+b|Y_{0}=a+b)$
		\item 
		\begin{equation*}
			\prob(Y_{n}=j|Y_{0}=a)=\prob\left(\sum_{i=1}^{n}X_{i}=j-a\right)=\prob\left(\sum_{i=m+1}^{m+n}X_{i}=j-a\right)=\prob(Y_{m+n}=j|Y_{m}=a)
		\end{equation*}
		\item If we know $Y_{m}$, then distribution of $Y_{m+n}$ depends only on $X_{m+1},X_{m+2},\cdots,X_{m+n}$ and $Y_{0},Y_{1},\cdots,Y_{m-1}$ does not influence the dependency.
	\end{enumerate}
\end{proofing}
\begin{eg}(Probability via sample path counting)
	Let \textbf{sample path} $\mathbf{s}=(s_{0},s_{1},\cdots,s_{n})$ (outcome/realization of the random walk), with $s_{0}=a$ and $s_{i+1}-s_{i}=\pm 1$.
	\begin{align*}
		\prob((Y_{0},Y_{1},\cdots,Y_{n})=(s_{0},s_{1},\cdots,s_{n}))&=p^{r}q^{\ell} & r&=\#\{i:s_{i+1}-s_{i}=1\} & \ell&=\#\{i:s_{i+1}-s_{i}=-1\}
	\end{align*}
\end{eg}
\begin{eg}
	Let $M_{n}^{r}(a,b)$ be number of paths $(s_{0},s_{1},\cdots,s_{n})$ with $s_{0}=a$, $s_{n}=b$ and having $r$ rightward steps.
	\begin{equation*}
		\prob(Y_{n}=b)=\sum_{r} M_{n}^{r}(a,b)p^{r}q^{n-r}
	\end{equation*}
	By $r+\ell=n$ and $r-\ell=b-a$, $r=\frac{1}{2}(n+b-a)$ and $\ell=(n-b+a)$. If $\frac{1}{2}(n+b-a)\in\{0,1,\cdots,n\}$,
	\begin{equation*}
		\prob(Y_{n}=b)=\binom{n}{\frac{1}{2}(n+b-a)}p^{\frac{1}{2}(n+b-a)}q^{\frac{1}{2}(n-b+a)}
	\end{equation*}
	Otherwise, $\prob(Y_{n}=b)=0$.
\end{eg}
\begin{thm}(Reflection principle)
	Let $N_{n}(a,b)$ be number of possible paths from $(0,a)$ to $(n,b)$ and let $N_{n}^{0}(a,b)$ be number of such paths which contains some point $(k,0)$ on the $x$-axis. If $a,b>0$, then:
	\begin{equation*}
		N_{n}^{0}(a,b)=N_{n}(-a,b)
	\end{equation*}
\end{thm}
\begin{proofing}
	Each path from $(0,-a)$ to $(n,b)$ intersects the $x$-axis at some earliest point $(k,0)$.\\
	Reflect the segment of the path with $0\leq x\leq k$ in the $x$-axis to obtain a path joining $(0,a)$ to $(n,b)$ which intersects the $x$-axis.\\
	This operation gives a one-to-one correspondence between the collections of such paths.
\end{proofing}
\begin{lem}
	\label{Appendix A (Lemma) Number of paths calculation}
	\begin{equation*}
		N_{n}(a,b)=\binom{n}{\frac{1}{2}(n+b-a)}
	\end{equation*}
\end{lem}
\begin{proofing}
	Choose a path from $(0,a)$ to $(n,b)$ and let $\alpha$ and $\beta$ be numbers of positive and negative steps in this path respectively.\\
	Then $\alpha+\beta=n$ and $\alpha-\beta=b-a$, which we have $\alpha=\frac{1}{2}(n+b-a)$.\\
	Number of such paths is the number of ways of picking $\alpha$ positive steps from $n$ available. Therefore,
	\begin{equation*}
		N_{n}(a,b)=\binom{n}{\alpha}=\binom{n}{\frac{1}{2}(n+b-a)}
	\end{equation*}
\end{proofing}
\begin{eg}
	We want to find the probability that the walk does not revisit its starting point in the first $n$ steps.\\
	Without loss of generality, we assume $Y_{0}=0$ so that $Y_{1},Y_{2},\cdots,Y_{n}\neq 0$ if and only if $Y_{1}Y_{2}\cdots Y_{n}\neq 0$.\\
	Event $Y_{1}Y_{2}\cdots Y_{n}\neq 0$ occurs if and only if the path of the walk does not visit the $x$-axis in the time interval $[1,n]$.\\
	If $b>0$, first step must be $(1,1)$, so, by Lemma \ref{Appendix A (Lemma) Number of paths calculation} and Reflection principle, number of such path is:
	\begin{align*}
		N_{n-1}(1,b)-N_{n-1}^{0}(1,b)&=N_{n-1}(1,b)-N_{n-1}(-1,b)\\
		&=\binom{n-1}{\frac{1}{2}(n+b-2)}-\binom{n-1}{\frac{1}{2}(n+b)}\\
		&=\left(\frac{n+b}{2n}-\frac{n-b}{2n}\right)\binom{n}{\frac{1}{2}(n+b)}\\
		&=\frac{b}{n}\binom{n}{\frac{1}{2}(n+b)}
	\end{align*}
	There are $\frac{1}{2}(n+b)$ rightward steps and $\frac{1}{2}(n-b)$ leftward steps. Therefore,
	\begin{equation*}
		\prob(Y_{1}Y_{2}\cdots Y_{n}\neq 0,Y_{n}=b)=\frac{b}{n}N_{n}(0,b)p^{\frac{1}{2}(n+b)}q^{\frac{1}{2}(n-b)}=\frac{b}{n}\prob(Y_{n}=b).
	\end{equation*}
\end{eg}
\begin{eg}
	Let $M_{n}=\max\{Y_{i}:0\leq i\leq n\}$ be the maximum value attained by random walk up to time $n$. Suppose that $Y_{0}=0$ so that $M_{n}\geq 0$. We have $M_{n}\geq Y_{n}$.
\end{eg}
\begin{thm}
	Suppose that $Y_{0}=0$. Then, for $r\geq 1$,
	\begin{equation*}
		\prob(M_{n}\geq r,Y_{n}=b)=\begin{cases}
			\prob(Y_{n}=b), &\text{if }b\geq r\\
			\left(\frac{q}{p}\right)^{r-b}\prob(Y_{n}=2r-b), &\text{if }b<r
		\end{cases}
	\end{equation*}
	It follows that, for $r\geq 1$,
	\begin{equation*}
		\prob(M_{n}\geq r)=\prob(Y_{n}\geq r)+\sum_{b=-\infty}^{r-1}\left(\frac{q}{p}\right)^{r-b}\prob(Y_{n}=2r-b)=\prob(Y_{n}=r)+\sum_{c=r+1}^{\infty}\left(1+\left(\frac{q}{p}\right)^{c-r}\right)\prob(Y_{n}=c)
	\end{equation*}
	For symmetric case when $p=q=\frac{1}{2}$,
	\begin{equation*}
		\prob(M_{n}\geq r)=2\prob(Y_{n}\geq r+1)+\prob(Y_{n}=r)
	\end{equation*}
\end{thm}

\newpage
\begin{proofing}
	Assume that $r\geq 1$ and $b<r$. Let $N_{n}^{r}(0,b)$ be number of paths from $(0,0)$ to $(n,b)$ which include some points having height $r$ (Some point $(i,r)$ with $0<i<n$). For a path $\pi$, let $(i_{\pi},r)$ be the earliest point.\\
	We reflect the segment of path with $i_{\pi}\leq x\leq n$ in the line $y=r$ to obtain path $\pi'$ joining $(0,0)$ to $(n,2r-b)$.\\
	We have $N_{n}^{r}(0,b)=N_{n}(0,2r-b)$.
	\begin{equation*}
		\prob(M_{n}\geq r,Y_{n}=b)=N_{n}^{r}(0,b)p^{\frac{1}{2}(n+b)}q^{\frac{1}{2}(n-b)}=\left(\frac{q}{p}\right)^{r-b}N_{n}(0,2r-b)p^{\frac{1}{2}(n+2r-b)}q^{\frac{1}{2}(n-2r+b)}=\left(\frac{q}{p}\right)^{r-b}\prob(Y_{n}=2r-b)
	\end{equation*}
\end{proofing}
\chapter{Terminologies in other fields of mathematics}
\begin{defn}
	\textbf{Supremum} of subset $S$ is the lowest upper bound $x$ such that for all $a\in S$, $x\geq a$. We write it as
	\begin{equation*}
		x=\sup S
	\end{equation*}
\end{defn}
\begin{defn}
	\textbf{Infimum} of subset $S$ is the highest lower bound $x$ such that for all $b\in S$, $x\leq b$. We write it as
	\begin{equation*}
		x=\inf S
	\end{equation*}
\end{defn}
\begin{defn}
	\textbf{Limit superior} and \textbf{limit inferior} of a sequence $x_{1},x_{2},\cdots$ are defined by
	\begin{align*}
		\limsup_{n\to\infty}x_{n}&=\lim_{n\to\infty}\sup_{m\geq n}x_{m} & \liminf_{n\to\infty}x_{n}&=\lim_{n\to\infty}\inf_{m\geq n}x_{m}
	\end{align*}
\end{defn}
\begin{defn}
	Infinite series $\sum_{n=0}^{\infty}a_{n}$ is \textbf{absolutely convergent} if for some real numbers $L$,
	\begin{equation*}
		\sum_{n=0}^{\infty}\abs{a_{n}}=L
	\end{equation*}
	Any groupings and rearrangings of absolutely convergent infinite series do not change the result of the infinite series.\\
	An infinite series is \textbf{conditionally convergent} if it converges but does not satisfy the condition.
\end{defn}
\begin{defn}(\textbf{Monotonicity})
	\textbf{Monotonic} function is a function that is either entirely non-increasing or entirely non-decreasing.\\
	\textbf{Strictly monotonic} function is a function that is either entirely strictly increasing or entirely strictly decreasing.
\end{defn}
\begin{defn}
	\textbf{Arguments of the maxima} are the input points at which a function output is maximized. It is defined as
	\begin{equation*}
		\argmax_{x\in S}f(x)=\{x\in S:f(x)\geq f(s)\text{ for all }s\in S\}
	\end{equation*}
\end{defn}
\begin{defn}
	\textbf{Arguments of the minima} are the input points at which a function output is minimized. It is defined as
	\begin{equation*}
		\argmin_{x\in S}f(x)=\{x\in S:f(x)\leq f(s)\text{ for all }s\in S\}
	\end{equation*}
\end{defn}
\begin{defn}(\textbf{Linearity})
	\textbf{Linear} function is a function $f$ that satisfies the following two properties:
	\begin{enumerate}
		\item $f(x+y)=f(x)+f(y)$
		\item $f(ax)=af(x)$ for all $a$
	\end{enumerate}
\end{defn}
\begin{defn}
	\textbf{Regular} function is a function $f$ that is
	\begin{enumerate}
		\item single-valued (any values in the domain will map to exactly one value)
		\item analytic ($f$ can be written as a convergent power series)
	\end{enumerate}
\end{defn}
\begin{defn}
	Let $V$ be a space of all real functions on $[0,1]$. $\norm{\cdot}:V\to\mathbb{R}$ is a \textbf{norm} of a function $f$ if
	\begin{enumerate}
		\item $\norm{f}\geq 0$ for all $f\in V$
		\item If $\norm{f}=0$, then $f=0$.
		\item $\norm{af}=\abs{a}\norm{f}$ for all $f\in V$ and $a\in\mathbb{R}$
		\item (Triangle inequality) $\norm{f+g}\leq\norm{f}+\norm{g}$ for all $f,g\in V$
	\end{enumerate}
	The $L_{p}$ norm for $p\geq 1$ is defined as
	\begin{equation*}
		\norm{f}_{p}=\left(\intlu{0}{1}\abs{f(x)}^{p}\,dx\right)^{\frac{1}{p}}
	\end{equation*}
	The \textbf{infinity norm} of a function $f\in V$ is defined to be
	\begin{equation*}
		\norm{f}_{\infty}=\max_{0\leq x\leq 1}\abs{f(x)}
	\end{equation*}
\end{defn}
\begin{defn}
	Let $f$ and $g$ be real-valued functions.
	\begin{enumerate}
		\item We write $f(x)=O(g(x))$ if and only if:
		\begin{equation*}
			\limsup_{x\to\infty}\frac{f(x)}{g(x)}<\infty
		\end{equation*}
		It is called \textbf{big O notation}.
		\item We write $f(x)=o(g(x))$ if and only if:
		\begin{equation*}
			\lim_{x\to\infty}\frac{f(x)}{g(x)}=0
		\end{equation*}
		It is called \textbf{small o notation}.
		\item We write $f(x)=\Omega(g(x))$ if and only if:
		\begin{equation*}
			\liminf_{x\to\infty}\frac{f(x)}{g(x)}>0
		\end{equation*}
		It is called \textbf{big Omega notation}.
		\item We write $f(x)=\omega(g(x))$ if and only if:
		\begin{equation*}
			\lim_{x\to\infty}\frac{f(x)}{g(x)}=\infty
		\end{equation*}
		It is called \textbf{small omega notation}.
		\item Functions $f$ and $g$ are \textbf{asymptotic equivalent} ($f\sim g$) if and only if
		\begin{equation*}
			\lim_{x\to\infty}\frac{f(x)}{g(x)}=1
		\end{equation*}
		\item If $f(x)=O(g(x))$ and $g(x)=O(f(x))$, then we write:
		\begin{equation*}
			f(x)\asymp g(x)
		\end{equation*}
	\end{enumerate}
\end{defn}
\chapter{Some useful inequalities}
\begin{thm}(Triangle inequality)
	Let $X$ and $Y$ be random variables. Then
	\begin{equation*}
		\abs{X+Y}\leq\abs{X}+\abs{Y}
	\end{equation*}
\end{thm}
\begin{thm}(Reverse triangle inequality)
	Let $X$ and $Y$ be random variables. Then
	\begin{equation*}
		\abs{X-Y}\geq\abs{\abs{X}-\abs{Y}}
	\end{equation*}
\end{thm}
\begin{thm}(Cauchy-Schwarz inequality)
	Let $X$ and $Y$ be random variables. Then
	\begin{equation*}
		\abs{\expect(XY)}^{2}\leq\expect(X^{2})\expect(Y^{2})
	\end{equation*}
\end{thm}
\begin{thm}(Covariance inequality)
	Let $X$ and $Y$ be random variables. Then
	\begin{equation*}
		\abs{\cov(X,Y)}^{2}\leq\Var(X)\Var(Y)
	\end{equation*}
\end{thm}
\begin{thm}(Markov's inequality)
	Let $X$ be a random variable with finite mean, then for all $k>0$ and any non-negative function $\gamma$ that is increasing on $[0,\infty)$,
	\begin{equation*}
		\prob(\abs{X}\geq k)\leq\frac{\expect(\gamma(\abs{X}))}{\gamma(k)}
	\end{equation*}
\end{thm}
\begin{thm}(Chebyshev's inequality)
	Let $X$ be a random variable with $\expect X=\mu$ and $\Var(X)=\sigma^{2}$. Then for all $k>0$,
	\begin{equation*}
		\prob(\abs{X-\mu}\geq k\sigma)\leq\frac{1}{k^{2}}
	\end{equation*}
\end{thm}
\begin{thm}(H\"older's inequality)
	Let $X$ and $Y$ be random variables. For any $p>1$, let $q=\frac{p}{p-1}$, then
	\begin{equation*}
		\expect\abs{XY}\leq(\expect\abs{X}^{p})^{\frac{1}{p}}(\expect\abs{Y}^{q})^{\frac{1}{q}}
	\end{equation*}
\end{thm}
\begin{thm}(Lyapunov's inequality)
	Let $X$ be a random variable. For all $0<s\leq r$,
	\begin{equation*}
		(\expect\abs{X}^{s})^{\frac{1}{s}}\leq(\expect\abs{X}^{r})^{\frac{1}{r}}
	\end{equation*}
\end{thm}
\begin{thm}(Minkowski inequality)
	Let $X$ and $Y$ be random variables. For any $r\geq 1$,
	\begin{equation*}
		(\expect\abs{X+Y}^{r})^{\frac{1}{r}}\leq(\expect\abs{X}^{r})^{\frac{1}{r}}+(\expect\abs{Y}^{r})^{\frac{1}{r}}
	\end{equation*}
\end{thm}
\begin{thm}(Jensen's inequality)
	Let $X$ be a random variables and $\gamma$ be a convex function. Then
	\begin{equation*}
		\gamma(\expect X)\leq\expect(\gamma(X))
	\end{equation*}
\end{thm}
For better memorization,\\
Triangle inequality $\implies$ Reverse triangle inequality\\
Markov's inequality $\implies$ Chebyshev's inequality\\
H\"older's inequality $\implies$ Cauchy-Schwarz inequality $\implies$ Covariance inequality

\addcontentsline{toc}{chapter}{Change Log}
\chapter*{Change Log}
\begin{description}[style=nextline]
	\item[1.0]
	Create the notes
	\item[1.1]
	Add the definition "Parametric distribution" in Chapter 5.5 "Examples of continuous random variables"
	\item[1.2]
	Add the Student's t-distribution and the properties of chi-squared distribution\\
	Add theorems that relate sample mean and sample variance with distributions\\
	Create a new Chapter 7.8 "Sampling"\\
	Add a remark in De Moivre-Laplace Limit Theorem\\
	Fix some typos
	\item[1.3]
	Modify the wording of some theorems\\
	Add the definition of random sample and combine it with sample mean and sample variance\\
	Add the definitions related to asymptotic notations\\
	Change how Change Log is produced
	\item[1.4] Add a Lemma linking Gamma distribution and Chi-squared distribution\\
	Add Slutsky's Theorem\\
	Add multivariate normal distribution\\
	Modify the appearance of random vectors
	\item[2.0]
	Combining all expectation related topic into a separate chapter\\
	Greatly modify the ordering of topics\\
	Add MGF of some of the distribution taught\\
	Add a theorem that allows estimating population variance from population mean\\
	Finish the proof of a theorem that allows estimating population variance from sample variance\\
	Add a theorem that correlates uncorrelated bivariate normal and independent normal\\
	Add a lemma regarding the properties of covariance\\
	Add definition of conditional variance and Law of total variance\\
	Modify notations for regular convergence
\end{description}

\end{document}